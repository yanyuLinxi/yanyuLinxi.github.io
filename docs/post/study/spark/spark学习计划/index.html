<!doctype html><html lang=zh-cn dir=content/zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=content-security-policy content="upgrade-insecure-requests"><title>Spark学习计划 - 阳阳的人间旅游日记</title><meta name=keywords content="博客,程序员,思考,读书,笔记,技术,分享"><meta name=author content="阳阳"><meta property="og:title" content="Spark学习计划"><meta property="og:site_name" content="阳阳的人间旅游日记"><meta property="og:image" content="/img/author.jpg"><meta name=title content="Spark学习计划 - 阳阳的人间旅游日记"><meta name=description content="欢迎来到临溪的博客站，个人主要专注于机器学习、深度学习的相关研究。在这里分享自己的学习心得。"><link rel="shortcut icon" href=/img/favicon.ico><link rel=apple-touch-icon href=/img/apple-touch-icon.png><link rel=apple-touch-icon-precomposed href=/img/apple-touch-icon.png><link href=//cdn.bootcdn.net/ajax/libs/font-awesome/4.6.2/css/font-awesome.min.css rel=stylesheet type=text/css><link href=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.css rel=stylesheet><link href=/css/main.css rel=stylesheet type=text/css><link href=/css/syntax.css rel=stylesheet type=text/css></head><body itemscope itemtype=http://schema.org/WebPage lang=zh-hans><div class="container one-collumn sidebar-position-left page-home"><div class=headband></div><header id=header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle role=button style=opacity:1;top:0><span class=toggle-line></span><span class=toggle-line></span><span class=toggle-line></span></div></div><div class=site-meta><div class=multi-lang-switch><i class="fa fa-fw fa-language" style=margin-right:5px></i><a class=lang-link id=zh-cn href=#>中文</a></div><div class=custom-logo-site-title><a href=/ class=brand rel=start><span class=logo-line-before><i></i></span><span class=site-title>阳阳的人间旅游日记</span>
<span class=logo-line-after><i></i></span></a></div><p class=site-subtitle>让我们消除隔阂的，不是无所不知的脑袋，而是手拉手，坚决不放弃的那颗心</p></div><div class=site-nav-right><div class="toggle popup-trigger" style=opacity:1;top:0><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul id=menu class=menu><li class=menu-item><a href=/ rel=section><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class=menu-item><a href=/post rel=section><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class=menu-item><a href=/about.html rel=section><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于我</a></li><li class=menu-item><a href=/404.html rel=section><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href=javascript:; class=popup-trigger><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class=site-search><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class=search-icon><i class="fa fa-search"></i></span><span class=popup-btn-close><i class="fa fa-times-circle"></i></span><div class=local-search-input-wrapper><input autocomplete=off placeholder=搜索关键字... spellcheck=false type=text id=local-search-input autocapitalize=none autocorrect=off></div></div><div id=local-search-result></div></div></div></nav></div></header><main id=main class=main><div class=main-inner><div class=content-wrap><div id=content class=content><section id=posts class=posts-expand><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><header class=post-header><h1 class=post-title itemprop="name headline"><a class=post-title-link href=https://yanyulinxi.github.io/post/study/spark/spark%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/ itemprop=url>Spark学习计划</a></h1><div class=post-meta><span class=post-time><i class="fa fa-calendar-o fa-fw"></i><span class=post-meta-item-text>时间：</span>
<time itemprop=dateCreated datetime=2016-03-22T13:04:35+08:00 content="2022-05-24">2022-05-24</time></span>
<span class=post-category>&nbsp; | &nbsp;
<i class="fa fa-folder-o fa-fw"></i><span class=post-meta-item-text>分类：</span>
<span itemprop=about itemscope itemtype=https://schema.org/Thing><a href=/categories/%E5%AD%A6%E4%B9%A0 itemprop=url rel=index style=text-decoration:underline><span itemprop=name>学习</span></a>
&nbsp;</span></span>
<span>|
<i class="fa fa-file-word-o fa-fw"></i><span class=post-meta-item-text>字数：</span>
<span class=leancloud-world-count>19429 字</span></span>
<span>|
<i class="fa fa-eye fa-fw"></i><span class=post-meta-item-text>阅读：</span>
<span class=leancloud-view-count>39分钟</span></span>
<span id=/post/study/spark/spark%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/ class=leancloud_visitors data-flag-title=Spark学习计划>|
<i class="fa fa-binoculars fa-fw"></i><span class=post-meta-item-text>阅读次数：</span>
<span class=leancloud-visitors-count></span></span></div></header><div class=post-body itemprop=articleBody><p>spark学习计划：https://zhuanlan.zhihu.com/p/384903354
Scala学习《Scala实用指南》
然后书籍路线，我目前自己的学习路线是：《Spark权威指南》->《Spark内核设计的艺术 架构设计与实现》->《SparkSQL内核剖析》，也就是先从第一本API表层原理的入门书开始</p><p>学习spark2。然后到时候转spark3。先做到会用。
<a href=https://blog.csdn.net/chengyuqiang/category_9270040.html>https://blog.csdn.net/chengyuqiang/category_9270040.html</a></p><p>spark中文教程
<a href=https://spark-reference-doc-cn.readthedocs.io/zh_CN/latest/deploy-guide/cluster-overview.html>https://spark-reference-doc-cn.readthedocs.io/zh_CN/latest/deploy-guide/cluster-overview.html</a></p><h1 id=spark生产环境开发环境远程开发环境搭建>spark生产环境开发环境远程开发环境搭建</h1><p><a href=https://zhuanlan.zhihu.com/p/55450219>https://zhuanlan.zhihu.com/p/55450219</a></p><p><a href=https://cloud.tencent.com/developer/article/1482700>https://cloud.tencent.com/developer/article/1482700</a>
<a href=https://blog.csdn.net/lijingjingchn/article/details/83143093>https://blog.csdn.net/lijingjingchn/article/details/83143093</a>
<a href=https://developer.aliyun.com/article/108549>https://developer.aliyun.com/article/108549</a></p><h1 id=教程>教程</h1><p>可能还不错的：
<a href=https://sparkbyexamples.com/>https://sparkbyexamples.com/</a></p><h1 id=速看视频随记>速看视频随记</h1><ol><li>集群模式，主从模式，一个控制端，几个从属端。可以在一个机子上部署。主从之间通过端口7077进行通信。<ol><li>高可用模式，建立备用的master。当主要的master断开之后，后续的master会顶替上来。</li></ol></li><li>yarn模式。</li><li>主要运行模式：编写脚本然后提交运行。</li></ol><h2 id=rdd>RDD</h2><ol><li>RDD体现了装饰者的设计模式，所有的操作在前面的RDD上进行封装。<ol><li>装饰者模式，允许向现有对象上添加新的功能，同时不改变其结构。将现有类进行包装。</li></ol></li><li>RDD是一个抽象类，代表弹性、不可变、可分区、里面元素可并行计算的集和<ol><li>Dataset：数据的集合</li><li>Distributed：分布式存储的对象</li><li>Resilient 弹性：<ol><li>存储的弹性：内存、与磁盘的自动切换：内存有限制。所以内存不能装入所有的数据。RDD支持数据在内存和磁盘的自动切换。对上层透明。</li><li>容错的弹性：数据丢失可以自动恢复：数据丢失了，可以重新读取数据。</li><li>计算的弹性：计算出错重试机制。计算出错后，可以重新计算。容错机制。</li><li>分区的弹性：可根据需要重新分片。根据计算资源的需要重新分区。</li></ol></li></ol></li><li>为什么需要RDD：<ol><li>分布式计算需要：分区控制、Shuffle、控制数据存储\序列化、发送数据计算API等一系列功能。这些功能在分布式框架中，需要由一个统一的数据抽象对象实现。这个对象就是RDD</li></ol></li><li>RDD特性：<ol><li>分布式：数据存储在大数据集群不同节点上。对上层透明。</li><li>数据抽象（数据集）：只封装了计算逻辑，并不保存数据。需要子类实现计算逻辑。</li><li>不可变。RDD不可以改变，想要改变只能生成新的RDD。在新RDD里封装计算逻辑。</li><li>可分区，可并行计算：</li><li>惰性计算。只有在真正需要的时候，才会执行业务逻辑。</li></ol></li><li>五大属性<ol><li>RDD是有分区列表。多个分区用来执行并行计算。getPartitions函数</li><li>RDD的逻辑函数作用于分区：每个分区的计算逻辑。每个分区的计算逻辑相同。computing函数</li><li>RDD依赖关系：一个依赖于其他RDD依赖的列表。getDependencies获取依赖。</li><li>RDD拥有分区器partitioner。可选的。数据如何分区。</li><li>RDD分区数据尽量靠近数据所在地。首选位置prefer location。可选的。判断计算发送到哪个节点效率最优（计算和数据在同一个集群效率更优）</li></ol></li><li>RDD的作用<ol><li>进行逻辑的封装，并生成Task，发送给Executor节点。</li></ol></li><li>RDD分区和并行度<ol><li>RDD根据分区来生成Task。<ol><li>在行动算子触发Task后，是将一个Task所有的逻辑执行完毕后，再执行下一个Task的。</li><li>同一分区内执行顺序是有序的。不同分区执行是无序的（并行的）</li></ol></li><li>根据Task和Executor的数量来影响并行度。</li></ol></li></ol><h2 id=rdd-注意>RDD 注意</h2><ol><li>RDD中的value可以是值、list、字典等等。</li></ol><h1 id=提交到集群>提交到集群</h1><ol><li>提交到yarn集群<ol><li>代码： bin/spark-submit &ndash;master yarn &ndash;py-files ./defs.py &ndash;executor-memory 2g &ndash;executor-cores 1 &ndash;num-executors 3 ./main.py</li><li>&ndash;py-files指定py文件。&ndash;executor-memory 指定执行器内存，&ndash;executor-cores指定核心。最后一个py文件是执行的文件。</li></ol></li></ol><h1 id=spark执行原理>Spark执行原理</h1><ol><li>Spark执行<ol><li>申请资源。</li><li>将数据处理逻辑，分成一个一个的计算任务。</li><li>将任务分配到资源的计算节点上。</li></ol></li><li>以Yarn为例的工作原理<ol><li>启动Yarn集群环境。资源管理器ResourceManager和若干个节点管理器NodeManager。</li><li>Spark申请资源创建调度节点Driver和计算节点Executor。</li><li>Driver将计算逻辑根据<strong>分区</strong>划分成不同的任务Task，并将Task组成TaskPool。<ol><li>Driver主要来做调度。</li></ol></li><li>Driver根据计算节点的状态将TaskPool中的Task发送到对应的计算节点进行计算。</li></ol></li></ol><h1 id=spark-core>Spark Core</h1><ol><li>准备环境<ol><li>基础登录设置</li><li>val sparkConf = new SparkConf().setMaster(&ldquo;local&rdquo;).setAppName("") # 设置本地环境。<ol><li>&ldquo;local"单线程单核。local[*]多线程模拟多集群。</li></ol></li><li>val sc = new SparkContext(sparkConf)</li></ol></li><li>关闭环境：sc.stop()</li><li>RDD相关<ol><li>RDD的内部方法将计算逻辑发送到Executor端进行执行。所以将RDD方法称为算子。</li><li>RDD创建<ol><li>从内存创建<ol><li><strong>sc.parallelize</strong>(data=数据集合, numSlices=Option[n])<ol><li>parallelize意思是并行。</li><li>numSlices表示分区的数量。可以缺省。缺省的话即使用默认的并行度defaultParallelism。<ol><li>defaultParallelism默认值来自scheduler.conf.getInt(&ldquo;spark.default.parallelism&rdquo;, totalCores)</li><li>默认情况下，从配置对象conf中获取配置参数spark.default.parallelism</li><li>如果取不到，会因为设置为local[*]为当前环境的最大可用核数。</li><li>当<strong>numSlices不可以整分</strong>的时候。通过读源码可以看到。不是均匀划分。是依次划分。</li></ol></li></ol></li><li><strong>sc.makeRDD</strong>(数据集合) 调用同parallelize函数。<ol><li>这个函数实现时调用了parallelize函数。</li></ol></li></ol></li><li>从文件中创建。<ol><li><strong>sc.textFile</strong>(path="",minPartitions)。以行为单位读取数据<ol><li>path可以是文件名、可以是目录名</li><li>path以当前环境的根路径为基准（项目路径）。可以写绝对路径。也可以写相对路径。</li><li>path中可以使用通配符 path=&ldquo;dataste/1*.txt&rdquo;</li><li>minPartitions为最小的分区数量=math.min(defaultParallelism, 2)。在默认最小分区下，可以往上增加分区，同下。</li><li>如果不使用默认的分区数量，可以指定。spark读取文件，底层使用的hadoop的方式，统计字节数（注意换行算一个字节），字节数除以分区数，得到一个余数，如果余数大于百分之十，则加一个分区，否则就不加分区。<ol><li>例子：7个字节，分两个分区。7（字节）/2（分区）=3（字节/分区）。7/3（字节/分区）=2&mldr;1分区。余出来的分区书来给你超过了百分之10，加一个分区。</li><li>总结。以文件字节数为单位统计分区。以行为单位读取文件并存入分区。</li></ol></li><li>如果读取文件，会按行进行读取，并切分作为数据。数据将会根据字节数分配到不同的分区。glom可以查看分区数据。</li></ol></li><li><strong>sc.wholeTextFiles</strong> 以文件夹为单位读取文件。会显示数据来源。</li></ol></li></ol></li><li>RDD存储<ol><li><strong>saveAsTextFile</strong>(path)<ol><li>存成文本文件。注意这个函数是将数据存成一个文件夹。</li><li>这个函数会直接由execute端执行。</li></ol></li></ol></li><li>RDD逻辑相关方法：RDD算子（解决问题就是将问题的状态进行改变。算子就是改变问题状态 不能拒的操作）<ol><li>分为两大类（转换和行动）转换算子会返回一个新的算子。只有在执行行动算子后，才会触发惰性计算，开始计算，并返回结果。</li><li>转换算子：将一个旧的RDD包装为新的RDD。可根据参数类型分为：value类型，double Value类型，key-value类型。<strong>默认窄依赖，宽依赖标注。</strong><ol><li>value类型<ol><li><strong>map</strong>(转换函数=>Int)<ol><li>使用转换函数将RDD中所有的元素进行转换。</li></ol></li><li><strong>mapPartitions</strong>(转换函数=>Iter)<ol><li>以分区为单位进行map。获取Iter。注意**需要返回Iter。**但不要求迭代器的数量保持一致。</li><li><strong>注意：容易内存溢出</strong><ol><li>会将分区的数据加载再内存中进行引用。Task没执行完时，处理完的数据不会被释放掉，会在内存中，因为存在对象的引用。所以当数据量很大的时候容易溢出。会长时间占用内存。</li><li>当数据量大时，建议使用map</li></ol></li></ol></li><li><strong>mapValues</strong>(f)<ol><li>只对每个值的values做map。</li></ol></li><li><strong>mapPartitionsWithIndex</strong>((index, Iter)=> Iter)<ol><li>获取分区和对应下标。</li></ol></li><li><strong>flatMap</strong>(f:T=>TraversableOnce[U]): RDD[U] 扁平化操作。一个元素返回一个list，再拼接。<ol><li>将数据map后转为一个集合。Map函数中传入为元素，传出为数组，一个元素对应一个数组。将处理的数据进行扁平化后再进行映射处理。所以算子也称作为扁平映射</li><li>flatMap就是先按照传入的函数对每个元素进行map，然后再将map后的元素一起扁平化。<ol><li>flatMap 的传入是一个函数对象。参数是集和的元素。返回值是一个可迭代的集和。</li></ol></li><li>样例：[[1,2],[3,4]]=>[1,2,3,4]。flatmap接收的参数是数组，flat将多个数组所有元素聚合。</li><li>只有flatmap，没有flat</li><li>是一个窄依赖。就是当前分区内的数据进行操作。</li></ol></li><li><strong>glom()</strong> 无参数=> RDD[Array[T]] 逆扁平化操作，<strong>可以用来查看数据分区</strong>。<ol><li>将一个分区所有数据转为一个集合。将同一个分区的数据转换为相同类型的内存数组进行处理。分区不变。可以理解为flat的反面。将一个分区组成一个数组返回。</li><li>样例：[1,2,3,4] 两个分区 => [[1,2],[3,4]] 将同一个分区的内容聚集在一起。</li></ol></li><li><strong>groupBy</strong><a href="f:T=%3EK">K</a> => RDD[(K, Iterable[T])] <strong>宽依赖</strong><ol><li>根据f返回的key进行分组。这个操作会对数据shuffle。</li><li>将数组根据制定规则进行分组。分区默认不变，但是<strong>数据会被打乱重新组合</strong>。这样的操作称作shuffle。极限情况下，数据可能被分为同一个分区中。一个组的数据在一个分区中，并不是一个分区只有一个组。</li><li><strong>分组和分区没有必然逻辑</strong></li><li>python:<ol><li>rdd.groupBy(lambda num: &ldquo;even&rdquo; if num%2 else &ldquo;odd&rdquo;).</li><li>out=>[(&ldquo;even&rdquo;, [2,4]), (&ldquo;odd&rdquo;, [1,3,5])]</li></ol></li></ol></li><li><strong>filter</strong>(f:T=>Boolean):RDD[T]<ol><li>将数据根据f进行筛选，符合规则的留下。区内filter</li><li>筛选过后，分区不变。可能会出现<strong>数据倾斜。</strong></li></ol></li><li><strong>sample</strong>(withReplacement:Boolean, fraction:Double, seed:Long=Utils.random.nextLong)=>RDD[T]<ol><li>抽样选取部分样本</li><li>withReplacement是否放回。fraction选取每一个数的概率为多少，不放回时，fraction最高1。有放回fraction大于1时，则是每个值可能被抽到的次数。seed种子。抽取数据特殊场合有用。</li><li>不放回：伯努利算法</li><li>有放回：泊松算法</li></ol></li><li><strong>distinct(Option[numPartitions:Int]):=>RDD[T]</strong></li><li>根据传入的参数去重。numPartitions分区数量</li><li>底层实现时，需要考虑数据所在的分区，根据分区去重。</li><li>python<ol><li>rdd = sc.parallelize([1,1,1,2,2,2,3]).distinct()</li><li>rdd=>[2,1,3]</li><li>如果是键值类型RDD(Key-value)，则会同时根据key和value进行distinct。</li><li>返回值才是去重的结果</li></ol></li><li><strong>coalesce</strong>(numPartitions:Int, shuffle:Boolean=false, partitionCoalescer:Option[PartitionCoalescer]=Option.empty)=>RDD[T] <strong>宽依赖</strong><ol><li>缩减分区，用于大数据过滤后，提高小数据集的执行效率。使用这个函数减少分区个数。numPartitions 分区数量。</li><li>默认不会将分区的数据打乱重新组合。一个分区的数据会整个加入另一个分区。这种情况下的缩减分区会导致数据不均衡。</li><li>如果想要数据均衡。可以设置shuffle为True。但是会打乱分区。</li></ol></li><li><strong>repartition</strong>(numPartitions:Int) <strong>宽依赖</strong><ol><li>扩大分区。底层调用coalesce，默认shuffle为true。</li><li>尽量别动分区。如果非要动，尽量减少分区，而不是增加分区。</li></ol></li><li><strong>sortBy</strong>[K](f:(T)=>K, ascending=True) <strong>宽依赖</strong><ol><li>根据key进行排序。但默认不改变分区，但存在shuffle操作。ascending为升序。</li></ol></li></ol></li><li>doubule-value类型。双值函数。两个RDD。以下都是shuffle<ol><li>交集 <strong>intersection</strong><ol><li>用法 rdd1.intersection(rdd2)。</li><li>要求两个数据源数据类型保持一致。可以是value、key-value类型的rdd。</li></ol></li><li>差集 <strong>subtract</strong></li><li>并集 <strong>union</strong><ol><li>并集会将两个rdd合成一起。不会去重交集后合并。</li><li>RDD值的类型不同也可以合并。</li><li>python:<ol><li>rdd1=sc.parallelize([1,2]), rdd2=sc.parallelize([&ldquo;a&rdquo;,&ldquo;b&rdquo;]), rdd1.union(rdd2)</li><li>out=>[1,2,&ldquo;a&rdquo;,&ldquo;b&rdquo;]</li></ol></li></ol></li><li>拉链 <strong>zip</strong><ol><li>将对应位置对应值打包成tuple。</li><li>数据源要求<strong>分区数量</strong>，<strong>分组中元素的数量</strong>保持一致。</li></ol></li></ol></li><li>key-value类型。要求RDD是键值类型的PairRDD：RDD[K, V]。RDD[(K, V)]会隐式转换为PairRDD[K, V]。<strong>以下的操作大部分按照Key进行</strong>。这里面操作的对象都要求是键值型对象。<strong>这里面传入的函数的操作对象都是对value进行操作</strong>。<ol><li><strong>partitionBy</strong>(partitioner:Partitioner):RDD[(K, V)]<ol><li>将数据按照传入的Partitioner重新进行分区。</li><li>Spark有一个默认分区器为HashPartitioner(numPartition=n)获取分区数量和数据对应分区。</li></ol></li><li><strong>reduceByKey</strong>(f:(V,V)=>V, Option[numPartitions]):RDD[(K, V)] <strong>宽依赖</strong><ol><li>将相同的数据按照相同的key对value进行聚合。然后根据f对value依次操作。</li><li>当一个key只有一个元素时，不会参与训练。</li><li>python: rdd=([(&ldquo;a&rdquo;, 1), (&ldquo;b&rdquo;, 1), (&ldquo;a&rdquo;, 1)]), rdd.reduceByKey(add)=> a,2 b, 1</li></ol></li><li><strong>groupByKey</strong>(Option[numPartitions], Option[[partitioner])=>RDD[(K, Iterable[V])] <strong>宽依赖</strong><ol><li>根据key 组合数据的value为List。注意没有f。</li><li>和groupBy的区别就是groupByKey会将value组合在一起。和前一个不会。</li><li>和reduceBy的对比见下面。落盘前预聚合。</li></ol></li><li><strong>aggregateByKey</strong>(zeroValu e:U)(seqOp:(U, V)) => U, combOp(U, U)=> U):RDD[(K, U)]<ol><li>将数据根据不同的规则进行分区内计算和分区间计算。传参是两个函数。一共两个要传两个参数列表<ol><li>第一个参数(括号)为初始值。制定分区内计算时，碰到的第一个值和初始值如何操作。就是分区内reduce的初始值。</li><li>第二个参数(括号)为函数列表。第一个函数区间内计算，对象是分区内每个元素，第二个函数分区间计算，对象是每个分区提取到的每个元素。</li><li>注意每个函数的参数类型，初始值参数类型为U。最终返回类型和初始值保持一致。</li></ol></li><li>python语法：aggregateByKey(zeroValue, seqFunc, combineFunc)。 seqFunc用于区间内，combineFunc用于区间间。</li></ol></li><li><strong>foldByKey</strong>(zeroValue:U)(combOp(U, U)=> U):RDD[(K, U)]<ol><li>aggregateByKey的简化版本。当分区内和分区间操作相同的时候，只用一个函数就可以了。</li></ol></li><li><strong>combineByKey</strong>[C]( createCombiner: V => C, mergeValue: (C, V) => C,mergeCombiners: (C, C) => C): RDD[(K, C)]<ol><li>三个参数</li><li>第一个参数：将相同的第一个数据进行结构转换。实现操作。</li><li>第二个参数：分区内的计算规则</li><li>第三个参数：分区间的计算规则</li></ol></li><li>上述四个聚合API分析：最简单的reduceByKey，只有一个聚合函数。foldByKey有初始值。aggregateByKey，有初始值，有组内组间。combineByKey有初始转换函数，有组内组间。</li><li><strong>join</strong>[W](other: RDD[(K, W)]):RDD[(K, (V, W))]<ol><li>在类型(K, V)和(K, W)的RDD上调用，返回一个相同key对应的所有元素连接在一起的(K, (V, W))的RDD。<strong>类似于内连接。</strong></li><li>如果两个rdd中的key没有匹配上，则数据<strong>不会</strong>出现在结果中。</li><li>如果一个源中存在多个相同key，则另一个源会和这个源的每个相同key连接。类似笛卡尔积。</li><li>使用 rdd1.join(rdd2)。数据经过join会几何增长。会影响shuffle的性能。不推荐使用。</li><li>{(“spark”,1)、(“spark”,2)、(“hadoop”,3)} join {(“spark”,”fast”)} => {(“spark”,1,”fast”),(“spark”,2,”fast”)}</li></ol></li><li><strong>leftOuterJoin</strong>[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))]<ol><li>左外连接。在右表中不存在的key会用None显示。</li></ol></li><li><strong>rightOuterJoin</strong></li><li>右外连接。在左表中不存在的key会用None显示。</li><li><strong>cogroup</strong>[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))]<ol><li>和<strong>外连接</strong>类似。但会先在RDD内聚合后再进行外连接。先group再外连接。等于connect+group.</li><li>样例：[(&ldquo;a&rdquo;,1),(&ldquo;b&rdquo;,1)][(&ldquo;a&rdquo;,1),(&ldquo;a&rdquo;, 2)] = >[(&ldquo;a&rdquo;, (1), (1,2)), (&ldquo;b&rdquo;, (1),())]。就是RDD内先按照key聚合，聚合的值放在一起。然后rdd间按照key进行外连接。没有的值为空。</li></ol></li><li>sortByKey(ascending, numPartitions, keyFunc):<ol><li>根据Key进行排序。</li><li>ascending = True， 升序</li><li>numPartitions 按照几个分区进行排序，全局有序则为1</li><li>keyfunc, 在排序前，对key进行处理。比如对key的大小写进行修改。</li></ol></li></ol></li></ol></li><li>行动算子：出发任务的调度和作业的执行。<ol><li>解释。行动算子会触发作业的执行。底层调用的是sc.runJob方法。创建ActiveJob 并执行</li><li><strong>reduce</strong>(f: (T, T) => T): T<ol><li>聚合元素并输出。</li></ol></li><li><strong>collect</strong>(): Array[T]</li><li>collect收集所有的数据并打印。</li><li><strong>count</strong>(): Long<ol><li>返回数据源中的个数。</li></ol></li><li><strong>first</strong>()T:<ol><li>返回第一个</li></ol></li><li><strong>take</strong>(num: Int): Array[T]：<ol><li>获取num个数据</li></ol></li><li>takeSample(Bool, nums, seeds):<ol><li>取部分样本。bool表示是否可以取同一个数据, True表示可以。nums表示个数，seeds表示种子</li><li>python: [1,2,3].takeSample(True, 2) => [1,1]</li></ol></li><li><strong>takeOrdered</strong>(num: Int)(implicit ord: Ordering[T]): Array[T]<ol><li>将RDD排序后返回前n个元素。默认升序。</li><li>降序takeOrdered(n)(降序)</li><li>python: takerOrdered(num, func), num表示个数，func表示排序前应用的函数.<ol><li>[1,2,3].takeOrdered(2, lambda x:-x)=>[3, 2]</li></ol></li></ol></li><li><strong>aggregate</strong>[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U</li><li>初始值、分区内、分区间的计算规则。</li><li>会直接返回结果。不需要考虑键值类型，普通类型就能用。</li><li>和aggergateByKey的不同<ol><li>aggergateByKey仅参与分区内的运算</li><li>aggregate会参与分区内和分区间的运算。</li></ol></li><li><strong>fold</strong>(zeroValue: T)(op: (T, T) => T): T</li><li>分区内、分区间的规则相同时，使用fold。其他没有不同</li><li><strong>countByKey</strong>(): Map[K, Long]<ol><li>统计key-value RDD中key出现的次数</li></ol></li><li><strong>countByValue</strong>():<ol><li>获取每个value和其对应的出现次数</li></ol></li><li>foreach<ol><li><strong>executor端</strong>对分区里的数据执行foreach。不是按照executor顺序打印。</li><li>collect后foreach是先收集数据并返回到driver，按照分区顺序采集executor并打印。</li><li>由于RDD需要发送到executor端。所以foreach中用到的对象都需要序列化。样例类可以自动序列化。</li><li>RDD算子传递的函数是会包含闭包操作，就会进行检测功能。会检测是否能序列化。</li><li>这个不太懂，算了。</li></ol></li><li>foreachPartition<ol><li>按照分区进行foreach</li></ol></li></ol></li></ol></li><li>分区器：<ol><li>分区器有两个需要实现的。numPartitions, getPartition</li><li>numPartitions为分区数量。</li><li>getPartition根据Key获取该Key所在的分区。</li><li>分区器有HashPartitioner, RangePartitioner, PythonPartitioner.</li></ol></li></ol></li><li>RDD序列化<ol><li><strong>算子以外的代码都是在Driver 端执行, 算子里面的代码都是在 Executor 端执行</strong>。那么在 scala 的函数式编程中，就会导致<strong>算子内经常会用到算子外的数据</strong>，这样就 形成了闭包的效果，如果使用的算子外的数据<strong>无法序列化</strong>，就意味着无法传值给 Executor 端执行，就会发生错误，所以需要在执行任务计算前，检测<strong>闭包内的对象</strong>是否可以进行序列化，这个操作我们称之为<strong>闭包检测</strong>。即检测RDD操作内部（RDD函数执行的操作）是否使用的都是能序列化的值。<ol><li>rdd.filter(this.query) 这个时候因为使用了this,所以会检测this所代表的对象能否序列化</li><li>scala中类的构造参数是类的属性。构造参数需要进行闭包检测.类也需要进行闭包检测</li></ol></li><li>可以使用kryo序列化操作来序列化，减少字节传输量。</li></ol></li><li>RDD依赖关系<ol><li>直接依赖关系称为依赖。间接依赖关系为血缘关系</li><li>每个RDD保存血缘关系。当某一个RDD出现运算错误的时候，该RDD可以通过血缘关系从数据源重新获取数据。</li><li>rdd.dependencies 打印依赖关系<ol><li>新RDD依赖于旧RDD。当不存在shuffle时，是窄依赖（一对一）依赖，指分区一对一的依赖。（OneToOneDependency)</li><li>当存在shuffle的时候，是宽依赖。叫shuffle依赖。(ShuffleDependency)</li></ol></li><li>rdd.toDebugString ：打印血缘关系。没有括号注意。<ol><li>当<strong>存在shuffle操作</strong>的时候，血缘关系会产生<strong>缩进</strong>的效果</li><li>(2) ShuffledRDD[2] at reduceByKey at <console>:25 []<ol><li>+-(2) MapPartitionsRDD[1] at map at <console>:25 []</li><li>| ParallelCollectionRDD[0] at makeRDD at <console>:25 []</li><li>其中(2)表示的是分区数量。+-表示shuffle操作。</li></ol></li></ol></li></ol></li><li>Spark并行<ol><li>一个分区的多个连续窄依赖会集成成一个Task后再执行。宽依赖会等多个分区的窄依赖Task运行结束后执行。上述两个操作是分阶段进行的。先进行窄依赖Task阶段。再进行宽依赖Task阶段。</li><li>阶段：根据DAG（Directed Acyclic Graph)有向无环图组成拓扑图，构建转换过程和任务阶段。<strong>shuffle操作产生一个新阶段(stage)</strong>。</li><li><strong>RDD的阶段划分</strong>：<ol><li>每存在一个shuffleRDD时，阶段会自动增加一个。阶段数量=shuffle依赖的数量+1.ResultStage只会出现一次，在最后一次执行。</li></ol></li><li>RDD的任务划分：<ol><li>RDD任务切分中间分为：Application、Job、Stage 和 Task</li><li>⚫ Application：初始化一个 SparkContext 即生成一个Application；</li><li>⚫ Job：一个<strong>Action 算子</strong>行动算子就会生成一个 Job；</li><li>⚫ Stage：Stage 等于宽依赖(ShuffleDependency)的个数加 1；stage内部一定是窄依赖。</li><li>⚫ Task：一个<strong>Stage 阶段</strong>中，<strong>最后一个RDD的分区</strong>个数就是 Task 的个数。</li><li>注意：Application->Job->Stage->Task 每一层都是 1 对 n 的关系。</li><li>阶段名称和任务名称相同，如ShuffleMaskStage对应任务ShuffleMaskTask</li><li>一个task是一个executor中的线程，完成纯内存中的并行计算。每一个分区一个task（executor的线程）。当发生分区的扩充和减少时，一定存在shuffle操作，所以肯定不在一个stage。</li><li>Spark运行流程总结：Spark应用(Application)中，每一个行动算子会产生一个Job，产生一个DAG图(DAGScheduler)。DAG图会基于分区和宽窄依赖关系划分阶段。一个阶段的内部都是窄依赖，<strong>窄依赖内每一个分区对应一个Task</strong>(TaskScheduler)，对应于Executor中的一个内存迭代的管道，即一个线程。</li></ol></li><li>Spark并行度决定了Task的数量，也就决定了分区的数量。<ol><li>设置并行度：<ol><li>在代码中和配置文件中设置并行度，优先级从高到低：代码中、客户端提交参数中、配置文件中、默认1.</li><li>配置文件设置：conf/spark-defaults.conf 中设置<ol><li>spark.default.parallelism 100</li></ol></li><li>客户端提交参数设置： bin/spark-submit &ndash;conf &ldquo;spark.default.parallelism=100&rdquo;</li><li>代码中设置：conf = SparkConf() conf.set(&ldquo;spark.default.parallelism&rdquo;, &ldquo;100&rdquo;)</li></ol></li><li>最好设置全局并行度，不要针对RDD单独设置并行度，会产生额外的shuffle。</li></ol></li><li>Spark并行度规划<ol><li>一般并行度设置为CPU核心数的2~10倍。<ol><li>当并行度等于cpu核心数时，每一个核心会有一个task。但是有些task会先执行完，这会导致核心等待。所以多设置一些task。但是不易太多，因为task相当于线程，task的切换会造成时间开销。</li></ol></li><li>规划并行度只看集群总的CPU核数。</li></ol></li><li>Driver端两个组件：<ol><li>DAG调度器：将逻辑DAG进行处理，得到Task划分</li><li>Task调度器：根据DAG Scheduler产出，规划逻辑的Task应该在哪些物理executor上运行。</li></ol></li></ol></li><li>RDD持久化<ol><li>如果多个行动算子存在相同的前部分操作，则RDD对象可以重用。但由于RDD中不存储数据，所以数据无法重用，每个行动算子会从头获取数据并计算。</li><li>如果为了性能需要重用中间数据，我们应该<strong>持久化存储</strong>中间存储。可以存在内存中（速度快不安全）或者磁盘中（慢但安全）。<ol><li>rdd.persist()则这个rdd会持久化存储。后面重用这个rdd对象的时候，会从cache中读取数据，而不是重复计算。默认保存到内存中，如果需要保存到磁盘中，需要传入参数。 StorageLevel.***<ol><li>***可以为：DISK_ONLY, MEMORY_ONLY, MEMORY_AND_DISK等等。</li><li>persist会在血缘关系中添加新的依赖。如果出现问题，可以重头读取数据。cache一样。</li><li>python: rdd.persist(storageLevel = StorageLevel.***).<ol><li>StorageLevel.DISK_ONLY</li><li>StorageLevel.MEMORY_ONLY</li><li>StorageLevel.MEMORY_AND_DISK</li><li>StorageLevel.MEMORY_AND_DISK_DESER</li><li>StorageLevel.OFF_HEAP</li></ol></li><li>rdd.persist 是在executor的服务器上存储。</li></ol></li><li>代码rdd.cache() 调用了persist()。将数据保存在内存中。<ol><li>注意cache持久化操作，会在行动算子执行时才触发。</li><li>cache会保存在临时文件夹下。作业结束后，会被删除。</li></ol></li><li>rdd.unpersist()释放内存。</li><li>rdd.checkpoint() 检查点操作。会将算子落盘。一般保存hdfs中。且checkpoint操作会独立执行作业。<ol><li>sc.setCheckpointDir(&ldquo;cp&rdquo;)设置检查点目录。</li><li>checkpoint执行过程中，由于从磁盘中读取数据，会切断血缘关系，重新建立从磁盘中读取数据的血缘关系。等同于改变数据源。</li><li>checkpoint可以控制存储的地方。比如hdfs。checkpoint不保存血缘关系。</li></ol></li></ol></li><li>持久化区别<ol><li>cache: 将数据临时存储在内存中进行数据重用。</li><li>persist：将数据临时存在磁盘中进行重用。涉及IO性能较低。作业执行完毕。临时存储的文件就会丢失。</li><li>checkpoint：将数据长久的保存在磁盘中。性能较低。为了保证数据安全。一般情况下，会独立执行一个作业（额外的作业），产生数据文件来长期存储。一般情况和cache联合使用。<ol><li>解释：checkpoint()调用的时候，会触发作业执行操作。相当于一个行动算子。如果在这之前调用了cache()，则会从cache中取出数据，而避免重复运算。</li></ol></li><li>总结：cache更快（保存在executor端），checkpoint慢（网络通信集中存储），但设计上更安全。当数据量比较大时，重新计算成本高，则用checkpoint</li></ol></li><li>持久化操作的应用：<ol><li>rdd被多个行动算子重复使用。</li><li>单个rdd走的路程太长，为了避免出错，或者数据比较重要的场合。可以持久化中间步骤。</li></ol></li></ol></li><li>RDD分区器。<ol><li>分区相关API：<ol><li>3个算子：partitionBy，根据传入的自定义分区器进行分区。coalesce重新设置数据分区。repartition同coalesce。</li></ol></li><li>自定义分区器。<ol><li>继承类： extends Partitioner{}</li><li>重写两个方法：<ol><li>override def numPartitions: Int。 分区数</li><li>override def getPartition(key:Any): Int=??? 根据数据的key返回数据所在的分区索引（从0开始）。</li></ol></li></ol></li><li>Spark 目前支持<strong>Hash 分区</strong>和<strong>Range 分区</strong>，和用户自定义分区。Hash 分区为当前的默认 分区。分区器直接决定了RDD中分区的个数、RDD中每条数据经过 Shuffle 后进入哪个分区，进而决定了Reduce 的个数。<ol><li>只有Key-Value 类型的RDD才有分区器，非Key-Value 类型的RDD分区的值是None</li><li>每个RDD的分区 ID 范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的</li></ol></li><li><strong>Hash 分区</strong>：对于给定的 key，计算其 hashCode,并除以分区个数取余</li><li><strong>Range 分区</strong>：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而 且分区间有序</li></ol></li><li>RDD文件读取和保存：<ol><li>Spark 的数据读取及数据保存可以从两个维度来作区分：<strong>文件格式</strong>以及<strong>文件系统</strong>。 文件格式分为：text 文件、csv 文件、sequence 文件以及Object 文件；文件系统分为：本地文件系统、HDFS、HBASE 以及数据库。</li><li>存储：<ol><li>RDD.saveAsTextFile(path)</li><li>RDD.saveAsObjectFile(path)</li><li>RDD.saveAsSequenceFile(path) 要求必须是key-value RDD</li></ol></li><li>读取<ol><li>sc.textFile(&ldquo;output&rdquo;)</li><li>sc.sequenceFile<a href=%22output%22>Int,Int</a></li><li>sc.objectFile[Int](&ldquo;output&rdquo;</li></ol></li></ol></li><li>三大数据结构：</li><li>RDD：弹性分布式数据集</li><li>累加器：分布式共享只写变量<ol><li>ac = sc.accumulator(0), 取值 ac.value</li></ol></li><li>广播变量：分布式共享只读变量<ol><li>python: bcVar = sc.broadcast([1,2,3,4]). 取值：bcVar.value。</li><li>这个广播变量被创建以后，那么在集群中的任何函数中，都应该使用广播变量broadcastVar的值，而不是使用v的值，这样就不会把v重复分发到这些节点上。</li></ol></li><li>累加器<ol><li>为什么需要累加器？<ol><li>spark分布式框架，相当于一个多线程的task。而且是发送到执行端executor执行。而且executor执行的task没有返回操作。无法修改同时修改本地的数据。累加器的作用就是在task执行完毕后，从task返回到driver端。</li><li>累加器用来把 Executor 端变量信息聚合到Driver 端。在Driver 程序中定义的变量，在 Executor 端的<strong>每个 Task</strong>都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后，<strong>传回Driver 端进行merge</strong>。</li></ol></li><li>使用<ol><li>sc.longAccumulator(name=&ldquo;sum&rdquo;)<ol><li>整型累加器。</li><li>ac.add</li><li>ac.value</li></ol></li><li>sc.doubleAccumulator</li><li>sc.collectionAccumulator<ol><li>List类型的累加器。</li></ol></li><li>python使用<ol><li>定义：acc = sc.accumulator(0)</li><li>使用，函数内部使用的时候需要使用global 申明这是外部变量。</li></ol></li></ol></li><li>自定义累加器：<ol><li>继承： extends AccumulatorV2[In, Out]<ol><li>定义泛型In :累加器输入的数据类型 Out: 累加器返回的数据类型。<ol><li>如: extends AccumulatorV2[String, mutable.Map[String, Long]]</li></ol></li><li>重写方法：<ol><li>isZero: Boolean 判断是否是初始状态。</li><li>add(In): 获取累加器需要计算的值</li><li>copy(): 复制一个新的累加器</li><li>reset(): 重置累加器</li><li>merge(): 合并多个累加器</li><li>value(): Out 返回累加器的结果</li></ol></li></ol></li><li>使用步骤：<ol><li>创建累加器对象</li><li>向spark注册。sc.register(累加器，name=&ldquo;xx&rdquo;)</li></ol></li></ol></li><li>python 自定义累加器<ol><li>定义AccumulatorParam的子类，来创建累加器。需要实现的方法同java。</li></ol></li><li>累加器的问题：<ol><li><strong>少加</strong>。如果转换算子中调用累加器。如果没有行动算子，那么不会执行。</li><li><strong>多加</strong>。如果多次调用行动算子，则会多次执行，导致多次累加。<ol><li>所以一般情况下，累加器会在行动算子中进行操作。即写在行动算子中。</li></ol></li><li>多个Task的累加器是不能相互访问的。</li></ol></li></ol></li><li>广播变量<ol><li>**广播变量用来高效分发较大的对象。**向所有工作节点发送一个较大的只读值，以供一个 或多个 Spark 操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表， 广播变量用起来都很顺手。<strong>在多个并行操作中使用同一个变量，但是 Spark 会为每个任务分别发送</strong>。</li><li>广播变量使用比较快的原因是，对一个executor只用发送一个，一个executor的多个task使用无需反序列化，可以直接使用。无需对每个task都序列化。</li><li>闭包数据，都是以task为单位发送的，每个任务中包含闭包数据，这样可能导致一个executor中含有大量重复的数据，并且占用大量的内存。Executor就是一个JVM，启动时会分配内存。完全可以将闭包数据放在executor中，达到共享的目的。这些放在内存的数据称为广播变量。广播变量不能被更改。分布式的只读变量。</li><li>使用(同python)：<ol><li>初始化bc = sc.boardcast(variable)</li><li>方位bc.value 就是variable</li></ol></li></ol></li></ol><h1 id=sparksql>SparkSQL</h1><ol><li>介绍：<ol><li>SparkSQL处理结构化数据。</li></ol></li><li>创建DataFrame<ol><li>从Spark数据源创建</li><li>从存在的RDD创建</li><li>从Hive Table查询。</li></ol></li><li>SparkSQL环境创建<ol><li>sparkConf = SparkConf().setMaster(&ldquo;local[*]").setAppName(&rdquo;..")</li><li>spark = SparkSession.builder().config(sparkConf).getOrCreate()</li><li>SQL相关操作都由SparkSession所创建的spark对象所访问。</li><li>python:<ol><li>from pyspark.sql import SparkSession</li><li>spark = SparkSession.builder.appName(&rdquo;&mldr;").config().getOrCreate()</li><li>sc = spark.sparkContext() # 通过session获取context对象。</li></ol></li></ol></li><li>DataFrame的API<ol><li>DataFrame对象创建<ol><li>从RDD中创建<ol><li>df = spark.createDataFrame(rdd, schema=[列名的列表， 或者是structType])</li><li>传入的rdd要求是属性的列表:[value1, value2]等等。</li><li>设置StructType<ol><li>from pyspark.sql.types import StructType</li><li>schema = StructType()</li><li>schema.add(fields=&ldquo;name&rdquo;, data_type, nullable=True, metadata=None)。 往schema中添加数据类型，列名、data_type等。data_type是spark中的数据类型，可以参考sql.types.__all__获取。<ol><li>距离 schema.add(&ldquo;user_id&rdquo;, StringType())</li></ol></li><li>df = spark.createDataFrame(rdd, schema=schema)</li></ol></li></ol></li><li>RDD直接转换为DataFrame<ol><li>spark.createDataFrame(rdd)可以直接使用rdd.toDF()来转换，参数相同，可以使用schema等。</li></ol></li><li>从pandas对象转换<ol><li>df = spark.createDataFrame(pandasDataFrame) 直接转换就行。</li></ol></li><li>直接从文件中读取<ol><li>df = sparksession.read.format (&ldquo;text csv json parquet orc avro jdbc&mldr;&mldr;").option(&ldquo;K”，&ldquo;v&rdquo;).schema(StructType|String).load(&ldquo;被读取文件的路径，支持本地文件系统和HDFS&rdquo;)。 读取的时候设置format格式，option是指可选的设置。当数据没有提供schema信息时，设置schema。</li><li>从csv读取举例：<ol><li>df = spark.read.format(&ldquo;csv&rdquo;).option(&ldquo;sep&rdquo;, &ldquo;:").option(&ldquo;header&rdquo;, False).option(&ldquo;encoding&rdquo;, &ldquo;utf-8&rdquo;).schema(&ldquo;name STRING, age INT, job STRING&rdquo;).load()</li><li>每一个option都是传入给csv读取的参数，参数是和spark.read_csv()参数相同的</li><li>schema是列出每一个列数据的名称和类型。类型是sql的类型。不设置的话，是默认字符串。如果列名在数据中给出的话，可以直接指定列名的类型。</li></ol></li></ol></li></ol></li><li>DataFrame的DSL语法API<ol><li>dataFrame对象，column对象，groupby对象。row对象。</li><li>df.printSchema(). 展示表结构。</li><li>df.show( n=20, truncate=True, vertical=False), n表示展示多少数据，默认20.truncate表示每一列的数据长度是否截断。vertical表示逐行打印每一列数据。</li><li>获取column对象。 id_column = df[&ldquo;id&rdquo;]。 id_column表示了id这一列。在使用DSL语法时，可以传入字符串比如"id &lt; 10&rdquo; ，也可以直接使用column对象，比如id_column &lt; 10.</li><li>分组API，如groupBy，会返回GroupedData对象，拥有聚合方法。</li><li>和sql同义的API：df.filter, where, filter, count</li><li>高级API：<ol><li>df.withColumn(origin_column, modify_column), df对列的操作，都只能通过这两个函数来，或者通过function语法进行操作。</li><li>df.withColumnRenamed(origin_column_name, renamed_column_name)</li><li>df.first()获取第一行返回一个row对象。row对象通过名称访问值。</li><li>df.agg(&mldr;) 其中可以写多个聚合。用来展示<ol><li>df.agg(F.round(), F.max(), F.min())</li></ol></li><li>df.dropDuplicates() 针对给出的字段进行去重, 不加参数时，完成整体去重。<ol><li>df.dropDuplicates([&ldquo;age&rdquo;, &ldquo;name&rdquo;])，仅针对age和name进行去重。</li></ol></li><li>df.dropna(how=&ldquo;any&rdquo;, threshold, subset) 去除空值。<ol><li>how为all时，一行都为空才去除。</li><li>threshold 指定一个子列有效值的阈值。有效列个数必须大于threshold才保存</li><li>subset是指定的列子集。</li></ol></li><li>df.fillna(值或者字典)， 字典是对应列的缺失值填入的数。</li></ol></li><li>columns的function语法<ol><li>from pyspark.sql import functions as F</li><li>F中拥有一些函数，比如split,explode等函数，其接收columns为对象，并返回columns对象。</li><li>举例：df.withColumn(&ldquo;avg_rank&rdquo;, F.round(&ldquo;avg_rank&rdquo;, 2))</li><li>列对象的方法 columns.alias() 重命名。</li></ol></li><li>DF数据导出<ol><li>df.write.mode().format().option(K, V).save(PATH)</li><li>mode 传入字符串设定模式，可选：append, overwrite, ignore, error(重复报错,默认)</li><li>format，格式字符串，可选：text,csv, json, parquet, orc, avro, jdbc, 注意text只支持单列df, 默认parquet</li><li>option设置属性，可累加， 举例：.option(&ldquo;sep&rdquo;, &ldquo;,") 设置分隔符。</li><li>save：导出的目标路径，支持本地文件和hdfs</li><li>举例：<ol><li>df.select(F.concat_ws("&mdash;&rdquo;, &ldquo;user_id&rdquo;)).write.mode(&ldquo;overwrite&rdquo;).option(&ldquo;header&rdquo;, True).save(path)</li></ol></li></ol></li></ol></li><li>SparkSql的SQL语法API<ol><li>DataFrame转换为SQL表<ol><li>df.createTempView(表名)创建了临时表后，就可以通过spark.sql进行查询</li><li>df.createOrReplaceTempView(表名)。创建或者替换。</li><li>df.createGlobalTempView(表名)<ol><li>全局表在查询时需要加上global_temp, 比如global_temp.score.</li></ol></li></ol></li><li>spark.sql(&ldquo;select * from score&rdquo;) => DataFrame 返回的是dataFrame对象。</li></ol></li><li>sparkSQL优化<ol><li>sparkSQL中job产生shuffle时，默认分区数为200， 由 spark.sql.shuffle.partitions指定。在实际项目中要合理设置。</li><li>代码设置 SparkSession.builder.config(&ldquo;spark.sql.shuffle.partitions&rdquo;, &ldquo;2&rdquo;)。</li></ol></li><li>UDF语法。<ol><li>python只支持UDF(user-defined-function)，表示一对一关系, 其他的UDAF(user-defined aggregation Function), 多对一关系， UDTF(user defined table-generating Functions)一对多关系，只能由scala语言定义</li><li>注册方式：<ol><li>sparksession.udf.register(udf_name, udf_function, return_type)<ol><li>udf_name 是udf的名称，可以用于sql。</li><li>udf_function是用户定义的函数</li><li>return_type是udf_function返回的数据类型。</li><li>举例：sparksession.udf.regisiter(&ldquo;udf1&rdquo;, num_add_1, IntergerType())</li><li>使用 spark.sql.selectExpr(&ldquo;udf1(df[&lsquo;nums&rsquo;]"))。 或者df.udf1(df[&ldquo;nums&rdquo;])</li><li>structType可以使用自定的structType，即返回数据表类型，比如StructType().add(&ldquo;nums1&rdquo;, IntergerType())这样，就会返回一个表，相当于嵌套类型。</li></ol></li><li>pyspark.sql.functions.udf(udf_function, retury_type)<ol><li>参数同上。但是这个只能通过dsl进行访问。</li><li>举例： F.udf(split_line, ArrayType(StringType()))</li></ol></li></ol></li><li>当需要用到UDAF的时候，使用df.rdd 转为rdd对象，然后使用rdd语法处理。注意转为rdd对象后，返回的是row对象的集和。</li></ol></li><li>开窗函数<ol><li>聚合函数是将多行变为一行，比如count, avg等等</li><li>开窗函数是将一行变为多行。比如OVER() 选项，比如NTILE()选项。简单来说，就是spark的sql支持数据库的sql。</li></ol></li><li>SQL优化器<ol><li>SparkSQL在执行过程中，由catalyst优化器进行优化，然后解析成RDD进行执行。SparkSQL可以优化，而RDD的操作不能优化，因为和他的存储结构有关系，rdd的类型不固定。</li><li>catalyst优化器的流程<ol><li>解析SQL生成AST树。</li><li>在AST中加入元数据信息，做这一步的主要是为了一些优化。</li><li>对加入元数据的AST进行优化<ol><li>断言（谓词）下推。predicate pushdown。<ol><li>将Filter这种可以减少数据集的操作下推。这样可以减少操作时需要的数据量。</li></ol></li><li>列值裁剪。经过第一步的元数据标记，可以将没有用到的列进行预先裁剪，这样可以减少数据量，从而优化处理速度。<ol><li>SQL执行顺序：From, Where, GroupBY, Having, Select, OrderBy, Limit.</li></ol></li></ol></li><li>将AST生成为逻辑计划，生成物理计划。将计划翻译成RDD代码。后面就是用RDD的运行逻辑进行运行。</li><li>总结两点优化比较突出的：行过滤：提取执行where。列裁剪，提前规划select字段。并且和parquet这种列值存储的系统很配。</li></ol></li><li>执行流程总结<ol><li>1.提交SparkSQL代码</li><li>catalyst优化
a. 生成原始AST语法数
b. 标记AST元数据
c. 进行断言下推和列值裁剪以及其它方面的优化作用在AST上
d. 将最终AST得到，生成执行计划
e. 将执行计划翻译为RDD代码</li><li>Driver执行环境入口构建(SparkSession)</li><li>DAG调度器规划逻辑任务</li><li>TASK调度区分配逻辑任务到具体Executor.上工作并监控管理任务</li><li>Worker干活。</li></ol></li><li>Spark没有自己的资源管理器。所以SparkOnHive就是使用Hive的元数据管理器 MetaStore<ol><li>spark中有一个服务：ThriftServer服务，可以启动并监听在1000端口。利用这个工具操作spark。</li></ol></li></ol></li></ol></li></ol><h1 id=spark-mllib>Spark MLlib</h1><ol><li>应用场景<ol><li>由于spark的mllib可扩展性比较差（不能修改内部的结构）等原因，使用spark进行机器学习需要结合具体的场景。当单机无法训练大数据，或者处理时间过长的时候可以使用spark。具体来说，有两点：1. 希望spark进行数据预处理和特征生成，以减少训练集合测试集所需要的时间。2. 输入数据和模型难以在单机上处理时，可以使用spark</li></ol></li><li>基本概念<ol><li>转换器transformer<ol><li>转换器将原始数据以某种方式进行转换。</li></ol></li><li>估计器estimator<ol><li>估计器需要传递两次值。第一次传值生成初始化，第二次在数据上应用生成函数。</li><li>注意，第一次fit数据后，返回的是一个模型。需要使用模型进行第二次transform</li><li>举例：<ol><li>lr = LogisticRegression(labelCol=&ldquo;label&rdquo;, featuresCol = &ldquo;features&rdquo;)</li><li>lr = LogisticRegression().setLabelCol().setFeaturesCol()</li><li>lr.fit(data)</li><li>lr.transform(data)</li></ol></li></ol></li><li>评估器evaluator<ol><li>评估器允许我们根据某种效果评价指标。</li></ol></li><li>流水线pipeline<ol><li>流水线即是可以将若干转换器、估计器组装起来的一个工具</li><li>举例：<ol><li>stages = [ss, lr]</li><li>pipeline = Pipeline().setStages(stages)</li></ol></li></ol></li><li>通用API<ol><li>大多数的transformer和estimator都需要指定inputCol和 ouputCol</li><li>大多数方法有explainParams()方法，用来查看参数。</li></ol></li></ol></li><li>数据类型<ol><li>SparkMLlib中的数据都是double类型的。</li><li>Vector<ol><li>Vectors.dense(1.0, 2.0, 3.0) 稠密数据。</li><li>Vectors.sparse(size, idx, values) 稀疏类型数据</li></ol></li><li>VectorAssembler<ol><li>将所有的特征组合成一个向量，送入估计器中</li><li>va = VectorAssembler().setInputCols([&ldquo;int1&rdquo;, &ldquo;int2&rdquo;])</li></ol></li></ol></li><li>模型评估<ol><li>多折模型<ol><li>params = ParamGridBuilder().addGrid(lr.elasticNetParam, [0.0, 0.5]).addGrid()</li></ol></li><li>评估器<ol><li>evaluator = BinaryClassificationEvaluator().setMetricName(&ldquo;areaUnderROC).setRawPredictionCol(&ldquo;prediction&rdquo;).setLabelCol(&ldquo;label&rdquo;)</li></ol></li><li>数据集分割<ol><li>tvs = TrainValidateionSplit().setTrainRatio().setEstimatorParamMaps(params).setEstimator(pipeline).setEvaluator(evaluator) 这样就构造了一个流水线。</li></ol></li></ol></li><li>模型展示结果输出</li><li>模型持久化</li><li>高级转换器RFormula</li><li>SQL转换器<ol><li>允许使用SQL语句一样，对数据进行转换。需要注意的就是将表名换为__THIS__</li><li>from pyspark.ml.feature import SQLTransformer</li><li>btf = SQLTransformer().setStatement(&rdquo;&rdquo;&rdquo; select sum(Quantity), count(*) from <strong>THIS</strong> group by customerID &ldquo;"")。</li></ol></li><li>总体来说sparkmlib就和sklearn很类似。</li></ol><h1 id=spark性能优化相关名词解释>spark性能优化相关名词解释</h1><ol><li>shuffle<ol><li>不同分区内的数据需要聚合。这种操作即宽依赖操作。即是shuffle操作。</li><li>针对同一个数据的多次shuffle，spark会在底层优化缓存。</li></ol></li><li>shuffle慢的原因<ol><li>落盘。当涉及shuffle操作的时候，即宽依赖的时候，不能在内存中存储数据，会容易溢出。必须<strong>落盘</strong>处理。即将符合条件的数据存储到一个文件中。再从文件中读取。所以shuffle的性能非常低。</li><li>网络传输。落盘后的数据会传输给目标task，这会引起大量的网络传输。</li></ol></li><li>shuffle的优化<ol><li>预聚合功能<ol><li>在落盘之前，在分区内预聚合。有效的而减少shuffle时落盘的数据量。</li></ol></li></ol></li><li>HashShuffle优化(待确定，感觉和上面的预聚合是一个东西。)<ol><li>Executor内预聚合，这个和上一个预聚合不同。上一个预聚合是同一个key的预先聚合，是在分区内。这个预聚合是在executor内。能聚合的原因是因为走的内存通道。</li></ol></li><li>SortShuffleManager<ol><li>和普通的不同的是，数据会先进行排序，然后放入内存缓冲区进行运算，并落盘，最后整合成一个大文件。排序的好处是，当下游的task索取数据的时候，会从当前task的索引文件获取数据的索引下标，根据索引索取指定的数据。排序就是为了使用索引下标，这样就能只发送部分数据，而不是全部数据给下游的task。（注意，这个排序在一个task内部是分批排序的）<ol><li>sortshuffle的优点：节省了一个task中磁盘文件的数量（只有一个文件）。减少了网络中的数据交互的数量。</li><li>缺点：排序非常耗时。</li></ol></li><li>ByPass机制下的SortShuffleManager。当满足：1. shuffle map task的数量小于预定的 bypassMergeThreshold参数定义 2. 不是聚合类的shuffle 算子（比如reduceByKey）。 这两个条件的时候，不会在task内进行预排序，直接写成一个文件。</li></ol></li></ol><h2 id=pyspark运行解释>pyspark运行解释</h2><p>在driver端，python代码会翻译成java代码，并运行在jvm上。Driver和Executor交互用的是java的RPC(Remote Procedure Call(远程过程调用))。然后在Executor，java代码会通过python守护进程进行中转，由python进程（python解释器）进行执行。</p><p>整个流程：Driver[python=>(py4j)=>JVM] &ndash;RPC&ndash;> Executor[JVM=>python]</p><h2 id=分布式集群讲解>分布式集群讲解</h2><p>在Spark中，非任务处理部分由Driver执行（非RDD代码）。任务处理部分由Executor执行（RDD代码）。Executor运行在分布式机器上，数量可以很多。</p><h2 id=spark30讲解>Spark3.0讲解</h2><ol><li>AQE 自适应查询<ol><li>动态合并。在运行时，将相邻的小分区合并为大分区。</li><li>动态join。在执行sql语句的两表合并的时候。如果检测到一个表的数据量很小，则将这个表转为广播表，发送给所有的task。另一个表的每个task都会拿到广播表，减少了shuffle的操作、网络传播量。</li><li>动态倾斜join。当某一个类型的数据量不平衡过多的时候，会将这部分数据拆分成多个更小的分区。让所有分区的数据更加均衡（优化倾斜)。以获得更好的整体性能（运算速度上的优化）。</li><li>总结：通过参数设置就能开启AQE。它是自动优化设置，符合条件就会开启。极大的提升了SparkSQL的性能。</li></ol></li><li>动态分区裁剪<ol><li>优化器无法识别可跳过的分区的时候，使用动态分区裁剪。</li></ol></li><li>Koalas：基于Apache Spark的pandasAPI。运行于分布式的pandasAPI。<ol><li>直接将pandas转为databricks的操作。（白学这么久了）</li></ol></li></ol><h1 id=架构>架构</h1><p>Java的架构：
MVC: Model View Controller 架构。
Model，模型：业务模型和数据模型（封装）。
View视图是数据展示
Controller是控制层。控制数据的流转</p><p>大数据中一般不需要View。所以不用MVC模式。改用三层架构：
Controller(控制层), service(服务层), dao(持久层)。
Application=》controller进行调度=》service(启动服务)=>dao(持久层)访问数据。
common 一些共同的代码放在这。
util 一些工具类放在这。哪里都能用的工具。
bean 实体类
application 应用类
controller 控制类
service 服务类
dao 持久层，操作数据。</p><p>问题：</p><ol><li>reduceByKey和groupByKey的区别<ol><li>性能上：<ol><li>reduceByKey会在区内进行聚合，然后再落盘。所以数据量会小很多。比groupByKey快很多，性能更高。就是所谓的预聚合功能</li></ol></li><li>功能上：<ol><li>groupByKey只分组，不聚合。reduceByKey分组且聚合。</li></ol></li></ol></li><li>reduceByKey, aggregateByKey, foldByKey, combineByKey底层调用都是combineByKeyWithTags(createCombiner, mergeValue, mergeCombines).设定初始值，然后分区内聚合，然后分区间聚合。</li><li>Spark为什么比MapReduce快<ol><li>Spark算子丰富，可以使用少量算子完成更多的工作，所以速度更快。MapReduce算子匮乏(Map和Reduce)，Map和reduce的交互通过硬盘完成。所以慢。</li><li>Spark通过task机制可以实现多个算子走内存迭代计算。但是MapReduce的Map和Reduce多个MR的串联仍然只能走磁盘交互，就会很慢。</li></ol></li></ol><p>题目：（做过的题目都得记录，不然很容易忘）</p><ol><li><p>创建数据</p><ol><li>sc.makeRDD(List(1,2,3,4), numSlices=2)</li></ol></li><li><p>分区内最大值，分区间最大值求和。</p><ol><li>使用mapPartitions求出分区最大值，然后collect。<ol><li>rdd.mapPartitions(list=>Iterator(list.max)).collect().sum</li><li>这里的list是val的名称。Iterator是获得一个Iterator对象。</li></ol></li><li>使用glom求出分区，然后map取最大值，然后collect求和。<ol><li>rdd.glom().map(_.max).collect()</li></ol></li><li>当使用key、value组成对时即List((&ldquo;a&rdquo;, 1), (&ldquo;a&rdquo;, 2), (&ldquo;a&rdquo;, 3), (&ldquo;a&rdquo;, 4))这样子。可用aggregateByKey<ol><li>rdd.aggregateByKey(0).(math.max(<em>,</em>), <em>+</em>).collect()</li></ol></li></ol></li><li><p>wordCount</p><ol><li>数据 sc.makeRDD(List(&ldquo;Hello spark&rdquo;, &ldquo;Hello scala&rdquo;), numSlices=2).flatMap(_.split(&rdquo; &ldquo;))</li><li>方法：<ol><li>使用groupBy 转换为iter后统计数量<ol><li>rdd.groupBy(word=>word).mapValues(iter=>iter.size)</li></ol></li><li>使用map转为key, value RDD，使用reduceByKey, aggregateByKey, foldByKey中的一个<ol><li>rdd.map((<em>, 1)).foldByKey(0)(</em>+_).collect()</li></ol></li><li>双值countByKey</li><li>countValue单值。</li><li>累加器实现。</li></ol></li></ol></li><li><p>相同Key的平均值</p><ol><li>数据List((&ldquo;a&rdquo;, 1), (&ldquo;b&rdquo;, 3),(&ldquo;b&rdquo;, 4), (&ldquo;a&rdquo;, 3), (&ldquo;a&rdquo;, 3),(&ldquo;b&rdquo;, 4))</li><li>统计Key的总和和出现的次数<ol><li>rdd.aggregrateByKey((0,0))((t, v)=>(t._1+v, t._2+1), (t1, t2)=>(t1._1+t2._1, t1._2+t2._2)).map(a=>a._1, a._2._1/a._2._2)</li><li>rdd.combineByKey(v=>(v,1), (t:(Int, Int), n:Int)=>(t._1+n, t._2+1), (t1:(Int, Int), t2:(Int, Int))=>(t1._1+t2._1, t1._2+t2._2))<ol><li>这个操作是对相同key的value做聚合。所以key是不会传进来的。</li><li>这里需要注意Scala的语法。由于tuple是中途产生的，所以scala并不理解t的类型是什么。scala是静态语言，这样没法编译。所以我们需要标注类型，才能让它正常的编译。</li></ol></li></ol></li><li>python:<ol><li>combine_rdd = data_rdd.combineByKey(lambda x: (x, 1), lambda x, y:(x[0]+y[0], x[1]+y[1]), lambda x, y:(x[0]+y[0], x[1]+y[1]))</li><li>result_rdd = combine_rdd.mapValues(lambda x:x[0]/x[1])</li><li>print(result_rdd.collect())</li></ol></li></ol></li><li><p>统计每一个省份每个广告被点击数量排行的Top3</p><ol><li>数据： xxxx 河北 北京 张三 广告A</li><li>思路，首先仅提取需要的特征 河北、广告A。思路按照省分类统计广告的点击次数，然后再按照省份进行归纳。你的思路是首先按照省分类，再统计广告次数。为什么这么做不好？不好写</li><li>首先用map创造数据，使用split分割。分割后的数据 List((省份,广告),1)</li><li>然后按照 省份-广告 作为key 进行聚合</li><li>再根据 省份 作为key 进行聚合。</li></ol><p>rdd.map(data=>{val datas=data.split(&rdquo; &ldquo;);((datas(1), datas(4)),1)})</p><p>rdd.reduceByKey(<em>+</em>)</p><p>rdd.map{case ((prv, ad), sum) =>{(prv, (ad, sum))}}</p><p>rdd.groupByKey().mapValues(iter=>iter.toList.sortBy(_._2)(Ordering.Int.reverse).take(3))</p></li><li><p>top10热门种类。</p><ol><li>需求：按照点击数排名。靠前的就排名高。再比较点击数、下单数、支付数。</li><li>实现一：<ol><li>首先filter出点击数量等分类存在的值=》进行map为(种类，1)=>reduceByKey。统计每个种类的个数=>三个种类进行连接。(种类ID，(点击数量)，(下单数量),(支付数量))</li><li>这里有多种连接方式：join,zip, leftOuterJoin, cogroup. join不行，因为可能存在三个种类一个不存在。leftOuterJoin不行，同样必须外连接。zip不行，zip连接不会按照key连接，而是按照分区内数据的数量和位置连接。cogroup是外连接。所以可行: a.cogroup(b,c)</li><li>连接起来后，将同id的几个种类值相加 a.mapValues()</li><li>进行排序a.sortBy(_._2, ascending=false).take(10)。按照第二个就是元组进行排序。先排元组第一个值，再排第二个值。</li></ol></li><li>优化一：<ol><li>多个种类调用读取数据textFile。所以cache这个RDD。</li><li>cogroup是shuffle操作，性能较低。<ol><li>map数据 => (品类ID，(点击数量,0,0))。<strong>将所有数据转换为相同格式后相加。</strong></li><li>然后a.union(b).union(c)</li></ol></li><li>再排序等等。</li></ol></li><li>优化二：<ol><li>存在大量的reduceByKey操作。</li><li>将数据转换结构：（品类ID，（1，0，0）），（品类ID，（0，1，0））</li><li>完整代码：</li></ol></li></ol></li></ol><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=color:#a2f;font-weight:700>val</span> flatRDD<span style=color:#a2f;font-weight:700>=</span>actionRDD<span style=color:#666>.</span>flatMap<span style=color:#666>(</span>
   action<span style=color:#666>=&gt;{</span>
      <span style=color:#a2f;font-weight:700>val</span> datas<span style=color:#a2f;font-weight:700>=</span>action<span style=color:#666>.</span>split<span style=color:#666>(</span><span style=color:#b44>&#34;_&#34;</span><span style=color:#666>);</span> 
      <span style=color:#a2f;font-weight:700>if</span><span style=color:#666>(</span>datas<span style=color:#666>(</span><span style=color:#666>6</span><span style=color:#666>)!=</span><span style=color:#b44>&#34;-1&#34;</span><span style=color:#666>)</span>
      <span style=color:#666>{</span><span style=color:#00f>List</span><span style=color:#666>((</span>datas<span style=color:#666>(</span><span style=color:#666>6</span><span style=color:#666>),(</span><span style=color:#666>1</span><span style=color:#666>,</span><span style=color:#666>0</span><span style=color:#666>,</span><span style=color:#666>0</span><span style=color:#666>)))}</span>
      <span style=color:#a2f;font-weight:700>else</span> <span style=color:#a2f;font-weight:700>if</span> <span style=color:#666>(</span>datas<span style=color:#666>(</span><span style=color:#666>8</span><span style=color:#666>)!=</span><span style=color:#b44>&#34;null&#34;</span><span style=color:#666>)</span>
      <span style=color:#666>{</span>datas<span style=color:#666>(</span><span style=color:#666>8</span><span style=color:#666>).</span>split<span style=color:#666>(</span><span style=color:#b44>&#34;,&#34;</span><span style=color:#666>).</span>map<span style=color:#666>(</span>id<span style=color:#666>=&gt;(</span>id<span style=color:#666>,</span> <span style=color:#666>(</span><span style=color:#666>0</span><span style=color:#666>,</span><span style=color:#666>1</span><span style=color:#666>,</span><span style=color:#666>0</span><span style=color:#666>)))}</span>
      <span style=color:#a2f;font-weight:700>else</span> <span style=color:#a2f;font-weight:700>if</span><span style=color:#666>(</span>datas<span style=color:#666>(</span><span style=color:#666>10</span><span style=color:#666>)!=</span><span style=color:#b44>&#34;null&#34;</span><span style=color:#666>){</span>datas<span style=color:#666>(</span><span style=color:#666>10</span><span style=color:#666>).</span>split<span style=color:#666>(</span><span style=color:#b44>&#34;,&#34;</span><span style=color:#666>).</span>map<span style=color:#666>(</span>id<span style=color:#666>=&gt;(</span>id<span style=color:#666>,(</span><span style=color:#666>0</span><span style=color:#666>,</span><span style=color:#666>0</span><span style=color:#666>,</span><span style=color:#666>1</span><span style=color:#666>)))}</span>
      <span style=color:#a2f;font-weight:700>else</span><span style=color:#666>{</span><span style=color:#00f>Nil</span><span style=color:#666>}})</span>
flatRDD<span style=color:#666>.</span>reduceByKey<span style=color:#666>((</span>t1<span style=color:#666>,</span>t2<span style=color:#666>)=&gt;(</span>t1<span style=color:#666>.</span>_1<span style=color:#666>+</span>t2<span style=color:#666>.</span>_1<span style=color:#666>,</span>t1<span style=color:#666>.</span>_2<span style=color:#666>+</span>t2<span style=color:#666>.</span>_2<span style=color:#666>,</span>t1<span style=color:#666>.</span>_3<span style=color:#666>+</span>t2<span style=color:#666>.</span>_3<span style=color:#666>)).</span>sortBy<span style=color:#666>(</span><span style=color:#a2f;font-weight:700>_</span><span style=color:#666>.</span>_2<span style=color:#666>,</span><span style=color:#a2f;font-weight:700>false</span><span style=color:#666>).</span>take<span style=color:#666>(</span><span style=color:#666>10</span><span style=color:#666>)</span>
</code></pre></div><ol start=5><li>优化三：<ol><li>不用shuffle操作。使用累加器。</li></ol></li></ol><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=color:#a2f;font-weight:700>case</span> <span style=color:#a2f;font-weight:700>class</span> <span style=color:#00f>HotCategory</span><span style=color:#666>(</span>cid<span style=color:#a2f;font-weight:700>:</span><span style=color:#0b0;font-weight:700>String</span><span style=color:#666>,</span> clickCnt<span style=color:#a2f;font-weight:700>:</span><span style=color:#0b0;font-weight:700>Int</span><span style=color:#666>,</span> <span style=color:#00f>OrderCnt</span><span style=color:#a2f;font-weight:700>:</span><span style=color:#0b0;font-weight:700>Int</span><span style=color:#666>,</span> payCnt<span style=color:#a2f;font-weight:700>:</span><span style=color:#0b0;font-weight:700>Int</span><span style=color:#666>)</span>

<span style=color:#a2f;font-weight:700>class</span> <span style=color:#00f>HotCategoryAccumulator</span> <span style=color:#a2f;font-weight:700>extends</span> <span style=color:#00f>AccumulatorV2</span><span style=color:#666>[(</span><span style=color:#0b0;font-weight:700>String</span>,<span style=color:#0b0;font-weight:700>String</span><span style=color:#666>)</span>,<span style=color:#0b0;font-weight:700>mutable.Map</span><span style=color:#666>[</span><span style=color:#0b0;font-weight:700>String</span>,<span style=color:#0b0;font-weight:700>HotCategory</span><span style=color:#666>]]{</span>
   <span style=color:#080;font-style:italic>// IN:(品类ID，行为类型)
</span><span style=color:#080;font-style:italic></span>   <span style=color:#080;font-style:italic>// out: mutable.Map[String, HotCategory]
</span><span style=color:#080;font-style:italic></span><span style=color:#666>}</span>

</code></pre></div><ol start=7><li>top10品类中，sessionID top10统计。<ol><li>需求分析：top10热门品类中，每个品类点击Top10的session。可以先统计出top10的热门品类。然后统计session。将项目映射为 (品类id, session, 1)。然后按照品类、session进行相加。再按照品类id进行聚合起来。然后再进行排序取前10个。思路和各省份广告点击前10相同。</li><li>首先map((省份，广告)，1)。然后按照省份广告相加。然后按照省份进行聚合。然后按照次数进行排序。</li></ol></li><li>页面单跳转换率统计。<ol><li>需求分析：数据： session id， 访问页面， 时间。</li><li>目标是求 A->B 的次数/ A的次数。所以首先统计A的次数。直接wordcount。然后统计A->B的次数。按照session id分组groupBy，然后按照时间排列。然后拉链 形成 A->B。然后再wordCount.</li><li>优化:<ol><li>不用统计所有页面。只用统计部分页面。所以我们对部分页面过滤一下就行。</li></ol></li></ol></li></ol><h2 id=资料>资料</h2><ol><li><a href=http://dblab.xmu.edu.cn/blog/1709-2/>林子雨Spark入门教程（Python版本）</a></li><li><a href="https://www.bilibili.com/video/BV1Jq4y1z7VP?p=149&spm_id_from=pageDriver">黑马视频教程</a></li></ol></div><footer class=post-footer><div class=post-tags><a href=/tags/spark rel=tag title=spark>#spark#</a></div><div class=addthis_inline_share_toolbox></div><div class=post-nav><div class=article-copyright><div class=article-copyright-img><img src=/img/qq_qrcode.png width=129px height=129px><div style=text-align:center>QQ扫一扫交流</div></div><div class=article-copyright-info><p><span>声明：</span>Spark学习计划</p><p style=word-break:break-all><span>链接：</span>
https://yanyulinxi.github.io/post/study/spark/spark%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/</p><p><span>作者：</span>阳阳</p><p><span>邮箱：</span>yanyulinxi@qq.com</p><p><span>声明： </span>本博客文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/ target=_blank style=text-decoration:underline>CC BY-NC-SA 3.0</a>许可协议，转载请注明出处！</p></div></div><div class=clear></div></div><div class=reward-qr-info><div>创作实属不易，如有帮助，那就打赏博主些许茶钱吧 ^_^</div><button id=rewardButton disable=enable onclick="var qr=document.getElementById('QR');qr.style.display==='none'?qr.style.display='block':qr.style.display='none'">
<span>赏</span></button><div id=QR style=display:none><div id=wechat style=display:inline-block><img id=wechat_qr src=/img/wechat-pay.png alt="WeChat Pay"><p>微信打赏</p></div><div id=alipay style=display:inline-block><img id=alipay_qr src=/img/ali-pay.png alt=Alipay><p>支付宝打赏</p></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=https://yanyulinxi.github.io/post/essay/thought/%E6%9C%89%E5%85%B3%E7%BB%93%E5%A9%9A%E6%89%BE%E5%AF%B9%E8%B1%A1/ rel=next title=有关结婚找对象><i class="fa fa-chevron-left"></i>有关结婚找对象</a></div><div class="post-nav-prev post-nav-item"><a href=https://yanyulinxi.github.io/post/study/kaggle/writeup/%E9%94%80%E9%87%8F%E9%A2%84%E6%B5%8B%E6%AF%94%E8%B5%9B/ rel=prev title=销量预测比赛>销量预测比赛
<i class="fa fa-chevron-right"></i></a></div></div><div id=wcomments></div></footer></article></section></div></div><div class=sidebar-toggle><div class=sidebar-toggle-line-wrap><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id=sidebar class=sidebar><div class=sidebar-inner><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image src=/img/linxi_icon.png alt=阳阳><p class=site-author-name itemprop=name>阳阳</p><p class="site-description motion-element" itemprop=description>再平凡的人也有属于他自己的梦想!</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href=/post/><span class=site-state-item-count>141</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>6</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>35</span>
<span class=site-state-item-name>标签</span></a></div></nav><div class="links-of-author motion-element"><span class=links-of-author-item><a href=https://github.com/yanyuLinxi target=_blank title=GitHub><i class="fa fa-fw fa-github"></i>GitHub</a></span>
<span class=links-of-author-item><a href=https://space.bilibili.com/19237450 target=_blank title=哔哩哔哩><i class="fa fa-fw fa-globe"></i>哔哩哔哩</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class=links-of-blogroll-title><i class="fa fa-fw fa-globe"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://www.liaoxuefeng.com/ title=廖雪峰 target=_blank>廖雪峰</a></li></ul></div><div class="tagcloud-of-blogroll motion-element tagcloud-of-blogroll-inline"><div class=tagcloud-of-blogroll-title><i class="fa fa-fw fa-tags"></i>标签云</div><ul class=tagcloud-of-blogroll-list><li class=tagcloud-of-blogroll-item><a href=/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0>论文阅读笔记</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/python%E7%9B%B8%E5%85%B3%E5%BA%93%E5%AD%A6%E4%B9%A0>Python相关库学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E5%BC%82%E5%B8%B8%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90>异常行为分析</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7>深度学习辅助工具</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E5%BA%93>机器学习相关库</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/kaggle>Kaggle</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/java%E5%AD%A6%E4%B9%A0>Java学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/insider-threat>Insider threat</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/leetcode%E5%AD%A6%E4%B9%A0>Leetcode学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/spark>Spark</a></li></ul></div></section></div></aside></div></main><footer id=footer class=footer><div class=footer-inner><div class=copyright><span class=copyright-year>&copy; 2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span><span class=copyright-author>阳阳的人间旅游日记</span></div><div class=powered-info><span class=powered-by>Powered by - <a class=powered-link href=//gohugo.io target=_blank title=hugo>Hugo v0.81.0</a></span>
<span class=separator-line>/</span>
<span class=theme-info>Theme by - <a class=powered-link href=//github.com/elkan1788/hugo-theme-next target=_blank>NexT</a></span></div><div class=vistor-info><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span class=site-uv><i class="fa fa-user"></i><span class=busuanzi-value id=busuanzi_value_site_uv></span></span><span class=separator-line>/</span>
<span class=site-pv><i class="fa fa-eye"></i><span class=busuanzi-value id=busuanzi_value_site_pv></span></span></div><div class=license-info><span class=storage-info>Storage by
<a href style=font-weight:700 target=_blank></a></span><span class=separator-line>/</span>
<span class=license-num><a href target=_blank></a></span></div></div></footer><div class=back-to-top><i class="fa fa-arrow-up"></i><span id=scrollpercent><span>0</span>%</span></div></div><script type=text/javascript src=//cdn.bootcdn.net/ajax/libs/jquery/2.1.4/jquery.min.js></script><script type=text/javascript src=/js/search.js></script><script type=text/javascript src=/js/affix.js></script><script type=text/javascript src=/js/scrollspy.js></script><script type=text/javascript>function detectIE(){var a=window.navigator.userAgent,b=a.indexOf('MSIE '),c=a.indexOf('Trident/'),d=a.indexOf('Edge/');return b>0||c>0||d>0?-1:1}function getCntViewHeight(){var b=$('#content').height(),a=$(window).height(),c=b>a?b-a:$(document).height()-a;return c}function getScrollbarWidth(){var a=$('<div />').addClass('scrollbar-measure').prependTo('body'),b=a[0],c=b.offsetWidth-b.clientWidth;return a.remove(),c}function registerBackTop(){var b=50,a=$('.back-to-top');$(window).on('scroll',function(){var d,e,f,c,g;a.toggleClass('back-to-top-on',window.pageYOffset>b),d=$(window).scrollTop(),e=getCntViewHeight(),f=d/e,c=Math.round(f*100),g=c>100?100:c,$('#scrollpercent>span').html(g)}),a.on('click',function(){$("html,body").animate({scrollTop:0,screenLeft:0},800)})}function initScrollSpy(){var a='.post-toc',d=$(a),b='.active-current';d.on('activate.bs.scrollspy',function(){var b=$(a+' .active').last();c(),b.addClass('active-current')}).on('clear.bs.scrollspy',c),$('body').scrollspy({target:a});function c(){$(a+' '+b).removeClass(b.substring(1))}}function initAffix(){var a=$('.header-inner').height(),b=parseInt($('.main').css('padding-bottom'),10),c=a+10;$('.sidebar-inner').affix({offset:{top:c,bottom:b}}),$(document).on('affixed.bs.affix',function(){updateTOCHeight(document.body.clientHeight-100)})}function initTOCDimension(){var a,b;$(window).on('resize',function(){a&&clearTimeout(a),a=setTimeout(function(){var a=document.body.clientHeight-100;updateTOCHeight(a)},0)}),updateTOCHeight(document.body.clientHeight-100),b=getScrollbarWidth(),$('.post-toc').css('width','calc(100% + '+b+'px)')}function updateTOCHeight(a){a=a||'auto',$('.post-toc').css('max-height',a)}$(function(){var b=$('.header-inner').height()+10,c,d,a,e;$('#sidebar').css({'margin-top':b}).show(),c=parseInt($('#sidebar').css('margin-top')),d=parseInt($('.sidebar-inner').css('height')),a=c+d,e=$('.content-wrap').height(),e<a&&$('.content-wrap').css('min-height',a),$('.site-nav-toggle').on('click',function(){var a=$('.site-nav'),e=$('.toggle'),b='site-nav-on',f='toggle-close',c=a.hasClass(b),g=c?'slideUp':'slideDown',d=c?'removeClass':'addClass';a.stop()[g]('normal',function(){a[d](b),e[d](f)})}),registerBackTop(),initScrollSpy(),initAffix(),initTOCDimension(),$('.sidebar-nav-toc').click(function(){$(this).addClass('sidebar-nav-active'),$(this).next().removeClass('sidebar-nav-active'),$('.'+$(this).next().attr('data-target')).toggle(500),$('.'+$(this).attr('data-target')).toggle(500)}),$('.sidebar-nav-overview').click(function(){$(this).addClass('sidebar-nav-active'),$(this).prev().removeClass('sidebar-nav-active'),$('.'+$(this).prev().attr('data-target')).toggle(500),$('.'+$(this).attr('data-target')).toggle(500)})})</script><script src=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.js></script><script type=text/javascript>$(function(){$('.post-body').viewer()})</script><script type=text/javascript>$(function(){detectIE()>0?$.getScript(document.location.protocol+'//cdn.jsdelivr.net/npm/@waline/client/dist/Waline.min.js',function(){new Waline({el:'#wcomments',visitor:!0,avatar:'wavatar',avatarCDN:'https://sdn.geekzu.org/avatar/',avatarForce:!1,wordLimit:'200',placeholder:' 欢迎留下您的宝贵建议，请填写您的昵称和邮箱便于后续交流. ^_^ ',requiredFields:['nick','mail'],serverURL:"Your WalineSerURL",lang:"zh-cn"})}):$('#wcomments').html('抱歉，Waline插件不支持IE或Edge，建议使用Chrome浏览器。')})</script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=Your%20AddthisId"></script><script>(function(){var a=document.createElement('script'),c=window.location.protocol.split(':')[0],b;c==='https'?a.src='https://zz.bdstatic.com/linksubmit/push.js':a.src='http://push.zhanzhang.baidu.com/push.js',b=document.getElementsByTagName("script")[0],b.parentNode.insertBefore(a,b)})()</script></body></html>