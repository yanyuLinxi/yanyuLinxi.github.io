<!doctype html><html lang=zh-cn dir=content/zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=content-security-policy content="upgrade-insecure-requests"><title>Kaggle学习计划 - 阳阳的人间旅游日记</title><meta name=keywords content="博客,程序员,思考,读书,笔记,技术,分享"><meta name=author content="阳阳"><meta property="og:title" content="Kaggle学习计划"><meta property="og:site_name" content="阳阳的人间旅游日记"><meta property="og:image" content="/img/author.jpg"><meta name=title content="Kaggle学习计划 - 阳阳的人间旅游日记"><meta name=description content="欢迎来到临溪的博客站，个人主要专注于机器学习、深度学习的相关研究。在这里分享自己的学习心得。"><link rel="shortcut icon" href=/img/favicon.ico><link rel=apple-touch-icon href=/img/apple-touch-icon.png><link rel=apple-touch-icon-precomposed href=/img/apple-touch-icon.png><link href=//cdn.bootcdn.net/ajax/libs/font-awesome/4.6.2/css/font-awesome.min.css rel=stylesheet type=text/css><link href=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.css rel=stylesheet><link href=/css/main.css rel=stylesheet type=text/css><link href=/css/syntax.css rel=stylesheet type=text/css></head><body itemscope itemtype=http://schema.org/WebPage lang=zh-hans><div class="container one-collumn sidebar-position-left page-home"><div class=headband></div><header id=header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle role=button style=opacity:1;top:0><span class=toggle-line></span><span class=toggle-line></span><span class=toggle-line></span></div></div><div class=site-meta><div class=multi-lang-switch><i class="fa fa-fw fa-language" style=margin-right:5px></i><a class=lang-link id=zh-cn href=#>中文</a></div><div class=custom-logo-site-title><a href=/ class=brand rel=start><span class=logo-line-before><i></i></span><span class=site-title>阳阳的人间旅游日记</span>
<span class=logo-line-after><i></i></span></a></div><p class=site-subtitle>让我们消除隔阂的，不是无所不知的脑袋，而是手拉手，坚决不放弃的那颗心</p></div><div class=site-nav-right><div class="toggle popup-trigger" style=opacity:1;top:0><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul id=menu class=menu><li class=menu-item><a href=/ rel=section><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class=menu-item><a href=/post rel=section><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class=menu-item><a href=/about.html rel=section><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于我</a></li><li class=menu-item><a href=/404.html rel=section><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href=javascript:; class=popup-trigger><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class=site-search><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class=search-icon><i class="fa fa-search"></i></span><span class=popup-btn-close><i class="fa fa-times-circle"></i></span><div class=local-search-input-wrapper><input autocomplete=off placeholder=搜索关键字... spellcheck=false type=text id=local-search-input autocapitalize=none autocorrect=off></div></div><div id=local-search-result></div></div></div></nav></div></header><main id=main class=main><div class=main-inner><div class=content-wrap><div id=content class=content><section id=posts class=posts-expand><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><header class=post-header><h1 class=post-title itemprop="name headline"><a class=post-title-link href=https://yanyulinxi.github.io/post/study/kaggle/kaggle%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/ itemprop=url>Kaggle学习计划</a></h1><div class=post-meta><span class=post-time><i class="fa fa-calendar-o fa-fw"></i><span class=post-meta-item-text>时间：</span>
<time itemprop=dateCreated datetime=2016-03-22T13:04:35+08:00 content="2022-06-06">2022-06-06</time></span>
<span class=post-category>&nbsp; | &nbsp;
<i class="fa fa-folder-o fa-fw"></i><span class=post-meta-item-text>分类：</span>
<span itemprop=about itemscope itemtype=https://schema.org/Thing><a href=/categories/%E5%AD%A6%E4%B9%A0 itemprop=url rel=index style=text-decoration:underline><span itemprop=name>学习</span></a>
&nbsp;</span></span>
<span>|
<i class="fa fa-file-word-o fa-fw"></i><span class=post-meta-item-text>字数：</span>
<span class=leancloud-world-count>5463 字</span></span>
<span>|
<i class="fa fa-eye fa-fw"></i><span class=post-meta-item-text>阅读：</span>
<span class=leancloud-view-count>11分钟</span></span>
<span id=/post/study/kaggle/kaggle%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/ class=leancloud_visitors data-flag-title=Kaggle学习计划>|
<i class="fa fa-binoculars fa-fw"></i><span class=post-meta-item-text>阅读次数：</span>
<span class=leancloud-visitors-count></span></span></div></header><div class=post-body itemprop=articleBody><h2 id=1-学习计划>1. 学习计划</h2><p>先按照<a href=https://github.com/datawhalechina/team-learning-data-mining>datawhale</a>的学习计划进行学习。</p><p>学玩一个notebook后，我们将机器学习分为几个部分
数据分析
数据预处理
模型训练
调优
整体步骤
这几个部分挨个填充内容。</p><p>一个kaggle怎么学？学习一个kaggle后。默写从头复现。</p><p>把集成学习两个案例学完。然后总结。</p><h2 id=2-数据分析>2. 数据分析</h2><h3 id=21-autoeda>2.1. AutoEDA</h3><h4 id=211-pandas-profiling>2.1.1. pandas-profiling</h4><p>pandas-profiling是一个自动化数据分析工具，使用代码如下：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>import</span> <span style=color:#00f;font-weight:700>pandas_profiling</span>

<span style=color:#080;font-style:italic># 如果是使用notebook进行分析，则直接可以在notebook中查看。</span>
<span style=color:#080;font-style:italic># notebook查看非常的费时间，建议转为html</span>
profile <span style=color:#666>=</span> pandas_profiling<span style=color:#666>.</span>ProfileReport(df)

<span style=color:#080;font-style:italic># 如果需要生成html文件</span>
profile<span style=color:#666>.</span>to_file(<span style=color:#b44>&#34;your_report.html&#34;</span>)
</code></pre></div><h4 id=sweetviz>Sweetviz</h4><p>Sweetviz相比于pandas-profiling功能更多。</p><p>概览：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>import</span> <span style=color:#00f;font-weight:700>sweetviz</span> <span style=color:#a2f;font-weight:700>as</span> <span style=color:#00f;font-weight:700>sv</span>

my_report <span style=color:#666>=</span> sv<span style=color:#666>.</span>analyze(df)
my_report<span style=color:#666>.</span>show_html() <span style=color:#080;font-style:italic># Default arguments will generate to &#34;SWEETVIZ_REPORT.html&#34;</span>
</code></pre></div><h5 id=生成分析报告>生成分析报告</h5><p>主要有三个函数创造report：analyze、compare、compare_intra</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>sv<span style=color:#666>.</span>analyze(source: Union[pd<span style=color:#666>.</span>DataFrame, Tuple[pd<span style=color:#666>.</span>DataFrame, <span style=color:#a2f>str</span>]],
            target_feat: <span style=color:#a2f>str</span> <span style=color:#666>=</span> None,
            feat_cfg: FeatureConfig <span style=color:#666>=</span> None,
            pairwise_analysis: <span style=color:#a2f>str</span> <span style=color:#666>=</span> <span style=color:#b44>&#39;auto&#39;</span>):
</code></pre></div><p>source表示输入的数据，支持的格式 source=df, source=[df, &ldquo;df&rsquo;s name&rdquo;]
target_feat 是一个字符串，指定的是df中为标签的那一栏。目前只支持boolean和数字
feat_cfg 是一个FeatureConfig对象，表示在分析中要跳过或强制某种类型的特征。参数可以是单个字符串或字符串列表。构建方式如：<code>feature_config = sv.FeatureConfig(skip="PassengerId", force_text=["Age"])</code>, 它的参数skip, force_cat, force_num 和 force_text， skip表示跳过，force表示前置的类型检测。
pairwise_analysis: Union[&ldquo;auto&rdquo;, &ldquo;on&rdquo;, &ldquo;off], 相关性和其他关联可能需要二次时间 (n^2) 才能完成直到数据集包含“association_auto_threshold”特征。超过该阈值，您需要显式传递参数pairwise_analysis=&ldquo;on&rdquo;（或=&ldquo;off&rdquo;），因为处理这么多特征需要很长时间。该参数还涵盖了关联图的生成</p><p>注意，现在的target_feat仅支持数字或者布尔类型，如果不是这两两种类型，需要指定为这两种。示例：
<code>report = sv.analyze(train, target_feat="happiness", feat_cfg=sv.FeatureConfig(force_num=["happiness"]))</code></p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>sv<span style=color:#666>.</span>compare(source, compare_source, <span style=color:#666>**</span>kwargs)
my_report <span style=color:#666>=</span> sv<span style=color:#666>.</span>compare([my_dataframe, <span style=color:#b44>&#34;Training Data&#34;</span>], [test_df, <span style=color:#b44>&#34;Test Data&#34;</span>], <span style=color:#b44>&#34;Survived&#34;</span>, feature_config)
</code></pre></div><p>compare是比较两个数据集的特征。source是一个数据集，compare_source是另一个数据集。其他参数和analyze相同。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>my_report <span style=color:#666>=</span> sv<span style=color:#666>.</span>compare_intra(my_dataframe, my_dataframe[<span style=color:#b44>&#34;Sex&#34;</span>] <span style=color:#666>==</span> <span style=color:#b44>&#34;male&#34;</span>, [<span style=color:#b44>&#34;Male&#34;</span>, <span style=color:#b44>&#34;Female&#34;</span>], feature_config)
</code></pre></div><p>compare_intra比较同一dataframe的两个子集, 如dataframe中的男性和女性两个子集。会创建两个单独的数据框进行展示。</p><h5 id=输出分析结果>输出分析结果</h5><p>一旦您创建了您的报告对象，只需将其传递给两个“显示”函数之一：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#080;font-style:italic># to html</span>
report<span style=color:#666>.</span>show_html( filepath<span style=color:#666>=</span><span style=color:#b44>&#39;SWEETVIZ_REPORT.html&#39;</span>, 
            open_browser<span style=color:#666>=</span>True, 
            layout<span style=color:#666>=</span><span style=color:#b44>&#39;widescreen&#39;</span>, 
            scale<span style=color:#666>=</span>None)



<span style=color:#080;font-style:italic># 在notebook中输出</span>
report<span style=color:#666>.</span>show_notebook(  
   w<span style=color:#666>=</span>None, 
   h<span style=color:#666>=</span>None, 
   scale<span style=color:#666>=</span>None,
   layout<span style=color:#666>=</span><span style=color:#b44>&#39;widescreen&#39;</span>,
   filepath<span style=color:#666>=</span>None)

</code></pre></div><h5 id=相关性分析说明>相关性分析说明</h5><p>Sweetviz 关联图和分析的主要洞察力和独特功能是它统一在一个图中
正方形代表分类特征相关变量，圆圈代表数值-数值相关性。</p><h5 id=对结果进行分析>对结果进行分析</h5><p>单数据集分析的时候，需要看：</p><ol><li>缺失值。是否有缺失值，缺失值的数量</li><li>和其他特征的相关性。高相关性的特征，应予以重视。</li><li>特征中值的占比。某个值占比太高，这个特征应予以剔除。</li><li>看数据类型。分类值应该用one-hot， 连续值应该考虑是否归一化，时间特征应该进行翻译。</li></ol><p>需要记录的信息：</p><ol><li>时间列</li><li>含异常值的列</li><li>类别值中需要处理为one-hot的列</li><li>连续值需要归一化的列</li><li>含缺失值的列，和缺失值的比例。（缺失值比例过高应该删除）</li><li>列中单个值占比过高的列。</li><li>记录类别特征</li><li>记录连续特征</li></ol><p>训练集、测试集对比的时候，需要看数据的分布。
数据分布不一致的列，应该考虑剔除。</p><h4 id=dataprep>Dataprep</h4><p>Dataprep是一个灵活性高、功能强大的数据分析软件。</p><p>API:</p><ol><li><code>plot(data, Optional[List[cols]])</code><ol><li>展示数据。如果后面的cols为空的时候，则分析所有数据。否则会分析传入的指定的列。</li></ol></li><li><code>plot_correlation</code><ol><li>参数同上。分析数据中的相关性</li></ol></li><li><code>plot_missing</code></li><li><code>plot_diff([df1, df2])</code><ol><li>对两个数据集进行对比。分析它们之间分布差异。</li></ol></li><li>create_report 生成数据集的综合报告。</li></ol><p>获取中间数据：对于每个plot函数，都有一个对应的compute函数，该函数返回用于渲染的计算中间体：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>dataprep.eda</span> <span style=color:#a2f;font-weight:700>import</span> compute_correlation
imdt <span style=color:#666>=</span> compute_correlation(df)
</code></pre></div><p>展示数据:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>report <span style=color:#666>=</span> plot(df)
report<span style=color:#666>.</span>show_browser()
</code></pre></div><p>Dataprep同时拥有清洗数据的功能。可以快速方便的对数据进行转换。并将不合法的数据标价为NAN。感觉有用。</p><h3 id=22-手动eda>2.2. 手动EDA</h3><p>需要明确三件事，EDA如何分析，能分析出什么？分析出来后怎么做？</p><ol><li>为什么要分析？</li><li>如何分析？</li><li>分析出来怎么解决？</li></ol><h4 id=221-核密度查看数据特征分布>2.2.1. 核密度查看数据特征分布</h4><p>训练集、测试集分布不一致会影响泛化性能。查看特征分布的目的就是将分布不一致的特征去掉，这些特征会影响到训练。</p><p>具体方法：</p><ol><li>使用自动化工具或者使用核密度估计来显示训练集和测试集特征的分布情况</li><li>分析：核密度图中，波峰趋势一致就是均匀的。波峰不一致就可能是不均匀的。如果波峰有偏移，但是图像整体偏移一点距离，可能是某些部分数据少，其他部分数据多，也算是均匀的。如果某些波峰特别高，则注意是不是异常数据。尽可能的保存特征。相差特别大的才去除。明显不一样的考虑去除。</li></ol><p>代码举例：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>sns<span style=color:#666>.</span>kdeplot(data[column], color<span style=color:#666>=</span><span style=color:#b44>&#34;Red&#34;</span>, shade <span style=color:#666>=</span> True)
</code></pre></div><h4 id=222-查看特征之间的相关性>2.2.2. 查看特征之间的相关性</h4><p>这里有两种相关性需要查看，特征之间的相关性，特征和标签之间的相关性。</p><ol><li><p>特征之间的相关性越高，则说明特征之间拥有共线性。对于部分模型，比如逻辑回归，特征共线性的危害比较大，其会放大噪音，降低模型的可解释性。这一点可以参考本站《机器学习基础知识面试》部分的内容。如果特征之间相关性非常高，则应该通过直接删除或降维等方式去除高共线性特征。</p></li><li><p>如果特征和标签之间的相关性很低，则特征意义不大，需要将这些特征剔除。</p></li></ol><p>具体方法：</p><ol><li>使用AutoEDA工具或者scipy,pandas工具计算相关系数。</li></ol><p>代码举例：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#080;font-style:italic># 获取相关性的绝对值</span>
corr_matrix <span style=color:#666>=</span> data<span style=color:#666>.</span>corr()<span style=color:#666>.</span>abs()
<span style=color:#080;font-style:italic># 将和标签相关性低的特征删除</span>
threshold<span style=color:#666>=</span><span style=color:#666>0.1</span>
drop_col<span style=color:#666>=</span>corr_matrix[corr_matrix[<span style=color:#b44>&#34;y&#34;</span>]<span style=color:#666>&lt;</span>threshold]<span style=color:#666>.</span>index
data <span style=color:#666>=</span> data<span style=color:#666>.</span>drop(drop_col,axis<span style=color:#666>=</span><span style=color:#666>1</span>)
</code></pre></div><h4 id=223-box-cox变换>2.2.3. Box-Cox变换</h4><p>我们测得一些数据，要对数据进行分析的时候，会发现数据有一些问题使得我们不能满足我们以前分析方法的一些要求（正态分布、平稳性）</p><p>为了满足经典线性模型的正态性假设，常常需要使用指数变换或者对数转化，使其转换后的数据接近正态，比如数据是非单峰分布的，或者各种混合分布，我们就需要进行一些转化</p><p>box-cox变换的目标有两个：一个是变换后，可以一定程度上减小不可观测的误差和预测变量的相关性。主要操作是对因变量转换，使得变换后的因变量于回归自变量具有线性相依关系，误差也服从正态分布，误差各分量是等方差且相互独立。第二个是用这个变换来使得因变量获得一些性质，比如在时间序列分析中的平稳性，或者使得因变量分布为正态分布。</p><p>这个还没有弄懂，只知道它会对数据分布进行转换，使得其满足正态分布。</p><p>参考<a href=https://github.com/datawhalechina/team-learning-data-mining/blob/master/EnsembleLearning/CH6-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%A1%88%E4%BE%8B%E5%88%86%E4%BA%AB/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%902/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%902.ipynb>team-learning-data-mining</a>的内容Box-Cox了解更多</p><h5 id=2231-资料>2.2.3.1. 资料</h5><ol><li><a href=https://zhuanlan.zhihu.com/p/36284359>box-cox变换</a></li><li><a href=https://blog.csdn.net/u012193416/article/details/83210790>stats.probplot解释</a></li></ol><h4 id=224-对数变换>2.2.4. 对数变换</h4><h5 id=2241-资料>2.2.4.1. 资料</h5><ol><li><a href=https://www.zhihu.com/question/22012482>对变量取对数</a></li><li><a href=https://github.com/datawhalechina/team-learning-data-mining/blob/master/EnsembleLearning/CH6-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%A1%88%E4%BE%8B%E5%88%86%E4%BA%AB/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%902/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%902.ipynb>操作实践</a></li></ol><h4 id=225-寻找离群值>2.2.5. 寻找离群值</h4><p>这里寻找数据中的离群数据（特征中的离群值应该视为异常值，按照异常值进行处理）。使用岭回归使用特征进行预测。将预测结果和真实结果相减得到差值。将归一化后的差值大于阈值的剔除。</p><p>TODO: 这里待改，图画的不好。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>find_outliers</span>(model, X, y, sigma<span style=color:#666>=</span><span style=color:#666>3</span>):
   <span style=color:#080;font-style:italic># predict y values using model</span>
   <span style=color:#080;font-style:italic># 使用模型进行预测，一般使用Ridge岭回归、逻辑回归等。</span>
   model<span style=color:#666>.</span>fit(X,y)
   y_pred <span style=color:#666>=</span> pd<span style=color:#666>.</span>Series(model<span style=color:#666>.</span>predict(X), index<span style=color:#666>=</span>y<span style=color:#666>.</span>index)
      
   <span style=color:#080;font-style:italic># calculate residuals between the model prediction and true y values</span>
   <span style=color:#080;font-style:italic># 计算差值，并归一化</span>
   resid <span style=color:#666>=</span> y <span style=color:#666>-</span> y_pred
   mean_resid <span style=color:#666>=</span> resid<span style=color:#666>.</span>mean()
   std_resid <span style=color:#666>=</span> resid<span style=color:#666>.</span>std()

   <span style=color:#080;font-style:italic># calculate z statistic, define outliers to be where |z|&gt;sigma</span>
   <span style=color:#080;font-style:italic># 将归一化后的点大于阈值的点标记为离群值。</span>
   z <span style=color:#666>=</span> (resid <span style=color:#666>-</span> mean_resid)<span style=color:#666>/</span>std_resid    
   outliers <span style=color:#666>=</span> z[<span style=color:#a2f>abs</span>(z)<span style=color:#666>&gt;</span>sigma]<span style=color:#666>.</span>index

   <span style=color:#080;font-style:italic># 打印展示结果</span>
   <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#a2f>len</span>(outliers),<span style=color:#b44>&#39;outliers:&#39;</span>)
   <span style=color:#a2f;font-weight:700>print</span>(outliers<span style=color:#666>.</span>tolist())

   plt<span style=color:#666>.</span>figure(figsize<span style=color:#666>=</span>(<span style=color:#666>15</span>,<span style=color:#666>5</span>))
   ax_131 <span style=color:#666>=</span> plt<span style=color:#666>.</span>subplot(<span style=color:#666>1</span>,<span style=color:#666>3</span>,<span style=color:#666>1</span>)
   plt<span style=color:#666>.</span>plot(y,y_pred,<span style=color:#b44>&#39;.&#39;</span>)
   plt<span style=color:#666>.</span>plot(y<span style=color:#666>.</span>loc[outliers],y_pred<span style=color:#666>.</span>loc[outliers],<span style=color:#b44>&#39;ro&#39;</span>)
   plt<span style=color:#666>.</span>legend([<span style=color:#b44>&#39;Accepted&#39;</span>,<span style=color:#b44>&#39;Outlier&#39;</span>])
   plt<span style=color:#666>.</span>xlabel(<span style=color:#b44>&#39;y&#39;</span>)
   plt<span style=color:#666>.</span>ylabel(<span style=color:#b44>&#39;y_pred&#39;</span>);
   

   ax_132<span style=color:#666>=</span>plt<span style=color:#666>.</span>subplot(<span style=color:#666>1</span>,<span style=color:#666>3</span>,<span style=color:#666>2</span>)
   plt<span style=color:#666>.</span>plot(y,y<span style=color:#666>-</span>y_pred,<span style=color:#b44>&#39;.&#39;</span>)
   plt<span style=color:#666>.</span>plot(y<span style=color:#666>.</span>loc[outliers],y<span style=color:#666>.</span>loc[outliers]<span style=color:#666>-</span>y_pred<span style=color:#666>.</span>loc[outliers],<span style=color:#b44>&#39;ro&#39;</span>)
   plt<span style=color:#666>.</span>legend([<span style=color:#b44>&#39;Accepted&#39;</span>,<span style=color:#b44>&#39;Outlier&#39;</span>])
   plt<span style=color:#666>.</span>xlabel(<span style=color:#b44>&#39;y&#39;</span>)
   plt<span style=color:#666>.</span>ylabel(<span style=color:#b44>&#39;y - y_pred&#39;</span>);

   ax_133<span style=color:#666>=</span>plt<span style=color:#666>.</span>subplot(<span style=color:#666>1</span>,<span style=color:#666>3</span>,<span style=color:#666>3</span>)
   z<span style=color:#666>.</span>plot<span style=color:#666>.</span>hist(bins<span style=color:#666>=</span><span style=color:#666>50</span>,ax<span style=color:#666>=</span>ax_133)
   z<span style=color:#666>.</span>loc[outliers]<span style=color:#666>.</span>plot<span style=color:#666>.</span>hist(color<span style=color:#666>=</span><span style=color:#b44>&#39;r&#39;</span>,bins<span style=color:#666>=</span><span style=color:#666>50</span>,ax<span style=color:#666>=</span>ax_133)
   plt<span style=color:#666>.</span>legend([<span style=color:#b44>&#39;Accepted&#39;</span>,<span style=color:#b44>&#39;Outlier&#39;</span>])
   plt<span style=color:#666>.</span>xlabel(<span style=color:#b44>&#39;z&#39;</span>)
   
   <span style=color:#a2f;font-weight:700>return</span> outliers

outliers <span style=color:#666>=</span> find_outliers(Ridge(), X_train, y_train)
X_outliers<span style=color:#666>=</span>X_train<span style=color:#666>.</span>loc[outliers]
y_outliers<span style=color:#666>=</span>y_train<span style=color:#666>.</span>loc[outliers]
X_t<span style=color:#666>=</span>X_train<span style=color:#666>.</span>drop(outliers)
y_t<span style=color:#666>=</span>y_train<span style=color:#666>.</span>drop(outliers)
</code></pre></div><h2 id=3-数据预处理>3. 数据预处理</h2><h3 id=31-训练测试数据组合>3.1. 训练、测试数据组合</h3><p>整个数据处理部分，应该将训练集和测试集放在一起统一处理，处理后，再将训练集和测试集分开。将训练集分成训练和验证集。</p><p>一种方式是，将测试数据加在训练数据后面，并添加一列来区分训练数据和测试数据。</p><p>TODO: 思考如果来到未知的新数据如何处理。这些操作能否组成pipeline？</p><h3 id=无用特征处理>无用特征处理</h3><p>检查特征中异常值、缺失值的占比。如果某个特征的有效样本，则应该删除这个特征。这些信息可以从AutoEDA中看出啦</p><p>代码</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>df[cols]<span style=color:#666>.</span>isna()<span style=color:#666>.</span>sum(axis<span style=color:#666>=</span><span style=color:#666>0</span>) <span style=color:#080;font-style:italic># 统计无效特征的个数。</span>
<span style=color:#080;font-style:italic># 特征中，某一个值占比超过0.9则应该予以重视或者删除。</span>
first_feature_proportion <span style=color:#666>=</span> train[col]<span style=color:#666>.</span>value_counts(dropna<span style=color:#666>=</span>False, normalize<span style=color:#666>=</span>True, ascending<span style=color:#666>=</span>False)<span style=color:#666>.</span>iloc[<span style=color:#666>0</span>]
<span style=color:#a2f;font-weight:700>if</span> first_feature_proportion<span style=color:#666>&gt;</span><span style=color:#666>0.9</span>:
   train<span style=color:#666>.</span>drop(col, axis<span style=color:#666>=</span><span style=color:#666>0</span>)
</code></pre></div><h3 id=32-异常值处理>3.2. 异常值处理</h3><p>找到异常值：</p><ol><li>对数据进行查看，常见的异常值：0, 负数, -1, -999, 999, NAN, 数字中的异常值。<ol><li>代码 <code>df.isnull()</code></li></ol></li><li>可视化分析：使用箱线图、hist图查看数据，远离太远的应该视为异常值。<ol><li>代码：<code>sns.boxplot(), sns.histplot()</code></li></ol></li><li>不合字段含义的异常值<ol><li>如收入是1块钱，身高是200cm，这些需要根据字段含义进一步判断。并修改。</li></ol></li></ol><p>异常值处理：</p><ol><li>统计每行数据的异常值的个数，添加为新特征。</li><li>删除异常值记录</li><li>视为缺失值进行处理</li><li>不处理。（部分异常值可能是对真实情况的记录，直接挖掘可能会保留最真实的信息）</li></ol><p>TODO: 特征处理：https://cloud.tencent.com/developer/article/1488245</p><h3 id=33-缺失值处理>3.3. 缺失值处理</h3><p>找到缺失值：</p><ol><li>区分缺失值：存在形式：None，空，特殊值（-1，-999）<ol><li>代码 <code>df.isnull(), df.isna(), df[df.isna().T.any()]</code></li></ol></li></ol><p>缺失值处理方法，对缺失的值进行填充：</p><ol><li>类别特征：进行填充众数、填充新类</li><li>数值特征：填充平均数、中位数、众数、最大值、最小值<ol><li>代码: <code>df.fillna(values)</code></li><li>注意替换mode时:<code>df.apply(lambda col:col.fillna(col.mode()[0]))</code></li></ol></li><li>有序数据：next、previous</li><li>模型预测填充：对含有缺失值的那一列进行建模并预测。</li><li>根据字段含义进行填充。比如大部分人不信教，宗教可以填充为不信教。</li></ol><h3 id=34-连续值离散化>3.4. 连续值离散化</h3><ol><li>连续的时间<ol><li>将连续的时间处理成年、月、日。具体也需要根据情况。</li><li>将时间转换为新特征，如年龄。</li><li>时间转换函数 <code>pd.to_datatime(df["time"], format="%Y-%m-%d", errors='coerce')</code></li></ol></li><li>年龄<ol><li>对年龄进行分桶。分桶可以参考关键年龄[0, 18, 24, 30, 50, 60,100]。也可以将年龄histplot展示出来，再分桶。</li><li>代码：<code>pd.cut(df["age"], bins, labels=[0,1,2...])</code></li></ol></li></ol><p>TODO: 如何分桶？</p><p>Notes：</p><ol><li>连续值处理成了离散值后，为什么不进一步处理成one-hot值？是因为连续值通常有偏序关系，one-hot无法题先偏序关系。</li></ol><h3 id=35-连续值无量纲化>3.5. 连续值无量纲化</h3><p>部分连续值不宜离散化，需要当作连续值处理。这个时候多个连续值就需要消除量纲。当数值直接影响结果或者需要梯度下降进行训练的模型的时候，往往需要消除连续值之间的量纲。</p><p>方法：</p><ol><li>标准化。缩放到标准差为0，方差为1</li><li>区间缩放，minmax归一化。</li></ol><p>Notes:</p><ol><li>注意标准化应该只对连续变量进行操作。minmax归一化对one-hot无影响。顺序应该为先归一化再进行one-hot。</li></ol><h3 id=36-离散特征处理>3.6. 离散特征处理</h3><ol><li>离散特征处理成one-hot.<ol><li>处理成one-hot, 代码: <code>df = sklearn.preprocessing.OneHotEncoder(categories="auto").fit_transform(df).toarray()</code></li><li>处理成二进制编码。在one_hot维度过大的时候，二进制编码可以缩小特征维度，同时比连续值提高一定的效果。</li></ol></li></ol><p>Notes:</p><ol><li>One-hot主要用来编码类别特征，即采用哑变量（dummy variables）对类别进行编码。 one-hot的意义：<ol><li>避免因将类别用数字作为表示而给函数带来抖动。 直接使用数字会将人工误差而导致的假设引入到类别特征中，比如类别之间的大小关系，以及差异关系等等</li><li>将连续值转为欧式空间。在机器学习算法中，距离、相似度的计算是十分重要的，而这些计算都是定义于欧式空间上的。高中低本来不可以区分，但转为000就可分了。可以理解为连续值是线性模型，转为为高维空间后就可以划分了。</li></ol></li><li>注意部分模型不需要处理成one-hot，比如树模型，数值对树模型仅是符号，处理成one-hot没有意义。</li></ol><h3 id=37-特征变换>3.7. 特征变换</h3><ol><li>多元特征组合<ol><li>可以根据一元特征的含义构造多元特征，构造方向应该朝着和目标有关联去构造。如预测幸福度的模型，可以通过年龄、子女年龄，构造出生孩子的时间、孩子的大小，这都可能直接对幸福值数造成影响。再比如收入、支出的比例，和同省人收入支出的比例。</li></ol></li></ol><h3 id=38-标签处理>3.8. 标签处理</h3><p>LabelEncoder</p><h2 id=4-模型训练>4. 模型训练</h2><h3 id=41-常用模型>4.1. 常用模型</h3><p>xgboost, lgbm, randomforest, gradientBoostingRegressor, ExtraTreesRegressor, GradientBoostingRegressor, Kernel Ridge Regression(常用于Stack的二层模型), Ridge岭回归, ElasticNet 弹性网络, BayesianRidge 贝叶斯回归, LinearRegression简单的线性回归</p><h3 id=42-数据分组>4.2. 数据分组</h3><ol><li>单次的数据分组：train_test_split</li><li>多折训练：<ol><li>KFold, StratifiedKFold</li><li>代码:</li></ol></li></ol><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>folds <span style=color:#666>=</span> StratifiedKFold(n_splits<span style=color:#666>=</span><span style=color:#666>5</span>, shuffle<span style=color:#666>=</span>True, random_state<span style=color:#666>=</span><span style=color:#666>4</span>)   <span style=color:#080;font-style:italic>#交叉切分：5 train:val=4:1</span>
oof <span style=color:#666>=</span> np<span style=color:#666>.</span>zeros(<span style=color:#a2f>len</span>(X_train))
test_prediction <span style=color:#666>=</span> np<span style=color:#666>.</span>zeros(<span style=color:#a2f>len</span>(X_test))

<span style=color:#a2f;font-weight:700>for</span> fold_, (trn_idx, val_idx) <span style=color:#a2f;font-weight:700>in</span> <span style=color:#a2f>enumerate</span>(folds<span style=color:#666>.</span>split(X_train, y_train)):
    trn_data <span style=color:#666>=</span> X_train[trn_idx], y_train[trn_idx]
    val_data <span style=color:#666>=</span> X_train[val_idx], y_train[val_idx]
    <span style=color:#080;font-style:italic># model.fit()</span>
    oof[val_idx] <span style=color:#666>=</span> model<span style=color:#666>.</span>predict()
    <span style=color:#080;font-style:italic># 这里由于每一折都会对test进行预测，所以每一折预测的结果除以折数加起来。</span>
    test_prediction <span style=color:#666>+=</span> model<span style=color:#666>.</span>predict()<span style=color:#666>/</span>folds<span style=color:#666>.</span>n_splits
<span style=color:#080;font-style:italic># 使用oof留出验证后，将oof的预测作为结果。</span>
<span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#34;CV score: {:&lt;8.8f}&#34;</span><span style=color:#666>.</span>format(mean_squared_error(oof, target)))
</code></pre></div><h3 id=43-模型训练>4.3. 模型训练</h3><p>因为有众多模型都支持fit进行训练，prediction进行预测，所以可以抽象下，写一个类似的接口，接收所有模型的训练并输出指标。</p><p>这个是一个例子，这个train_model支持传入模型，支持grid,search, 支持KFold。训练模型并输出结果。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>train_model</span>(model, param_grid<span style=color:#666>=</span>[], X<span style=color:#666>=</span>[], y<span style=color:#666>=</span>[], 
                splits<span style=color:#666>=</span><span style=color:#666>5</span>, repeats<span style=color:#666>=</span><span style=color:#666>5</span>):
    <span style=color:#080;font-style:italic># 系统化的训练函数，需要自己总结一个。</span>
    <span style=color:#080;font-style:italic># 先要自己能写出来一个完整的流程。然后总结一下代码。</span>
    <span style=color:#080;font-style:italic># 获取数据</span>
    <span style=color:#a2f;font-weight:700>if</span> <span style=color:#a2f>len</span>(y)<span style=color:#666>==</span><span style=color:#666>0</span>:
        X,y <span style=color:#666>=</span> get_trainning_data_omitoutliers()
        
    <span style=color:#080;font-style:italic># 交叉验证</span>
    <span style=color:#080;font-style:italic># TODO: 再总结下，repeated K Fold有什么用？</span>
    rkfold <span style=color:#666>=</span> RepeatedKFold(n_splits<span style=color:#666>=</span>splits, n_repeats<span style=color:#666>=</span>repeats)
    
    <span style=color:#080;font-style:italic># 网格搜索最佳参数</span>
    <span style=color:#a2f;font-weight:700>if</span> <span style=color:#a2f>len</span>(param_grid)<span style=color:#666>&gt;</span><span style=color:#666>0</span>:
        gsearch <span style=color:#666>=</span> GridSearchCV(model, param_grid, cv<span style=color:#666>=</span>rkfold,
                               scoring<span style=color:#666>=</span><span style=color:#b44>&#34;neg_mean_squared_error&#34;</span>,
                               verbose<span style=color:#666>=</span><span style=color:#666>1</span>, return_train_score<span style=color:#666>=</span>True)

        <span style=color:#080;font-style:italic># 训练</span>
        gsearch<span style=color:#666>.</span>fit(X,y)

        <span style=color:#080;font-style:italic># 最好的模型</span>
        model <span style=color:#666>=</span> gsearch<span style=color:#666>.</span>best_estimator_        
        best_idx <span style=color:#666>=</span> gsearch<span style=color:#666>.</span>best_index_

        <span style=color:#080;font-style:italic># 获取交叉验证评价指标</span>
        grid_results <span style=color:#666>=</span> pd<span style=color:#666>.</span>DataFrame(gsearch<span style=color:#666>.</span>cv_results_)
        cv_mean <span style=color:#666>=</span> <span style=color:#a2f>abs</span>(grid_results<span style=color:#666>.</span>loc[best_idx,<span style=color:#b44>&#39;mean_test_score&#39;</span>])
        cv_std <span style=color:#666>=</span> grid_results<span style=color:#666>.</span>loc[best_idx,<span style=color:#b44>&#39;std_test_score&#39;</span>]

    <span style=color:#080;font-style:italic># 没有网格搜索  </span>
    <span style=color:#a2f;font-weight:700>else</span>:
        grid_results <span style=color:#666>=</span> []
        cv_results <span style=color:#666>=</span> cross_val_score(model, X, y, scoring<span style=color:#666>=</span><span style=color:#b44>&#34;neg_mean_squared_error&#34;</span>, cv<span style=color:#666>=</span>rkfold)
        cv_mean <span style=color:#666>=</span> <span style=color:#a2f>abs</span>(np<span style=color:#666>.</span>mean(cv_results))
        cv_std <span style=color:#666>=</span> np<span style=color:#666>.</span>std(cv_results)
    
    <span style=color:#080;font-style:italic># 合并数据</span>
    cv_score <span style=color:#666>=</span> pd<span style=color:#666>.</span>Series({<span style=color:#b44>&#39;mean&#39;</span>:cv_mean,<span style=color:#b44>&#39;std&#39;</span>:cv_std})

    <span style=color:#080;font-style:italic># 预测</span>
    y_pred <span style=color:#666>=</span> model<span style=color:#666>.</span>predict(X)
    
    <span style=color:#080;font-style:italic># 模型性能的统计数据        </span>
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;----------------------&#39;</span>)
    <span style=color:#a2f;font-weight:700>print</span>(model)
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;----------------------&#39;</span>)
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;score=&#39;</span>,model<span style=color:#666>.</span>score(X,y))
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;rmse=&#39;</span>,rmse(y, y_pred))
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;mse=&#39;</span>,mse(y, y_pred))
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;cross_val: mean=&#39;</span>,cv_mean,<span style=color:#b44>&#39;, std=&#39;</span>,cv_std)
    
    <span style=color:#080;font-style:italic># 残差分析与可视化</span>
    y_pred <span style=color:#666>=</span> pd<span style=color:#666>.</span>Series(y_pred,index<span style=color:#666>=</span>y<span style=color:#666>.</span>index)
    resid <span style=color:#666>=</span> y <span style=color:#666>-</span> y_pred
    mean_resid <span style=color:#666>=</span> resid<span style=color:#666>.</span>mean()
    std_resid <span style=color:#666>=</span> resid<span style=color:#666>.</span>std()
    z <span style=color:#666>=</span> (resid <span style=color:#666>-</span> mean_resid)<span style=color:#666>/</span>std_resid    
    n_outliers <span style=color:#666>=</span> <span style=color:#a2f>sum</span>(<span style=color:#a2f>abs</span>(z)<span style=color:#666>&gt;</span><span style=color:#666>3</span>)
    outliers <span style=color:#666>=</span> z[<span style=color:#a2f>abs</span>(z)<span style=color:#666>&gt;</span><span style=color:#666>3</span>]<span style=color:#666>.</span>index
    
    <span style=color:#a2f;font-weight:700>return</span> model, cv_score, grid_results
</code></pre></div><h3 id=44-特征重要性查看>4.4. 特征重要性查看</h3><p>TODO: 查看特征重要性后，有什么用？能干什么？</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>
df[<span style=color:#b44>&#39;importance&#39;</span>]<span style=color:#666>=</span><span style=color:#a2f>list</span>(lgb_model<span style=color:#666>.</span>feature_importance())
df <span style=color:#666>=</span> df<span style=color:#666>.</span>sort_values(by<span style=color:#666>=</span><span style=color:#b44>&#39;importance&#39;</span>,ascending<span style=color:#666>=</span>False)
plt<span style=color:#666>.</span>figure(figsize<span style=color:#666>=</span>(<span style=color:#666>14</span>,<span style=color:#666>28</span>))
sns<span style=color:#666>.</span>barplot(x<span style=color:#666>=</span><span style=color:#b44>&#34;importance&#34;</span>, y<span style=color:#666>=</span><span style=color:#b44>&#34;feature&#34;</span>, data<span style=color:#666>=</span>df<span style=color:#666>.</span>head(<span style=color:#666>50</span>))
plt<span style=color:#666>.</span>title(<span style=color:#b44>&#39;Features importance (averaged/folds)&#39;</span>)
plt<span style=color:#666>.</span>tight_layout()

</code></pre></div><h3 id=45-模型交叉验证>4.5. 模型交叉验证</h3><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#080;font-style:italic># 3折交叉验证。</span>
cross_val_score(model, X, y, cv<span style=color:#666>=</span><span style=color:#666>3</span>)
</code></pre></div><h2 id=5-模型调优>5. 模型调优</h2><h3 id=51-网格调参>5.1. 网格调参</h3><p>常用的参数搜索：网格调参、贝叶斯调参。</p><p>Notes: 网格调参应该先使用大网格，再逐步细化。</p><h4 id=511-gridsearchcv>5.1.1. GridSearchCV</h4><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>params <span style=color:#666>=</span> {<span style=color:#b44>&#39;kneighborsclassifier-1__n_neighbors&#39;</span>: [<span style=color:#666>1</span>, <span style=color:#666>5</span>],
          <span style=color:#b44>&#39;kneighborsclassifier-2__n_neighbors&#39;</span>: [<span style=color:#666>1</span>, <span style=color:#666>5</span>],
          <span style=color:#b44>&#39;randomforestclassifier__n_estimators&#39;</span>: [<span style=color:#666>10</span>, <span style=color:#666>50</span>],
          <span style=color:#b44>&#39;meta_classifier__C&#39;</span>: [<span style=color:#666>0.1</span>, <span style=color:#666>10.0</span>]}
grid <span style=color:#666>=</span> GridSearchCV(estimator<span style=color:#666>=</span>sclf, 
                    param_grid<span style=color:#666>=</span>params, 
                    cv<span style=color:#666>=</span><span style=color:#666>5</span>,
                    refit<span style=color:#666>=</span>True)
grid<span style=color:#666>.</span>fit(X, y)

cv_keys <span style=color:#666>=</span> (<span style=color:#b44>&#39;mean_test_score&#39;</span>, <span style=color:#b44>&#39;std_test_score&#39;</span>, <span style=color:#b44>&#39;params&#39;</span>)
<span style=color:#a2f;font-weight:700>for</span> r, _ <span style=color:#a2f;font-weight:700>in</span> <span style=color:#a2f>enumerate</span>(grid<span style=color:#666>.</span>cv_results_[<span style=color:#b44>&#39;mean_test_score&#39;</span>]):
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>%0.3f</span><span style=color:#b44> +/- </span><span style=color:#b68;font-weight:700>%0.2f</span><span style=color:#b44> </span><span style=color:#b68;font-weight:700>%r</span><span style=color:#b44>&#34;</span>
          <span style=color:#666>%</span> (grid<span style=color:#666>.</span>cv_results_[cv_keys[<span style=color:#666>0</span>]][r],
             grid<span style=color:#666>.</span>cv_results_[cv_keys[<span style=color:#666>1</span>]][r] <span style=color:#666>/</span> <span style=color:#666>2.0</span>,
             grid<span style=color:#666>.</span>cv_results_[cv_keys[<span style=color:#666>2</span>]][r]))

<span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;Best parameters: </span><span style=color:#b68;font-weight:700>%s</span><span style=color:#b44>&#39;</span> <span style=color:#666>%</span> grid<span style=color:#666>.</span>best_params_)
<span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;Accuracy: </span><span style=color:#b68;font-weight:700>%.2f</span><span style=color:#b44>&#39;</span> <span style=color:#666>%</span> grid<span style=color:#666>.</span>best_score_)

</code></pre></div><h3 id=52-模型融合>5.2. 模型融合</h3><p>blending, stacking(mlxtend)</p><h4 id=521-stacking>5.2.1. Stacking</h4><h5 id=5211-使用mlxtend工具包>5.2.1.1. 使用mlxtend工具包</h5><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>mlxtend.classifier</span> <span style=color:#a2f;font-weight:700>import</span> StackingCVClassifier

clf1 <span style=color:#666>=</span> KNeighborsClassifier(n_neighbors<span style=color:#666>=</span><span style=color:#666>1</span>)
clf2 <span style=color:#666>=</span> RandomForestClassifier(random_state<span style=color:#666>=</span>RANDOM_SEED)
clf3 <span style=color:#666>=</span> GaussianNB()
lr <span style=color:#666>=</span> LogisticRegression()

<span style=color:#080;font-style:italic># Starting from v0.16.0, StackingCVRegressor supports</span>
sclf <span style=color:#666>=</span> StackingCVClassifier(
    classifiers<span style=color:#666>=</span>[clf1, clf2, clf3],  <span style=color:#080;font-style:italic># 第一层分类器</span>
    use_probas<span style=color:#666>=</span>True,  <span style=color:#080;font-style:italic># </span>
    meta_classifier<span style=color:#666>=</span>lr,   <span style=color:#080;font-style:italic># 第二层分类器</span>
    random_state<span style=color:#666>=</span>RANDOM_SEED)
sclf<span style=color:#666>.</span>fit()
</code></pre></div><h5 id=5212-使用sklearn>5.2.1.2. 使用sklearn</h5><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>sklearn.ensemble</span> <span style=color:#a2f;font-weight:700>import</span> StackingClassifier
estimators <span style=color:#666>=</span> [
    (<span style=color:#b44>&#39;rf&#39;</span>, RandomForestClassifier(n_estimators<span style=color:#666>=</span><span style=color:#666>10</span>, random_state<span style=color:#666>=</span><span style=color:#666>42</span>)),
    (<span style=color:#b44>&#39;svr&#39;</span>, make_pipeline(StandardScaler(),
                          LinearSVC(random_state<span style=color:#666>=</span><span style=color:#666>42</span>)))
]
clf <span style=color:#666>=</span> StackingClassifier(
    estimators<span style=color:#666>=</span>estimators, final_estimator<span style=color:#666>=</span>LogisticRegression()
)
</code></pre></div><h2 id=6-整体步骤>6. 整体步骤</h2><h2 id=7-参考资料>7. 参考资料</h2><ol><li>非常详细全面的动手学习:<a href=https://github.com/datawhalechina/team-learning-data-mining>team-learning-data-mining</a></li></ol></div><footer class=post-footer><div class=post-tags><a href=/tags/kaggle rel=tag title=Kaggle>#Kaggle#</a></div><div class=addthis_inline_share_toolbox></div><div class=post-nav><div class=article-copyright><div class=article-copyright-img><img src=/img/qq_qrcode.png width=129px height=129px><div style=text-align:center>QQ扫一扫交流</div></div><div class=article-copyright-info><p><span>声明：</span>Kaggle学习计划</p><p style=word-break:break-all><span>链接：</span>
https://yanyulinxi.github.io/post/study/kaggle/kaggle%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/</p><p><span>作者：</span>阳阳</p><p><span>邮箱：</span>yanyulinxi@qq.com</p><p><span>声明： </span>本博客文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/ target=_blank style=text-decoration:underline>CC BY-NC-SA 3.0</a>许可协议，转载请注明出处！</p></div></div><div class=clear></div></div><div class=reward-qr-info><div>创作实属不易，如有帮助，那就打赏博主些许茶钱吧 ^_^</div><button id=rewardButton disable=enable onclick="var qr=document.getElementById('QR');qr.style.display==='none'?qr.style.display='block':qr.style.display='none'">
<span>赏</span></button><div id=QR style=display:none><div id=wechat style=display:inline-block><img id=wechat_qr src=/img/wechat-pay.png alt="WeChat Pay"><p>微信打赏</p></div><div id=alipay style=display:inline-block><img id=alipay_qr src=/img/ali-pay.png alt=Alipay><p>支付宝打赏</p></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=https://yanyulinxi.github.io/post/study/python%E5%BA%93/argparse%E4%BD%BF%E7%94%A8%E8%A7%84%E8%8C%83/ rel=next title=Argparse使用规范><i class="fa fa-chevron-left"></i>Argparse使用规范</a></div><div class="post-nav-prev post-nav-item"><a href=https://yanyulinxi.github.io/post/study/spark/spark%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B/ rel=prev title=Spark代码示例>Spark代码示例
<i class="fa fa-chevron-right"></i></a></div></div><div id=wcomments></div></footer></article></section></div></div><div class=sidebar-toggle><div class=sidebar-toggle-line-wrap><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id=sidebar class=sidebar><div class=sidebar-inner><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target=post-toc-wrap>文章目录</li><li class=sidebar-nav-overview data-target=site-overview>站点概览</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image src=/img/linxi_icon.png alt=阳阳><p class=site-author-name itemprop=name>阳阳</p><p class="site-description motion-element" itemprop=description>再平凡的人也有属于他自己的梦想!</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href=/post/><span class=site-state-item-count>140</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>6</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>35</span>
<span class=site-state-item-name>标签</span></a></div></nav><div class="links-of-author motion-element"><span class=links-of-author-item><a href=https://github.com/yanyuLinxi target=_blank title=GitHub><i class="fa fa-fw fa-github"></i>GitHub</a></span>
<span class=links-of-author-item><a href=https://space.bilibili.com/19237450 target=_blank title=哔哩哔哩><i class="fa fa-fw fa-globe"></i>哔哩哔哩</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class=links-of-blogroll-title><i class="fa fa-fw fa-globe"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://www.liaoxuefeng.com/ title=廖雪峰 target=_blank>廖雪峰</a></li></ul></div><div class="tagcloud-of-blogroll motion-element tagcloud-of-blogroll-inline"><div class=tagcloud-of-blogroll-title><i class="fa fa-fw fa-tags"></i>标签云</div><ul class=tagcloud-of-blogroll-list><li class=tagcloud-of-blogroll-item><a href=/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0>论文阅读笔记</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/python%E7%9B%B8%E5%85%B3%E5%BA%93%E5%AD%A6%E4%B9%A0>Python相关库学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E5%BC%82%E5%B8%B8%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90>异常行为分析</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7>深度学习辅助工具</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E5%BA%93>机器学习相关库</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/kaggle>Kaggle</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/java%E5%AD%A6%E4%B9%A0>Java学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/insider-threat>Insider threat</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/leetcode%E5%AD%A6%E4%B9%A0>Leetcode学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/spark>Spark</a></li></ul></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class=post-toc><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#1-学习计划>1. 学习计划</a></li><li><a href=#2-数据分析>2. 数据分析</a><ul><li><a href=#21-autoeda>2.1. AutoEDA</a></li><li><a href=#22-手动eda>2.2. 手动EDA</a></li></ul></li><li><a href=#3-数据预处理>3. 数据预处理</a><ul><li><a href=#31-训练测试数据组合>3.1. 训练、测试数据组合</a></li><li><a href=#无用特征处理>无用特征处理</a></li><li><a href=#32-异常值处理>3.2. 异常值处理</a></li><li><a href=#33-缺失值处理>3.3. 缺失值处理</a></li><li><a href=#34-连续值离散化>3.4. 连续值离散化</a></li><li><a href=#35-连续值无量纲化>3.5. 连续值无量纲化</a></li><li><a href=#36-离散特征处理>3.6. 离散特征处理</a></li><li><a href=#37-特征变换>3.7. 特征变换</a></li><li><a href=#38-标签处理>3.8. 标签处理</a></li></ul></li><li><a href=#4-模型训练>4. 模型训练</a><ul><li><a href=#41-常用模型>4.1. 常用模型</a></li><li><a href=#42-数据分组>4.2. 数据分组</a></li><li><a href=#43-模型训练>4.3. 模型训练</a></li><li><a href=#44-特征重要性查看>4.4. 特征重要性查看</a></li><li><a href=#45-模型交叉验证>4.5. 模型交叉验证</a></li></ul></li><li><a href=#5-模型调优>5. 模型调优</a><ul><li><a href=#51-网格调参>5.1. 网格调参</a></li><li><a href=#52-模型融合>5.2. 模型融合</a></li></ul></li><li><a href=#6-整体步骤>6. 整体步骤</a></li><li><a href=#7-参考资料>7. 参考资料</a></li></ul></nav></div></div></section></div></aside></div></main><footer id=footer class=footer><div class=footer-inner><div class=copyright><span class=copyright-year>&copy; 2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span><span class=copyright-author>阳阳的人间旅游日记</span></div><div class=powered-info><span class=powered-by>Powered by - <a class=powered-link href=//gohugo.io target=_blank title=hugo>Hugo v0.81.0</a></span>
<span class=separator-line>/</span>
<span class=theme-info>Theme by - <a class=powered-link href=//github.com/elkan1788/hugo-theme-next target=_blank>NexT</a></span></div><div class=vistor-info><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span class=site-uv><i class="fa fa-user"></i><span class=busuanzi-value id=busuanzi_value_site_uv></span></span><span class=separator-line>/</span>
<span class=site-pv><i class="fa fa-eye"></i><span class=busuanzi-value id=busuanzi_value_site_pv></span></span></div><div class=license-info><span class=storage-info>Storage by
<a href style=font-weight:700 target=_blank></a></span><span class=separator-line>/</span>
<span class=license-num><a href target=_blank></a></span></div></div></footer><div class=back-to-top><i class="fa fa-arrow-up"></i><span id=scrollpercent><span>0</span>%</span></div></div><script type=text/javascript src=//cdn.bootcdn.net/ajax/libs/jquery/2.1.4/jquery.min.js></script><script type=text/javascript src=/js/search.js></script><script type=text/javascript src=/js/affix.js></script><script type=text/javascript src=/js/scrollspy.js></script><script type=text/javascript>function detectIE(){var a=window.navigator.userAgent,b=a.indexOf('MSIE '),c=a.indexOf('Trident/'),d=a.indexOf('Edge/');return b>0||c>0||d>0?-1:1}function getCntViewHeight(){var b=$('#content').height(),a=$(window).height(),c=b>a?b-a:$(document).height()-a;return c}function getScrollbarWidth(){var a=$('<div />').addClass('scrollbar-measure').prependTo('body'),b=a[0],c=b.offsetWidth-b.clientWidth;return a.remove(),c}function registerBackTop(){var b=50,a=$('.back-to-top');$(window).on('scroll',function(){var d,e,f,c,g;a.toggleClass('back-to-top-on',window.pageYOffset>b),d=$(window).scrollTop(),e=getCntViewHeight(),f=d/e,c=Math.round(f*100),g=c>100?100:c,$('#scrollpercent>span').html(g)}),a.on('click',function(){$("html,body").animate({scrollTop:0,screenLeft:0},800)})}function initScrollSpy(){var a='.post-toc',d=$(a),b='.active-current';d.on('activate.bs.scrollspy',function(){var b=$(a+' .active').last();c(),b.addClass('active-current')}).on('clear.bs.scrollspy',c),$('body').scrollspy({target:a});function c(){$(a+' '+b).removeClass(b.substring(1))}}function initAffix(){var a=$('.header-inner').height(),b=parseInt($('.main').css('padding-bottom'),10),c=a+10;$('.sidebar-inner').affix({offset:{top:c,bottom:b}}),$(document).on('affixed.bs.affix',function(){updateTOCHeight(document.body.clientHeight-100)})}function initTOCDimension(){var a,b;$(window).on('resize',function(){a&&clearTimeout(a),a=setTimeout(function(){var a=document.body.clientHeight-100;updateTOCHeight(a)},0)}),updateTOCHeight(document.body.clientHeight-100),b=getScrollbarWidth(),$('.post-toc').css('width','calc(100% + '+b+'px)')}function updateTOCHeight(a){a=a||'auto',$('.post-toc').css('max-height',a)}$(function(){var b=$('.header-inner').height()+10,c,d,a,e;$('#sidebar').css({'margin-top':b}).show(),c=parseInt($('#sidebar').css('margin-top')),d=parseInt($('.sidebar-inner').css('height')),a=c+d,e=$('.content-wrap').height(),e<a&&$('.content-wrap').css('min-height',a),$('.site-nav-toggle').on('click',function(){var a=$('.site-nav'),e=$('.toggle'),b='site-nav-on',f='toggle-close',c=a.hasClass(b),g=c?'slideUp':'slideDown',d=c?'removeClass':'addClass';a.stop()[g]('normal',function(){a[d](b),e[d](f)})}),registerBackTop(),initScrollSpy(),initAffix(),initTOCDimension(),$('.sidebar-nav-toc').click(function(){$(this).addClass('sidebar-nav-active'),$(this).next().removeClass('sidebar-nav-active'),$('.'+$(this).next().attr('data-target')).toggle(500),$('.'+$(this).attr('data-target')).toggle(500)}),$('.sidebar-nav-overview').click(function(){$(this).addClass('sidebar-nav-active'),$(this).prev().removeClass('sidebar-nav-active'),$('.'+$(this).prev().attr('data-target')).toggle(500),$('.'+$(this).attr('data-target')).toggle(500)})})</script><script src=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.js></script><script type=text/javascript>$(function(){$('.post-body').viewer()})</script><script type=text/javascript>$(function(){detectIE()>0?$.getScript(document.location.protocol+'//cdn.jsdelivr.net/npm/@waline/client/dist/Waline.min.js',function(){new Waline({el:'#wcomments',visitor:!0,avatar:'wavatar',avatarCDN:'https://sdn.geekzu.org/avatar/',avatarForce:!1,wordLimit:'200',placeholder:' 欢迎留下您的宝贵建议，请填写您的昵称和邮箱便于后续交流. ^_^ ',requiredFields:['nick','mail'],serverURL:"Your WalineSerURL",lang:"zh-cn"})}):$('#wcomments').html('抱歉，Waline插件不支持IE或Edge，建议使用Chrome浏览器。')})</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=Your%20AddthisId"></script><script>(function(){var a=document.createElement('script'),c=window.location.protocol.split(':')[0],b;c==='https'?a.src='https://zz.bdstatic.com/linksubmit/push.js':a.src='http://push.zhanzhang.baidu.com/push.js',b=document.getElementsByTagName("script")[0],b.parentNode.insertBefore(a,b)})()</script></body></html>