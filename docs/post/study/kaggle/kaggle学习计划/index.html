<!doctype html><html lang=zh-cn dir=content/zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=content-security-policy content="upgrade-insecure-requests"><title>Kaggle学习计划 - 阳阳的人间旅游日记</title><meta name=keywords content="博客,程序员,思考,读书,笔记,技术,分享"><meta name=author content="阳阳"><meta property="og:title" content="Kaggle学习计划"><meta property="og:site_name" content="阳阳的人间旅游日记"><meta property="og:image" content="/img/author.jpg"><meta name=title content="Kaggle学习计划 - 阳阳的人间旅游日记"><meta name=description content="欢迎来到临溪的博客站，个人主要专注于机器学习、深度学习的相关研究。在这里分享自己的学习心得。"><link rel="shortcut icon" href=/img/favicon.ico><link rel=apple-touch-icon href=/img/apple-touch-icon.png><link rel=apple-touch-icon-precomposed href=/img/apple-touch-icon.png><link href=//cdn.bootcdn.net/ajax/libs/font-awesome/4.6.2/css/font-awesome.min.css rel=stylesheet type=text/css><link href=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.css rel=stylesheet><link href=/css/main.css rel=stylesheet type=text/css><link href=/css/syntax.css rel=stylesheet type=text/css></head><body itemscope itemtype=http://schema.org/WebPage lang=zh-hans><div class="container one-collumn sidebar-position-left page-home"><div class=headband></div><header id=header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle role=button style=opacity:1;top:0><span class=toggle-line></span><span class=toggle-line></span><span class=toggle-line></span></div></div><div class=site-meta><div class=multi-lang-switch><i class="fa fa-fw fa-language" style=margin-right:5px></i><a class=lang-link id=zh-cn href=#>中文</a></div><div class=custom-logo-site-title><a href=/ class=brand rel=start><span class=logo-line-before><i></i></span><span class=site-title>阳阳的人间旅游日记</span>
<span class=logo-line-after><i></i></span></a></div><p class=site-subtitle>让我们消除隔阂的，不是无所不知的脑袋，而是手拉手，坚决不放弃的那颗心</p></div><div class=site-nav-right><div class="toggle popup-trigger" style=opacity:1;top:0><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul id=menu class=menu><li class=menu-item><a href=/ rel=section><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class=menu-item><a href=/post rel=section><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class=menu-item><a href=/about.html rel=section><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于我</a></li><li class=menu-item><a href=/404.html rel=section><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href=javascript:; class=popup-trigger><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class=site-search><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class=search-icon><i class="fa fa-search"></i></span><span class=popup-btn-close><i class="fa fa-times-circle"></i></span><div class=local-search-input-wrapper><input autocomplete=off placeholder=搜索关键字... spellcheck=false type=text id=local-search-input autocapitalize=none autocorrect=off></div></div><div id=local-search-result></div></div></div></nav></div></header><main id=main class=main><div class=main-inner><div class=content-wrap><div id=content class=content><section id=posts class=posts-expand><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><header class=post-header><h1 class=post-title itemprop="name headline"><a class=post-title-link href=https://yanyulinxi.github.io/post/study/kaggle/kaggle%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/ itemprop=url>Kaggle学习计划</a></h1><div class=post-meta><span class=post-time><i class="fa fa-calendar-o fa-fw"></i><span class=post-meta-item-text>时间：</span>
<time itemprop=dateCreated datetime=2016-03-22T13:04:35+08:00 content="2022-06-06">2022-06-06</time></span>
<span class=post-category>&nbsp; | &nbsp;
<i class="fa fa-folder-o fa-fw"></i><span class=post-meta-item-text>分类：</span>
<span itemprop=about itemscope itemtype=https://schema.org/Thing><a href=/categories/%E5%AD%A6%E4%B9%A0 itemprop=url rel=index style=text-decoration:underline><span itemprop=name>学习</span></a>
&nbsp;</span></span>
<span>|
<i class="fa fa-file-word-o fa-fw"></i><span class=post-meta-item-text>字数：</span>
<span class=leancloud-world-count>8856 字</span></span>
<span>|
<i class="fa fa-eye fa-fw"></i><span class=post-meta-item-text>阅读：</span>
<span class=leancloud-view-count>18分钟</span></span>
<span id=/post/study/kaggle/kaggle%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/ class=leancloud_visitors data-flag-title=Kaggle学习计划>|
<i class="fa fa-binoculars fa-fw"></i><span class=post-meta-item-text>阅读次数：</span>
<span class=leancloud-visitors-count></span></span></div></header><div class=post-body itemprop=articleBody><h2 id=1-学习计划>1. 学习计划</h2><p>先按照<a href=https://github.com/datawhalechina/team-learning-data-mining>datawhale</a>的学习计划进行学习。</p><p>学玩一个notebook后，我们将机器学习分为几个部分
数据分析
数据预处理
模型训练
调优
整体步骤
这几个部分挨个填充内容。</p><p>一个kaggle怎么学？学习一个kaggle后。默写从头复现。</p><p>把集成学习两个案例学完。然后总结。</p><h2 id=2-赛题分析>2. 赛题分析</h2><p>详见：<a href=https://github.com/datawhalechina/team-learning-data-mining/blob/master/SecondHandCarPriceForecast/Task1%20%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3.md>二手车交易</a></p><p>赛题分析的目的：</p><ol><li>理清任务，对数据有基本的了解</li><li>对题目进行分析：<ol><li>比如这题的难点可能在哪里，</li><li>关键点可能在哪里，</li><li>哪些地方可以挖掘更好的特征，</li><li>用什么样得线下验证方式更为稳定，</li><li>出现了过拟合或者其他问题，</li><li>估摸可以用什么方法去解决这些问题，</li><li>哪些数据是可靠的，</li><li>哪些数据是需要精密的处理的，</li><li>哪部分数据应该是关键数据</li></ol></li><li>对评价指标进行分析：<ol><li>本地模型的验证方式，很多情况下，线上验证是有一定的时间和次数限制的，所以在比赛中构建一个合理的<strong>本地的验证集</strong>和<strong>验证的评价指标</strong>是很关键的步骤，能有效的节省很多时间。</li><li>不同的指标对于同样的预测结果是具有误差敏感的差异性的，比如AUC，logloss, MAE，RSME，或者一些特定的评价函数。是会有很大可能会影响后续一些预测的侧重点。</li></ol></li></ol><h2 id=解题步骤>解题步骤</h2><p>首先应该进行数据分析，记录数据可能存在的问题，需要操作的事情。
再对数据进行操作。</p><h2 id=3-数据分析>3. 数据分析</h2><p>数据分析阶段需要做的事情：</p><ol><li>查看特征类型。</li><li>查看特征缺失、异常值情况</li><li>查看预测值分布</li><li>针对数字特征：<ol><li>相关性分析</li><li>查看几个特征的偏度和峰值</li><li>每个数字特征得分布可视化</li><li>数字特征相互之间的关系可视化</li><li>多变量互相回归关系可视化</li></ol></li><li>针对类别特征：<ol><li>unique分布</li><li>类别特征箱形图可视化</li><li>类别特征的小提琴图可视化、</li><li>类别特征的柱形图可视化类别</li><li>特征的每个类别频数可视化(count_plot)</li></ol></li></ol><h3 id=资料>资料</h3><p><a href=https://github.com/datawhalechina/team-learning-data-mining/blob/master/SecondHandCarPriceForecast/Task2%20%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90.md#%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%87%BA%E5%8C%BF%E5%90%8D%E7%89%B9%E5%BE%81%E7%9B%B8%E5%AF%B9%E5%88%86%E5%B8%83%E5%9D%87%E5%8C%80>二手车交易</a></p><h3 id=31-autoeda>3.1. AutoEDA</h3><h4 id=311-pandas-profiling>3.1.1. pandas-profiling</h4><p>pandas-profiling是一个自动化数据分析工具，使用代码如下：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>import</span> <span style=color:#00f;font-weight:700>pandas_profiling</span>

<span style=color:#080;font-style:italic># 如果是使用notebook进行分析，则直接可以在notebook中查看。</span>
<span style=color:#080;font-style:italic># notebook查看非常的费时间，建议转为html</span>
profile <span style=color:#666>=</span> pandas_profiling<span style=color:#666>.</span>ProfileReport(df)

<span style=color:#080;font-style:italic># 如果需要生成html文件</span>
profile<span style=color:#666>.</span>to_file(<span style=color:#b44>&#34;your_report.html&#34;</span>)
</code></pre></div><h4 id=312-sweetviz>3.1.2. Sweetviz</h4><p>Sweetviz相比于pandas-profiling功能更多。</p><p>概览：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>import</span> <span style=color:#00f;font-weight:700>sweetviz</span> <span style=color:#a2f;font-weight:700>as</span> <span style=color:#00f;font-weight:700>sv</span>

my_report <span style=color:#666>=</span> sv<span style=color:#666>.</span>analyze(df)
my_report<span style=color:#666>.</span>show_html() <span style=color:#080;font-style:italic># Default arguments will generate to &#34;SWEETVIZ_REPORT.html&#34;</span>
</code></pre></div><h5 id=3121-生成分析报告>3.1.2.1. 生成分析报告</h5><p>主要有三个函数创造report：analyze、compare、compare_intra</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>sv<span style=color:#666>.</span>analyze(source: Union[pd<span style=color:#666>.</span>DataFrame, Tuple[pd<span style=color:#666>.</span>DataFrame, <span style=color:#a2f>str</span>]],
            target_feat: <span style=color:#a2f>str</span> <span style=color:#666>=</span> None,
      
            feat_cfg: FeatureConfig <span style=color:#666>=</span> None,
            pairwise_analysis: <span style=color:#a2f>str</span> <span style=color:#666>=</span> <span style=color:#b44>&#39;auto&#39;</span>):
</code></pre></div><p>source表示输入的数据，支持的格式 source=df, source=[df, &ldquo;df&rsquo;s name&rdquo;]
target_feat 是一个字符串，指定的是df中为标签的那一栏。目前只支持boolean和数字
feat_cfg 是一个FeatureConfig对象，表示在分析中要跳过或强制某种类型的特征。参数可以是单个字符串或字符串列表。构建方式如：<code>feature_config = sv.FeatureConfig(skip="PassengerId", force_text=["Age"])</code>, 它的参数skip, force_cat, force_num 和 force_text， skip表示跳过，force表示前置的类型检测。
pairwise_analysis: Union[&ldquo;auto&rdquo;, &ldquo;on&rdquo;, &ldquo;off], 相关性和其他关联可能需要二次时间 (n^2) 才能完成直到数据集包含“association_auto_threshold”特征。超过该阈值，您需要显式传递参数pairwise_analysis=&ldquo;on&rdquo;（或=&ldquo;off&rdquo;），因为处理这么多特征需要很长时间。该参数还涵盖了关联图的生成</p><p>注意，现在的target_feat仅支持数字或者布尔类型，如果不是这两两种类型，需要指定为这两种。示例：
<code>report = sv.analyze(train, target_feat="happiness", feat_cfg=sv.FeatureConfig(force_num=["happiness"]))</code></p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>sv<span style=color:#666>.</span>compare(source, compare_source, <span style=color:#666>**</span>kwargs)
my_report <span style=color:#666>=</span> sv<span style=color:#666>.</span>compare([my_dataframe, <span style=color:#b44>&#34;Training Data&#34;</span>], [test_df, <span style=color:#b44>&#34;Test Data&#34;</span>], <span style=color:#b44>&#34;Survived&#34;</span>, feature_config)
</code></pre></div><p>compare是比较两个数据集的特征。source是一个数据集，compare_source是另一个数据集。其他参数和analyze相同。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>my_report <span style=color:#666>=</span> sv<span style=color:#666>.</span>compare_intra(my_dataframe, my_dataframe[<span style=color:#b44>&#34;Sex&#34;</span>] <span style=color:#666>==</span> <span style=color:#b44>&#34;male&#34;</span>, [<span style=color:#b44>&#34;Male&#34;</span>, <span style=color:#b44>&#34;Female&#34;</span>], feature_config)
</code></pre></div><p>compare_intra比较同一dataframe的两个子集, 如dataframe中的男性和女性两个子集。会创建两个单独的数据框进行展示。</p><h5 id=3122-输出分析结果>3.1.2.2. 输出分析结果</h5><p>一旦您创建了您的报告对象，只需将其传递给两个“显示”函数之一：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#080;font-style:italic># to html</span>
report<span style=color:#666>.</span>show_html( filepath<span style=color:#666>=</span><span style=color:#b44>&#39;SWEETVIZ_REPORT.html&#39;</span>, 
            open_browser<span style=color:#666>=</span>True, 
            layout<span style=color:#666>=</span><span style=color:#b44>&#39;widescreen&#39;</span>, 
            scale<span style=color:#666>=</span>None)



<span style=color:#080;font-style:italic># 在notebook中输出</span>
report<span style=color:#666>.</span>show_notebook(  
   w<span style=color:#666>=</span>None, 
   h<span style=color:#666>=</span>None, 
   scale<span style=color:#666>=</span>None,
   layout<span style=color:#666>=</span><span style=color:#b44>&#39;widescreen&#39;</span>,
   filepath<span style=color:#666>=</span>None)

</code></pre></div><h5 id=3123-相关性分析说明>3.1.2.3. 相关性分析说明</h5><p>Sweetviz 关联图和分析的主要洞察力和独特功能是它统一在一个图中
正方形代表分类特征相关变量，圆圈代表数值-数值相关性。</p><h5 id=3124-对结果进行分析>3.1.2.4. 对结果进行分析</h5><p>单数据集分析的时候，需要看：</p><ol><li>缺失值。是否有缺失值，缺失值的数量</li><li>和其他特征的相关性。高相关性的特征，应予以重视。</li><li>特征中值的占比。某个值占比太高，这个特征应予以剔除。</li><li>看数据类型。分类值应该用one-hot， 连续值应该考虑是否归一化，时间特征应该进行翻译。</li></ol><p>需要记录的信息：</p><ol><li>时间列</li><li>含异常值的列</li><li>类别值中需要处理为one-hot的列</li><li>连续值需要归一化的列</li><li>含缺失值的列，和缺失值的比例。（缺失值比例过高应该删除）</li><li>列中单个值占比过高的列。</li><li>记录类别特征</li><li>记录连续特征</li></ol><p>训练集、测试集对比的时候，需要看数据的分布。
数据分布不一致的列，应该考虑剔除。</p><h4 id=313-dataprep>3.1.3. Dataprep</h4><p>Dataprep是一个灵活性高、功能强大的数据分析软件。</p><p>API:</p><ol><li><code>plot(data, Optional[List[cols]])</code><ol><li>展示数据。如果后面的cols为空的时候，则分析所有数据。否则会分析传入的指定的列。</li></ol></li><li><code>plot_correlation</code><ol><li>参数同上。分析数据中的相关性</li></ol></li><li><code>plot_missing</code></li><li><code>plot_diff([df1, df2])</code><ol><li>对两个数据集进行对比。分析它们之间分布差异。</li></ol></li><li>create_report 生成数据集的综合报告。</li></ol><p>获取中间数据：对于每个plot函数，都有一个对应的compute函数，该函数返回用于渲染的计算中间体：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>dataprep.eda</span> <span style=color:#a2f;font-weight:700>import</span> compute_correlation
imdt <span style=color:#666>=</span> compute_correlation(df)
</code></pre></div><p>展示数据:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>report <span style=color:#666>=</span> plot(df)
report<span style=color:#666>.</span>show_browser()
</code></pre></div><p>Dataprep同时拥有清洗数据的功能。可以快速方便的对数据进行转换。并将不合法的数据标价为NAN。感觉有用。</p><h3 id=32-手动eda>3.2. 手动EDA</h3><p>需要明确三件事，EDA如何分析，能分析出什么？分析出来后怎么做？</p><ol><li>为什么要分析？</li><li>如何分析？</li><li>分析出来怎么解决？</li></ol><h4 id=查看数据类型>查看数据类型</h4><p>查看数据类型，可以找出类别特征、数字特征。帮助后面进一步分析。</p><p>代码:<code>df.info()</code></p><h4 id=321-核密度查看数据特征分布>3.2.1. 核密度查看数据特征分布</h4><p>训练集、测试集分布不一致会影响泛化性能。查看特征分布的目的就是将分布不一致的特征去掉，这些特征会影响到训练。</p><p>具体方法：</p><ol><li>使用自动化工具或者使用核密度估计来显示训练集和测试集特征的分布情况</li><li>分析：核密度图中，波峰趋势一致就是均匀的。波峰不一致就可能是不均匀的。如果波峰有偏移，但是图像整体偏移一点距离，可能是某些部分数据少，其他部分数据多，也算是均匀的。如果某些波峰特别高，则注意是不是异常数据。尽可能的保存特征。相差特别大的才去除。明显不一样的考虑去除。</li></ol><p>代码举例：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#080;font-style:italic># melt函数将列名转为variable列，将值转为value列。可以根据sns画出图。</span>
f <span style=color:#666>=</span> pd<span style=color:#666>.</span>melt(Train_data, value_vars<span style=color:#666>=</span>numeric_features)
g <span style=color:#666>=</span> sns<span style=color:#666>.</span>FacetGrid(f, col<span style=color:#666>=</span><span style=color:#b44>&#34;variable&#34;</span>,  col_wrap<span style=color:#666>=</span><span style=color:#666>2</span>, sharex<span style=color:#666>=</span>False, sharey<span style=color:#666>=</span>False)
g <span style=color:#666>=</span> g<span style=color:#666>.</span>map(sns<span style=color:#666>.</span>distplot, <span style=color:#b44>&#34;value&#34;</span>)
</code></pre></div><p>查看特征的偏度和峰值</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>for</span> col <span style=color:#a2f;font-weight:700>in</span> numeric_features:
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;{:15}&#39;</span><span style=color:#666>.</span>format(col), 
          <span style=color:#b44>&#39;Skewness: {:05.2f}&#39;</span><span style=color:#666>.</span>format(Train_data[col]<span style=color:#666>.</span>skew()) , 
          <span style=color:#b44>&#39;   &#39;</span> ,
          <span style=color:#b44>&#39;Kurtosis: {:06.2f}&#39;</span><span style=color:#666>.</span>format(Train_data[col]<span style=color:#666>.</span>kurt())  
         )
</code></pre></div><p>特征之间分布可视化:<a href=https://github.com/datawhalechina/team-learning-data-mining/blob/master/SecondHandCarPriceForecast/Task2%20%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90.md>特征分布</a></p><h4 id=322-查看特征之间的相关性>3.2.2. 查看特征之间的相关性</h4><p>这里有两种相关性需要查看，特征之间的相关性，特征和标签之间的相关性。</p><ol><li><p>特征之间的相关性越高，则说明特征之间拥有共线性。对于部分模型，比如逻辑回归，特征共线性的危害比较大，其会放大噪音，降低模型的可解释性。这一点可以参考本站《机器学习基础知识面试》部分的内容。如果特征之间相关性非常高，则应该通过直接删除或降维等方式去除高共线性特征。</p></li><li><p>如果特征和标签之间的相关性很低，则特征意义不大，需要将这些特征剔除。</p></li></ol><p>具体方法：</p><ol><li>使用AutoEDA工具或者scipy,pandas工具计算相关系数。</li></ol><p>代码举例：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#080;font-style:italic># 获取相关性的绝对值</span>
corr_matrix <span style=color:#666>=</span> data<span style=color:#666>.</span>corr()<span style=color:#666>.</span>abs()

<span style=color:#080;font-style:italic># 展示相关性</span>
<span style=color:#a2f;font-weight:700>print</span>(corr_matrix[<span style=color:#b44>&#34;y&#34;</span>]<span style=color:#666>.</span>sort_values(ascending<span style=color:#666>=</span>False))

<span style=color:#080;font-style:italic># 相关性热力图</span>
sns<span style=color:#666>.</span>heatmap(corr_matrix, square<span style=color:#666>=</span>True, vmax<span style=color:#666>=</span><span style=color:#666>0.8</span>)

<span style=color:#080;font-style:italic># 将和标签相关性低的特征删除</span>
threshold<span style=color:#666>=</span><span style=color:#666>0.1</span>
drop_col<span style=color:#666>=</span>corr_matrix[corr_matrix[<span style=color:#b44>&#34;y&#34;</span>]<span style=color:#666>&lt;</span>threshold]<span style=color:#666>.</span>index
data <span style=color:#666>=</span> data<span style=color:#666>.</span>drop(drop_col,axis<span style=color:#666>=</span><span style=color:#666>1</span>)
</code></pre></div><h4 id=323-box-cox变换>3.2.3. Box-Cox变换</h4><p>我们测得一些数据，要对数据进行分析的时候，会发现数据有一些问题使得我们不能满足我们以前分析方法的一些要求（正态分布、平稳性）</p><p>为了满足经典线性模型的正态性假设，常常需要使用指数变换或者对数转化，使其转换后的数据接近正态，比如数据是非单峰分布的，或者各种混合分布，我们就需要进行一些转化</p><p>box-cox变换的目标有两个：一个是变换后，可以一定程度上减小不可观测的误差和预测变量的相关性。主要操作是对因变量转换，使得变换后的因变量于回归自变量具有线性相依关系，误差也服从正态分布，误差各分量是等方差且相互独立。第二个是用这个变换来使得因变量获得一些性质，比如在时间序列分析中的平稳性，或者使得因变量分布为正态分布。</p><p>这个还没有弄懂，只知道它会对数据分布进行转换，使得其满足正态分布。</p><p>参考<a href=https://github.com/datawhalechina/team-learning-data-mining/blob/master/EnsembleLearning/CH6-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%A1%88%E4%BE%8B%E5%88%86%E4%BA%AB/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%902/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%902.ipynb>team-learning-data-mining</a>的内容Box-Cox了解更多</p><h5 id=3231-资料>3.2.3.1. 资料</h5><ol><li><a href=https://zhuanlan.zhihu.com/p/36284359>box-cox变换</a></li><li><a href=https://blog.csdn.net/u012193416/article/details/83210790>stats.probplot解释</a></li></ol><h4 id=324-对数变换>3.2.4. 对数变换</h4><h5 id=3241-资料>3.2.4.1. 资料</h5><ol><li><a href=https://www.zhihu.com/question/22012482>对变量取对数</a></li><li><a href=https://github.com/datawhalechina/team-learning-data-mining/blob/master/EnsembleLearning/CH6-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%A1%88%E4%BE%8B%E5%88%86%E4%BA%AB/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%902/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0-%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%902.ipynb>操作实践</a></li></ol><h4 id=325-寻找离群值>3.2.5. 寻找离群值</h4><p>这里寻找数据中的离群数据（特征中的离群值应该视为异常值，按照异常值进行处理）。使用岭回归使用特征进行预测。将预测结果和真实结果相减得到差值。将归一化后的差值大于阈值的剔除。</p><p>TODO: 这里待改，图画的不好。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#080;font-style:italic># 使用数据对标签进行预测，删除训练集的标签的离群数据。</span>
<span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>find_outliers</span>(model, X, y, sigma<span style=color:#666>=</span><span style=color:#666>3</span>):
   <span style=color:#080;font-style:italic># predict y values using model</span>
   <span style=color:#080;font-style:italic># 使用模型进行预测，一般使用Ridge岭回归、逻辑回归等。</span>
   model<span style=color:#666>.</span>fit(X,y)
   y_pred <span style=color:#666>=</span> pd<span style=color:#666>.</span>Series(model<span style=color:#666>.</span>predict(X), index<span style=color:#666>=</span>y<span style=color:#666>.</span>index)
      
   <span style=color:#080;font-style:italic># calculate residuals between the model prediction and true y values</span>
   <span style=color:#080;font-style:italic># 计算差值，并归一化</span>
   resid <span style=color:#666>=</span> y <span style=color:#666>-</span> y_pred
   mean_resid <span style=color:#666>=</span> resid<span style=color:#666>.</span>mean()
   std_resid <span style=color:#666>=</span> resid<span style=color:#666>.</span>std()

   <span style=color:#080;font-style:italic># calculate z statistic, define outliers to be where |z|&gt;sigma</span>
   <span style=color:#080;font-style:italic># 将归一化后的点大于阈值的点标记为离群值。</span>
   z <span style=color:#666>=</span> (resid <span style=color:#666>-</span> mean_resid)<span style=color:#666>/</span>std_resid    
   outliers <span style=color:#666>=</span> z[<span style=color:#a2f>abs</span>(z)<span style=color:#666>&gt;</span>sigma]<span style=color:#666>.</span>index

   <span style=color:#080;font-style:italic># 打印展示结果</span>
   <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#a2f>len</span>(outliers),<span style=color:#b44>&#39;outliers:&#39;</span>)
   <span style=color:#a2f;font-weight:700>print</span>(outliers<span style=color:#666>.</span>tolist())

   plt<span style=color:#666>.</span>figure(figsize<span style=color:#666>=</span>(<span style=color:#666>15</span>,<span style=color:#666>5</span>))
   ax_131 <span style=color:#666>=</span> plt<span style=color:#666>.</span>subplot(<span style=color:#666>1</span>,<span style=color:#666>3</span>,<span style=color:#666>1</span>)
   plt<span style=color:#666>.</span>plot(y,y_pred,<span style=color:#b44>&#39;.&#39;</span>)
   plt<span style=color:#666>.</span>plot(y<span style=color:#666>.</span>loc[outliers],y_pred<span style=color:#666>.</span>loc[outliers],<span style=color:#b44>&#39;ro&#39;</span>)
   plt<span style=color:#666>.</span>legend([<span style=color:#b44>&#39;Accepted&#39;</span>,<span style=color:#b44>&#39;Outlier&#39;</span>])
   plt<span style=color:#666>.</span>xlabel(<span style=color:#b44>&#39;y&#39;</span>)
   plt<span style=color:#666>.</span>ylabel(<span style=color:#b44>&#39;y_pred&#39;</span>);
   

   ax_132<span style=color:#666>=</span>plt<span style=color:#666>.</span>subplot(<span style=color:#666>1</span>,<span style=color:#666>3</span>,<span style=color:#666>2</span>)
   plt<span style=color:#666>.</span>plot(y,y<span style=color:#666>-</span>y_pred,<span style=color:#b44>&#39;.&#39;</span>)
   plt<span style=color:#666>.</span>plot(y<span style=color:#666>.</span>loc[outliers],y<span style=color:#666>.</span>loc[outliers]<span style=color:#666>-</span>y_pred<span style=color:#666>.</span>loc[outliers],<span style=color:#b44>&#39;ro&#39;</span>)
   plt<span style=color:#666>.</span>legend([<span style=color:#b44>&#39;Accepted&#39;</span>,<span style=color:#b44>&#39;Outlier&#39;</span>])
   plt<span style=color:#666>.</span>xlabel(<span style=color:#b44>&#39;y&#39;</span>)
   plt<span style=color:#666>.</span>ylabel(<span style=color:#b44>&#39;y - y_pred&#39;</span>);

   ax_133<span style=color:#666>=</span>plt<span style=color:#666>.</span>subplot(<span style=color:#666>1</span>,<span style=color:#666>3</span>,<span style=color:#666>3</span>)
   z<span style=color:#666>.</span>plot<span style=color:#666>.</span>hist(bins<span style=color:#666>=</span><span style=color:#666>50</span>,ax<span style=color:#666>=</span>ax_133)
   z<span style=color:#666>.</span>loc[outliers]<span style=color:#666>.</span>plot<span style=color:#666>.</span>hist(color<span style=color:#666>=</span><span style=color:#b44>&#39;r&#39;</span>,bins<span style=color:#666>=</span><span style=color:#666>50</span>,ax<span style=color:#666>=</span>ax_133)
   plt<span style=color:#666>.</span>legend([<span style=color:#b44>&#39;Accepted&#39;</span>,<span style=color:#b44>&#39;Outlier&#39;</span>])
   plt<span style=color:#666>.</span>xlabel(<span style=color:#b44>&#39;z&#39;</span>)
   
   <span style=color:#a2f;font-weight:700>return</span> outliers

outliers <span style=color:#666>=</span> find_outliers(Ridge(), X_train, y_train)
X_outliers<span style=color:#666>=</span>X_train<span style=color:#666>.</span>loc[outliers]
y_outliers<span style=color:#666>=</span>y_train<span style=color:#666>.</span>loc[outliers]
X_t<span style=color:#666>=</span>X_train<span style=color:#666>.</span>drop(outliers)
y_t<span style=color:#666>=</span>y_train<span style=color:#666>.</span>drop(outliers)
</code></pre></div><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#080;font-style:italic># 使用箱线图来去除异常</span>
<span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>outliers_proc</span>(data, col_name, scale<span style=color:#666>=</span><span style=color:#666>3</span>):
    <span style=color:#b44>&#34;&#34;&#34;
</span><span style=color:#b44>    用于清洗异常值，默认用 box_plot（scale=3）进行清洗
</span><span style=color:#b44>    :param data: 接收 pandas 数据格式
</span><span style=color:#b44>    :param col_name: pandas 列名
</span><span style=color:#b44>    :param scale: 尺度
</span><span style=color:#b44>    :return:
</span><span style=color:#b44>    &#34;&#34;&#34;</span>

    <span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>box_plot_outliers</span>(data_ser, box_scale):
        <span style=color:#b44>&#34;&#34;&#34;
</span><span style=color:#b44>        利用箱线图去除异常值
</span><span style=color:#b44>        :param data_ser: 接收 pandas.Series 数据格式
</span><span style=color:#b44>        :param box_scale: 箱线图尺度，
</span><span style=color:#b44>        :return: 大于、低于
</span><span style=color:#b44>        &#34;&#34;&#34;</span>
        iqr <span style=color:#666>=</span> box_scale <span style=color:#666>*</span> (data_ser<span style=color:#666>.</span>quantile(<span style=color:#666>0.75</span>) <span style=color:#666>-</span> data_ser<span style=color:#666>.</span>quantile(<span style=color:#666>0.25</span>)) <span style=color:#080;font-style:italic># 四分位间距*scale</span>
        val_low <span style=color:#666>=</span> data_ser<span style=color:#666>.</span>quantile(<span style=color:#666>0.25</span>) <span style=color:#666>-</span> iqr <span style=color:#080;font-style:italic># 最大上限 Q3 + scale*四分位间距</span>
        val_up <span style=color:#666>=</span> data_ser<span style=color:#666>.</span>quantile(<span style=color:#666>0.75</span>) <span style=color:#666>+</span> iqr <span style=color:#080;font-style:italic># 最低下限 Q1 - scale*四分位间距</span>
        rule_low <span style=color:#666>=</span> (data_ser <span style=color:#666>&lt;</span> val_low)
        rule_up <span style=color:#666>=</span> (data_ser <span style=color:#666>&gt;</span> val_up)
        <span style=color:#a2f;font-weight:700>return</span> (rule_low, rule_up), (val_low, val_up)

   
    data_n <span style=color:#666>=</span> data<span style=color:#666>.</span>copy()
    data_series <span style=color:#666>=</span> data_n[col_name]
    rule, value <span style=color:#666>=</span> box_plot_outliers(data_series, box_scale<span style=color:#666>=</span>scale)
    index <span style=color:#666>=</span> np<span style=color:#666>.</span>arange(data_series<span style=color:#666>.</span>shape[<span style=color:#666>0</span>])[rule[<span style=color:#666>0</span>] <span style=color:#666>|</span> rule[<span style=color:#666>1</span>]]
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#34;Delete number is: {}&#34;</span><span style=color:#666>.</span>format(<span style=color:#a2f>len</span>(index)))
    data_n <span style=color:#666>=</span> data_n<span style=color:#666>.</span>drop(index)
    data_n<span style=color:#666>.</span>reset_index(drop<span style=color:#666>=</span>True, inplace<span style=color:#666>=</span>True)
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#34;Now column number is: {}&#34;</span><span style=color:#666>.</span>format(data_n<span style=color:#666>.</span>shape[<span style=color:#666>0</span>]))
    index_low <span style=color:#666>=</span> np<span style=color:#666>.</span>arange(data_series<span style=color:#666>.</span>shape[<span style=color:#666>0</span>])[rule[<span style=color:#666>0</span>]]
    outliers <span style=color:#666>=</span> data_series<span style=color:#666>.</span>iloc[index_low]
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#34;Description of data less than the lower bound is:&#34;</span>)
    <span style=color:#a2f;font-weight:700>print</span>(pd<span style=color:#666>.</span>Series(outliers)<span style=color:#666>.</span>describe())
    index_up <span style=color:#666>=</span> np<span style=color:#666>.</span>arange(data_series<span style=color:#666>.</span>shape[<span style=color:#666>0</span>])[rule[<span style=color:#666>1</span>]]
    outliers <span style=color:#666>=</span> data_series<span style=color:#666>.</span>iloc[index_up]
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#34;Description of data larger than the upper bound is:&#34;</span>)
    <span style=color:#a2f;font-weight:700>print</span>(pd<span style=color:#666>.</span>Series(outliers)<span style=color:#666>.</span>describe())
    
    fig, ax <span style=color:#666>=</span> plt<span style=color:#666>.</span>subplots(<span style=color:#666>1</span>, <span style=color:#666>2</span>, figsize<span style=color:#666>=</span>(<span style=color:#666>10</span>, <span style=color:#666>7</span>))
    sns<span style=color:#666>.</span>boxplot(y<span style=color:#666>=</span>data[col_name], data<span style=color:#666>=</span>data, palette<span style=color:#666>=</span><span style=color:#b44>&#34;Set1&#34;</span>, ax<span style=color:#666>=</span>ax[<span style=color:#666>0</span>])
    sns<span style=color:#666>.</span>boxplot(y<span style=color:#666>=</span>data_n[col_name], data<span style=color:#666>=</span>data_n, palette<span style=color:#666>=</span><span style=color:#b44>&#34;Set1&#34;</span>, ax<span style=color:#666>=</span>ax[<span style=color:#666>1</span>])
    <span style=color:#a2f;font-weight:700>return</span> data_n

<span style=color:#080;font-style:italic># 使用</span>
train <span style=color:#666>=</span> outliers_proc(train, <span style=color:#b44>&#39;power&#39;</span>, scale<span style=color:#666>=</span><span style=color:#666>3</span>)
</code></pre></div><h2 id=4-特征工程>4. 特征工程</h2><h3 id=41-训练测试数据组合>4.1. 训练、测试数据组合</h3><p>整个数据处理部分，应该将训练集和测试集放在一起统一处理，处理后，再将训练集和测试集分开。将训练集分成训练和验证集。并需要将标签给独立出来。</p><p>一种方式是，将测试数据加在训练数据后面，并添加一列来区分训练数据和测试数据。</p><p>TODO: 思考如果来到未知的新数据如何处理。这些操作能否组成pipeline？</p><h3 id=回归问题的标签处理>回归问题的标签处理</h3><h4 id=标签分布转换>标签分布转换</h4><p>经常会把原始数据取对数后进一步处理。这样做的原理是基于对数函数在其定义域内是单调增函数，取对数后不会改变数据的相对关系。如果数据集中有负数当然就不能取对数了。实践中，取对数的一般是水平量，而不是比例数据，例如变化率等。</p><p>这样做的好处：</p><ol><li>缩小数据的绝对数值，方便计算。</li><li>某些情况下，在数据的整个值域中的在不同区间的差异带来的影响不同。<ol><li>从log函数的图像可以看到，自变量x的值越小，函数值y的变化越快。也就是说，对数值小的部分差异的敏感程度比数值大的部分的差异敏感程度更高。这也是符合生活常识的，例如对于价格，买个家电，如果价格相差几百元能够很大程度影响你决策，但是你买汽车时相差几百元你会忽略不计了。</li><li>即小数据的差异更敏感，而大数据的差异的敏感性较弱。</li></ol></li><li>取对数之后不会改变数据的性质和相关关系，但压缩了变量的尺度。数据更加平稳，也消弱了模型的共线性、异方差性等。</li><li>所得到的数据易消除异方差问题。</li><li>在经济学中，常取自然对数再做回归，</li></ol><p>资料：</p><ol><li><a href=https://blog.csdn.net/anshuai_aw1/article/details/83866105>为什么对预测任务里的标签Y做对数处理？</a></li><li><a href=https://blog.csdn.net/Noob_daniel/article/details/76087829>回归分析的五个基本假设，数据需要满足的基本假设</a></li></ol><p>综上，预测值的分布需要处理成适宜的分布</p><p>查看数据适宜的分布</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>import</span> <span style=color:#00f;font-weight:700>scipy.stats</span> <span style=color:#a2f;font-weight:700>as</span> <span style=color:#00f;font-weight:700>st</span>
y <span style=color:#666>=</span> Train_data[<span style=color:#b44>&#39;price&#39;</span>]
plt<span style=color:#666>.</span>figure(<span style=color:#666>1</span>); plt<span style=color:#666>.</span>title(<span style=color:#b44>&#39;Johnson SU&#39;</span>)
sns<span style=color:#666>.</span>distplot(y, kde<span style=color:#666>=</span>False, fit<span style=color:#666>=</span>st<span style=color:#666>.</span>johnsonsu) <span style=color:#080;font-style:italic># 无界约翰逊分布进心拟合</span>
plt<span style=color:#666>.</span>figure(<span style=color:#666>2</span>); plt<span style=color:#666>.</span>title(<span style=color:#b44>&#39;Normal&#39;</span>)
sns<span style=color:#666>.</span>distplot(y, kde<span style=color:#666>=</span>False, fit<span style=color:#666>=</span>st<span style=color:#666>.</span>norm)
plt<span style=color:#666>.</span>figure(<span style=color:#666>3</span>); plt<span style=color:#666>.</span>title(<span style=color:#b44>&#39;Log Normal&#39;</span>)
sns<span style=color:#666>.</span>distplot(y, kde<span style=color:#666>=</span>False, fit<span style=color:#666>=</span>st<span style=color:#666>.</span>lognorm)
</code></pre></div><p>观察数据对哪个分布拟合最好，就将数据转为那个分布。</p><p>将代码转为log分布。注意log的定义域为正数，这里为了避免0，所以加了1。但如果存在负数，则需要转到正数再取log，或者寻求其他方法。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>data[column] <span style=color:#666>=</span> np<span style=color:#666>.</span>log(data[column] <span style=color:#666>+</span> <span style=color:#666>1</span>) 

</code></pre></div><h4 id=标签异常值处理>标签异常值处理</h4><p>标签的异常值仍然需要处理。</p><p>当标签存在异常值时，可以考虑对异常值进行处理。如对异常值进行删除、填充。</p><h3 id=42-倾斜特征处理>4.2. 倾斜特征处理</h3><p>检查特征中异常值、缺失值的占比。如果某个特征的有效样本过少，或者样本过于倾斜，则应该删除这个特征。这些信息可以从AutoEDA中看出。</p><p>代码</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#080;font-style:italic># 统计无效特征的个数。</span>
df[cols]<span style=color:#666>.</span>isna()<span style=color:#666>.</span>sum(axis<span style=color:#666>=</span><span style=color:#666>0</span>)

<span style=color:#080;font-style:italic># 举例：特征中某一个值占比超过0.95则应该予以重视或者删除。</span>
first_feature_proportion <span style=color:#666>=</span> train[col]<span style=color:#666>.</span>value_counts(dropna<span style=color:#666>=</span>False, normalize<span style=color:#666>=</span>True, ascending<span style=color:#666>=</span>False)<span style=color:#666>.</span>iloc[<span style=color:#666>0</span>]
<span style=color:#a2f;font-weight:700>if</span> first_feature_proportion<span style=color:#666>&gt;</span><span style=color:#666>0.95</span>:
   train<span style=color:#666>.</span>drop(col, axis<span style=color:#666>=</span><span style=color:#666>0</span>)
</code></pre></div><h3 id=43-异常值处理>4.3. 异常值处理</h3><p>找到异常值：</p><ol><li>对数据进行查看，常见的异常值：0, 负数, -1, -999, 999, NAN, 数字中的异常值。<ol><li>代码 <code>df.isnull()</code></li></ol></li><li>可视化分析：使用箱线图、hist图查看数据，远离太远的应该视为异常值。<ol><li>代码：<code>sns.boxplot(), sns.histplot()</code></li></ol></li><li>不合字段含义的异常值<ol><li>如收入是1块钱，身高是200cm，这些需要根据字段含义进一步判断。并修改。</li></ol></li></ol><p>异常值处理：</p><ol><li>统计每行数据的异常值的个数，添加为新特征。</li><li>删除异常值记录</li><li>视为缺失值进行处理</li><li>不处理。（部分异常值可能是对真实情况的记录，直接挖掘可能会保留最真实的信息）</li></ol><p>TODO: 特征处理：https://cloud.tencent.com/developer/article/1488245</p><h3 id=44-缺失值处理>4.4. 缺失值处理</h3><p>找到缺失值：</p><ol><li>区分缺失值：存在形式：None，空，特殊值（-1，-999）<ol><li>代码 <code>df.isnull().sum(), df.isna(), df[df.isna().T.any()]</code></li><li>展示所有缺失列:</li></ol></li></ol><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>missing <span style=color:#666>=</span> Train_data<span style=color:#666>.</span>isnull()<span style=color:#666>.</span>sum()
missing <span style=color:#666>=</span> missing[missing<span style=color:#666>&gt;</span><span style=color:#666>0</span>]
missing<span style=color:#666>.</span>sort_values(inplace<span style=color:#666>=</span>True)
sns<span style=color:#666>.</span>histplot(missing)
</code></pre></div><p>解决思路：</p><ol><li>当缺失值数量很小的时候，可以选择填充。如果使用树模型，也可以直接空缺。</li><li>如果缺失值过多，可以直接删除</li></ol><p>对缺失的值进行填充：</p><ol><li>类别特征：进行填充众数、填充新类</li><li>数值特征：填充平均数、中位数、众数、最大值、最小值<ol><li>代码: <code>df.fillna(values)</code></li><li>注意替换mode时:<code>df.apply(lambda col:col.fillna(col.mode()[0]))</code></li></ol></li><li>有序数据：next、previous</li><li>模型预测填充：对含有缺失值的那一列进行建模并预测。</li><li>根据字段含义进行填充。比如大部分人不信教，宗教可以填充为不信教。</li></ol><h3 id=数字特征处理>数字特征处理</h3><p>数字特征即特征中的数据是数字，可以进一步分为连续特征、类别特征。</p><p>获取数字特征:</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>numeric_features <span style=color:#666>=</span> Train_data<span style=color:#666>.</span>select_dtypes(include<span style=color:#666>=</span>[np<span style=color:#666>.</span>number])
numeric_features<span style=color:#666>.</span>columns
</code></pre></div><h4 id=45-连续值分桶>4.5. 连续值分桶</h4><p>为什么要分桶：</p><ol><li>离散后稀疏向量内积乘法运算速度更快，计算结果也方便存储，容易扩展；</li><li>离散后的特征对异常值更具鲁棒性，如 age>30 为 1 否则为 0，对于年龄为 200 的也不会对模型造成很大的干扰；</li><li>LR 属于广义线性模型，表达能力有限，经过离散化后，每个变量有单独的权重，这相当于引入了非线性，能够提升模型的表达能力，加大拟合；</li><li>离散后特征可以进行特征交叉，提升表达能力，由 M+N 个变量编程 M*N 个变量，进一步引入非线形，提升了表达能力；</li><li>特征离散后模型更稳定，如用户年龄区间，不会因为用户年龄长了一岁就变化，增加了模型的泛化性。</li></ol><p>需要分桶的变量：</p><ol><li>连续的时间<ol><li>将连续的时间处理成年、月、日。具体也需要根据情况。</li><li>将时间转换为新特征，如年龄。</li><li>时间转换函数 <code>pd.to_datatime(df["time"], format="%Y-%m-%d", errors='coerce')</code></li></ol></li><li>年龄<ol><li>对年龄进行分桶。分桶可以参考关键年龄[0, 18, 24, 30, 50, 60,100]。也可以将年龄histplot展示出来，再分桶。</li><li>代码：<code>pd.cut(df["age"], bins, labels=[0,1,2...])</code></li></ol></li></ol><p>TODO: 如何分桶？</p><p>Notes：</p><ol><li>连续值处理成了离散值后，为什么不进一步处理成one-hot值？是因为连续值通常有偏序关系，one-hot无法体现偏序关系。</li></ol><h4 id=46-连续值无量纲化>4.6. 连续值无量纲化</h4><p>部分连续值不宜离散化，需要当作连续值处理。这个时候多个连续值就需要消除量纲。当数值直接影响结果或者需要梯度下降进行训练的模型的时候，往往需要消除连续值之间的量纲。</p><p>在树模型中一般无需无量纲化。但是其他模型如线性回归等模型，需要归一化。</p><p>方法：</p><ol><li>标准化。缩放到标准差为0，方差为1</li><li>区间缩放，minmax归一化。</li></ol><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>max_min</span>(x):
    <span style=color:#a2f;font-weight:700>return</span> (x <span style=color:#666>-</span> np<span style=color:#666>.</span>min(x)) <span style=color:#666>/</span> (np<span style=color:#666>.</span>max(x) <span style=color:#666>-</span> np<span style=color:#666>.</span>min(x))
data[column] <span style=color:#666>=</span> max_min(data[column])
</code></pre></div><p>Notes:</p><ol><li>注意树模型不需要无量纲化。</li><li>注意标准化应该只对连续变量进行操作。minmax归一化对one-hot无影响。顺序应该为先归一化再进行one-hot。</li><li>对于数据分布过分偏移的数据，应该先取log后，再进行归一化。</li></ol><h3 id=47-类别特征处理>4.7. 类别特征处理</h3><p>获取类别特征：</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>categorical_features <span style=color:#666>=</span> Train_data<span style=color:#666>.</span>select_dtypes(include<span style=color:#666>=</span>[np<span style=color:#666>.</span>object])
categorical_features<span style=color:#666>.</span>columns
</code></pre></div><ol><li>离散特征处理成one-hot.<ol><li>处理成one-hot,<ol><li>代码: <code>df = sklearn.preprocessing.OneHotEncoder(categories="auto").fit_transform(df).toarray()</code></li><li>代码：<code>data = pd.get_dummies(data, columns=[...])</code></li></ol></li><li>处理成二进制编码。在one_hot维度过大的时候，二进制编码可以缩小特征维度，同时比连续值提高一定的效果。</li></ol></li></ol><p>Notes:</p><ol><li>One-hot主要用来编码类别特征，即采用哑变量（dummy variables）对类别进行编码。 one-hot的意义：<ol><li>避免因将类别用数字作为表示而给函数带来抖动。 直接使用数字会将人工误差而导致的假设引入到类别特征中，比如类别之间的大小关系，以及差异关系等等</li><li>将连续值转为欧式空间。在机器学习算法中，距离、相似度的计算是十分重要的，而这些计算都是定义于欧式空间上的。高中低本来不可以区分，但转为000就可分了。可以理解为连续值是线性模型，转为为高维空间后就可以划分了。</li></ol></li><li>注意部分模型不需要处理成one-hot，比如树模型，数值对树模型仅是符号，处理成one-hot没有意义。但是可以给如lr等模型用。</li></ol><h3 id=构造新特征>构造新特征</h3><ol><li>多元特征组合<ol><li>可以根据一元特征的含义构造多元特征，构造方向应该朝着和目标有关联去构造。如预测幸福度的模型，可以通过年龄、子女年龄，构造出生孩子的时间、孩子的大小，这都可能直接对幸福值数造成影响。再比如收入、支出的比例，和同省人收入支出的比例。</li></ol></li><li>统计特征构造：<ol><li>有些比赛的特征是匿名特征，这导致我们并不清楚特征相互直接的关联性，这时我们就只有单纯基于特征进行处理，比如装箱，groupby，agg 等这样一些操作进行一些特征统计，此外还可以对特征进行进一步的 log，exp 等变换，或者对多个特征进行四则运算（如上面我们算出的使用时长），多项式组合等然后进行筛选。由于特性的匿名性其实限制了很多对于特征的处理，当然有些时候用** NN **去提取一些特征也会达到意想不到的良好效果。</li><li>对于知道特征含义（非匿名）的特征工程，特别是在工业类型比赛中，会基于信号处理，频域提取，丰度，偏度等构建更为有实际意义的特征，这就是结合背景的特征构建，在推荐系统中也是这样的，各种类型点击率统计，各时段统计，加用户属性的统计等等，这样一种特征构建往往要深入分析背后的业务逻辑或者说物理原理，从而才能更好的找到 magic。</li></ol></li><li>结合标签进行构造<ol><li>一些标签蕴含的信息，也可以添加到数据当中</li></ol></li></ol><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>data_all_gb <span style=color:#666>=</span> data_all[choose_cols]<span style=color:#666>.</span>groupby(<span style=color:#b44>&#34;地理区域&#34;</span>)
<span style=color:#a2f;font-weight:700>for</span> col <span style=color:#a2f;font-weight:700>in</span> choose_cols:
    data_all[<span style=color:#b44>&#34;平均&#34;</span><span style=color:#666>+</span>col] <span style=color:#666>=</span> data_all_gb[col]<span style=color:#666>.</span>transform(<span style=color:#b44>&#39;mean&#39;</span>)

    

train_gb <span style=color:#666>=</span> train<span style=color:#666>.</span>groupby(<span style=color:#b44>&#34;brand&#34;</span>)
all_info <span style=color:#666>=</span> {}
<span style=color:#a2f;font-weight:700>for</span> kind, kind_data <span style=color:#a2f;font-weight:700>in</span> train_gb:
    info <span style=color:#666>=</span> {}
    kind_data <span style=color:#666>=</span> kind_data[kind_data[<span style=color:#b44>&#39;price&#39;</span>] <span style=color:#666>&gt;</span> <span style=color:#666>0</span>]
    info[<span style=color:#b44>&#39;brand_amount&#39;</span>] <span style=color:#666>=</span> <span style=color:#a2f>len</span>(kind_data)
    info[<span style=color:#b44>&#39;brand_price_max&#39;</span>] <span style=color:#666>=</span> kind_data<span style=color:#666>.</span>price<span style=color:#666>.</span>max()
    info[<span style=color:#b44>&#39;brand_price_median&#39;</span>] <span style=color:#666>=</span> kind_data<span style=color:#666>.</span>price<span style=color:#666>.</span>median()
    info[<span style=color:#b44>&#39;brand_price_min&#39;</span>] <span style=color:#666>=</span> kind_data<span style=color:#666>.</span>price<span style=color:#666>.</span>min()
    info[<span style=color:#b44>&#39;brand_price_sum&#39;</span>] <span style=color:#666>=</span> kind_data<span style=color:#666>.</span>price<span style=color:#666>.</span>sum()
    info[<span style=color:#b44>&#39;brand_price_std&#39;</span>] <span style=color:#666>=</span> kind_data<span style=color:#666>.</span>price<span style=color:#666>.</span>std()
    info[<span style=color:#b44>&#39;brand_price_average&#39;</span>] <span style=color:#666>=</span> <span style=color:#a2f>round</span>(kind_data<span style=color:#666>.</span>price<span style=color:#666>.</span>sum() <span style=color:#666>/</span> (<span style=color:#a2f>len</span>(kind_data) <span style=color:#666>+</span> <span style=color:#666>1</span>), <span style=color:#666>2</span>)
    all_info[kind] <span style=color:#666>=</span> info
brand_fe <span style=color:#666>=</span> pd<span style=color:#666>.</span>DataFrame(all_info)<span style=color:#666>.</span>T<span style=color:#666>.</span>reset_index()<span style=color:#666>.</span>rename(columns<span style=color:#666>=</span>{<span style=color:#b44>&#34;index&#34;</span>: <span style=color:#b44>&#34;brand&#34;</span>})
data <span style=color:#666>=</span> data<span style=color:#666>.</span>merge(brand_fe, how<span style=color:#666>=</span><span style=color:#b44>&#39;left&#39;</span>, on<span style=color:#666>=</span><span style=color:#b44>&#39;brand&#39;</span>)

</code></pre></div><p><a href=https://github.com/datawhalechina/team-learning-data-mining/blob/master/SecondHandCarPriceForecast/Task3%20%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B.md#34-%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93>特征构造资料</a></p><h3 id=49-标签处理>4.9. 标签处理</h3><p>LabelEncoder</p><h3 id=特征选择>特征选择</h3><p><a href=https://github.com/datawhalechina/team-learning-data-mining/blob/master/SecondHandCarPriceForecast/Task3%20%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B.md#333-%E7%89%B9%E5%BE%81%E7%AD%9B%E9%80%89>特征筛选</a></p><p>查看特征重要性后，能够做什么事情？</p><h2 id=5-模型训练>5. 模型训练</h2><p><a href=https://github.com/datawhalechina/team-learning-data-mining/blob/master/SecondHandCarPriceForecast/Task4%20%E5%BB%BA%E6%A8%A1%E8%B0%83%E5%8F%82%20.md>模型解释</a></p><h3 id=51-常用模型>5.1. 常用模型</h3><p>xgboost, lgbm, randomforest, gradientBoostingRegressor, ExtraTreesRegressor, GradientBoostingRegressor, Kernel Ridge Regression(常用于Stack的二层模型), Ridge岭回归, ElasticNet 弹性网络, BayesianRidge 贝叶斯回归, LinearRegression简单的线性回归</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>sklearn.linear_model</span> <span style=color:#a2f;font-weight:700>import</span> LinearRegression
<span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>sklearn.svm</span> <span style=color:#a2f;font-weight:700>import</span> SVC
<span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>sklearn.tree</span> <span style=color:#a2f;font-weight:700>import</span> DecisionTreeRegressor
<span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>sklearn.ensemble</span> <span style=color:#a2f;font-weight:700>import</span> RandomForestRegressor
<span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>sklearn.ensemble</span> <span style=color:#a2f;font-weight:700>import</span> GradientBoostingRegressor
<span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>sklearn.neural_network</span> <span style=color:#a2f;font-weight:700>import</span> MLPRegressor
<span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>xgboost.sklearn</span> <span style=color:#a2f;font-weight:700>import</span> XGBRegressor
<span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>lightgbm.sklearn</span> <span style=color:#a2f;font-weight:700>import</span> LGBMRegressor

models <span style=color:#666>=</span> [LinearRegression(),
          DecisionTreeRegressor(),
          RandomForestRegressor(),
          GradientBoostingRegressor(),
          MLPRegressor(solver<span style=color:#666>=</span><span style=color:#b44>&#39;lbfgs&#39;</span>, max_iter<span style=color:#666>=</span><span style=color:#666>100</span>), 
          XGBRegressor(n_estimators <span style=color:#666>=</span> <span style=color:#666>100</span>, objective<span style=color:#666>=</span><span style=color:#b44>&#39;reg:squarederror&#39;</span>), 
          LGBMRegressor(n_estimators <span style=color:#666>=</span> <span style=color:#666>100</span>)]
</code></pre></div><h3 id=模型数据类型调整>模型数据类型调整</h3><p>reduce_mem_usage 函数通过调整数据类型，帮助我们减少数据在内存中占用的空间</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>reduce_mem_usage</span>(df):
    <span style=color:#b44>&#34;&#34;&#34; iterate through all the columns of a dataframe and modify the data type
</span><span style=color:#b44>        to reduce memory usage.        
</span><span style=color:#b44>    &#34;&#34;&#34;</span>
    start_mem <span style=color:#666>=</span> df<span style=color:#666>.</span>memory_usage()<span style=color:#666>.</span>sum() 
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;Memory usage of dataframe is {:.2f} MB&#39;</span><span style=color:#666>.</span>format(start_mem))
    
    <span style=color:#a2f;font-weight:700>for</span> col <span style=color:#a2f;font-weight:700>in</span> df<span style=color:#666>.</span>columns:
        col_type <span style=color:#666>=</span> df[col]<span style=color:#666>.</span>dtype
        
        <span style=color:#a2f;font-weight:700>if</span> col_type <span style=color:#666>!=</span> <span style=color:#a2f>object</span>:
            c_min <span style=color:#666>=</span> df[col]<span style=color:#666>.</span>min()
            c_max <span style=color:#666>=</span> df[col]<span style=color:#666>.</span>max()
            <span style=color:#a2f;font-weight:700>if</span> <span style=color:#a2f>str</span>(col_type)[:<span style=color:#666>3</span>] <span style=color:#666>==</span> <span style=color:#b44>&#39;int&#39;</span>:
                <span style=color:#a2f;font-weight:700>if</span> c_min <span style=color:#666>&gt;</span> np<span style=color:#666>.</span>iinfo(np<span style=color:#666>.</span>int8)<span style=color:#666>.</span>min <span style=color:#a2f;font-weight:700>and</span> c_max <span style=color:#666>&lt;</span> np<span style=color:#666>.</span>iinfo(np<span style=color:#666>.</span>int8)<span style=color:#666>.</span>max:
                    df[col] <span style=color:#666>=</span> df[col]<span style=color:#666>.</span>astype(np<span style=color:#666>.</span>int8)
                <span style=color:#a2f;font-weight:700>elif</span> c_min <span style=color:#666>&gt;</span> np<span style=color:#666>.</span>iinfo(np<span style=color:#666>.</span>int16)<span style=color:#666>.</span>min <span style=color:#a2f;font-weight:700>and</span> c_max <span style=color:#666>&lt;</span> np<span style=color:#666>.</span>iinfo(np<span style=color:#666>.</span>int16)<span style=color:#666>.</span>max:
                    df[col] <span style=color:#666>=</span> df[col]<span style=color:#666>.</span>astype(np<span style=color:#666>.</span>int16)
                <span style=color:#a2f;font-weight:700>elif</span> c_min <span style=color:#666>&gt;</span> np<span style=color:#666>.</span>iinfo(np<span style=color:#666>.</span>int32)<span style=color:#666>.</span>min <span style=color:#a2f;font-weight:700>and</span> c_max <span style=color:#666>&lt;</span> np<span style=color:#666>.</span>iinfo(np<span style=color:#666>.</span>int32)<span style=color:#666>.</span>max:
                    df[col] <span style=color:#666>=</span> df[col]<span style=color:#666>.</span>astype(np<span style=color:#666>.</span>int32)
                <span style=color:#a2f;font-weight:700>elif</span> c_min <span style=color:#666>&gt;</span> np<span style=color:#666>.</span>iinfo(np<span style=color:#666>.</span>int64)<span style=color:#666>.</span>min <span style=color:#a2f;font-weight:700>and</span> c_max <span style=color:#666>&lt;</span> np<span style=color:#666>.</span>iinfo(np<span style=color:#666>.</span>int64)<span style=color:#666>.</span>max:
                    df[col] <span style=color:#666>=</span> df[col]<span style=color:#666>.</span>astype(np<span style=color:#666>.</span>int64)  
            <span style=color:#a2f;font-weight:700>else</span>:
                <span style=color:#a2f;font-weight:700>if</span> c_min <span style=color:#666>&gt;</span> np<span style=color:#666>.</span>finfo(np<span style=color:#666>.</span>float16)<span style=color:#666>.</span>min <span style=color:#a2f;font-weight:700>and</span> c_max <span style=color:#666>&lt;</span> np<span style=color:#666>.</span>finfo(np<span style=color:#666>.</span>float16)<span style=color:#666>.</span>max:
                    df[col] <span style=color:#666>=</span> df[col]<span style=color:#666>.</span>astype(np<span style=color:#666>.</span>float16)
                <span style=color:#a2f;font-weight:700>elif</span> c_min <span style=color:#666>&gt;</span> np<span style=color:#666>.</span>finfo(np<span style=color:#666>.</span>float32)<span style=color:#666>.</span>min <span style=color:#a2f;font-weight:700>and</span> c_max <span style=color:#666>&lt;</span> np<span style=color:#666>.</span>finfo(np<span style=color:#666>.</span>float32)<span style=color:#666>.</span>max:
                    df[col] <span style=color:#666>=</span> df[col]<span style=color:#666>.</span>astype(np<span style=color:#666>.</span>float32)
                <span style=color:#a2f;font-weight:700>else</span>:
                    df[col] <span style=color:#666>=</span> df[col]<span style=color:#666>.</span>astype(np<span style=color:#666>.</span>float64)
        <span style=color:#a2f;font-weight:700>else</span>:
            df[col] <span style=color:#666>=</span> df[col]<span style=color:#666>.</span>astype(<span style=color:#b44>&#39;category&#39;</span>)

    end_mem <span style=color:#666>=</span> df<span style=color:#666>.</span>memory_usage()<span style=color:#666>.</span>sum() 
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;Memory usage after optimization is: {:.2f} MB&#39;</span><span style=color:#666>.</span>format(end_mem))
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;Decreased by {:.1f}%&#39;</span><span style=color:#666>.</span>format(<span style=color:#666>100</span> <span style=color:#666>*</span> (start_mem <span style=color:#666>-</span> end_mem) <span style=color:#666>/</span> start_mem))
    <span style=color:#a2f;font-weight:700>return</span> df
sample_feature <span style=color:#666>=</span> reduce_mem_usage(pd<span style=color:#666>.</span>read_csv(<span style=color:#b44>&#39;data_for_tree.csv&#39;</span>))
</code></pre></div><h3 id=52-数据分组>5.2. 数据分组</h3><ol><li>单次的数据分组：train_test_split</li><li>多折训练：<ol><li>KFold, StratifiedKFold</li><li>代码:</li></ol></li></ol><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>folds <span style=color:#666>=</span> StratifiedKFold(n_splits<span style=color:#666>=</span><span style=color:#666>5</span>, shuffle<span style=color:#666>=</span>True, random_state<span style=color:#666>=</span><span style=color:#666>4</span>)   <span style=color:#080;font-style:italic>#交叉切分：5 train:val=4:1</span>
oof <span style=color:#666>=</span> np<span style=color:#666>.</span>zeros(<span style=color:#a2f>len</span>(X_train))
test_prediction <span style=color:#666>=</span> np<span style=color:#666>.</span>zeros(<span style=color:#a2f>len</span>(X_test))

<span style=color:#a2f;font-weight:700>for</span> fold_, (trn_idx, val_idx) <span style=color:#a2f;font-weight:700>in</span> <span style=color:#a2f>enumerate</span>(folds<span style=color:#666>.</span>split(X_train, y_train)):
    trn_data <span style=color:#666>=</span> X_train[trn_idx], y_train[trn_idx]
    val_data <span style=color:#666>=</span> X_train[val_idx], y_train[val_idx]
    <span style=color:#080;font-style:italic># model.fit()</span>
    oof[val_idx] <span style=color:#666>=</span> model<span style=color:#666>.</span>predict()
    <span style=color:#080;font-style:italic># 这里由于每一折都会对test进行预测，所以每一折预测的结果除以折数加起来。</span>
    test_prediction <span style=color:#666>+=</span> model<span style=color:#666>.</span>predict()<span style=color:#666>/</span>folds<span style=color:#666>.</span>n_splits
<span style=color:#080;font-style:italic># 使用oof留出验证后，将oof的预测作为结果。</span>
<span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#34;CV score: {:&lt;8.8f}&#34;</span><span style=color:#666>.</span>format(mean_squared_error(oof, target)))
</code></pre></div><h3 id=53-模型训练>5.3. 模型训练</h3><p>因为有众多模型都支持fit进行训练，prediction进行预测，所以可以抽象下，写一个类似的接口，接收所有模型的训练并输出指标。</p><p>这个是一个例子，这个train_model支持传入模型，支持grid,search, 支持KFold。训练模型并输出结果。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>train_model</span>(model, param_grid<span style=color:#666>=</span>[], X<span style=color:#666>=</span>[], y<span style=color:#666>=</span>[], 
                splits<span style=color:#666>=</span><span style=color:#666>5</span>, repeats<span style=color:#666>=</span><span style=color:#666>5</span>):
    <span style=color:#080;font-style:italic># 系统化的训练函数，需要自己总结一个。</span>
    <span style=color:#080;font-style:italic># 先要自己能写出来一个完整的流程。然后总结一下代码。</span>
    <span style=color:#080;font-style:italic># 获取数据</span>
    <span style=color:#a2f;font-weight:700>if</span> <span style=color:#a2f>len</span>(y)<span style=color:#666>==</span><span style=color:#666>0</span>:
        X,y <span style=color:#666>=</span> get_trainning_data_omitoutliers()
        
    <span style=color:#080;font-style:italic># 交叉验证</span>
    <span style=color:#080;font-style:italic># TODO: 再总结下，repeated K Fold有什么用？</span>
    rkfold <span style=color:#666>=</span> RepeatedKFold(n_splits<span style=color:#666>=</span>splits, n_repeats<span style=color:#666>=</span>repeats)
    
    <span style=color:#080;font-style:italic># 网格搜索最佳参数</span>
    <span style=color:#a2f;font-weight:700>if</span> <span style=color:#a2f>len</span>(param_grid)<span style=color:#666>&gt;</span><span style=color:#666>0</span>:
        gsearch <span style=color:#666>=</span> GridSearchCV(model, param_grid, cv<span style=color:#666>=</span>rkfold,
                               scoring<span style=color:#666>=</span><span style=color:#b44>&#34;neg_mean_squared_error&#34;</span>,
                               verbose<span style=color:#666>=</span><span style=color:#666>1</span>, return_train_score<span style=color:#666>=</span>True)

        <span style=color:#080;font-style:italic># 训练</span>
        gsearch<span style=color:#666>.</span>fit(X,y)

        <span style=color:#080;font-style:italic># 最好的模型</span>
        model <span style=color:#666>=</span> gsearch<span style=color:#666>.</span>best_estimator_        
        best_idx <span style=color:#666>=</span> gsearch<span style=color:#666>.</span>best_index_

        <span style=color:#080;font-style:italic># 获取交叉验证评价指标</span>
        grid_results <span style=color:#666>=</span> pd<span style=color:#666>.</span>DataFrame(gsearch<span style=color:#666>.</span>cv_results_)
        cv_mean <span style=color:#666>=</span> <span style=color:#a2f>abs</span>(grid_results<span style=color:#666>.</span>loc[best_idx,<span style=color:#b44>&#39;mean_test_score&#39;</span>])
        cv_std <span style=color:#666>=</span> grid_results<span style=color:#666>.</span>loc[best_idx,<span style=color:#b44>&#39;std_test_score&#39;</span>]

    <span style=color:#080;font-style:italic># 没有网格搜索  </span>
    <span style=color:#a2f;font-weight:700>else</span>:
        grid_results <span style=color:#666>=</span> []
        cv_results <span style=color:#666>=</span> cross_val_score(model, X, y, scoring<span style=color:#666>=</span><span style=color:#b44>&#34;neg_mean_squared_error&#34;</span>, cv<span style=color:#666>=</span>rkfold)
        cv_mean <span style=color:#666>=</span> <span style=color:#a2f>abs</span>(np<span style=color:#666>.</span>mean(cv_results))
        cv_std <span style=color:#666>=</span> np<span style=color:#666>.</span>std(cv_results)
    
    <span style=color:#080;font-style:italic># 合并数据</span>
    cv_score <span style=color:#666>=</span> pd<span style=color:#666>.</span>Series({<span style=color:#b44>&#39;mean&#39;</span>:cv_mean,<span style=color:#b44>&#39;std&#39;</span>:cv_std})

    <span style=color:#080;font-style:italic># 预测</span>
    y_pred <span style=color:#666>=</span> model<span style=color:#666>.</span>predict(X)
    
    <span style=color:#080;font-style:italic># 模型性能的统计数据        </span>
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;----------------------&#39;</span>)
    <span style=color:#a2f;font-weight:700>print</span>(model)
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;----------------------&#39;</span>)
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;score=&#39;</span>,model<span style=color:#666>.</span>score(X,y))
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;rmse=&#39;</span>,rmse(y, y_pred))
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;mse=&#39;</span>,mse(y, y_pred))
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;cross_val: mean=&#39;</span>,cv_mean,<span style=color:#b44>&#39;, std=&#39;</span>,cv_std)
    
    <span style=color:#080;font-style:italic># 残差分析与可视化</span>
    y_pred <span style=color:#666>=</span> pd<span style=color:#666>.</span>Series(y_pred,index<span style=color:#666>=</span>y<span style=color:#666>.</span>index)
    resid <span style=color:#666>=</span> y <span style=color:#666>-</span> y_pred
    mean_resid <span style=color:#666>=</span> resid<span style=color:#666>.</span>mean()
    std_resid <span style=color:#666>=</span> resid<span style=color:#666>.</span>std()
    z <span style=color:#666>=</span> (resid <span style=color:#666>-</span> mean_resid)<span style=color:#666>/</span>std_resid    
    n_outliers <span style=color:#666>=</span> <span style=color:#a2f>sum</span>(<span style=color:#a2f>abs</span>(z)<span style=color:#666>&gt;</span><span style=color:#666>3</span>)
    outliers <span style=color:#666>=</span> z[<span style=color:#a2f>abs</span>(z)<span style=color:#666>&gt;</span><span style=color:#666>3</span>]<span style=color:#666>.</span>index
    
    <span style=color:#a2f;font-weight:700>return</span> model, cv_score, grid_results
</code></pre></div><h3 id=54-特征重要性查看>5.4. 特征重要性查看</h3><p>TODO: 查看特征重要性后，有什么用？能干什么？</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>
df[<span style=color:#b44>&#39;importance&#39;</span>]<span style=color:#666>=</span><span style=color:#a2f>list</span>(lgb_model<span style=color:#666>.</span>feature_importance())
df <span style=color:#666>=</span> df<span style=color:#666>.</span>sort_values(by<span style=color:#666>=</span><span style=color:#b44>&#39;importance&#39;</span>,ascending<span style=color:#666>=</span>False)
plt<span style=color:#666>.</span>figure(figsize<span style=color:#666>=</span>(<span style=color:#666>14</span>,<span style=color:#666>28</span>))
sns<span style=color:#666>.</span>barplot(x<span style=color:#666>=</span><span style=color:#b44>&#34;importance&#34;</span>, y<span style=color:#666>=</span><span style=color:#b44>&#34;feature&#34;</span>, data<span style=color:#666>=</span>df<span style=color:#666>.</span>head(<span style=color:#666>50</span>))
plt<span style=color:#666>.</span>title(<span style=color:#b44>&#39;Features importance (averaged/folds)&#39;</span>)
plt<span style=color:#666>.</span>tight_layout()

</code></pre></div><h4 id=线性回归查看截距和权重>线性回归查看截距和权重</h4><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>sklearn.linear_model</span> <span style=color:#a2f;font-weight:700>import</span> LinearRegression
model <span style=color:#666>=</span> LinearRegression(normalize<span style=color:#666>=</span>True)
model <span style=color:#666>=</span> model<span style=color:#666>.</span>fit(train_X, train_y)
<span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;intercept:&#39;</span><span style=color:#666>+</span> <span style=color:#a2f>str</span>(model<span style=color:#666>.</span>intercept_)) <span style=color:#080;font-style:italic># 查看截距intercept</span>

<span style=color:#a2f>sorted</span>(<span style=color:#a2f>dict</span>(<span style=color:#a2f>zip</span>(continuous_feature_names, model<span style=color:#666>.</span>coef_))<span style=color:#666>.</span>items(), key<span style=color:#666>=</span><span style=color:#a2f;font-weight:700>lambda</span> x:x[<span style=color:#666>1</span>], reverse<span style=color:#666>=</span>True)
</code></pre></div><h3 id=模型结果查看>模型结果查看</h3><p>可视化查看模型的预测值和真实值。如果模型的预测值和真实值存在整体偏差，则说明数据、模型存在问题。</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>random_index <span style=color:#666>=</span> np<span style=color:#666>.</span>random<span style=color:#666>.</span>choice(a<span style=color:#666>=</span>train_x<span style=color:#666>.</span>shape[<span style=color:#666>0</span>], size<span style=color:#666>=</span><span style=color:#666>200</span>, replace<span style=color:#666>=</span>False)
sns<span style=color:#666>.</span>scatter(train_y[random_index])
sns<span style=color:#666>.</span>scatter(predict[random_index])

</code></pre></div><h3 id=绘制学习率曲线验证曲线>绘制学习率曲线、验证曲线</h3><p>绘制学习率曲线、验证曲线，可以帮助查看训练是否存在问题</p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>plot_learning_curve</span>(estimator, title, X, y, ylim<span style=color:#666>=</span>None, cv<span style=color:#666>=</span>None,n_jobs<span style=color:#666>=</span><span style=color:#666>1</span>, train_size<span style=color:#666>=</span>np<span style=color:#666>.</span>linspace(<span style=color:#666>.</span><span style=color:#666>1</span>, <span style=color:#666>1.0</span>, <span style=color:#666>5</span> )):  
    plt<span style=color:#666>.</span>figure()  
    plt<span style=color:#666>.</span>title(title)  
    <span style=color:#a2f;font-weight:700>if</span> ylim <span style=color:#a2f;font-weight:700>is</span> <span style=color:#a2f;font-weight:700>not</span> None:  
        plt<span style=color:#666>.</span>ylim(<span style=color:#666>*</span>ylim)  
    plt<span style=color:#666>.</span>xlabel(<span style=color:#b44>&#39;Training example&#39;</span>)  
    plt<span style=color:#666>.</span>ylabel(<span style=color:#b44>&#39;score&#39;</span>)  
    train_sizes, train_scores, test_scores <span style=color:#666>=</span> learning_curve(estimator, X, y, cv<span style=color:#666>=</span>cv, n_jobs<span style=color:#666>=</span>n_jobs, train_sizes<span style=color:#666>=</span>train_size, scoring <span style=color:#666>=</span> make_scorer(mean_absolute_error))  
    train_scores_mean <span style=color:#666>=</span> np<span style=color:#666>.</span>mean(train_scores, axis<span style=color:#666>=</span><span style=color:#666>1</span>)  
    train_scores_std <span style=color:#666>=</span> np<span style=color:#666>.</span>std(train_scores, axis<span style=color:#666>=</span><span style=color:#666>1</span>)  
    test_scores_mean <span style=color:#666>=</span> np<span style=color:#666>.</span>mean(test_scores, axis<span style=color:#666>=</span><span style=color:#666>1</span>)  
    test_scores_std <span style=color:#666>=</span> np<span style=color:#666>.</span>std(test_scores, axis<span style=color:#666>=</span><span style=color:#666>1</span>)  
    plt<span style=color:#666>.</span>grid()<span style=color:#080;font-style:italic>#区域  </span>
    plt<span style=color:#666>.</span>fill_between(train_sizes, train_scores_mean <span style=color:#666>-</span> train_scores_std,  
                     train_scores_mean <span style=color:#666>+</span> train_scores_std, alpha<span style=color:#666>=</span><span style=color:#666>0.1</span>,  
                     color<span style=color:#666>=</span><span style=color:#b44>&#34;r&#34;</span>)  
    plt<span style=color:#666>.</span>fill_between(train_sizes, test_scores_mean <span style=color:#666>-</span> test_scores_std,  
                     test_scores_mean <span style=color:#666>+</span> test_scores_std, alpha<span style=color:#666>=</span><span style=color:#666>0.1</span>,  
                     color<span style=color:#666>=</span><span style=color:#b44>&#34;g&#34;</span>)  
    plt<span style=color:#666>.</span>plot(train_sizes, train_scores_mean, <span style=color:#b44>&#39;o-&#39;</span>, color<span style=color:#666>=</span><span style=color:#b44>&#39;r&#39;</span>,  
             label<span style=color:#666>=</span><span style=color:#b44>&#34;Training score&#34;</span>)  
    plt<span style=color:#666>.</span>plot(train_sizes, test_scores_mean,<span style=color:#b44>&#39;o-&#39;</span>,color<span style=color:#666>=</span><span style=color:#b44>&#34;g&#34;</span>,  
             label<span style=color:#666>=</span><span style=color:#b44>&#34;Cross-validation score&#34;</span>)  
    plt<span style=color:#666>.</span>legend(loc<span style=color:#666>=</span><span style=color:#b44>&#34;best&#34;</span>)  
    <span style=color:#a2f;font-weight:700>return</span> plt  

plot_learning_curve(LinearRegression(), <span style=color:#b44>&#39;Liner_model&#39;</span>, train_X[:<span style=color:#666>1000</span>], train_y_ln[:<span style=color:#666>1000</span>], ylim<span style=color:#666>=</span>(<span style=color:#666>0.0</span>, <span style=color:#666>0.5</span>), cv<span style=color:#666>=</span><span style=color:#666>5</span>, n_jobs<span style=color:#666>=</span><span style=color:#666>1</span>)  
</code></pre></div><h3 id=55-模型交叉验证>5.5. 模型交叉验证</h3><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#080;font-style:italic># 3折交叉验证。</span>
cross_val_score(model, X, y, cv<span style=color:#666>=</span><span style=color:#666>3</span>)
</code></pre></div><h2 id=6-模型调优>6. 模型调优</h2><h3 id=61-网格调参>6.1. 网格调参</h3><p>常用的参数搜索：网格调参、贝叶斯调参。</p><p>Notes: 网格调参应该先使用大网格，再逐步细化。</p><h4 id=611-gridsearchcv>6.1.1. GridSearchCV</h4><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python>params <span style=color:#666>=</span> {<span style=color:#b44>&#39;kneighborsclassifier-1__n_neighbors&#39;</span>: [<span style=color:#666>1</span>, <span style=color:#666>5</span>],
          <span style=color:#b44>&#39;kneighborsclassifier-2__n_neighbors&#39;</span>: [<span style=color:#666>1</span>, <span style=color:#666>5</span>],
          <span style=color:#b44>&#39;randomforestclassifier__n_estimators&#39;</span>: [<span style=color:#666>10</span>, <span style=color:#666>50</span>],
          <span style=color:#b44>&#39;meta_classifier__C&#39;</span>: [<span style=color:#666>0.1</span>, <span style=color:#666>10.0</span>]}
grid <span style=color:#666>=</span> GridSearchCV(estimator<span style=color:#666>=</span>sclf, 
                    param_grid<span style=color:#666>=</span>params, 
                    cv<span style=color:#666>=</span><span style=color:#666>5</span>,
                    refit<span style=color:#666>=</span>True)
grid<span style=color:#666>.</span>fit(X, y)

cv_keys <span style=color:#666>=</span> (<span style=color:#b44>&#39;mean_test_score&#39;</span>, <span style=color:#b44>&#39;std_test_score&#39;</span>, <span style=color:#b44>&#39;params&#39;</span>)
<span style=color:#a2f;font-weight:700>for</span> r, _ <span style=color:#a2f;font-weight:700>in</span> <span style=color:#a2f>enumerate</span>(grid<span style=color:#666>.</span>cv_results_[<span style=color:#b44>&#39;mean_test_score&#39;</span>]):
    <span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#34;</span><span style=color:#b68;font-weight:700>%0.3f</span><span style=color:#b44> +/- </span><span style=color:#b68;font-weight:700>%0.2f</span><span style=color:#b44> </span><span style=color:#b68;font-weight:700>%r</span><span style=color:#b44>&#34;</span>
          <span style=color:#666>%</span> (grid<span style=color:#666>.</span>cv_results_[cv_keys[<span style=color:#666>0</span>]][r],
             grid<span style=color:#666>.</span>cv_results_[cv_keys[<span style=color:#666>1</span>]][r] <span style=color:#666>/</span> <span style=color:#666>2.0</span>,
             grid<span style=color:#666>.</span>cv_results_[cv_keys[<span style=color:#666>2</span>]][r]))

<span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;Best parameters: </span><span style=color:#b68;font-weight:700>%s</span><span style=color:#b44>&#39;</span> <span style=color:#666>%</span> grid<span style=color:#666>.</span>best_params_)
<span style=color:#a2f;font-weight:700>print</span>(<span style=color:#b44>&#39;Accuracy: </span><span style=color:#b68;font-weight:700>%.2f</span><span style=color:#b44>&#39;</span> <span style=color:#666>%</span> grid<span style=color:#666>.</span>best_score_)

</code></pre></div><h4 id=贪心调参>贪心调参</h4><p>资料：<a href=https://www.jianshu.com/p/ab89df9759c8>贪心调参</a></p><h4 id=贝叶斯调参>贝叶斯调参</h4><p>资料：<a href=https://blog.csdn.net/linxid/article/details/81189154>贝叶斯调参</a></p><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#080;font-style:italic># 函数。返回分数。</span>
<span style=color:#a2f;font-weight:700>def</span> <span style=color:#00a000>rf_cv</span>(num_leaves, max_depth, subsample, min_child_samples):
    val <span style=color:#666>=</span> cross_val_score(
        LGBMRegressor(objective <span style=color:#666>=</span> <span style=color:#b44>&#39;regression_l1&#39;</span>,
            num_leaves<span style=color:#666>=</span><span style=color:#a2f>int</span>(num_leaves),
            max_depth<span style=color:#666>=</span><span style=color:#a2f>int</span>(max_depth),
            subsample <span style=color:#666>=</span> subsample,
            min_child_samples <span style=color:#666>=</span> <span style=color:#a2f>int</span>(min_child_samples)
        ),
        X<span style=color:#666>=</span>train_X, y<span style=color:#666>=</span>train_y_ln, verbose<span style=color:#666>=</span><span style=color:#666>0</span>, cv <span style=color:#666>=</span> <span style=color:#666>5</span>, scoring<span style=color:#666>=</span>make_scorer(mean_absolute_error)
    )<span style=color:#666>.</span>mean()
    <span style=color:#a2f;font-weight:700>return</span> <span style=color:#666>1</span> <span style=color:#666>-</span> val

<span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>bayes_opt</span> <span style=color:#a2f;font-weight:700>import</span> BayesianOptimization
rf_bo <span style=color:#666>=</span> BayesianOptimization(
    rf_cv,
    {
    <span style=color:#b44>&#39;num_leaves&#39;</span>: (<span style=color:#666>2</span>, <span style=color:#666>100</span>),
    <span style=color:#b44>&#39;max_depth&#39;</span>: (<span style=color:#666>2</span>, <span style=color:#666>100</span>),
    <span style=color:#b44>&#39;subsample&#39;</span>: (<span style=color:#666>0.1</span>, <span style=color:#666>1</span>),
    <span style=color:#b44>&#39;min_child_samples&#39;</span> : (<span style=color:#666>2</span>, <span style=color:#666>100</span>)
    }
)
<span style=color:#080;font-style:italic># 使得分数最大的贝叶斯调参。</span>
rf_bo<span style=color:#666>.</span>maximize()
<span style=color:#080;font-style:italic># 获取最大的target</span>
<span style=color:#a2f;font-weight:700>print</span>(<span style=color:#666>1</span> <span style=color:#666>-</span> rf_bo<span style=color:#666>.</span>max[<span style=color:#b44>&#39;target&#39;</span>])
</code></pre></div><h3 id=62-模型融合>6.2. 模型融合</h3><p>blending, stacking(mlxtend)</p><h4 id=621-stacking>6.2.1. Stacking</h4><h5 id=6211-使用mlxtend工具包>6.2.1.1. 使用mlxtend工具包</h5><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>mlxtend.classifier</span> <span style=color:#a2f;font-weight:700>import</span> StackingCVClassifier

clf1 <span style=color:#666>=</span> KNeighborsClassifier(n_neighbors<span style=color:#666>=</span><span style=color:#666>1</span>)
clf2 <span style=color:#666>=</span> RandomForestClassifier(random_state<span style=color:#666>=</span>RANDOM_SEED)
clf3 <span style=color:#666>=</span> GaussianNB()
lr <span style=color:#666>=</span> LogisticRegression()

<span style=color:#080;font-style:italic># Starting from v0.16.0, StackingCVRegressor supports</span>
sclf <span style=color:#666>=</span> StackingCVClassifier(
    classifiers<span style=color:#666>=</span>[clf1, clf2, clf3],  <span style=color:#080;font-style:italic># 第一层分类器</span>
    use_probas<span style=color:#666>=</span>True,  <span style=color:#080;font-style:italic># </span>
    meta_classifier<span style=color:#666>=</span>lr,   <span style=color:#080;font-style:italic># 第二层分类器</span>
    random_state<span style=color:#666>=</span>RANDOM_SEED)
sclf<span style=color:#666>.</span>fit()
</code></pre></div><h5 id=6212-使用sklearn>6.2.1.2. 使用sklearn</h5><div class=highlight><pre style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=color:#a2f;font-weight:700>from</span> <span style=color:#00f;font-weight:700>sklearn.ensemble</span> <span style=color:#a2f;font-weight:700>import</span> StackingClassifier
estimators <span style=color:#666>=</span> [
    (<span style=color:#b44>&#39;rf&#39;</span>, RandomForestClassifier(n_estimators<span style=color:#666>=</span><span style=color:#666>10</span>, random_state<span style=color:#666>=</span><span style=color:#666>42</span>)),
    (<span style=color:#b44>&#39;svr&#39;</span>, make_pipeline(StandardScaler(),
                          LinearSVC(random_state<span style=color:#666>=</span><span style=color:#666>42</span>)))
]
clf <span style=color:#666>=</span> StackingClassifier(
    estimators<span style=color:#666>=</span>estimators, final_estimator<span style=color:#666>=</span>LogisticRegression()
)
</code></pre></div><h2 id=7-整体步骤>7. 整体步骤</h2><h2 id=8-参考资料>8. 参考资料</h2><ol><li>非常详细全面的动手学习:<a href=https://github.com/datawhalechina/team-learning-data-mining>team-learning-data-mining</a></li></ol></div><footer class=post-footer><div class=post-tags><a href=/tags/kaggle rel=tag title=Kaggle>#Kaggle#</a></div><div class=addthis_inline_share_toolbox></div><div class=post-nav><div class=article-copyright><div class=article-copyright-img><img src=/img/qq_qrcode.png width=129px height=129px><div style=text-align:center>QQ扫一扫交流</div></div><div class=article-copyright-info><p><span>声明：</span>Kaggle学习计划</p><p style=word-break:break-all><span>链接：</span>
https://yanyulinxi.github.io/post/study/kaggle/kaggle%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/</p><p><span>作者：</span>阳阳</p><p><span>邮箱：</span>yanyulinxi@qq.com</p><p><span>声明： </span>本博客文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/ target=_blank style=text-decoration:underline>CC BY-NC-SA 3.0</a>许可协议，转载请注明出处！</p></div></div><div class=clear></div></div><div class=reward-qr-info><div>创作实属不易，如有帮助，那就打赏博主些许茶钱吧 ^_^</div><button id=rewardButton disable=enable onclick="var qr=document.getElementById('QR');qr.style.display==='none'?qr.style.display='block':qr.style.display='none'">
<span>赏</span></button><div id=QR style=display:none><div id=wechat style=display:inline-block><img id=wechat_qr src=/img/wechat-pay.png alt="WeChat Pay"><p>微信打赏</p></div><div id=alipay style=display:inline-block><img id=alipay_qr src=/img/ali-pay.png alt=Alipay><p>支付宝打赏</p></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=https://yanyulinxi.github.io/post/study/python%E5%BA%93/argparse%E4%BD%BF%E7%94%A8%E8%A7%84%E8%8C%83/ rel=next title=Argparse使用规范><i class="fa fa-chevron-left"></i>Argparse使用规范</a></div><div class="post-nav-prev post-nav-item"><a href=https://yanyulinxi.github.io/post/study/spark/spark%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B/ rel=prev title=Spark代码示例>Spark代码示例
<i class="fa fa-chevron-right"></i></a></div></div><div id=wcomments></div></footer></article></section></div></div><div class=sidebar-toggle><div class=sidebar-toggle-line-wrap><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id=sidebar class=sidebar><div class=sidebar-inner><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target=post-toc-wrap>文章目录</li><li class=sidebar-nav-overview data-target=site-overview>站点概览</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image src=/img/linxi_icon.png alt=阳阳><p class=site-author-name itemprop=name>阳阳</p><p class="site-description motion-element" itemprop=description>再平凡的人也有属于他自己的梦想!</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href=/post/><span class=site-state-item-count>142</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>6</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>35</span>
<span class=site-state-item-name>标签</span></a></div></nav><div class="links-of-author motion-element"><span class=links-of-author-item><a href=https://github.com/yanyuLinxi target=_blank title=GitHub><i class="fa fa-fw fa-github"></i>GitHub</a></span>
<span class=links-of-author-item><a href=https://space.bilibili.com/19237450 target=_blank title=哔哩哔哩><i class="fa fa-fw fa-globe"></i>哔哩哔哩</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class=links-of-blogroll-title><i class="fa fa-fw fa-globe"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://www.liaoxuefeng.com/ title=廖雪峰 target=_blank>廖雪峰</a></li></ul></div><div class="tagcloud-of-blogroll motion-element tagcloud-of-blogroll-inline"><div class=tagcloud-of-blogroll-title><i class="fa fa-fw fa-tags"></i>标签云</div><ul class=tagcloud-of-blogroll-list><li class=tagcloud-of-blogroll-item><a href=/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0>论文阅读笔记</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/python%E7%9B%B8%E5%85%B3%E5%BA%93%E5%AD%A6%E4%B9%A0>Python相关库学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E5%BC%82%E5%B8%B8%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90>异常行为分析</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7>深度学习辅助工具</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/kaggle>Kaggle</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E5%BA%93>机器学习相关库</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/java%E5%AD%A6%E4%B9%A0>Java学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/insider-threat>Insider threat</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/leetcode%E5%AD%A6%E4%B9%A0>Leetcode学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/spark>Spark</a></li></ul></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class=post-toc><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#1-学习计划>1. 学习计划</a></li><li><a href=#2-赛题分析>2. 赛题分析</a></li><li><a href=#解题步骤>解题步骤</a></li><li><a href=#3-数据分析>3. 数据分析</a><ul><li><a href=#资料>资料</a></li><li><a href=#31-autoeda>3.1. AutoEDA</a></li><li><a href=#32-手动eda>3.2. 手动EDA</a></li></ul></li><li><a href=#4-特征工程>4. 特征工程</a><ul><li><a href=#41-训练测试数据组合>4.1. 训练、测试数据组合</a></li><li><a href=#回归问题的标签处理>回归问题的标签处理</a></li><li><a href=#42-倾斜特征处理>4.2. 倾斜特征处理</a></li><li><a href=#43-异常值处理>4.3. 异常值处理</a></li><li><a href=#44-缺失值处理>4.4. 缺失值处理</a></li><li><a href=#数字特征处理>数字特征处理</a></li><li><a href=#47-类别特征处理>4.7. 类别特征处理</a></li><li><a href=#构造新特征>构造新特征</a></li><li><a href=#49-标签处理>4.9. 标签处理</a></li><li><a href=#特征选择>特征选择</a></li></ul></li><li><a href=#5-模型训练>5. 模型训练</a><ul><li><a href=#51-常用模型>5.1. 常用模型</a></li><li><a href=#模型数据类型调整>模型数据类型调整</a></li><li><a href=#52-数据分组>5.2. 数据分组</a></li><li><a href=#53-模型训练>5.3. 模型训练</a></li><li><a href=#54-特征重要性查看>5.4. 特征重要性查看</a></li><li><a href=#模型结果查看>模型结果查看</a></li><li><a href=#绘制学习率曲线验证曲线>绘制学习率曲线、验证曲线</a></li><li><a href=#55-模型交叉验证>5.5. 模型交叉验证</a></li></ul></li><li><a href=#6-模型调优>6. 模型调优</a><ul><li><a href=#61-网格调参>6.1. 网格调参</a></li><li><a href=#62-模型融合>6.2. 模型融合</a></li></ul></li><li><a href=#7-整体步骤>7. 整体步骤</a></li><li><a href=#8-参考资料>8. 参考资料</a></li></ul></nav></div></div></section></div></aside></div></main><footer id=footer class=footer><div class=footer-inner><div class=copyright><span class=copyright-year>&copy; 2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span><span class=copyright-author>阳阳的人间旅游日记</span></div><div class=powered-info><span class=powered-by>Powered by - <a class=powered-link href=//gohugo.io target=_blank title=hugo>Hugo v0.81.0</a></span>
<span class=separator-line>/</span>
<span class=theme-info>Theme by - <a class=powered-link href=//github.com/elkan1788/hugo-theme-next target=_blank>NexT</a></span></div><div class=vistor-info><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span class=site-uv><i class="fa fa-user"></i><span class=busuanzi-value id=busuanzi_value_site_uv></span></span><span class=separator-line>/</span>
<span class=site-pv><i class="fa fa-eye"></i><span class=busuanzi-value id=busuanzi_value_site_pv></span></span></div><div class=license-info><span class=storage-info>Storage by
<a href style=font-weight:700 target=_blank></a></span><span class=separator-line>/</span>
<span class=license-num><a href target=_blank></a></span></div></div></footer><div class=back-to-top><i class="fa fa-arrow-up"></i><span id=scrollpercent><span>0</span>%</span></div></div><script type=text/javascript src=//cdn.bootcdn.net/ajax/libs/jquery/2.1.4/jquery.min.js></script><script type=text/javascript src=/js/search.js></script><script type=text/javascript src=/js/affix.js></script><script type=text/javascript src=/js/scrollspy.js></script><script type=text/javascript>function detectIE(){var a=window.navigator.userAgent,b=a.indexOf('MSIE '),c=a.indexOf('Trident/'),d=a.indexOf('Edge/');return b>0||c>0||d>0?-1:1}function getCntViewHeight(){var b=$('#content').height(),a=$(window).height(),c=b>a?b-a:$(document).height()-a;return c}function getScrollbarWidth(){var a=$('<div />').addClass('scrollbar-measure').prependTo('body'),b=a[0],c=b.offsetWidth-b.clientWidth;return a.remove(),c}function registerBackTop(){var b=50,a=$('.back-to-top');$(window).on('scroll',function(){var d,e,f,c,g;a.toggleClass('back-to-top-on',window.pageYOffset>b),d=$(window).scrollTop(),e=getCntViewHeight(),f=d/e,c=Math.round(f*100),g=c>100?100:c,$('#scrollpercent>span').html(g)}),a.on('click',function(){$("html,body").animate({scrollTop:0,screenLeft:0},800)})}function initScrollSpy(){var a='.post-toc',d=$(a),b='.active-current';d.on('activate.bs.scrollspy',function(){var b=$(a+' .active').last();c(),b.addClass('active-current')}).on('clear.bs.scrollspy',c),$('body').scrollspy({target:a});function c(){$(a+' '+b).removeClass(b.substring(1))}}function initAffix(){var a=$('.header-inner').height(),b=parseInt($('.main').css('padding-bottom'),10),c=a+10;$('.sidebar-inner').affix({offset:{top:c,bottom:b}}),$(document).on('affixed.bs.affix',function(){updateTOCHeight(document.body.clientHeight-100)})}function initTOCDimension(){var a,b;$(window).on('resize',function(){a&&clearTimeout(a),a=setTimeout(function(){var a=document.body.clientHeight-100;updateTOCHeight(a)},0)}),updateTOCHeight(document.body.clientHeight-100),b=getScrollbarWidth(),$('.post-toc').css('width','calc(100% + '+b+'px)')}function updateTOCHeight(a){a=a||'auto',$('.post-toc').css('max-height',a)}$(function(){var b=$('.header-inner').height()+10,c,d,a,e;$('#sidebar').css({'margin-top':b}).show(),c=parseInt($('#sidebar').css('margin-top')),d=parseInt($('.sidebar-inner').css('height')),a=c+d,e=$('.content-wrap').height(),e<a&&$('.content-wrap').css('min-height',a),$('.site-nav-toggle').on('click',function(){var a=$('.site-nav'),e=$('.toggle'),b='site-nav-on',f='toggle-close',c=a.hasClass(b),g=c?'slideUp':'slideDown',d=c?'removeClass':'addClass';a.stop()[g]('normal',function(){a[d](b),e[d](f)})}),registerBackTop(),initScrollSpy(),initAffix(),initTOCDimension(),$('.sidebar-nav-toc').click(function(){$(this).addClass('sidebar-nav-active'),$(this).next().removeClass('sidebar-nav-active'),$('.'+$(this).next().attr('data-target')).toggle(500),$('.'+$(this).attr('data-target')).toggle(500)}),$('.sidebar-nav-overview').click(function(){$(this).addClass('sidebar-nav-active'),$(this).prev().removeClass('sidebar-nav-active'),$('.'+$(this).prev().attr('data-target')).toggle(500),$('.'+$(this).attr('data-target')).toggle(500)})})</script><script src=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.js></script><script type=text/javascript>$(function(){$('.post-body').viewer()})</script><script type=text/javascript>$(function(){detectIE()>0?$.getScript(document.location.protocol+'//cdn.jsdelivr.net/npm/@waline/client/dist/Waline.min.js',function(){new Waline({el:'#wcomments',visitor:!0,avatar:'wavatar',avatarCDN:'https://sdn.geekzu.org/avatar/',avatarForce:!1,wordLimit:'200',placeholder:' 欢迎留下您的宝贵建议，请填写您的昵称和邮箱便于后续交流. ^_^ ',requiredFields:['nick','mail'],serverURL:"Your WalineSerURL",lang:"zh-cn"})}):$('#wcomments').html('抱歉，Waline插件不支持IE或Edge，建议使用Chrome浏览器。')})</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=Your%20AddthisId"></script><script>(function(){var a=document.createElement('script'),c=window.location.protocol.split(':')[0],b;c==='https'?a.src='https://zz.bdstatic.com/linksubmit/push.js':a.src='http://push.zhanzhang.baidu.com/push.js',b=document.getElementsByTagName("script")[0],b.parentNode.insertBefore(a,b)})()</script></body></html>