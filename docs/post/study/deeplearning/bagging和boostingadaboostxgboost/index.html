<!doctype html><html lang=zh-cn dir=content/zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=content-security-policy content="upgrade-insecure-requests"><title>Bagging和Boosting,adboost,xgboost - 阳阳的人间旅游日记</title><meta name=keywords content="博客,程序员,思考,读书,笔记,技术,分享"><meta name=author content="阳阳"><meta property="og:title" content="Bagging和Boosting,adboost,xgboost"><meta property="og:site_name" content="阳阳的人间旅游日记"><meta property="og:image" content="/img/author.jpg"><meta name=title content="Bagging和Boosting,adboost,xgboost - 阳阳的人间旅游日记"><meta name=description content="欢迎来到临溪的博客站，个人主要专注于机器学习、深度学习的相关研究。在这里分享自己的学习心得。"><link rel="shortcut icon" href=/img/favicon.ico><link rel=apple-touch-icon href=/img/apple-touch-icon.png><link rel=apple-touch-icon-precomposed href=/img/apple-touch-icon.png><link href=//cdn.bootcdn.net/ajax/libs/font-awesome/4.6.2/css/font-awesome.min.css rel=stylesheet type=text/css><link href=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.css rel=stylesheet><link href=/css/main.css rel=stylesheet type=text/css><link href=/css/syntax.css rel=stylesheet type=text/css></head><body itemscope itemtype=http://schema.org/WebPage lang=zh-hans><div class="container one-collumn sidebar-position-left page-home"><div class=headband></div><header id=header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle role=button style=opacity:1;top:0><span class=toggle-line></span><span class=toggle-line></span><span class=toggle-line></span></div></div><div class=site-meta><div class=multi-lang-switch><i class="fa fa-fw fa-language" style=margin-right:5px></i><a class=lang-link id=zh-cn href=#>中文</a></div><div class=custom-logo-site-title><a href=/ class=brand rel=start><span class=logo-line-before><i></i></span><span class=site-title>阳阳的人间旅游日记</span>
<span class=logo-line-after><i></i></span></a></div><p class=site-subtitle>让我们消除隔阂的，不是无所不知的脑袋，而是手拉手，坚决不放弃的那颗心</p></div><div class=site-nav-right><div class="toggle popup-trigger" style=opacity:1;top:0><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul id=menu class=menu><li class=menu-item><a href=/ rel=section><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class=menu-item><a href=/post rel=section><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class=menu-item><a href=/about.html rel=section><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于我</a></li><li class=menu-item><a href=/404.html rel=section><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href=javascript:; class=popup-trigger><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class=site-search><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class=search-icon><i class="fa fa-search"></i></span><span class=popup-btn-close><i class="fa fa-times-circle"></i></span><div class=local-search-input-wrapper><input autocomplete=off placeholder=搜索关键字... spellcheck=false type=text id=local-search-input autocapitalize=none autocorrect=off></div></div><div id=local-search-result></div></div></div></nav></div></header><main id=main class=main><div class=main-inner><div class=content-wrap><div id=content class=content><section id=posts class=posts-expand><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><header class=post-header><h1 class=post-title itemprop="name headline"><a class=post-title-link href=https://yanyulinxi.github.io/post/study/deeplearning/bagging%E5%92%8Cboostingadaboostxgboost/ itemprop=url>Bagging和Boosting,adboost,xgboost</a></h1><div class=post-meta><span class=post-time><i class="fa fa-calendar-o fa-fw"></i><span class=post-meta-item-text>时间：</span>
<time itemprop=dateCreated datetime=2016-03-22T13:04:35+08:00 content="2021-11-17">2021-11-17</time></span>
<span>|
<i class="fa fa-file-word-o fa-fw"></i><span class=post-meta-item-text>字数：</span>
<span class=leancloud-world-count>11179 字</span></span>
<span>|
<i class="fa fa-eye fa-fw"></i><span class=post-meta-item-text>阅读：</span>
<span class=leancloud-view-count>23分钟</span></span>
<span id=/post/study/deeplearning/bagging%E5%92%8Cboostingadaboostxgboost/ class=leancloud_visitors data-flag-title=Bagging和Boosting,adboost,xgboost>|
<i class="fa fa-binoculars fa-fw"></i><span class=post-meta-item-text>阅读次数：</span>
<span class=leancloud-visitors-count></span></span></div></header><div class=post-body itemprop=articleBody><p><a href=https://cloud.tencent.com/developer/news/393218>https://cloud.tencent.com/developer/news/393218</a></p><p>Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个性能更加强大的分类器，更准确的说这是一种分类算法的组装方法。即将弱分类器组装成强分类器的方法。</p><p><a href=https://zhuanlan.zhihu.com/p/37730184>https://zhuanlan.zhihu.com/p/37730184</a>
<a href=https://github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning/EnsembleLearning.md>https://github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning/EnsembleLearning.md</a></p><h1 id=集成学习>集成学习</h1><ol><li>什么是集成学习算法？<ol><li>集成学习算法是一种优化手段或者策略，将多个较弱的模型集成模型组，一般的弱分类器可以是决策树，SVM，KNN等构成。其中的模型可以单独进行训练，并且它们的预测能以某种方式结合起来去做出一个总体预测。</li></ol></li><li>集成学习主要有哪几种框架？<ol><li>集成学习从集成思想的架构分为Bagging，Boosting，Stacking三种。</li></ol></li><li>简单介绍一下bagging，常用bagging算法有哪些？<ol><li>Bagging</li><li><strong>多次采样</strong>，训练多个分类器，<strong>集体投票</strong>，旨在减小方差，</li><li>基于数据随机重抽样的分类器构建方法。从训练集中进行<strong>子抽样</strong>组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果。</li><li>算法流程：<ol><li>输入为样本集D=(x1，y1)，(x2，y2)…(xm，ym)，弱学习器算法，弱分类器迭代次数T。</li><li>输出为最终的强分类器f(x)</li></ol></li><li>对于t=1，2…T<ol><li>对训练集进行第t次随机采样，共采集T次，得到包含T个样本的采样集Dt</li><li>用<strong>采样集Dt</strong>训练第t个弱学习器Gt(x)</li></ol></li><li>如果是分类算法预测，则T个弱学习器<strong>投出最多票数</strong>的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行<strong>算术平均</strong>得到的值为最终的模型输出。</li><li>常用bagging算法：随机森林算法</li></ol></li><li>简单介绍一下boosting，常用boosting算法有哪些？<ol><li><strong>Boosting</strong><ol><li>基分类器<strong>层层叠加</strong>，聚焦<strong>分错的样本</strong>，旨在<strong>减小偏差</strong></li></ol></li><li>训练过程为阶梯状，基模型<strong>按次序</strong>进行训练（实现上可以做到并行），基模型的训练集按照某种策略每次都进行一定的转化，每次都是<strong>提高前一次分错了的数据集的权值</strong>，最后对所有基模型预测的结果进行线性组合产生最终的预测结果。</li><li>算法流程：<ol><li>给定初始训练数据，由此训练出第一个基学习器；</li><li>根据基学习器的表现对样本进行调整，在之前<strong>学习器做错的样本</strong>上投入更多关注；</li><li>用调整后的样本，训练下一个基学习器；</li><li>重复<strong>上述过程T次</strong>，将<strong>T个学习器</strong>加权结合。</li></ol></li><li>常用boosting算法<ol><li>Adaboost</li><li>GBDT</li><li>XGBoost</li></ol></li><li>数学表达式：<ol><li>$f(x)=w_0+\sum_{m=1}^Mw_mϕ_m(x)$</li><li>就是给弱分类器拟合权重，来训练强分类器。</li></ol></li></ol></li><li>简单介绍一下stacking<ol><li><strong>多次采样</strong>，训练多个分类器，将<strong>输出作为最后的输入特征</strong></li><li>将训练好的所有基模型对训练集<strong>进行预测</strong>，第个i基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第i个特征值，**最后基于新的训练集进行训练。**同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测。</li><li>stacking常见的使用方式：<ol><li>由k-NN、随机森林和朴素贝叶斯基础分类器组成，它的预测结果由作为元分类器的逻回归组合。</li></ol></li></ol></li><li>使用场合<ol><li>应对高方差：（就是提升泛化性）<ol><li>使用正则化技术</li><li>使用可变重要性图表中的前n个特征。</li><li>bagging</li></ol></li></ol></li><li>常用的基分类器是什么？<ol><li>最常用的基分类器是决策树,原因:<ol><li>决策树可以较为方便地将<strong>样本的权重</strong>整合到训练过程中，而不需要使用过采样的方法来调整样本权重</li><li>决策树的<strong>表达能力和泛化能力</strong>，可以通过调节树的层数来做折中。</li><li>数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性较大，<strong>这样的“不稳定学习器”更适合作为基分类器</strong>。此外，在决策树节点分裂的时候，随机地选择一个特征子集，从中找出最优分裂属性，<strong>很好地引入了随机性。</strong></li></ol></li></ol></li></ol><p><strong>首先介绍Bootstraping，即自助法：它是一种有放回的抽样方法（可能抽到重复的样本）。</strong></p><ol><li>Bagging (bootstrap aggregating)</li></ol><p><strong>Bagging即套袋法</strong>，其算法过程如下：</p><p>从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）</p><p>每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）</p><p>对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）</p><h2 id=具体流程>具体流程：</h2><p>每次使用一份训练集训练一个模型，k 个训练集共得到 k 个基模型
利用这k个基模型对测试集进行预测，将k个预测结果进行聚合。</p><h2 id=特点>特点</h2><ol><li><p>可并行的集成方法。每个基模型可以分别、独立、互不影响地生成。</p></li><li><p>主要降低 Variance，对 Bias 无明显作用。因此，适用于 High Variance & Low Bias 的模型。</p></li><li><p>Boosting 助推法
其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。</p></li></ol><p>关于Boosting的两个核心问题：</p><p>2.1 在每一轮如何改变训练数据的权值或概率分布？</p><p>通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。</p><p>2.2 通过什么方式来组合弱分类器？</p><p>通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。</p><p>而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。</p><ol start=3><li>Bagging，Boosting二者之间的区别</li></ol><p>Bagging和Boosting的区别：</p><p>1）样本选择上：</p><p>Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。</p><p>Boosting：<strong>每一轮的训练集不变</strong>，只是训练集中<strong>每个样例在分类器中的权重发生变化</strong>。而<strong>权值是根据上一轮的分类结果进行调整。</strong></p><p>2）样例权重：</p><p>Bagging：使用均匀取样，每个样例的权重相等</p><p>Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。</p><p>3）预测函数：</p><p>Bagging：所有预测函数的权重相等。</p><p>Boosting：<strong>每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。</strong></p><p>4）并行计算：</p><p>Bagging：各个预测函数可以并行生成</p><p>Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。</p><p>5）其他：
Bagging主要关注降低方差，而Boosting主要关注降低偏差
Boosting是一族算法，其主要目标为将弱学习器“提升”为强学习器，大部分Boosting算法都是根据前一个学习器的训练效果对样本分布进行调整，再根据新的样本分布训练下一个学习器，如此迭代M次，最后将一系列弱学习器组合成一个强学习器。
而这些Boosting算法的不同点则主要体现在每轮样本分布的调整方式上</p><ol start=4><li>总结</li></ol><p>这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。</p><p>下面是将决策树与这些算法框架进行结合所得到的新的算法：</p><p>Bagging + 决策树 = 随机森林</p><p>AdaBoost + 决策树 = 提升树</p><p>Gradient Boosting + 决策树 = GBDT</p><h1 id=adaboost>AdaBoost</h1><p><a href=https://github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning/Adaboost.md>https://github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning/Adaboost.md</a></p><h2 id=adaboost-步骤>adaboost 步骤</h2><ol><li>初始化训练样本的权值分布，每个训练样本的权值应该相等（如果一共有N个样本，则每个样本的权值为1N）</li><li>依次构造训练集并训练弱分类器。如果一个样本被准确分类，那么它的权值在下一个训练集中就会降低；相反，如果它被分类错误，那么它在下个训练集中的权值就会提高。权值更新过后的训练集会用于训练下一个分类器。</li><li>将训练好的弱分类器集成为一个强分类器，误差率小的弱分类器会在最终的强分类器里占据更大的权重，否则较小。</li></ol><h2 id=步骤算法>步骤算法</h2><ol><li>初始化权重</li><li>进行t=1&mldr;T轮迭代<ol><li>选取当前误差最低的弱分类器h作为第t个分类器。计算弱分类器在分布D_t的分类误差e_t。</li><li>计算该分类器在最终分类器中所占权重。$\alpha_t = 1/2ln\frac{1-e_t}{e_t}$</li><li>更新权值分布。</li></ol></li><li>最终按照$\alpha_t$来组合所有的弱分类器。</li></ol><p><a href=https://www.cnblogs.com/willnote/p/6801496.html>https://www.cnblogs.com/willnote/p/6801496.html</a>
<a href=https://zhuanlan.zhihu.com/p/37358517>https://zhuanlan.zhihu.com/p/37358517</a></p><p>AdaBoost是一个具有里程碑意义的算法，因为其是第一个具有适应性的算法，即能适应弱学习器各自的训练误差率，这也是其名称的由来（Ada为Adaptive的简写）。</p><p><strong>AdaBoost的具体流程为先对每个样本赋予相同的初始权重，<strong>每一轮学习器训练过后都会根据其表现对每个样本的权重进行调整，<strong>增加分错样本的权</strong>重，这样先前做错的样本在后续就能得到更多关注，按这样的过程重复训练出</strong>M个学习器，最后进行加权组合，</strong></p><p>加法模型，第m次迭代中，前m-1个基学习器是固定的。
AdaBoost最后得到的强学习器是由一系列的弱学习器的线性组合，此即加法模型。这个是说训练第二个基学习器的时候，用的是第一个基学习器的结果。</p><p>这里有两个关键问题：</p><p>每轮训练过后<strong>如何调整样本权重</strong> w ？
如何确定<strong>最后各学习器的权重</strong> $\alpha$ ？</p><p>=></p><p><strong>前一轮正确分类的样本减少权重。</strong>
<strong>线性组合后，准确率越高的学习器赋予较大的系数。</strong></p><h2 id=adaboost损失函数>adaboost损失函数</h2><p>adaboost 表达式 $f(x) = sign(\sum_m^M \alpha_mG_m(x))$</p><p>adaboost采用指数损失函数 e^{-yf(x)}求出\alpha_m G_m</p><h2 id=adaboost采用指数损失的原因>AdaBoost采用指数损失的原因</h2><p>这说明指数损失函数是分类任务<strong>原本0-1损失函数的一致性替代函</strong>数。由于这个替代函数是单调连续可微函数，因此用它代替0-1损失函数作为优化目标。</p><h2 id=其他>其他</h2><p><strong>Weight Trimming</strong>
weight trimming不是正则化的方法，其主要目的是提高训练速度。在AdaBoost的每一轮基学习器训练过程中，只有小部分样本的权重较大，因而能产生较大的影响，而其他大部分权重小的样本则对训练影响甚微。</p><p>只用高权重样本进行训练。具体是设定一个阈值 (比如90%或99%)，再将所有样本按权重排序，计算权重的累积和，累积和大于阈值的权重 (样本) 被舍弃，不会用于训练。</p><p>注意每一轮训练完成后所有样本的权重依然会被重新计算，这意味着之前被舍弃的样本在之后的迭代中如果权重增加，可能会重新用于训练。</p><h2 id=adboost优缺点>adboost优缺点：</h2><p>优点：
能够基于泛化性能相当弱的的学习器构建出很强的集成，不容易发生过拟合。
缺点：</p><ol><li>对<strong>异常样本比较敏感</strong>，异常样本在迭代过程中会获得较高的权值，影响最终学习器的性能表现。</li><li>只能处理采用<strong>指数损失函数的二分类学习任务</strong>。</li></ol><p>这个优缺点和boost的优缺点没什么区别。</p><h1 id=gradient-boosting>Gradient Boosting</h1><p><a href=https://www.cnblogs.com/willnote/p/6801496.html>https://www.cnblogs.com/willnote/p/6801496.html</a>
<a href=https://zhuanlan.zhihu.com/p/38329631>https://zhuanlan.zhihu.com/p/38329631</a>
<a href=https://zhuanlan.zhihu.com/p/81016622>https://zhuanlan.zhihu.com/p/81016622</a></p><h2 id=adaboost缺点>adaboost缺点：</h2><ol><li>上一篇介绍了AdaBoost算法，在AdaBoost中每一轮基学习器训练过后都会更新样本权重，再训练下一个学习器，最后将所有的基学习器加权组合。AdaBoost使用的是指数损失，这个损失函数的缺点是对于<strong>异常点非常敏感，</strong>。因而通常在<strong>噪音比较多的数据集</strong>上表现不佳。<strong>Gradient Boosting在这方面进行了改进</strong>，使得可以<strong>使用任何损失函数</strong>(只要损失函数是连续可导的)，这样一些比较robust的损失函数就能得以应用，使模型抗噪音能力更强。</li></ol><h2 id=注意>注意</h2><ol><li><p><strong>Boosting的基本思想</strong>是通过某种方式使得每一轮基学习器在训练过程中<strong>更加关注上一轮学习错误的样本</strong>，区别在于是<strong>采用何种方式</strong>.AdaBoost采用的是增加上一轮<strong>学习错误样本的权重的策略</strong>，而在Gradient Boosting中则将<strong>负梯度</strong>作为上一轮基学习器犯错的衡量指标，在下一轮学习中通过<strong>拟合负梯度</strong>来纠正上一轮犯的错误。</p></li><li><p>对于分类问题。转换为回归问题进行计算。</p><p>为什么通过拟合负梯度就能纠正上一轮的错误了？Gradient Boosting的发明者给出的答案是：<strong>函数空间的梯度下降。</strong></p><p>Gradient Boosting 采用和AdaBoost同样的<strong>加法模型</strong>，在<strong>第m次迭代</strong>中，<strong>前m-1个基学习器</strong>都是固定的。</p><p>负梯度也被称为“<strong>响应 (response)</strong>”或“<strong>伪残差 (pseudo residual)</strong>”，从名字可以看出是一个与残差接近的概念。直觉上来看，残差 r=y-f(x) 越大，表明前一轮学习器 f(x)的结果与真实值 y 相差较大，那么下一轮学习器通过拟合残差或负梯度，就能纠正之前的学习器犯错较大的地方。</p><p>就是说让h_m(x)去拟合f(x)和h_{m-1}(x)的差值。</p><p>在Gradient Boosting框架中，最常用的基学习器是决策树 (一般是CART)，二者结合就成了著名的梯度提升树 (Gradient Boosting Decision Tree, GBDT)。注意GBDT不论是用于回归还是分类，<strong>其基学习器 (即单颗决策树) 都是回归树</strong>，即使是分类问题也是将最后的预测值映射为概率。</p></li></ol><h1 id=算法步骤>算法步骤</h1><ol><li>初始化第一个学习器。根据loss function的选择。</li><li>for m=1 to M:<ol><li>计算负梯度：y_i = - \frac{偏导L(y_i,f_{m-1}(x_i))}{偏导f_{m-1}(x_i)}</li><li>通过最小化平方误差，用基学习器来拟合y_i。(这个是回归树叶子决策时的公式。)</li><li>使用line search 确定一个p_m。用来最小化和标签的损失函数L<ol><li>确定p_m，$p_m = argmin_p \sum_i^N L(y_i, f_{m-1}(x_i)+ph_m(x_i;w_m))$ 来使得L最小</li><li>p_m就是最终加法模型当中每个基学习器的权重。</li></ol></li><li>f_m(x) = f_{m-1}(x) + p_mh_m(x; w_m)</li></ol></li></ol><h1 id=gbdt公式>gbdt公式</h1><p>依然采用前向分步算法，过程如下：
$$f_{0}(x)=0 \<br>f_{m}(x)=f_{m-1}(x)+T(x;\Theta), m=1,2,&mldr;,M\<br>f_{M}(x)=\sum_{m=1}^{M}T(x;\Theta_{m})$$
第一个式子首先定义初始提升树$f_{0}(x)=0$；之后第m步的模型即为第二个式子，其中$T(x;\Theta)$表示决策树，$\Theta$为决策树的参数；第三个式子表示GBDM的最终模型，其中M为树的个数。</p><p>在前向分步算法的第m步，给定当前模型$f_{m-1}(x)$，需求解
$$\hat{\Theta}_{m}=\underset{\Theta_{m}}{arg\ min}\sum_{i=1}^{N}L(y_{i},f_{m-1}(x_{i})+T(x_{i};\Theta_{m}))$$
得到的$\hat{\Theta}_{m}$即为第m颗树的参数。当采用平方误差作为损失函数时：
$$L(y,f(x))=(y-f(x))^{2}$$
带入上式中，则其损失函数变为：
$$L(y,f_{m-1}(x)+T(x;\Theta_{m}))\<br>=[y-f_{m-1}(x)-T(x;\Theta_{m})]^{2}\<br>=[r-T(x;\Theta_{m})]^{2}$$
这里
$$r=y-f_{m-1}(x)$$
是当前模型拟合数据的残差。所以，对于回归问题的提升树算法来说，只需简单地拟合当前模型的残差。即每一轮产生的残差作为下一轮回归树的输入，下一轮的回归树的目的就是尽可能的拟合这个输入残差。</p><h1 id=可供选择的损失函数>可供选择的损失函数</h1><p>常用的损失函数为平方损失 (squared loss)，
绝对值损失 (absolute loss)，
Huber损失 (huber loss)</p><h1 id=gbdt正则化>gbdt正则化：</h1><ol><li>设立一个步长。防止过快产生过拟合</li><li>Early stopping<ol><li>在训练过程中不断检查在测试集上的表现，如果测试集上的准确率下降到一定阈值之下，则停止训练，选用当前的迭代次数M</li></ol></li><li>限制树的复杂度</li><li>Subsampling<ol><li>借用bootstrap的思想，每一轮训练时只使用一部分样本，不同点是这里的采样是无放回抽样，</li></ol></li></ol><h2 id=面试相关>面试相关</h2><ol><li>RF和GBDT的区别（bagging和boosting的区别）<ol><li>相同点<ol><li>都是由多棵树组成，最终的结果都是由多棵树一起决定。</li></ol></li><li>不同点：
<strong>集成学习</strong>：RF属于Bagging思想，而GBDT是Boosting思想
<strong>偏差-方差权衡</strong>：RF不断的降低模型的方差，而GBDT不断的降低模型的偏差
<strong>并行性</strong>：RF的树可以并行生成，而GBDT只能顺序生成(需要等上一棵树完全生成)
<strong>最终结果</strong>：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合
<strong>数据敏感性</strong>：RF对<strong>异常值不敏</strong>感，而<strong>GBDT对异常值比较敏感</strong>
<strong>泛化能力</strong>：RF不易过拟合，而GBDT容易过拟合</li></ol></li><li>比较LR和GBDT，说说什么情景下GBDT不如LR<ol><li>先说说LR和GBDT的区别：<ol><li>LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程</li><li>GBDT 是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合；</li></ol></li><li><strong>高维稀疏特征</strong>的场景下，LR的效果一般会比GBDT好<ol><li>带正则化的线性模型不容易对稀疏特征过拟合。</li></ol></li></ol></li><li>GBDT的拟合值残差为什么用负梯度代替，而不是直接拟合残差<ol><li>使用<strong>残差拟合只是考虑到损失函数为平方损失的特殊情况</strong>，负梯度是更加广义上的拟合项，更具普适性。</li><li>代价函数除了loss还有正则项，梯度的本质也是一种方向导数，综合了各个方向（参数）的变化，选择了一个总是最优（下降最快）的方向；</li><li>最后目标函数可表达为由梯度构成，所以说成是拟合梯度，也好像不是不行</li></ol></li></ol><h1 id=总结>总结：</h1><ol><li>通过拟合上一个基学习器的负梯度来训练当前的基本学习器。负梯度就表示了哪些地方需要加大权重。常用损失函数为平方损失。<ol><li>然后为每一个基学习器赋予一个权重</li><li>都是回归树。</li><li>分类问题就用softmax</li></ol></li><li>而adaboost是为错误较大的样本赋予更多的权重。</li></ol><h1 id=xgboost>XGBOOST</h1><p><a href=https://zhuanlan.zhihu.com/p/46683728>https://zhuanlan.zhihu.com/p/46683728</a>
<a href=https://cloud.tencent.com/developer/article/1500914>https://cloud.tencent.com/developer/article/1500914</a>
<a href=https://github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning/XGBoost.md>https://github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning/XGBoost.md</a></p><p>公式推导：https://zhuanlan.zhihu.com/p/92837676</p><p>xgboost面经 <a href="https://mp.weixin.qq.com/s?__biz=MzI1MzY0MzE4Mg==&mid=2247485159&idx=1&sn=d429aac8370ca5127e1e786995d4e8ec&chksm=e9d01626dea79f30043ab80652c4a859760c1ebc0d602e58e13490bf525ad7608a9610495b3d&scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzI1MzY0MzE4Mg==&mid=2247485159&idx=1&sn=d429aac8370ca5127e1e786995d4e8ec&chksm=e9d01626dea79f30043ab80652c4a859760c1ebc0d602e58e13490bf525ad7608a9610495b3d&scene=21#wechat_redirect</a></p><p>eXtreme Gradient Boosting</p><p>注意：</p><ol><li>loss函数和F函数不一样。F就是机器学习学习出来的值。loss是一个函数用来求最小的值。</li><li>排序对单个特征进行排序。因为是二分类，所以可以用这种递增的方式进行求和。找出每个特征的最佳分裂。找出所有特征的最好分裂点（分裂后增益最大的特征及特征值）<ol><li>排序就是将特征排好序，不用每次都排序。</li></ol></li></ol><h2 id=和gbdt的不同>和GBDT的不同</h2><p><strong>基分类器</strong>：XGBoost的基分类器不仅支持CART决策树，还支持线性分类器，此时XGBoost相当于带L1和L2正则化项的LR回归（分类问题）或者线性回归（回归问题）。
<strong>导数信息</strong>：XGBoost对损失函数做了二阶泰勒展开，可以更为精准的逼近真实的损失函数，GBDT只用了一阶导数信息，并且XGBoost还支持自定义损失函数，只要损失函数一阶、二阶可导。
<strong>正则项</strong>：XGBoost的目标函数加了正则项， 相当于预剪枝，使得学习出来的模型更加不容易过拟合。
<strong>列抽样</strong>：XGBoost支持列采样，与随机森林类似，用于防止过拟合。
<strong>缺失值处理</strong>：对树中的每个非叶子结点，XGBoost可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。
<strong>并行化</strong>：注意不是树维度的并行，而是<strong>特征维度的并行</strong>。XGBoost预先将<strong>每个特征按特征值排好序，存储为块结构</strong>，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。
<strong>可扩展性</strong>：损失函数支持自定义，只需要新的损失函数二阶可导。还可以支持其他弱分类器。如LR。</p><h2 id=xgboost-并行训练>xgboost 并行训练</h2><ol><li>每个特征按特征值对样本进行预排序，并存储为block结构，在后面查找特征分割点时可以重复使用</li><li>而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。</li></ol><h2 id=xgboost-快>xgboost 快</h2><ol><li>分块并行</li><li>block 处理优化</li><li>CPU cache 命中优化。我不太懂。但我知道这个。</li></ol><h2 id=xgboost-如何处理过拟合>xgboost 如何处理过拟合</h2><ol><li>模型方面：<ol><li>增加正则</li><li>设置树深度</li></ol></li><li>训练方面<ol><li>设置增益阈值</li><li>设置样本权重和的阈值</li><li>设置步长</li></ol></li><li>数据方面：<ol><li>列抽样</li><li>子抽样</li></ol></li></ol><p><strong>调参:</strong></p><ol><li>直接控制模型复杂度：包括max_depth，min_child_weight，gamma 等参数</li><li>控制随机性：从而使得模型在训练时对于噪音不敏感。包括subsample，colsample_by树</li><li>减少learning rate，增加estimator参数。还有就是直接减小learning rate，但需要同时增加estimator 参数。</li></ol><h2 id=xgboost-处理缺失值>xgboost 处理缺失值</h2><ol><li>XGBoost 在构建树的节点过程中只考虑非缺失值的数据遍历。而为每个节点增加了一个缺省方向，分别枚举特征缺省的样本归为左右分支后的增益，选择增益最大的枚举项即为最优缺省方向</li><li>如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子结点。</li><li>树模型对缺失值的敏感度低，大部分时候可以在数据缺失时时使用。原因就是：一**棵树中每个结点在分裂时，寻找的是某个特征的最佳分裂点（特征值），**完全可以不考虑存在特征值缺失的样本</li></ol><h2 id=xgboost的优缺点>XGBoost​的优缺点</h2><p>优点</p><ol><li><strong>精度更高</strong>： GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。XGBoost 引入<strong>二阶导一方面是为了增加精度</strong>，另一方面也是为了<strong>能够自定义损失函数</strong>，二阶泰勒展开可以近似大量损失函数</li><li><strong>灵活性更强</strong>： GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，使用线性分类器的 XGBoost 相当于带 和 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。</li><li><strong>正则化</strong></li><li>**Shrinkage（缩减）**相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。</li><li><strong>列抽样</strong></li><li><strong>缺失值处理：</strong></li><li>XGBoost工具支持并行</li><li>可并行的近似算法：</li><li>所以XGBoost还提出了<strong>一种可并行的近似算法</strong>，用于高效地生成候选的分割点。常数个数的候选位置作为候选分裂点。</li></ol><h2 id=xgboost使用二阶泰勒展开的目的和优势>XGBoost使用二阶泰勒展开的目的和优势</h2><ol><li>XGBoost是以MSE为基础推导出来的，在MSE的情况下，XGBoost的目标函数展开就是一阶项+二阶项的形式。只要二阶可导即可，增强了模型的扩展性。而不需要对损失函数做出过高的要求。</li><li><strong>二阶信息</strong>能够让梯度收敛的更快，拟牛顿法比SGD收敛更快，一阶信息描述梯度变化方向，二阶信息可以描述梯度变化方向是如何变化的。所以二阶信息可以对一阶信息做出指引</li></ol><h2 id=xgboost-损失函数>xgboost 损失函数</h2><p>$Obj^t = \sum_i^n[g_if_t(x_i) + 1/2h_i(f_t(x_i))^2] + \omega(cart)$
\omega是正则项。</p><p>体来看，XGBoost 在原理方面的改进主要就是在损失函数上作文章。
一是在原损失函数的基础上添加了<strong>正则化项</strong>产生了新的目标函数，这类似于对每棵树进行了剪枝并限制了叶结点上的分数来防止过拟合。
二是对目标函数进行<strong>二阶泰勒展开</strong>，以类似牛顿法的方式来进行优化（事实上早在 Friedman, J., Hastie, T. and Tibshirani, R., 1999 中就已有类似方案，即利用二阶导信息来最小化目标函数，陈天奇在论文中也提到了这一点）。
XGBoost 之所以快的一大原因是在工程上实现了<strong>Column Block 方法</strong>，使得并行训练成为了可能。</p><p>由于已经预先保存为block 结构，所以在对叶结点进行分裂时，每个特征的增益计算就可以开多线程进行，训练速度也由此提升了很多。而且这种 block 结构也支持列抽样，只要每次从所有 block 特征中选择一个子集作为候选分裂特征就可以了，据我的使用经验，列抽样大部分时候都比行抽样的效果好。</p><p>最后总结一下 XGBoost 与传统 GBDT 的不同之处：</p><p>传统 GBDT 在优化时只用到一阶导数信息，XGBoost <strong>则对目标函数进行了二阶泰勒展开</strong>，同时用到了一阶和二阶导数。另外 <strong>XGBoost 工具支持自定义损失函数，只要函数可一阶和二阶求导。</strong>
XGBoost 在损失函数中加入了<strong>正则化项</strong>，用于控制模型的复杂度，防止过拟合，<strong>从而提高模型的泛化能力</strong>。
传统 GBDT 采用的是均方误差作为内部分裂的增益计算指标（因为用的都是回归树），<strong>而 XGBoost 使用的是经过优化推导后的式子</strong>，即式 [公式] 。
<strong>XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算量</strong>，这也是 XGBoost 异于传统 GBDT 的一个特性。
XGBoost 添加了对稀疏数据的支持，**在计算分裂增益时不会考虑带有缺失值的样本，这样就减少了时间开销。**在分裂点确定了之后，将带有缺失值的样本分别放在左子树和右子树，比较两者分裂增益，选择增益较大的那一边作为默认分裂方向。
<strong>并行化处理</strong>：由于 Boosting 本身的特性，无法像随机森林那样树与树之间的并行化。**XGBoost 的并行主要体现在特征粒度上，在对结点进行分裂时，由于已预先对特征排序并保存为block 结构，每个特征的增益计算就可以开多线程进行，**极大提升了训练速度。
传统 GBDT 在损失不再减少时会停止分裂，这是一种预剪枝的贪心策略，容易欠拟合。<strong>XGBoost采用的是后剪枝的策略，<strong>先分裂到指定的</strong>最大深度 (max_depth) 再进行剪枝</strong>。而且和一般的后剪枝不同， XGBoost 的后剪枝是不需要验证集的。 不过我并不觉得这是“纯粹”的后剪枝，<strong>因为一般还是要预先限制最大深度的呵呵。</strong>
​</p><p>说了这么多 XGBoost 的优点，其当然也有不完美之处，因为要在训练之前先对每个特征进行预排序并将结果存储起来，<strong>对于空间消耗较大</strong>。另外虽然相比传统的 GBDT 速度是快了很多，但和后来的 LightGBM 比起来还是慢了不少，不知以后还会不会出现更加快的 Boosting 实现。</p><p>xgboost优点：</p><ol><li>传统 GBDT 在优化时只用到一阶导数信息，XGBoost 则对目标函数进行了二阶泰勒展开，同时用到了一阶和二阶导数</li><li>XGBoost 在损失函数中加入了正则化项，用于控制模型的复杂度，防止过拟合，</li><li>XGBoost 的并行主要体现在特征粒度上，在对结点进行分裂时，由于已预先对特征排序并保存为block 结构，每个特征的增益计算就可以开多线程进行，极大提升了训练速度。</li></ol><p>空间消耗很大。</p><h1 id=lightgbm>lightgbm</h1><p><a href=https://zhuanlan.zhihu.com/p/99069186>https://zhuanlan.zhihu.com/p/99069186</a></p><p>LightGBM（Light Gradient Boosting Machine）是一个实现<strong>GBDT算法的框架</strong>，支持高效率的并行训练，并且具有更快的训练速度、更低的内存消耗、更好的准确率、支持分布式可以快速处理海量数据等优点。</p><h2 id=xgboost的缺点>XGBoost的缺点</h2><p>它是基于预排序方法的决策树算法。这种构建决策树的算法基本思想是：首先，对所有特征都按照特征的数值进行预排序。</p><ol><li>首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如，为了后续快速的计算分割点，保存了排序后的索引），这就需要消耗训练数据两倍的内存。</li><li>其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。</li><li>最后，对cache优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。</li></ol><h2 id=lightgbm的优化>LightGBM的优化</h2><ol><li>基于Histogram的决策树算法。</li><li>单边梯度采样 Gradient-based One-Side Sampling(GOSS)：使用GOSS可以减少大量只具有小梯度的数据实例，这样在计算信息增益的时候只利用剩下的具有高梯度的数据就可以了，相比XGBoost遍历所有特征值节省了不少时间和空间上的开销。</li><li>互斥特征捆绑 Exclusive Feature Bundling(EFB)：使用EFB可以将许多互斥的特征绑定为一个特征，这样达到了降维的目的。</li><li>带深度限制的Leaf-wise的叶子生长策略：大多数GBDT工具使用低效的按层生长 (level-wise) 的决策树生长策略，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销。实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。LightGBM使用了带有深度限制的按叶子生长 (leaf-wise) 算法。</li><li>直接支持类别特征(Categorical Feature)</li><li>支持高效并行</li><li>Cache命中率优化</li></ol><p>优点：
速度更快:</p><ol><li>速度更快。比如使用直方图减少内存消耗。降低时间复杂度。</li><li>使用单边梯度算法，减少了大量的计算。GOSS在进行数据采样的时候只保留了梯度较大的数据</li><li>基于leaf-wise算法的增长策略。减少了很多不必要的计算量</li><li>采用优化后的特征并行，数据并行方法加速计算。还可以使用投票并行</li><li>对缓存进行了优化。
内存更小：</li><li>使用直方图将特征转为bin，减少了内存消耗</li></ol><p>缺点：</p><ol><li>可能决策树较深，产生过拟合。在leaf_wise熵增加了一个最大深度限制，防止过拟合</li><li>不断降低偏差，所以对噪点铭感。</li><li>在寻找最优解时，依据的是最优切分变量，没有将最优解是全部特征的综合这一理念考虑进去；</li></ol><h2 id=xgboost和lgbm的对比>xgboost和lgbm的对比</h2><ol><li><strong>树生长策略</strong><ol><li>XGB采用level-wise的分裂策略：XGB对每一层所有节点做无差别分裂。这一层的节点都分裂。</li><li>LGB采用leaf-wise的分裂策略：Leaf-wise是在所有叶子节点中选取分裂收益最大的节点进行的，但是很容易出现过拟合问题，所以需要对最大深度做限制 。</li></ol></li><li><strong>分割点查找算法</strong><ol><li>XGB使用特征预排序算法，LGB使用基于直方图的切分点算法，<ol><li>减少内存占用，</li><li>计算效率提高</li></ol></li></ol></li><li><strong>直方图算法</strong><ol><li>XGB 在每一层都动态构建直方图</li><li>LGB中对每个特征都有一个直方图，</li></ol></li><li><strong>支持离散变量</strong><ol><li>XGB无法直接输入类别型变量因此需要事先对类别型变量进行编码（例如独热编码），</li><li>LGB可以直接处理类别型变量。</li></ol></li><li><strong>缓存命中率</strong><ol><li>XGB用block结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的，这将导致非连续的内存访问，</li><li>LGB是基于直方图分裂特征的，梯度信息都存储在一个个bin中，所以访问梯度是连续的，缓存命中率高。</li></ol></li><li><strong>并行策略-特征并行</strong><ol><li>XGB每个worker节点中仅有部分的列数据，也就是垂直切分，每个worker寻找局部最佳切分点，worker之间相互通信</li><li>LGB特征并行的前提是每个worker留有一份完整的数据集，但是每个worker仅在特征子集上进行最佳切分点的寻找</li></ol></li><li><strong>并行策略-数据并行</strong><ol><li>LGB中先对数据水平切分，每个worker上的数据先建立起局部的直方图，然后合并成全局的直方图，采用直方图相减的方式，先计算样本量少的节点的样本索引，然后直接相减得到另一子节点的样本索引，这个直方图算法使得worker间的通信成本降低一倍，因为只用通信以此样本量少的节点</li></ol></li><li>最后两个了解不多。只知道通过并行策略，可以减少线程之间的通信成本。</li></ol><h1 id=重学各个公式>重学各个公式</h1><p>adaboost:
西瓜书
<a href=https://zhuanlan.zhihu.com/p/274517564>https://zhuanlan.zhihu.com/p/274517564</a>
<a href=https://zhuanlan.zhihu.com/p/105515064>https://zhuanlan.zhihu.com/p/105515064</a></p><p>gbdt：
<a href=https://zhuanlan.zhihu.com/p/107751279>https://zhuanlan.zhihu.com/p/107751279</a>
<a href=https://zhuanlan.zhihu.com/p/38329631>https://zhuanlan.zhihu.com/p/38329631</a></p></div><footer class=post-footer><div class=addthis_inline_share_toolbox></div><div class=post-nav><div class=article-copyright><div class=article-copyright-img><img src=/img/qq_qrcode.png width=129px height=129px><div style=text-align:center>QQ扫一扫交流</div></div><div class=article-copyright-info><p><span>声明：</span>Bagging和Boosting,adboost,xgboost</p><p style=word-break:break-all><span>链接：</span>
https://yanyulinxi.github.io/post/study/deeplearning/bagging%E5%92%8Cboostingadaboostxgboost/</p><p><span>作者：</span>阳阳</p><p><span>邮箱：</span>yanyulinxi@qq.com</p><p><span>声明： </span>本博客文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/ target=_blank style=text-decoration:underline>CC BY-NC-SA 3.0</a>许可协议，转载请注明出处！</p></div></div><div class=clear></div></div><div class=reward-qr-info><div>创作实属不易，如有帮助，那就打赏博主些许茶钱吧 ^_^</div><button id=rewardButton disable=enable onclick="var qr=document.getElementById('QR');qr.style.display==='none'?qr.style.display='block':qr.style.display='none'">
<span>赏</span></button><div id=QR style=display:none><div id=wechat style=display:inline-block><img id=wechat_qr src=/img/wechat-pay.png alt="WeChat Pay"><p>微信打赏</p></div><div id=alipay style=display:inline-block><img id=alipay_qr src=/img/ali-pay.png alt=Alipay><p>支付宝打赏</p></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=https://yanyulinxi.github.io/post/paperreading/simclr-a-simple-framework-for-contrastive-learning-of-visual-representations/ rel=next title="SimCLR a Simple Framework for Contrastive Learning of Visual Representations"><i class="fa fa-chevron-left"></i>SimCLR a Simple Framework for Contrastive Learning of Visual Representations</a></div><div class="post-nav-prev post-nav-item"><a href=https://yanyulinxi.github.io/post/study/deeplearning/jcs%E9%9A%8F%E5%8D%B3%E6%A3%AE%E6%9E%97%E5%92%8C%E5%86%B3%E7%AD%96%E6%A0%91/ rel=prev title=随即森林和决策树>随即森林和决策树
<i class="fa fa-chevron-right"></i></a></div></div><div id=wcomments></div></footer></article></section></div></div><div class=sidebar-toggle><div class=sidebar-toggle-line-wrap><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id=sidebar class=sidebar><div class=sidebar-inner><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image src=/img/linxi_icon.png alt=阳阳><p class=site-author-name itemprop=name>阳阳</p><p class="site-description motion-element" itemprop=description>再平凡的人也有属于他自己的梦想!</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href=/post/><span class=site-state-item-count>112</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>6</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>31</span>
<span class=site-state-item-name>标签</span></a></div></nav><div class="links-of-author motion-element"><span class=links-of-author-item><a href=https://github.com/yanyuLinxi target=_blank title=GitHub><i class="fa fa-fw fa-github"></i>GitHub</a></span>
<span class=links-of-author-item><a href=https://space.bilibili.com/19237450 target=_blank title=哔哩哔哩><i class="fa fa-fw fa-globe"></i>哔哩哔哩</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class=links-of-blogroll-title><i class="fa fa-fw fa-globe"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://www.liaoxuefeng.com/ title=廖雪峰 target=_blank>廖雪峰</a></li></ul></div><div class="tagcloud-of-blogroll motion-element tagcloud-of-blogroll-inline"><div class=tagcloud-of-blogroll-title><i class="fa fa-fw fa-tags"></i>标签云</div><ul class=tagcloud-of-blogroll-list><li class=tagcloud-of-blogroll-item><a href=/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0>论文阅读笔记</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E5%BC%82%E5%B8%B8%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90>异常行为分析</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/python%E7%9B%B8%E5%85%B3%E5%BA%93%E5%AD%A6%E4%B9%A0>Python相关库学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E5%BA%93>机器学习相关库</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/java%E5%AD%A6%E4%B9%A0>Java学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/insider-threat>Insider threat</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/leetcode%E5%AD%A6%E4%B9%A0>Leetcode学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E7%BE%8E%E9%A3%9F%E7%82%B9%E8%AF%84>美食点评</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/rnn>Rnn</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0>科研学习笔记</a></li></ul></div></section></div></aside></div></main><footer id=footer class=footer><div class=footer-inner><div class=copyright><span class=copyright-year>&copy; 2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span><span class=copyright-author>阳阳的人间旅游日记</span></div><div class=powered-info><span class=powered-by>Powered by - <a class=powered-link href=//gohugo.io target=_blank title=hugo>Hugo v0.81.0</a></span>
<span class=separator-line>/</span>
<span class=theme-info>Theme by - <a class=powered-link href=//github.com/elkan1788/hugo-theme-next target=_blank>NexT</a></span></div><div class=vistor-info><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span class=site-uv><i class="fa fa-user"></i><span class=busuanzi-value id=busuanzi_value_site_uv></span></span><span class=separator-line>/</span>
<span class=site-pv><i class="fa fa-eye"></i><span class=busuanzi-value id=busuanzi_value_site_pv></span></span></div><div class=license-info><span class=storage-info>Storage by
<a href style=font-weight:700 target=_blank></a></span><span class=separator-line>/</span>
<span class=license-num><a href target=_blank></a></span></div></div></footer><div class=back-to-top><i class="fa fa-arrow-up"></i><span id=scrollpercent><span>0</span>%</span></div></div><script type=text/javascript src=//cdn.bootcdn.net/ajax/libs/jquery/2.1.4/jquery.min.js></script><script type=text/javascript src=/js/search.js></script><script type=text/javascript src=/js/affix.js></script><script type=text/javascript src=/js/scrollspy.js></script><script type=text/javascript>function detectIE(){var a=window.navigator.userAgent,b=a.indexOf('MSIE '),c=a.indexOf('Trident/'),d=a.indexOf('Edge/');return b>0||c>0||d>0?-1:1}function getCntViewHeight(){var b=$('#content').height(),a=$(window).height(),c=b>a?b-a:$(document).height()-a;return c}function getScrollbarWidth(){var a=$('<div />').addClass('scrollbar-measure').prependTo('body'),b=a[0],c=b.offsetWidth-b.clientWidth;return a.remove(),c}function registerBackTop(){var b=50,a=$('.back-to-top');$(window).on('scroll',function(){var d,e,f,c,g;a.toggleClass('back-to-top-on',window.pageYOffset>b),d=$(window).scrollTop(),e=getCntViewHeight(),f=d/e,c=Math.round(f*100),g=c>100?100:c,$('#scrollpercent>span').html(g)}),a.on('click',function(){$("html,body").animate({scrollTop:0,screenLeft:0},800)})}function initScrollSpy(){var a='.post-toc',d=$(a),b='.active-current';d.on('activate.bs.scrollspy',function(){var b=$(a+' .active').last();c(),b.addClass('active-current')}).on('clear.bs.scrollspy',c),$('body').scrollspy({target:a});function c(){$(a+' '+b).removeClass(b.substring(1))}}function initAffix(){var a=$('.header-inner').height(),b=parseInt($('.main').css('padding-bottom'),10),c=a+10;$('.sidebar-inner').affix({offset:{top:c,bottom:b}}),$(document).on('affixed.bs.affix',function(){updateTOCHeight(document.body.clientHeight-100)})}function initTOCDimension(){var a,b;$(window).on('resize',function(){a&&clearTimeout(a),a=setTimeout(function(){var a=document.body.clientHeight-100;updateTOCHeight(a)},0)}),updateTOCHeight(document.body.clientHeight-100),b=getScrollbarWidth(),$('.post-toc').css('width','calc(100% + '+b+'px)')}function updateTOCHeight(a){a=a||'auto',$('.post-toc').css('max-height',a)}$(function(){var b=$('.header-inner').height()+10,c,d,a,e;$('#sidebar').css({'margin-top':b}).show(),c=parseInt($('#sidebar').css('margin-top')),d=parseInt($('.sidebar-inner').css('height')),a=c+d,e=$('.content-wrap').height(),e<a&&$('.content-wrap').css('min-height',a),$('.site-nav-toggle').on('click',function(){var a=$('.site-nav'),e=$('.toggle'),b='site-nav-on',f='toggle-close',c=a.hasClass(b),g=c?'slideUp':'slideDown',d=c?'removeClass':'addClass';a.stop()[g]('normal',function(){a[d](b),e[d](f)})}),registerBackTop(),initScrollSpy(),initAffix(),initTOCDimension(),$('.sidebar-nav-toc').click(function(){$(this).addClass('sidebar-nav-active'),$(this).next().removeClass('sidebar-nav-active'),$('.'+$(this).next().attr('data-target')).toggle(500),$('.'+$(this).attr('data-target')).toggle(500)}),$('.sidebar-nav-overview').click(function(){$(this).addClass('sidebar-nav-active'),$(this).prev().removeClass('sidebar-nav-active'),$('.'+$(this).prev().attr('data-target')).toggle(500),$('.'+$(this).attr('data-target')).toggle(500)})})</script><script src=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.js></script><script type=text/javascript>$(function(){$('.post-body').viewer()})</script><script type=text/javascript>$(function(){detectIE()>0?$.getScript(document.location.protocol+'//cdn.jsdelivr.net/npm/@waline/client/dist/Waline.min.js',function(){new Waline({el:'#wcomments',visitor:!0,avatar:'wavatar',avatarCDN:'https://sdn.geekzu.org/avatar/',avatarForce:!1,wordLimit:'200',placeholder:' 欢迎留下您的宝贵建议，请填写您的昵称和邮箱便于后续交流. ^_^ ',requiredFields:['nick','mail'],serverURL:"Your WalineSerURL",lang:"zh-cn"})}):$('#wcomments').html('抱歉，Waline插件不支持IE或Edge，建议使用Chrome浏览器。')})</script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=Your%20AddthisId"></script><script>(function(){var a=document.createElement('script'),c=window.location.protocol.split(':')[0],b;c==='https'?a.src='https://zz.bdstatic.com/linksubmit/push.js':a.src='http://push.zhanzhang.baidu.com/push.js',b=document.getElementsByTagName("script")[0],b.parentNode.insertBefore(a,b)})()</script></body></html>