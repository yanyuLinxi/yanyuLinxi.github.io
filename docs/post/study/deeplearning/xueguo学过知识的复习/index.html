<!doctype html><html lang=zh-cn dir=content/zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=content-security-policy content="upgrade-insecure-requests"><title>Boost复习 - 阳阳的人间旅游日记</title><meta name=keywords content="博客,程序员,思考,读书,笔记,技术,分享"><meta name=author content="阳阳"><meta property="og:title" content="Boost复习"><meta property="og:site_name" content="阳阳的人间旅游日记"><meta property="og:image" content="/img/author.jpg"><meta name=title content="Boost复习 - 阳阳的人间旅游日记"><meta name=description content="欢迎来到临溪的博客站，个人主要专注于机器学习、深度学习的相关研究。在这里分享自己的学习心得。"><link rel="shortcut icon" href=/img/favicon.ico><link rel=apple-touch-icon href=/img/apple-touch-icon.png><link rel=apple-touch-icon-precomposed href=/img/apple-touch-icon.png><link href=//cdn.bootcdn.net/ajax/libs/font-awesome/4.6.2/css/font-awesome.min.css rel=stylesheet type=text/css><link href=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.css rel=stylesheet><link href=/css/main.css rel=stylesheet type=text/css><link href=/css/syntax.css rel=stylesheet type=text/css></head><body itemscope itemtype=http://schema.org/WebPage lang=zh-hans><div class="container one-collumn sidebar-position-left page-home"><div class=headband></div><header id=header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle role=button style=opacity:1;top:0><span class=toggle-line></span><span class=toggle-line></span><span class=toggle-line></span></div></div><div class=site-meta><div class=multi-lang-switch><i class="fa fa-fw fa-language" style=margin-right:5px></i><a class=lang-link id=zh-cn href=#>中文</a></div><div class=custom-logo-site-title><a href=/ class=brand rel=start><span class=logo-line-before><i></i></span><span class=site-title>阳阳的人间旅游日记</span>
<span class=logo-line-after><i></i></span></a></div><p class=site-subtitle>让我们消除隔阂的，不是无所不知的脑袋，而是手拉手，坚决不放弃的那颗心</p></div><div class=site-nav-right><div class="toggle popup-trigger" style=opacity:1;top:0><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul id=menu class=menu><li class=menu-item><a href=/ rel=section><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class=menu-item><a href=/post rel=section><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class=menu-item><a href=/about.html rel=section><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于我</a></li><li class=menu-item><a href=/404.html rel=section><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href=javascript:; class=popup-trigger><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class=site-search><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class=search-icon><i class="fa fa-search"></i></span><span class=popup-btn-close><i class="fa fa-times-circle"></i></span><div class=local-search-input-wrapper><input autocomplete=off placeholder=搜索关键字... spellcheck=false type=text id=local-search-input autocapitalize=none autocorrect=off></div></div><div id=local-search-result></div></div></div></nav></div></header><main id=main class=main><div class=main-inner><div class=content-wrap><div id=content class=content><section id=posts class=posts-expand><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><header class=post-header><h1 class=post-title itemprop="name headline"><a class=post-title-link href=https://yanyulinxi.github.io/post/study/deeplearning/xueguo%E5%AD%A6%E8%BF%87%E7%9F%A5%E8%AF%86%E7%9A%84%E5%A4%8D%E4%B9%A0/ itemprop=url>Boost复习</a></h1><div class=post-meta><span class=post-time><i class="fa fa-calendar-o fa-fw"></i><span class=post-meta-item-text>时间：</span>
<time itemprop=dateCreated datetime=2016-03-22T13:04:35+08:00 content="2021-12-26">2021-12-26</time></span>
<span>|
<i class="fa fa-file-word-o fa-fw"></i><span class=post-meta-item-text>字数：</span>
<span class=leancloud-world-count>4375 字</span></span>
<span>|
<i class="fa fa-eye fa-fw"></i><span class=post-meta-item-text>阅读：</span>
<span class=leancloud-view-count>9分钟</span></span>
<span id=/post/study/deeplearning/xueguo%E5%AD%A6%E8%BF%87%E7%9F%A5%E8%AF%86%E7%9A%84%E5%A4%8D%E4%B9%A0/ class=leancloud_visitors data-flag-title=Boost复习>|
<i class="fa fa-binoculars fa-fw"></i><span class=post-meta-item-text>阅读次数：</span>
<span class=leancloud-visitors-count></span></span></div></header><div class=post-body itemprop=articleBody><h1 id=boost>Boost</h1><p>boost是一种加法模型，通过串联的叠加多个弱学习器，每个学习器来学习上一个学习器的偏差，来得到一个强学习器。</p><ol><li>boost和bagging的区别</li></ol><ul><li>bagging的弱学习器使用部分特征。boost使用全部特征</li><li>bagging可以并行训练。boost只能串行训练</li><li>bagging解决方差，boost解决偏差。</li></ul><h2 id=提问>提问：</h2><ol><li><p>boost描述不精准（方法，目的）
boost通过层层叠加多个基分类器，聚焦错误的样本，来减少偏差。
bagging使用多个基分类器，多次采样集体投票，减小方差。</p></li><li><p>boost、bagging区别
从头想到尾，
输入上，boost使用全部的数据集。bagging使用部分数据集
运行上，boost串联运行，bagging支持并行
运行方式上，bagging均匀采样样本比例相同，boost对错误的样本赋予更大的权重。
结果（基学习器组合）bagging每个基学习器权重相等。boost分类误差小的基学习器拥有更大的权重。
目的上：bagging关注于降低方差。boost关注于降低偏差</p></li></ol><h1 id=adaboost>adaboost</h1><p>将若干个弱学习器通过加法模型连接起来，每一个弱学习器中，加大判断错误的样本的权重，减少判断正确的样本的权重。采用指数函数：$f(x) = e^{-yh(x)}$作为损失函数。</p><p>算法流程：
1). 初始化第一个基学习器
2). 对于1-T个基学习器
3). 计算误差率$\epsilon$
4). 计算基学习器在加法模型中的权重
5). 计算样本的分布
6). end For
7). $f(x) = sign(\sum \alpha h(x))$</p><ol><li>为什么采用指数函数</li></ol><p>指数函数是-1到1的替代性函数。
指数函数拥有连续可微等优秀的特性。</p><ol start=2><li>如何分配基学习器的权重？</li></ol><p>$\alpha = \frac{1}{2}ln\frac{1-\epsilon}{\epsilon}$
通过公式$f(x) = e^{-y \alpha h(x)}$求得。</p><ol start=3><li>如何分配样本的权重？</li></ol><p>$\D_t = \frac{\D_{t-1}exp(-y \alpha_t h_t(x))}{Z_t}$ 其中$Z_t$是归一化系数，值为$\sum exp(-y \alpha_t h_t(x))$</p><h2 id=提问-1>提问：</h2><ol><li><p>优缺点
优缺点就是boost的优缺点。
优点：1. 能够叠加弱学习器迅速构造出强学习器。
缺点：1. 异常样本的权重会不停的增加，影响最终的表现。对异常样本敏感。容易过拟合。
2. 无法并行运行，运行比较慢。</p></li><li><p>描述</p></li></ol><h1 id=gbdt>GBDT</h1><p>GBDT就是梯度提升树，和adaboost一样，通过加法模型串联多个弱学习器达到强学习器。不同的是，GBDT通过拟合负梯度来纠正偏差。</p><p>算法步骤：</p><ol><li>初始化第一个基学习器f_0。初始化方式根据损失函数来决定。</li><li>for 1 to T:</li><li><pre><code>计算f_{t-1}的负梯度 f_{t-1}'
</code></pre></li><li><pre><code>$\alpha = min_\alpha L(f_{t-1}', f_t(\alpha))$
</code></pre></li><li><pre><code>$\rho = min_\rho L(y, f_{t-1} + \rho f_t)$
</code></pre></li><li><pre><code>更新$f(x) = f_{t-1} + \rho f_t(\alpha)$
</code></pre></li><li>end For</li></ol><ol><li>为什么要拟合负梯度</li></ol><p>根据提升树的公式 $L(y, f(x)) = L(y, f_{t-1}(x) + f_t(x))$.对f_{t-1}(x)进行泰勒展开。即 $L(y, f(x)) = L(y, f_{t-1}(x)) + (梯度）f_t(x)$所以可以看到这里完全可以让f_t(x)去拟合负梯度。就可以降低损失，来达到训练效果。负梯度中错误的样本梯度大，正确的样本梯度小，所以也是在去加大错误样本的权重。</p><p>负梯度的引入，可以支持更多的损失函数。</p><p>残差是梯度的特殊情况，拟合负梯度效果更好。</p><ol><li>$\alpha是什么 \rho是什么$</li></ol><p>$\alpha$是基学习器拟合负梯度的参数。$\rho$是基学习器的步长。负梯度仅仅指明了方向，但是没有指明移动多长距离。</p><ol start=2><li>如何优化这个算法？</li></ol><p>对于基学习器$f_t$,其可以表示为叶子节点的值$f_t = \sum_{x\in R_j} b_j$, $b_j$为叶子节点学习的值，在CART中一般为叶子节点的均值。</p><p>所以$\rho = min_\rho L(y, f_{t-1}+ \sum_j \rho b_j$</p><p>另$\gamma_j = \rho b_j$ 就可以一次最小运算得到$\gamma$</p><p>最终公式变为 $f(x) = f_{t-1} + \sum_j \gamma_j$</p><h2 id=提问-2>提问</h2><ol><li><p>描述</p></li><li><p>优缺点</p></li><li><p>举例子</p></li></ol><p>如果需要举例子，可以使用对数损失函数L = log(1+e^{-2yF})来举例。</p><h1 id=xgboost>XGBoost</h1><p>xgboost属于一种梯度提升树，相比于GBDT，它在损失函数中引入了二阶导和正则化。并且使用了新的增益计算函数。在运行过程中，通过预排序和多线程特征并行查找来加快速度，引入了近似分裂算法来减少决策树特征选择的复杂度，引入了列抽样、shrinkage等避免过拟合的方法。</p><p>算法流程</p><ol><li>如何使用二阶导信息的？</li></ol><p>对于提升树的损失函数$L(y, f) = L(y, f_{t-1} + f_t) + \Omega(f_t)$其中$\Omega(f_t) = \gamma j + \frac{1}{2} \lambda \sum b_j ^2$，就是通过正则化方式，尽量减少叶子结点的数量和叶子节点的权重的L2正则化。其中$b_j$就是叶子节点的预测值。对损失函数进行f_{t-1}的二阶泰勒展开。</p><p>$$
L(y, f) = \sum_i^{N}(L(y, f_{t-1}) + g_i f_t + \frac{1}{2} h f_t^2) + \gamma j + \frac{1}{2} \lambda \sum b_j ^2
\<br>L(y, f) = \sum_j L(y, f_{t-1}) + \sum_{x\in R_j}g_i b_j + \frac{1}{2} \sum_{x\in R_j}h_i b_j^2 + \gamma j + \frac{1}{2} \lambda \sum b_j ^2
\<br>L(y, f) = \sum_j L(y, f_{t-1}) + G_j b_j + \frac{1}{2}(H_j+\lambda) b_j^2 + \gamma j
$$
其中g为一阶导，h为二阶导。将公式对$b_j$求导得到$b_j = -\sum_j \frac{G_j}{H_j+\lambda}$。这个值为叶子节点的权重。</p><p>将$b_j$带回式子得到<strong>损失函数</strong>$L = \sum_j -\frac{1}{2}\frac{G_j^2}{H_j+\lambda} +\gamma j$
所以<strong>信息增益</strong>计算方式为
$$ Gain = L - (L_{left} +L_{right})
\<br>= \frac{G_{left}^2}{H_{left}+\lambda} + \frac{G_{right}^2}{H_{right}+\lambda} - \frac{(G_{left}+G_{right})^2}{H_{left}+H_{right}+\lambda} - \gamma
$$</p><ol start=2><li>为何引入二阶导信息？</li></ol><ol><li>二阶导可以让结果更加精准。一阶导只提供了梯度方向，没有提供下降多少，二阶导提供了一阶导的变化趋势。可以使下降结果更加精确</li><li>二阶导的引入，更方便的支持扩展性。只要二阶可导的函数都可以作为损失函数。</li></ol><ol start=3><li>新的增益如何计算的？</li></ol><ol><li>使用Gain = L - L_left - L_right. $L = \frac{G^2}{H+\lambda}$</li></ol><ol start=4><li>如何剪枝</li></ol><ol><li>后剪枝。生长到指定的max_depth，然后从底到上进行剪枝，当某个节点以后的增益都小于gamma，则剪掉。一定程度上避免了欠拟合</li><li>达到max_depth停止。</li><li>当叶子权重小于最小的叶子权重时，避免分裂。</li></ol><ol start=5><li>如何避免过拟合</li></ol><ol><li>增加正则化项。控制复杂度。</li><li>修改min_child_weight，分裂gamma等</li><li>进行列抽样。</li><li>shrinkage。缩小当前叶子节点的权重，为后面的树留出训练空间。</li><li>子采样。每轮计算不适用全部样本。</li></ol><p>调参:</p><ol><li><p>调整控制模型复杂度的参数：max_depth, min_child_weight, gamma</p></li><li><p>调整随机性的参数，subsample。colsample（列采样）</p></li><li><p>调整模型学习率，基学习器的数量等。</p></li><li><p>如何处理缺失值</p></li></ol><p>1） 训练时，使用没有缺失值的部分进行训练。缺失值则依次放入左右叶子节点，计算最大增益。作为默认的缺省方向。
2） 预测时出现缺失值，默认送入右节点。
树模型对缺失值敏感度低。样本缺失不影响最优特征的选择，所以树模型对缺失值敏感度低</p><ol start=7><li><p>优点
（思考时和gbdt做对比）</p></li><li><p>二阶导，精准度更高。且可扩展性更好</p></li><li><p>正则化可以更好的避免过拟合。降低泛化误差。shrinkage缩减，削弱每棵树的影响，让后面有更大的学习空间。</p></li><li><p>支持列抽样</p></li><li><p>多线程特征查找速度更快。将特征预排序存为block。然后对每个特征的最有特征的查找使用多线程。</p></li><li><p>近似分裂算法，降低特征查找时的时间复杂度。提供了可并行的近似算法，高效的生成候选分割点。</p></li><li><p>缺失值的处理。</p></li><li><p>缺点</p></li><li><p>预排序消耗的时间空间大。</p></li><li><p>在寻找分裂点的时候，仍然需要遍历所有的数据。</p></li><li><p>对比</p></li><li><p>多线程特征并行查找如何实现的？
对特征进行预排序，然后存为blcok结构。每次分裂的时候。启用多个线程对多个特征的最优分裂点进行查找。</p></li></ol><h2 id=提问-3>提问</h2><h1 id=lightgbm>LightGBM</h1><p>LightGBM是GBDT的一个算法框架。为了解决GBDT速度慢、内存占用大的缺点。</p><p>有以下的改进：
1）基于Histogram的决策树算法
基于直方图的算法。特征划分多个区间，每个区间成为一个箱子。区间中的值更新为箱子的值。
优点：</p><ol><li>内存占用小</li><li>计算代价小。相比xgb不需要遍历一个特征值就需要计算一次分裂的增益，只需要计算k次(k为箱子的个数)</li><li>子直方图可以用父直方图做差得到。速度上加快。</li></ol><p>2）单边梯度采样 Gradient-based One-Side Sampling(GOSS)</p><p>减少样本的角度出发。排除大部分小梯度的<strong>样本</strong>。为了保证分布，按照梯度大小排序，选取最大的a个数，和小特征中的b个数。b个数乘以权重(1-a)/b。然后用a+b来计算信息增益。就减少了1-a-b个数。</p><p>3）互斥特征捆绑 Exclusive Feature Bundling(EFB)</p><p>如果将部分特征进行捆绑，可以降低特征数量。将互斥的特征进行捆绑（互斥即不同时为0，这样两个值可以叠加。如果同时为0）。且允许一小部分的冲突。可以得到更少的绑定特征。特征如何捆绑，A和B的取值空间叠加。</p><p>4）带深度限制的Leaf-wise的叶子生长策略</p><p>level-wise每次分裂一层的叶子。不容易过拟合，但是比较低效。
leaf-wise，找到分裂增益最大的叶子节点进行分裂，降低的误差更多，但是更容易过拟合。所以限制高度。</p><p>5）直接支持类别特征(Categorical Feature)</p><p>实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，通过 one-hot 编码，转化到多维的0/1特征，降低了空间和时间的效率。</p><p>但我们知道对于决策树来说并不推荐使用 one-hot 编码，尤其当类别特征中类别个数很多的情况下，会存在以下问题
1） 会产生样本切分不平衡问题，导致切分增益非常小（即浪费了这个特征）。即这个特征没有用。
2） 会影响决策树的学习。因为就算可以对这个类别特征进行切分，独热编码也会把数据切分到很多零散的小空间上，</p><p>LightGBM是第一个直接支持类别特征的GBDT工具。</p><p>6）支持高效并行</p><p>特征并行、数据并行、投票并行</p><p>7）Cache命中率优化</p><p>大概知道。并不很了解。</p><p>优点：
速度更快:</p><ol><li>速度更快。比如使用直方图减少内存消耗。降低时间复杂度。</li><li>使用单边梯度算法，减少了大量的计算。GOSS在进行数据采样的时候只保留了梯度较大的数据</li><li>基于leaf-wise算法的增长策略。减少了很多不必要的计算量</li><li>采用优化后的特征并行，数据并行方法加速计算。还可以使用投票并行</li><li>对缓存进行了优化。
内存更小：</li><li>使用直方图将特征转为bin，减少了内存消耗</li></ol><p>缺点：</p><ol><li>可能决策树较深，产生过拟合。在leaf_wise熵增加了一个最大深度限制，防止过拟合</li><li>不断降低偏差，所以对噪点铭感。</li><li>在寻找最优解时，依据的是最优切分变量，没有将最优解是全部特征的综合这一理念考虑进去；</li></ol><h1 id=决策树>决策树</h1><p>ID3
C4.5
CART</p><p>决策树根是树形结构的判决树，来完成分类。拥有以下特点：</p><ol><li><p>根节点拥有所有的样本</p></li><li><p>叶节点包含决策结果</p></li><li><p>每个节点包含样本集，根据属性测试的结果划分到左右子树中。</p></li><li><p>划分测试：
ID3 信息增益。
信息熵：所含的信息量。信息熵越大，信息量越多，混乱程度越高。公式$=-\sum p log p$。p就是第k类样本（标签）所占比例。
信息增益就是<strong>父节点的信息熵</strong>减去（<strong>左右子节点的信息熵</strong>乘以<strong>节点权重</strong>）。由于是多分类问题，每个节点拥有的样本数量不同，所以乘以这个权重。</p></li></ol><p>信息增益的缺陷就是容易偏爱特征种数多的特征。当一个特征种数等于样本个数，则每种特征都是纯的，那这个信息增益会非常大。</p><p>信息增益率 = 信息增益 / 固有值。固有值为每一类样本个数所占比例的熵。公式$=-\sum p log p, p=\frac{D^v}{D}$
信息增益率让特征选择更偏向种类少的特征。</p><p>基尼系数
$Gini(D) = 1-\sum p^2$ p为第k类样本所占比例。
基尼指数 = $\sum \frac{D^v}{D}Gini(D^v)$</p><ol start=2><li><p>连续值的处理。对于属性a，在a上出现了n个不同的取值，排序，然后取每两个相邻元素的中位点进行划分。进行二分。二分后的特征后面可以继续划分。</p></li><li><p>缺失值</p></li><li><p>剪枝</p></li></ol><h1 id=svm>SVM</h1><p>公式：$L = \frac{1}{2}||w||^2 + \sum\alpha_i(1-y_i(W^TX+b))$</p><p>怎么求解？先转为对偶问题，先求w，b再求$\alpha$。两个参数W，b求偏导，得到的值带入能求出$\alpha$, $W = \sum_i \alpha y_i x_i$</p><p>软间隔 $L = min (1/2 ||w||^2 + C\sum_i^n\mu)$ C为松弛变量。</p><p>$\mu > max(0, 1-y_i(WX+b))$ 这个就是合页损失。</p><h1 id=逻辑回归>逻辑回归</h1><h1 id=k-means>k-means</h1><h1 id=随机森林>随机森林</h1><h1 id=其他>其他</h1><ul><li><input disabled type=checkbox> 各个部分面经</li></ul></div><footer class=post-footer><div class=addthis_inline_share_toolbox></div><div class=post-nav><div class=article-copyright><div class=article-copyright-img><img src=/img/qq_qrcode.png width=129px height=129px><div style=text-align:center>QQ扫一扫交流</div></div><div class=article-copyright-info><p><span>声明：</span>Boost复习</p><p style=word-break:break-all><span>链接：</span>
https://yanyulinxi.github.io/post/study/deeplearning/xueguo%E5%AD%A6%E8%BF%87%E7%9F%A5%E8%AF%86%E7%9A%84%E5%A4%8D%E4%B9%A0/</p><p><span>作者：</span>阳阳</p><p><span>邮箱：</span>yanyulinxi@qq.com</p><p><span>声明： </span>本博客文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/ target=_blank style=text-decoration:underline>CC BY-NC-SA 3.0</a>许可协议，转载请注明出处！</p></div></div><div class=clear></div></div><div class=reward-qr-info><div>创作实属不易，如有帮助，那就打赏博主些许茶钱吧 ^_^</div><button id=rewardButton disable=enable onclick="var qr=document.getElementById('QR');qr.style.display==='none'?qr.style.display='block':qr.style.display='none'">
<span>赏</span></button><div id=QR style=display:none><div id=wechat style=display:inline-block><img id=wechat_qr src=/img/wechat-pay.png alt="WeChat Pay"><p>微信打赏</p></div><div id=alipay style=display:inline-block><img id=alipay_qr src=/img/ali-pay.png alt=Alipay><p>支付宝打赏</p></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=https://yanyulinxi.github.io/post/essay/thought/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%8A%AA%E5%8A%9B/ rel=next title=为什么要努力><i class="fa fa-chevron-left"></i>为什么要努力</a></div><div class="post-nav-prev post-nav-item"><a href=https://yanyulinxi.github.io/post/study/deeplearning/kmeans/ rel=prev title=Kmeans>Kmeans
<i class="fa fa-chevron-right"></i></a></div></div><div id=wcomments></div></footer></article></section></div></div><div class=sidebar-toggle><div class=sidebar-toggle-line-wrap><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id=sidebar class=sidebar><div class=sidebar-inner><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image src=/img/linxi_icon.png alt=阳阳><p class=site-author-name itemprop=name>阳阳</p><p class="site-description motion-element" itemprop=description>再平凡的人也有属于他自己的梦想!</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href=/post/><span class=site-state-item-count>140</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>6</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>35</span>
<span class=site-state-item-name>标签</span></a></div></nav><div class="links-of-author motion-element"><span class=links-of-author-item><a href=https://github.com/yanyuLinxi target=_blank title=GitHub><i class="fa fa-fw fa-github"></i>GitHub</a></span>
<span class=links-of-author-item><a href=https://space.bilibili.com/19237450 target=_blank title=哔哩哔哩><i class="fa fa-fw fa-globe"></i>哔哩哔哩</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class=links-of-blogroll-title><i class="fa fa-fw fa-globe"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://www.liaoxuefeng.com/ title=廖雪峰 target=_blank>廖雪峰</a></li></ul></div><div class="tagcloud-of-blogroll motion-element tagcloud-of-blogroll-inline"><div class=tagcloud-of-blogroll-title><i class="fa fa-fw fa-tags"></i>标签云</div><ul class=tagcloud-of-blogroll-list><li class=tagcloud-of-blogroll-item><a href=/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0>论文阅读笔记</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/python%E7%9B%B8%E5%85%B3%E5%BA%93%E5%AD%A6%E4%B9%A0>Python相关库学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E5%BC%82%E5%B8%B8%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90>异常行为分析</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7>深度学习辅助工具</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E5%BA%93>机器学习相关库</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/kaggle>Kaggle</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/java%E5%AD%A6%E4%B9%A0>Java学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/insider-threat>Insider threat</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/leetcode%E5%AD%A6%E4%B9%A0>Leetcode学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/spark>Spark</a></li></ul></div></section></div></aside></div></main><footer id=footer class=footer><div class=footer-inner><div class=copyright><span class=copyright-year>&copy; 2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span><span class=copyright-author>阳阳的人间旅游日记</span></div><div class=powered-info><span class=powered-by>Powered by - <a class=powered-link href=//gohugo.io target=_blank title=hugo>Hugo v0.81.0</a></span>
<span class=separator-line>/</span>
<span class=theme-info>Theme by - <a class=powered-link href=//github.com/elkan1788/hugo-theme-next target=_blank>NexT</a></span></div><div class=vistor-info><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span class=site-uv><i class="fa fa-user"></i><span class=busuanzi-value id=busuanzi_value_site_uv></span></span><span class=separator-line>/</span>
<span class=site-pv><i class="fa fa-eye"></i><span class=busuanzi-value id=busuanzi_value_site_pv></span></span></div><div class=license-info><span class=storage-info>Storage by
<a href style=font-weight:700 target=_blank></a></span><span class=separator-line>/</span>
<span class=license-num><a href target=_blank></a></span></div></div></footer><div class=back-to-top><i class="fa fa-arrow-up"></i><span id=scrollpercent><span>0</span>%</span></div></div><script type=text/javascript src=//cdn.bootcdn.net/ajax/libs/jquery/2.1.4/jquery.min.js></script><script type=text/javascript src=/js/search.js></script><script type=text/javascript src=/js/affix.js></script><script type=text/javascript src=/js/scrollspy.js></script><script type=text/javascript>function detectIE(){var a=window.navigator.userAgent,b=a.indexOf('MSIE '),c=a.indexOf('Trident/'),d=a.indexOf('Edge/');return b>0||c>0||d>0?-1:1}function getCntViewHeight(){var b=$('#content').height(),a=$(window).height(),c=b>a?b-a:$(document).height()-a;return c}function getScrollbarWidth(){var a=$('<div />').addClass('scrollbar-measure').prependTo('body'),b=a[0],c=b.offsetWidth-b.clientWidth;return a.remove(),c}function registerBackTop(){var b=50,a=$('.back-to-top');$(window).on('scroll',function(){var d,e,f,c,g;a.toggleClass('back-to-top-on',window.pageYOffset>b),d=$(window).scrollTop(),e=getCntViewHeight(),f=d/e,c=Math.round(f*100),g=c>100?100:c,$('#scrollpercent>span').html(g)}),a.on('click',function(){$("html,body").animate({scrollTop:0,screenLeft:0},800)})}function initScrollSpy(){var a='.post-toc',d=$(a),b='.active-current';d.on('activate.bs.scrollspy',function(){var b=$(a+' .active').last();c(),b.addClass('active-current')}).on('clear.bs.scrollspy',c),$('body').scrollspy({target:a});function c(){$(a+' '+b).removeClass(b.substring(1))}}function initAffix(){var a=$('.header-inner').height(),b=parseInt($('.main').css('padding-bottom'),10),c=a+10;$('.sidebar-inner').affix({offset:{top:c,bottom:b}}),$(document).on('affixed.bs.affix',function(){updateTOCHeight(document.body.clientHeight-100)})}function initTOCDimension(){var a,b;$(window).on('resize',function(){a&&clearTimeout(a),a=setTimeout(function(){var a=document.body.clientHeight-100;updateTOCHeight(a)},0)}),updateTOCHeight(document.body.clientHeight-100),b=getScrollbarWidth(),$('.post-toc').css('width','calc(100% + '+b+'px)')}function updateTOCHeight(a){a=a||'auto',$('.post-toc').css('max-height',a)}$(function(){var b=$('.header-inner').height()+10,c,d,a,e;$('#sidebar').css({'margin-top':b}).show(),c=parseInt($('#sidebar').css('margin-top')),d=parseInt($('.sidebar-inner').css('height')),a=c+d,e=$('.content-wrap').height(),e<a&&$('.content-wrap').css('min-height',a),$('.site-nav-toggle').on('click',function(){var a=$('.site-nav'),e=$('.toggle'),b='site-nav-on',f='toggle-close',c=a.hasClass(b),g=c?'slideUp':'slideDown',d=c?'removeClass':'addClass';a.stop()[g]('normal',function(){a[d](b),e[d](f)})}),registerBackTop(),initScrollSpy(),initAffix(),initTOCDimension(),$('.sidebar-nav-toc').click(function(){$(this).addClass('sidebar-nav-active'),$(this).next().removeClass('sidebar-nav-active'),$('.'+$(this).next().attr('data-target')).toggle(500),$('.'+$(this).attr('data-target')).toggle(500)}),$('.sidebar-nav-overview').click(function(){$(this).addClass('sidebar-nav-active'),$(this).prev().removeClass('sidebar-nav-active'),$('.'+$(this).prev().attr('data-target')).toggle(500),$('.'+$(this).attr('data-target')).toggle(500)})})</script><script src=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.js></script><script type=text/javascript>$(function(){$('.post-body').viewer()})</script><script type=text/javascript>$(function(){detectIE()>0?$.getScript(document.location.protocol+'//cdn.jsdelivr.net/npm/@waline/client/dist/Waline.min.js',function(){new Waline({el:'#wcomments',visitor:!0,avatar:'wavatar',avatarCDN:'https://sdn.geekzu.org/avatar/',avatarForce:!1,wordLimit:'200',placeholder:' 欢迎留下您的宝贵建议，请填写您的昵称和邮箱便于后续交流. ^_^ ',requiredFields:['nick','mail'],serverURL:"Your WalineSerURL",lang:"zh-cn"})}):$('#wcomments').html('抱歉，Waline插件不支持IE或Edge，建议使用Chrome浏览器。')})</script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=Your%20AddthisId"></script><script>(function(){var a=document.createElement('script'),c=window.location.protocol.split(':')[0],b;c==='https'?a.src='https://zz.bdstatic.com/linksubmit/push.js':a.src='http://push.zhanzhang.baidu.com/push.js',b=document.getElementsByTagName("script")[0],b.parentNode.insertBefore(a,b)})()</script></body></html>