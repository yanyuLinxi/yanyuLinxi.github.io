<!doctype html><html lang=zh-cn dir=content/zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=content-security-policy content="upgrade-insecure-requests"><title>Word2vec讲解和面经 - 阳阳的人间旅游日记</title><meta name=keywords content="博客,程序员,思考,读书,笔记,技术,分享"><meta name=author content="阳阳"><meta property="og:title" content="Word2vec讲解和面经"><meta property="og:site_name" content="阳阳的人间旅游日记"><meta property="og:image" content="/img/author.jpg"><meta name=title content="Word2vec讲解和面经 - 阳阳的人间旅游日记"><meta name=description content="欢迎来到临溪的博客站，个人主要专注于机器学习、深度学习的相关研究。在这里分享自己的学习心得。"><link rel="shortcut icon" href=/img/favicon.ico><link rel=apple-touch-icon href=/img/apple-touch-icon.png><link rel=apple-touch-icon-precomposed href=/img/apple-touch-icon.png><link href=//cdn.bootcdn.net/ajax/libs/font-awesome/4.6.2/css/font-awesome.min.css rel=stylesheet type=text/css><link href=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.css rel=stylesheet><link href=/css/main.css rel=stylesheet type=text/css><link href=/css/syntax.css rel=stylesheet type=text/css></head><body itemscope itemtype=http://schema.org/WebPage lang=zh-hans><div class="container one-collumn sidebar-position-left page-home"><div class=headband></div><header id=header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle role=button style=opacity:1;top:0><span class=toggle-line></span><span class=toggle-line></span><span class=toggle-line></span></div></div><div class=site-meta><div class=multi-lang-switch><i class="fa fa-fw fa-language" style=margin-right:5px></i><a class=lang-link id=zh-cn href=#>中文</a></div><div class=custom-logo-site-title><a href=/ class=brand rel=start><span class=logo-line-before><i></i></span><span class=site-title>阳阳的人间旅游日记</span>
<span class=logo-line-after><i></i></span></a></div><p class=site-subtitle>让我们消除隔阂的，不是无所不知的脑袋，而是手拉手，坚决不放弃的那颗心</p></div><div class=site-nav-right><div class="toggle popup-trigger" style=opacity:1;top:0><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul id=menu class=menu><li class=menu-item><a href=/ rel=section><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class=menu-item><a href=/post rel=section><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class=menu-item><a href=/about.html rel=section><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于我</a></li><li class=menu-item><a href=/404.html rel=section><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href=javascript:; class=popup-trigger><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class=site-search><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class=search-icon><i class="fa fa-search"></i></span><span class=popup-btn-close><i class="fa fa-times-circle"></i></span><div class=local-search-input-wrapper><input autocomplete=off placeholder=搜索关键字... spellcheck=false type=text id=local-search-input autocapitalize=none autocorrect=off></div></div><div id=local-search-result></div></div></div></nav></div></header><main id=main class=main><div class=main-inner><div class=content-wrap><div id=content class=content><section id=posts class=posts-expand><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><header class=post-header><h1 class=post-title itemprop="name headline"><a class=post-title-link href=https://yanyulinxi.github.io/post/study/deeplearning/word2vec%E8%AE%B2%E8%A7%A3%E5%92%8C%E9%9D%A2%E7%BB%8F/ itemprop=url>Word2vec讲解和面经</a></h1><div class=post-meta><span class=post-time><i class="fa fa-calendar-o fa-fw"></i><span class=post-meta-item-text>时间：</span>
<time itemprop=dateCreated datetime=2016-03-22T13:04:35+08:00 content="2022-02-26">2022-02-26</time></span>
<span>|
<i class="fa fa-file-word-o fa-fw"></i><span class=post-meta-item-text>字数：</span>
<span class=leancloud-world-count>3910 字</span></span>
<span>|
<i class="fa fa-eye fa-fw"></i><span class=post-meta-item-text>阅读：</span>
<span class=leancloud-view-count>8分钟</span></span>
<span id=/post/study/deeplearning/word2vec%E8%AE%B2%E8%A7%A3%E5%92%8C%E9%9D%A2%E7%BB%8F/ class=leancloud_visitors data-flag-title=Word2vec讲解和面经>|
<i class="fa fa-binoculars fa-fw"></i><span class=post-meta-item-text>阅读次数：</span>
<span class=leancloud-visitors-count></span></span></div></header><div class=post-body itemprop=articleBody><h1 id=word2vec-简介>word2vec 简介</h1><p>讲的非常好：https://zhuanlan.zhihu.com/p/27234078
负采样：https://www.cnblogs.com/pinard/p/7249903.html</p><h2 id=介绍>介绍</h2><p>Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型
通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。</p><p>它的输入都是one-hot编码，网络层包括两层隐藏层来学习一个滑动窗口内单词的共现关系。输出层是一个概率分布。</p><h2 id=详细定义>详细定义</h2><p>word2vec是一个3层的神经网络。输入层和输出层都可以看做词汇表的one-hot表示</p><p>CBOW: 每个滑动窗口是一个测试用例。输入 层V_1, V_m ，输出层 V_c 。即用中心词 的上下文(不包含 V_c )为输入，最大化预测输出为中心词 V_c的概率。</p><p>Skip-Gram: 每个滑动窗口是w个测试用例。输入层 V_c ,输出层 V_i 。即用中心词，最大化预测输出层为上下文词汇的概率。</p><p>$$Z ^ { [ 1 ] } = \sum _ { i } ^ { 4 } W _ { i * } x _ { i } + b _ { i }$$</p><p>$$h = A ^ { [ 1 ] } = \frac { 1 } { 4 } \sum _ { i } z _ { i } ^ { [ 1 ] }$$</p><p>$$Z ^ { [ 2 ] } = h * W ^ { \prime }$$</p><p>$$\hat{y} = a = softmax ( Z ^ { [ 2 ] } ) = \frac { e ^ { [ 2 ] } } { \sum _ { k } ^ { [ 2 ] } }$$</p><p>再与 W&rsquo;乘积后softmax归一化。因为输出层词汇的one-hot可以对应 y=[0,0..1..0] ，对于softmax输出一般采用最小化负的交叉熵的似然: ,然后使用梯度下降即可更新。</p><p>损失函数
$$E = - \log p ( W _ { 0 } | W _ { 1 } ) = - \log \frac { e x p ( u _j ) } { \sum _ { k \in V } e x p ( u _ { k } ) } = \log \sum _ { k \in v } exp(u_k)-u_j$$</p><p>这里的j是目标词真实的下标。u_j表示第j个词就是目标词的可能性。就是使目标次出现频率最高的概率的乘积。就是极大似然估计。</p><p>CBOW具体前向传播步骤：</p><ol><li>输入层: 输入C个单词x： x1k,⋯,xCk，并且每个 x 都是用 One-hot 编码表示，每一个 x 的<strong>维度为 V</strong>（词表长度）。</li><li>输入层到隐层<ol><li>首先，共享矩阵为 W_{V×N} ，V表示词表长度，W的每一行表示的就是一个N维的向量（训练结束后，W的每一行就表示一个词的词向量）。</li><li>然后，我们把所有输入的词转x化为对应词向量，然后取平均值，这样我们就得到了隐层输出值 ( 注意，隐层中无激活函数，也就是说这里是线性组合)。 其中，隐层输出 h 是一个N维的向量 。</li><li>$h=\frac{1}{C}W^T(x1+x2+⋯+xc)$</li></ol></li><li>隐层到输出层：隐层的输出为N维向量 h ， 隐层到输出层的权重矩阵为 W′N×V 。然后，通过矩阵运算我们得到一个 V×1 维向量u=W′T∗h</li><li>其中，向量 u 的第 i 行表示词汇表中第 i 个词的可能性，然后我们的目的就是取可能性最高的那个词。因此，在最后的输出层是一个softmax 层获取分数最高的词，那么就有我们的最终输出：$P(wj|context)=yi=\frac{exp(uj)}{\sum_{k∈V}exp(uk)}$</li></ol><p>Skip-gram:
Skip-Gram与CBOW的方法略有不同，每个词对（中心词，上下文词）都是一个训练样本</p><h2 id=步骤>步骤</h2><p>skip-gram处理步骤：</p><p>1.确定窗口大小window，对每个词生成2*window个训练样本，(i, i-window)，(i, i-window+1)，&mldr;，(i, i+window-1)，(i, i+window)</p><p>2.确定batch_size，注意batch_size的大小必须是2*window的整数倍，这确保每个batch包含了一个词汇对应的所有样本</p><p>3.训练算法有两种：层次Softmax和Negative Sampling</p><p>4.神经网络迭代训练一定次数，得到输入层到隐藏层的参数矩阵，矩阵中每一行的转置即是对应词的词向量</p><h3 id=损失函数>损失函数</h3><p>$$E= - log p(w_1, w_2, \cdots, w_C | w_I) \ = - log \prod_{c=1}^C P(w_c|w_i) \ = - log \prod_{c=1}^{C} \frac{exp(u_{c, j})}{\sum_{k=1}^{V} exp(u_{c,k}) } \ = - \sum_{c=1}^C u_{j,c} + C \cdot log \sum_{k=1}^{V} exp(u_k)$$</p><h2 id=层次softmax-哈夫曼树>层次softmax， 哈夫曼树。</h2><p><a href=https://zhuanlan.zhihu.com/p/59396559>https://zhuanlan.zhihu.com/p/59396559</a></p><p><a href=https://zhuanlan.zhihu.com/p/56139075>https://zhuanlan.zhihu.com/p/56139075</a></p><p>层次Softmax是一个二叉树结构，每个<strong>非叶子节点是个二分类器</strong>，每个<strong>叶子节点对应词汇表中的单词</strong>（ N个词汇，N-1 个二分类器）二分类器是逻辑回归。</p><p>二叉树是根据<strong>词频</strong>构建的哈夫曼树，词频越大的叶子节点距离根节点的路径越短。</p><p>softmax层的求和项被一系列二分类器代替，如果当前节点到目标叶子节点往左走，该节点的输出为二分类器的输出$\theta$ ,否则输出为$1-\theta$ 。最终输出为各个二分类器节点输出的乘积。$o ( h * W _ { 1 } ) * ( 1 - o ( h * W _ { 2 } ) ) * ( 1 - o ( h * W _ { 3 } ) )$.我们期望最大化上述路径乘积，即损失Cost 为最小化其负的log梯度。</p><p>所以层次Softmax相当于自动完成了归一化操作，故与普通Softmax可以等价</p><p>采用Softmax之后，需要需要更新参数的数量从 N降低到 logN ，对于一个大语料来说，这个改进可以说很大了。只有路径上的权重进行了更新。所以更新速度较快。</p><h3 id=损失函数-1>损失函数</h3><p>$l^w$项连乘，即从根节点到叶子节点一功有$l^w$个节点，求负对数后，变成连加。</p><p>$$L ( w , j ) = ( 1 - d _ { j } ) \cdot \log [ \delta ( x _ { i v } w _ { j - 1 } ) ] + d _ { j } ^ { w } \cdot \log [ 1 - \delta ( xw ) ]$$
就是按照词频构建二叉树，然后每一个非叶子节点就是而分类器，输出$\theta$,往左走输出$\theta$,往右走输出$1-\theta$，$d_j$表示向左走，即为0， $d_j$表示向右走，表示值为1.</p><p>最终计算出来的梯度信息是上下文单词的梯度信息，word2vec直接将梯度信息应用到每个窗口单词上面去。</p><h2 id=negative-sampling负采样>Negative Sampling(负采样)</h2><ol><li>负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。</li><li>当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新</li><li>采用二元逻辑回归来求解模型参数。通过负采样，得到neg个负例。接下来就是采用一个正例和negative负例，来使用逻辑回归求解<ol><li>$f = w_1x+b$</li><li>$f_2 = w_2f+b$</li><li>$p = sigmoid(f_2) = \frac{1}{1+e^{-f_2}}$,这是正例的概率，负例的概率就是1-p。然后对$w_1, w_2$进行梯度更新</li><li>损失函数为$L=\sum y_i log(\theta(xw))+(1-y_i)log(1-\theta(xw))$</li></ol></li><li>word2vec中负采样<ol><li>高频词采样率大，低频词采样率少。</li><li>公式：$p=\frac{f(w)^{\frac{3}{4}}}{\sum f(w)^{\frac{3}{4}}}$, f(w)为单词出现频率。</li><li>长度为1的线段，词频越大，则词对应的长度越大。</li><li>最后线段等份成m份。取neg个位置，每个位置对应一个词</li></ol></li><li>cbow 负采样<ol><li>上下文分别预测窗口词的平均去预测对应词和负采样的词</li></ol></li><li>skip-gram负采样<ol><li>分别使用当前窗口词去预测对应词和负采样的词。</li></ol></li></ol><h2 id=word2vec>word2vec</h2><p>输入是one-hot向量，B，N。N是字典大小。这里的N不是特征大小，如果是NLP的话，就是字典大小。就是整个集和中不重复样本数的个数。映射成one-hot向量
输出是经过两层隐藏层后，输出也是字典大小的向量，表示概率。</p><h2 id=word2vec优化>word2vec优化</h2><ol><li>输入的时候不会相乘，而是“查表”</li><li>word2vec在学习词向量之间的共现关系</li><li>优化<ol><li>对常见单词组合或词组作为单个words来处理</li><li>对高频词进行抽样减少训练样本个数<ol><li>对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关。</li><li>出现频率越高，越容易被删除</li></ol></li><li>使用negative sample，这样每个训练样本只会更新一小部分权重。</li></ol></li></ol><h2 id=面试题>面试题</h2><ol><li><p>word2vec看起来跟自动编码解码器很像，它们两者有什么关联吗？
word2vec和自编码器的差别。
word2vec是最大化共现概率。
自编码器是最小化重构误差。</p></li><li><p>cbow和skip-gram哪个更好
参数完全相同的情况下，大语料库cbow好。
这里使用skip-gram是速度更慢一些，但效果更好一点，因为中心词训练的次数更多。
skip-gram训练次数更多。所以学到的东西理应更多。</p></li><li><p>word2vec中softmax的优化</p><ol><li>softmax时间消耗主要在指数计算上，指数计算可以采用多项式逼近的方法，还可以采用查表法计算。将输入的值限制到一个范围内，这样查表就能得到高精度值。</li></ol></li><li><p>softmax求导</p><ol><li>$e^{z_j}$为第j个词的概率。对输入z_j求偏导为1，其余的$z_k$对z_j求偏导为0</li><li>a = softmax(z), a对z求偏导后=a(1-a)</li></ol></li></ol><p>$E = - log , p(w_1, w_2, \cdots, w_C | w_I) \ = - log \prod_{c=1}^C P(w_c|w_i) \ = - log \prod_{c=1}^{C} \frac{exp(u_{c, j})}{\sum_{k=1}^{V} exp(u_{c,k}) } \ = - \sum_{c=1}^C u_{j^c} + C \cdot log \sum{k=1}^{V} exp(u_k)$</p><h1 id=随机游走>随机游走</h1><p><a href=https://zhuanlan.zhihu.com/p/66836312>https://zhuanlan.zhihu.com/p/66836312</a></p><ol><li>DeepWalk</li></ol><p>DeepWalk提出了“随机游走”的思想，这个思想有点类似搜索算法中的DFS，从某一点出发，以深搜的方式获得一个节点序列。</p><p>这个序列即可以用来描述节点。</p><ol start=2><li>node2vec</li></ol><p>node2vec的作者针对DeepWalk不能用到带权图上的问题，提出了概率游走的策略</p><p>在一个图中，假设上一步走到的节点是t ，当前处于节点 v ，则下一步游走的概率为$\alpha$满足</p><p>$$\alpha = { \begin{array} { l } { \frac { 1 } { p } \quad d = 0 } \ { 1 \quad d = 1 } \ { \frac { 1 } { q } \quad d = 2 } \end{array}$$</p><p>d = 0 表示上一步走过的节点。d=1表示与节点t和节点v距离相同的节点（距离都为1）。d=2表示其他节点。</p><p>使用p和q控制了游走的宽度优先和深度优先。在调参的效果下可以取得比word2vec更好的效果</p><p>第一个节点由于没有上一个节点，所以直接选取游走节点。</p><ol start=3><li>metapath2vec</li></ol><p>应用于异质图。异质网络比同质网络包含了更多的语义信息，如果仅仅使用DeepWalk和node2vec这类方法，只能将不同的节点进行不同编号，并进行随机游走。但是这种方式忽略了节点的<strong>类型信息</strong>，而将作者、论文、会议都视作同一类型的节点，丢失了大量语义信息。</p><p>元路径可以理解为预定义的节点类型序列，比如APV表示【作者（Author）-论文（Paper）-期刊（Venue）】</p><p>通过设计首尾类型相同的元路径，我们可以不断地重复基于相同元路径的游走</p></div><footer class=post-footer><div class=addthis_inline_share_toolbox></div><div class=post-nav><div class=article-copyright><div class=article-copyright-img><img src=/img/qq_qrcode.png width=129px height=129px><div style=text-align:center>QQ扫一扫交流</div></div><div class=article-copyright-info><p><span>声明：</span>Word2vec讲解和面经</p><p style=word-break:break-all><span>链接：</span>
https://yanyulinxi.github.io/post/study/deeplearning/word2vec%E8%AE%B2%E8%A7%A3%E5%92%8C%E9%9D%A2%E7%BB%8F/</p><p><span>作者：</span>阳阳</p><p><span>邮箱：</span>yanyulinxi@qq.com</p><p><span>声明： </span>本博客文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/ target=_blank style=text-decoration:underline>CC BY-NC-SA 3.0</a>许可协议，转载请注明出处！</p></div></div><div class=clear></div></div><div class=reward-qr-info><div>创作实属不易，如有帮助，那就打赏博主些许茶钱吧 ^_^</div><button id=rewardButton disable=enable onclick="var qr=document.getElementById('QR');qr.style.display==='none'?qr.style.display='block':qr.style.display='none'">
<span>赏</span></button><div id=QR style=display:none><div id=wechat style=display:inline-block><img id=wechat_qr src=/img/wechat-pay.png alt="WeChat Pay"><p>微信打赏</p></div><div id=alipay style=display:inline-block><img id=alipay_qr src=/img/ali-pay.png alt=Alipay><p>支付宝打赏</p></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=https://yanyulinxi.github.io/post/essay/thought/%E5%86%99%E5%9C%A8%E9%9D%A2%E8%AF%95%E5%AD%97%E8%8A%82%E8%A2%AB%E6%8B%92/ rel=next title=写在面试字节被拒><i class="fa fa-chevron-left"></i>写在面试字节被拒</a></div><div class="post-nav-prev post-nav-item"><a href=https://yanyulinxi.github.io/post/journey/%E7%BE%8E%E9%A3%9F%E8%AF%84%E5%88%86/%E5%85%89%E8%B0%B7%E5%BA%97%E9%93%BA%E7%82%B9%E8%AF%84/ rel=prev title=光谷店铺点评>光谷店铺点评
<i class="fa fa-chevron-right"></i></a></div></div><div id=wcomments></div></footer></article></section></div></div><div class=sidebar-toggle><div class=sidebar-toggle-line-wrap><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id=sidebar class=sidebar><div class=sidebar-inner><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image src=/img/linxi_icon.png alt=阳阳><p class=site-author-name itemprop=name>阳阳</p><p class="site-description motion-element" itemprop=description>再平凡的人也有属于他自己的梦想!</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href=/post/><span class=site-state-item-count>124</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>6</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>33</span>
<span class=site-state-item-name>标签</span></a></div></nav><div class="links-of-author motion-element"><span class=links-of-author-item><a href=https://github.com/yanyuLinxi target=_blank title=GitHub><i class="fa fa-fw fa-github"></i>GitHub</a></span>
<span class=links-of-author-item><a href=https://space.bilibili.com/19237450 target=_blank title=哔哩哔哩><i class="fa fa-fw fa-globe"></i>哔哩哔哩</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class=links-of-blogroll-title><i class="fa fa-fw fa-globe"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://www.liaoxuefeng.com/ title=廖雪峰 target=_blank>廖雪峰</a></li></ul></div><div class="tagcloud-of-blogroll motion-element tagcloud-of-blogroll-inline"><div class=tagcloud-of-blogroll-title><i class="fa fa-fw fa-tags"></i>标签云</div><ul class=tagcloud-of-blogroll-list><li class=tagcloud-of-blogroll-item><a href=/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0>论文阅读笔记</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E5%BC%82%E5%B8%B8%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90>异常行为分析</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/python%E7%9B%B8%E5%85%B3%E5%BA%93%E5%AD%A6%E4%B9%A0>Python相关库学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7>深度学习辅助工具</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E5%BA%93>机器学习相关库</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/java%E5%AD%A6%E4%B9%A0>Java学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/insider-threat>Insider threat</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/leetcode%E5%AD%A6%E4%B9%A0>Leetcode学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E7%BE%8E%E9%A3%9F%E7%82%B9%E8%AF%84>美食点评</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/rnn>Rnn</a></li></ul></div></section></div></aside></div></main><footer id=footer class=footer><div class=footer-inner><div class=copyright><span class=copyright-year>&copy; 2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span><span class=copyright-author>阳阳的人间旅游日记</span></div><div class=powered-info><span class=powered-by>Powered by - <a class=powered-link href=//gohugo.io target=_blank title=hugo>Hugo v0.81.0</a></span>
<span class=separator-line>/</span>
<span class=theme-info>Theme by - <a class=powered-link href=//github.com/elkan1788/hugo-theme-next target=_blank>NexT</a></span></div><div class=vistor-info><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span class=site-uv><i class="fa fa-user"></i><span class=busuanzi-value id=busuanzi_value_site_uv></span></span><span class=separator-line>/</span>
<span class=site-pv><i class="fa fa-eye"></i><span class=busuanzi-value id=busuanzi_value_site_pv></span></span></div><div class=license-info><span class=storage-info>Storage by
<a href style=font-weight:700 target=_blank></a></span><span class=separator-line>/</span>
<span class=license-num><a href target=_blank></a></span></div></div></footer><div class=back-to-top><i class="fa fa-arrow-up"></i><span id=scrollpercent><span>0</span>%</span></div></div><script type=text/javascript src=//cdn.bootcdn.net/ajax/libs/jquery/2.1.4/jquery.min.js></script><script type=text/javascript src=/js/search.js></script><script type=text/javascript src=/js/affix.js></script><script type=text/javascript src=/js/scrollspy.js></script><script type=text/javascript>function detectIE(){var a=window.navigator.userAgent,b=a.indexOf('MSIE '),c=a.indexOf('Trident/'),d=a.indexOf('Edge/');return b>0||c>0||d>0?-1:1}function getCntViewHeight(){var b=$('#content').height(),a=$(window).height(),c=b>a?b-a:$(document).height()-a;return c}function getScrollbarWidth(){var a=$('<div />').addClass('scrollbar-measure').prependTo('body'),b=a[0],c=b.offsetWidth-b.clientWidth;return a.remove(),c}function registerBackTop(){var b=50,a=$('.back-to-top');$(window).on('scroll',function(){var d,e,f,c,g;a.toggleClass('back-to-top-on',window.pageYOffset>b),d=$(window).scrollTop(),e=getCntViewHeight(),f=d/e,c=Math.round(f*100),g=c>100?100:c,$('#scrollpercent>span').html(g)}),a.on('click',function(){$("html,body").animate({scrollTop:0,screenLeft:0},800)})}function initScrollSpy(){var a='.post-toc',d=$(a),b='.active-current';d.on('activate.bs.scrollspy',function(){var b=$(a+' .active').last();c(),b.addClass('active-current')}).on('clear.bs.scrollspy',c),$('body').scrollspy({target:a});function c(){$(a+' '+b).removeClass(b.substring(1))}}function initAffix(){var a=$('.header-inner').height(),b=parseInt($('.main').css('padding-bottom'),10),c=a+10;$('.sidebar-inner').affix({offset:{top:c,bottom:b}}),$(document).on('affixed.bs.affix',function(){updateTOCHeight(document.body.clientHeight-100)})}function initTOCDimension(){var a,b;$(window).on('resize',function(){a&&clearTimeout(a),a=setTimeout(function(){var a=document.body.clientHeight-100;updateTOCHeight(a)},0)}),updateTOCHeight(document.body.clientHeight-100),b=getScrollbarWidth(),$('.post-toc').css('width','calc(100% + '+b+'px)')}function updateTOCHeight(a){a=a||'auto',$('.post-toc').css('max-height',a)}$(function(){var b=$('.header-inner').height()+10,c,d,a,e;$('#sidebar').css({'margin-top':b}).show(),c=parseInt($('#sidebar').css('margin-top')),d=parseInt($('.sidebar-inner').css('height')),a=c+d,e=$('.content-wrap').height(),e<a&&$('.content-wrap').css('min-height',a),$('.site-nav-toggle').on('click',function(){var a=$('.site-nav'),e=$('.toggle'),b='site-nav-on',f='toggle-close',c=a.hasClass(b),g=c?'slideUp':'slideDown',d=c?'removeClass':'addClass';a.stop()[g]('normal',function(){a[d](b),e[d](f)})}),registerBackTop(),initScrollSpy(),initAffix(),initTOCDimension(),$('.sidebar-nav-toc').click(function(){$(this).addClass('sidebar-nav-active'),$(this).next().removeClass('sidebar-nav-active'),$('.'+$(this).next().attr('data-target')).toggle(500),$('.'+$(this).attr('data-target')).toggle(500)}),$('.sidebar-nav-overview').click(function(){$(this).addClass('sidebar-nav-active'),$(this).prev().removeClass('sidebar-nav-active'),$('.'+$(this).prev().attr('data-target')).toggle(500),$('.'+$(this).attr('data-target')).toggle(500)})})</script><script src=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.js></script><script type=text/javascript>$(function(){$('.post-body').viewer()})</script><script type=text/javascript>$(function(){detectIE()>0?$.getScript(document.location.protocol+'//cdn.jsdelivr.net/npm/@waline/client/dist/Waline.min.js',function(){new Waline({el:'#wcomments',visitor:!0,avatar:'wavatar',avatarCDN:'https://sdn.geekzu.org/avatar/',avatarForce:!1,wordLimit:'200',placeholder:' 欢迎留下您的宝贵建议，请填写您的昵称和邮箱便于后续交流. ^_^ ',requiredFields:['nick','mail'],serverURL:"Your WalineSerURL",lang:"zh-cn"})}):$('#wcomments').html('抱歉，Waline插件不支持IE或Edge，建议使用Chrome浏览器。')})</script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=Your%20AddthisId"></script><script>(function(){var a=document.createElement('script'),c=window.location.protocol.split(':')[0],b;c==='https'?a.src='https://zz.bdstatic.com/linksubmit/push.js':a.src='http://push.zhanzhang.baidu.com/push.js',b=document.getElementsByTagName("script")[0],b.parentNode.insertBefore(a,b)})()</script></body></html>