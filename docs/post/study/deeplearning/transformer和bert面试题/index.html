<!doctype html><html lang=zh-cn dir=content/zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=content-security-policy content="upgrade-insecure-requests"><title>Transformer和Bert面试题 - 阳阳的人间旅游日记</title><meta name=keywords content="博客,程序员,思考,读书,笔记,技术,分享"><meta name=author content="阳阳"><meta property="og:title" content="Transformer和Bert面试题"><meta property="og:site_name" content="阳阳的人间旅游日记"><meta property="og:image" content="/img/author.jpg"><meta name=title content="Transformer和Bert面试题 - 阳阳的人间旅游日记"><meta name=description content="欢迎来到临溪的博客站，个人主要专注于机器学习、深度学习的相关研究。在这里分享自己的学习心得。"><link rel="shortcut icon" href=/img/favicon.ico><link rel=apple-touch-icon href=/img/apple-touch-icon.png><link rel=apple-touch-icon-precomposed href=/img/apple-touch-icon.png><link href=//cdn.bootcdn.net/ajax/libs/font-awesome/4.6.2/css/font-awesome.min.css rel=stylesheet type=text/css><link href=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.css rel=stylesheet><link href=/css/main.css rel=stylesheet type=text/css><link href=/css/syntax.css rel=stylesheet type=text/css></head><body itemscope itemtype=http://schema.org/WebPage lang=zh-hans><div class="container one-collumn sidebar-position-left page-home"><div class=headband></div><header id=header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle role=button style=opacity:1;top:0><span class=toggle-line></span><span class=toggle-line></span><span class=toggle-line></span></div></div><div class=site-meta><div class=multi-lang-switch><i class="fa fa-fw fa-language" style=margin-right:5px></i><a class=lang-link id=zh-cn href=#>中文</a></div><div class=custom-logo-site-title><a href=/ class=brand rel=start><span class=logo-line-before><i></i></span><span class=site-title>阳阳的人间旅游日记</span>
<span class=logo-line-after><i></i></span></a></div><p class=site-subtitle>让我们消除隔阂的，不是无所不知的脑袋，而是手拉手，坚决不放弃的那颗心</p></div><div class=site-nav-right><div class="toggle popup-trigger" style=opacity:1;top:0><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul id=menu class=menu><li class=menu-item><a href=/ rel=section><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class=menu-item><a href=/post rel=section><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class=menu-item><a href=/about.html rel=section><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于我</a></li><li class=menu-item><a href=/404.html rel=section><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href=javascript:; class=popup-trigger><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class=site-search><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class=search-icon><i class="fa fa-search"></i></span><span class=popup-btn-close><i class="fa fa-times-circle"></i></span><div class=local-search-input-wrapper><input autocomplete=off placeholder=搜索关键字... spellcheck=false type=text id=local-search-input autocapitalize=none autocorrect=off></div></div><div id=local-search-result></div></div></div></nav></div></header><main id=main class=main><div class=main-inner><div class=content-wrap><div id=content class=content><section id=posts class=posts-expand><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><header class=post-header><h1 class=post-title itemprop="name headline"><a class=post-title-link href=https://yanyulinxi.github.io/post/study/deeplearning/transformer%E5%92%8Cbert%E9%9D%A2%E8%AF%95%E9%A2%98/ itemprop=url>Transformer和Bert面试题</a></h1><div class=post-meta><span class=post-time><i class="fa fa-calendar-o fa-fw"></i><span class=post-meta-item-text>时间：</span>
<time itemprop=dateCreated datetime=2016-03-22T13:04:35+08:00 content="2022-03-10">2022-03-10</time></span>
<span>|
<i class="fa fa-file-word-o fa-fw"></i><span class=post-meta-item-text>字数：</span>
<span class=leancloud-world-count>6900 字</span></span>
<span>|
<i class="fa fa-eye fa-fw"></i><span class=post-meta-item-text>阅读：</span>
<span class=leancloud-view-count>14分钟</span></span>
<span id=/post/study/deeplearning/transformer%E5%92%8Cbert%E9%9D%A2%E8%AF%95%E9%A2%98/ class=leancloud_visitors data-flag-title=Transformer和Bert面试题>|
<i class="fa fa-binoculars fa-fw"></i><span class=post-meta-item-text>阅读次数：</span>
<span class=leancloud-visitors-count></span></span></div></header><div class=post-body itemprop=articleBody><h1 id=transformer-和-bert-面试题>Transformer 和 Bert 面试题</h1><h2 id=资料>资料</h2><ol><li>视频资料<ol><li><a href=https://www.bilibili.com/video/BV1Di4y1c7Zm>https://www.bilibili.com/video/BV1Di4y1c7Zm</a></li></ol></li><li>面试资料<ol><li><a href=https://zhuanlan.zhihu.com/p/148656446>https://zhuanlan.zhihu.com/p/148656446</a></li></ol></li></ol><h2 id=讲解>讲解</h2><h3 id=来源>来源</h3><ol><li>机器翻译<ol><li>机器翻译常用RNN模型。RNN模型的缺点</li><li>RNN模型需要一个一个送入进去，无法并行</li><li>RNN模型的梯度信息被近距离梯度信息主导，远距离梯度信息被忽略。</li></ol></li><li>Transformer两列的结构<ol><li>左边6个（或更多）Encoder。Encoder的结构相同。但每个Encoder参数是不共享的。</li><li>右边6个（或更多）Decoder。Decoder和Encoder结构不同。</li></ol></li><li>具体结构<ol><li>Encoder：<ol><li>输入->position encoding</li><li>->多头注意机制</li><li>-> position+多头注意力机制（残差结构）并进行norm</li><li>-> 送入FF网络</li><li>-> ff输出和第三步相加（残差）并进行norm</li></ol></li><li>Decoder<ol><li>（解码器前一个时刻的输出）输入->position encoding</li><li>送入 masked 多头注意力机制</li><li>输出和第一步残差并norm</li><li>接收encoder输出和第三步输出送入多头注意力<ol><li>注意这里接收的encoder输出，是所有encoder计算完后产生的输出，不是同层encoder的输出。</li></ol></li><li>和第3步的残差并norm</li><li>送入FF网络</li><li>FF输出和第5步残差</li></ol></li><li>Decoder最终输出->线性层->softmax</li><li>总结Encoder：可以分为三个部分，输入部分，注意力机制部分，前馈神经网络部分。后面两个部分都有残差和norm的结构。</li><li>Encoder和Decoder差别。Decoder中间多了一层交互层，输入为Decoder上一层输出和同层Encoder输出。且Decoder第一层是mask操作。</li></ol></li></ol><h3 id=位置编码>位置编码</h3><ol><li><p>Encoder 输入部分</p><ol><li>Embedding</li><li>位置编码</li></ol></li><li><p>为什么需要位置编码</p><ol><li>相比于RNN来说，Transformer对于一个句子中的值是一起处理的（一起处理的好处加快了速度，但忽略了序列关系），而不是一个个处理的，所以需要加入位置编码来告诉网络每一个的位置。</li></ol></li><li><p>位置编码公式</p><ol><li>对于一个长度为512的词特征向量。对于词向量中的奇数位置使用sin编码，对偶数位置使用cos编码。得到一个新的512维度向量。</li><li>上述两个512维度的向量相加</li></ol></li><li><p>为什么这样编码有用？</p><ol><li>$${ \begin{array} { l } { \sin ( \alpha + \beta ) = \sin \alpha \cos \beta + \cos \alpha \sin \beta } \ { \cos ( \alpha + \beta ) = \cos \alpha \cos \beta - \sin \alpha \sin \beta } \end{array}$$</li><li>$${ P E ( p o s + k , 2 i ) = P E ( p o s , 2 i ) * P E ( k , 2 i + 1 ) + P E ( p o s , 2 i + 1 ) * P E ( k , 2 i ) } \ P E ( p o s + k , 2 i + 1 ) = P E ( p o s , 2 i + 1 ) * P E ( k , 2 i +1) - PE(pos, 2i) * PE(k, 2i)$$</li><li>所以对于pos+k的位置向量的某一维度2i或2i+1而言，可以表示为pos位置和k位置的位置向量的2i和2i+1的线性组合。这样的线性组合意味着向量中蕴含了相对位置信息。</li></ol></li><li><p>注意</p><ol><li>这种相对位置信息会在注意力机制中消失。</li><li>相对位置信息即$PE_{pos+k}$，可以由PE_{pos}线性表示。但经过点乘后$PE_{pos}W_qW_kPE_{pos+k}$，这两个值就没有相对位置信息了，两个w可以看成两个线性变化（实验得到的）。</li></ol></li><li></li></ol><h3 id=多头注意力机制>多头注意力机制</h3><ol><li>基础的注意力机制<ol><li>自注意力机制，见Attention章节更详细的知识。</li><li>多头注意力机制。</li></ol></li><li>多头注意力机制<ol><li>与其做一个单一的注意力函数，不如将整个query映射到低维（使用Linear）。然后使用多个注意力机制进行计算，最后再并起来，送入Linear</li><li>两个好处：<ol><li>原始的单一注意力方式并没有很多可以学习的参数。可学习的参数少，说明网络的容量小。</li><li>希望不同的注意力头可以学习到不同的投影方法，去匹配不同的注意力模式。和卷积中多个核很像。</li></ol></li><li>可以类比CNN中同时使用多个滤波器的作用，直观上讲，多头的注意力有助于网络捕捉到更丰富的特征/信息。</li></ol></li></ol><h3 id=残差和layernorm>残差和layerNorm</h3><ol><li>输入x1, x2</li><li>经过self-attention得到 z1, z2</li><li>进行残差 f1= x1+z1, f2=x2+z2<ol><li>残差为什么起作用，公式一写就明白了。它增加了一条梯度下降的路径。缓解了梯度消失。</li><li>小括号中的1表明短路机制可以无损地传播梯度，而另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。https://zhuanlan.zhihu.com/p/31852747</li></ol></li><li>进行layernorm<ol><li>为什么用layernorm不使用BN<ol><li>BN优点：<ol><li>解决ICS问题（数据分布）</li><li>解决梯度消失问题</li></ol></li><li>BN缺点：<ol><li>batch小的时候效果差。</li><li>在RNN中效果比较差（句子的输入是动态的，句子长度不一致，且任意一个词都可以放在第一个位置，所以batchnorm保存的信息不够好。所以BN效果不好）</li></ol></li></ol></li><li>LayerNorm对同一个样本内的所有值做归一化<ol><li>这种归一化就学习到了句子中的分布</li></ol></li></ol></li></ol><h3 id=前馈神经网络>前馈神经网络</h3><ol><li>接着上一步<ol><li>z1+Feed Forward, z2+Feed Forward。 FeedForward又叫多层感知器。多层的FC全连接。</li><li>加上残差和归一化</li></ol></li><li>前馈神经网络就是全连接+非线性激活函数。然后多层累加。</li><li>在实现中，就是单隐藏层的MLP。维度扩大四倍再缩小四倍。实现中，mlp是对每个词做运算得到输出。</li></ol><h3 id=decoder>Decoder</h3><ol><li>MASK<ol><li>需要对当前值和之前的值进行mask</li><li>因为预测的时候，无法知道当前值后面的值</li><li>如何做mask<ol><li>在decoder端计算注意力时，仍然计算，但在做softmax前，将第t时刻以后的注意力权重设置为极大的负数。这样softmax后这个值就变为了0。</li></ol></li></ol></li><li>交互层<ol><li>Encoder的输出（K，V）矩阵。是所有单词输出汇总计算k,v矩阵。这个过程和在Encoder中计算K/v矩阵是一样的。</li><li>Decoder生成Q矩阵。</li></ol></li></ol><h3 id=运行阶段>运行阶段</h3><ol><li>Encoder：<ol><li>长度为n的句子。输入是n个词的embedding，【我，爱，机器，学习】</li><li>计算position encoding。</li><li>然后计算多头注意力机制</li></ol></li><li>Decoder:<ol><li>第二个多头注意力中，key、value来自于编码器输出，query来自上一层masked attention.</li><li>masked注意力层的输出向量维度仍然是不变的</li><li>decoder的每一次输入都是上一个时间状态理应的输出，第一个时间状态下，上一个时间状态使用<bos>表示。所以未【<bos>】，【<bos>, i,】,【<bos>, i,love】这样。为了方便，变为【<bos>, i,love, machine, learning】，然后在第一个attention 的时候，使用mask。</li></ol></li><li>词映射到向量时的这个embedding和decoder输入的embedding和输出时softmax前的embedding，这些是共享权重，减少训练难度。这个embedding的权重除以了根号d，d为512。为了和position编码相加。</li></ol><h3 id=trm面试题讲解>TRM面试题讲解</h3><ol><li>所有Encoder和所有decoder做交互？</li><li>Encoder怎么生成Q矩阵的？</li><li>和RNN的区别。<ol><li>RNN是使用上一刻的语义信息辅助当前的语义信息。</li><li>Attention是关注全局的语义信息（Encoder）提取出来后，再通过mlp映射到相应语义空间。</li></ol></li><li>position embedding？</li><li>正则化<ol><li>在进入残差和layernorm前都做0.1的dropout。词嵌入、输入也用dropout</li><li>label smoothing，就是标签的value只用达到0.1的置信度就行。</li></ol></li><li>并行<ol><li>6个encoder、decoder都是串行。</li><li>encoder两个子模块之间是串行。子模块本身是可以并行的。</li></ol></li><li>self-attention的Q、K为什么要乘以一个矩阵<ol><li>Q、K本身的点积是为了计算相似度。但原空间的相似度不够，为了使模型有更强的表现力，所以进行映射</li><li>如果Q、K保持相同。则Q、K的计算拥有自反性，即Q、K计算出来的矩阵是对称的。实际上，我是男孩，“我”对“男孩”的重要性应该低于“男孩”修饰“我”的重要性。所以乘上了矩阵</li><li>实际上reformer也提出来，Q、K可以是同一个。Q、K相乘以后再乘一个矩阵。其实还是在做线性映射。</li></ol></li><li>为什么attention会除以$\sqrt{d_k}$<ol><li>对于输入[a, 10a, 100a]在输入的数据量级很大的时候，softmax会将几乎所有的概率分布分配给最大值对应的标签。会造成梯度消失为0，参数更新困难。</li><li>为什么使用$\sqrt{d_k}$:对于两个向量q、k，均值0，方差1。它们乘积后，均值0，方差为$d_k$。方差越大也就说明，点积的数量级越大（以越大的概率取大值）。那么一个自然的做法就是把方差稳定到1，做法是将点积除以$\sqrt{d_k}$</li><li>attention两种形式，加法Add然后映射到tanh，第二种乘法Mul。Add运算中的矩阵乘法，和Attention中的矩阵乘法不同。前者中只有随机变量X和参数矩阵W相乘，但是后者中包含随机变量X和随机变量X之间的乘法。对于Mul来说，如果S和h都分布在[0,1]，在相乘时引入一次对所有位置的∑求和，整体的分布就会扩大到[0,dk], 反过来看Add,右侧是被tanh0钳位后的值，分布在[-1,1]。整体分布和dk没有关系。</li><li><a href=https://www.cnblogs.com/hongdoudou/p/12594430.html>https://www.cnblogs.com/hongdoudou/p/12594430.html</a></li></ol></li><li>Bert模型attention部分如何加速。即如何一个矩阵完成多个投影。</li><li>多头运算加速<ol><li>原始特征 batch, seqlen, hidden，映射成hidden/head的特征。则可以优化为：</li><li>batch, head, seqlen, hidden/head. 这样可以进行多头的快速运算。</li><li>注意力的运算：<ol><li>hidden: B, S, Hidden</li><li>Q, K, V: B, S, Hidden2</li><li>Q*K=> S, Hidden2 * Hidden2, S => B, S, S</li><li>Q<em>K</em>V=> S, S* S, Hidden2 = > S, Hidden2</li><li>最终C=>B, S, Hidden2</li><li>无论多少维，最后运算的时候都是二维的。</li></ol></li></ol></li><li>attention相比于lstm的优点<ol><li>attention将任意两个词的距离拉到了1</li><li>attention的运算可以并行化。</li></ol></li><li>位置编码技术<ol><li>RPE(相对位置编码)</li><li>多头时，将key绝对位置转为相对query的位置。</li><li>复数域函数。</li></ol></li><li>并行化<ol><li>Transformer的并行化主要体现在self-attention模块，在Encoder端Transformer可以并行处理整个序列，并得到整个输入序列经过Encoder端的输出，但是rnn只能从前到后的执行</li><li>Decoder 交互的时候不能并行化，其他的时候可以。</li></ol></li><li>bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？<ol><li>BERT和transformer的目标不一致，bert是语言的预训练模型，需要充分考虑上下文的关系，而transformer主要考虑句子中第i个元素与前i-1个元素的关系。Transformer主要是用来做机器翻译的。</li></ol></li></ol><h1 id=bert>Bert</h1><h2 id=简单介绍>简单介绍</h2><ol><li>bert是用了transformer的encoder侧的网络，作为一个文本编码器，使用大规模数据进行预训练，预训练使用两个loss，一个是mask LM，遮蔽掉源端的一些字，然后根据上下文去预测这些字<ol><li>可能会被问到mask的具体做法，15%概率mask词，这其中80%用[mask]替换，10%随机替换一个其他字，10%不替换，至于为什么这么做，那就得问问BERT的作者了{捂脸}）</li></ol></li><li>一个是next sentence，判断两个句子是否在文章中互为上下句，然后使用了大规模的语料去预训练。<ol><li>roberta证明nsp这个loss并没有起到什么作用。</li></ol></li></ol><p>和GPT最大的区别在于它是双向的。</p><h2 id=概要>概要</h2><ol><li>用了双向信息和Transformer来做预测。</li><li>以前的预训练模型<ol><li>基于特征：将学到的特征和输入一起放入网络训练。</li><li>基于微调：将训练好的权重，在下一层进行微调。</li></ol></li><li>使用两个任务:<ol><li>带掩码的语言模型。masked language model.可以从双向看句子。</li><li>下一个句子预测。</li></ol></li><li>贡献：<ol><li>针对于Elmo的改动，Elmo使用了双向的信息，但是使用的是RNN的老旧模型。GPT使用了Transformer的结构，但是使用的是单向的模型。所以把它们结合起来。</li><li>使用了大量无标签的数据集进行预训练，比在小批量有标签的数据集上训练效果更好。</li></ol></li></ol><h2 id=模型>模型</h2><ol><li>预训练<ol><li>在没有标签上的数据上进行训练</li><li>参数量<ol><li>嵌入层：输入是30K，输出是隐藏层大小H。</li><li>自注意力块：多头注意力机制。<ol><li>投影矩阵：H*H。投影后合并。</li><li>拿到输出concat后，再做一次投影，矩阵为H*H</li></ol></li><li>MLP层：H * 4H + 4H * H = 8H^2</li></ol></li></ol></li><li>输入/输出。<ol><li>输入不再是一个句子，而是一个句子对。</li><li>使用wordpiece进行切词。如果一个词根可以表示一个词，则用词根来表示词。</li><li>输入进行处理：行首添加[CLS]，两个句子中间添加[SEP]。来表示这是第一个句子还是第二个句子。</li><li>在词送入网络时，添加一个嵌入层。<ol><li>token embedding层。节点、句子的嵌入</li><li>segment embedding层。用来标识该词时第一个句子还是第二个句子。cls是第一个句子，sep是第二个句子。输入就是2.</li><li>位置编码。和句子长度相关。位置编码的one-hot。对应值为1.</li></ol></li></ol></li><li>预训练的任务<ol><li>MLM任务。<ol><li>对于句子中的词，有15%的词会被替换为[MASK]。</li><li>为了避免预训练和微调时数据的不一致性。这15%的词中，80%被替换为[MASK], 10%的概率替换成随机的词源， 10%的概率什么也不干。</li></ol></li><li>预测下一个句子<ol><li>输入序列中有两个句子，50%样本就是下一个句子，50%的概率是随机选择一个句子。增大QA和语言推理中的信息。</li></ol></li></ol></li><li>微调<ol><li>要么使用对应词元做输出。</li><li>要么使用[cls]做输出。NSP任务也是用cls的embedding做预测的。</li></ol></li><li>实验<ol><li>glue：将CLS的输出拿出来接入softmax</li><li>QA：对于一段话中，找出语句中的开头和结尾。</li></ol></li><li>微调设置<ol><li>3 epoch， 学习率5e-5, batchsize 32. 增大epoch数量。（epoch太少是没办法达到好的效果）</li></ol></li><li>消融实验<ol><li>bert微调的特征比直接使用特征进行训练效果要好。（大家都得做微调）</li></ol></li></ol><p>写作贡献：</p><ol><li>突出你的主要贡献点。</li><li>分析你的贡献点带来的优点和缺点。</li></ol><h2 id=bert模型详解>bert模型详解</h2><ol><li>每个词的embedding加上位置编码，加上句子编码。（并不是concat，是直接相加）</li><li>乘以Q、K、V三个矩阵得到Q, K, V</li><li>使用一个词的Q对其他的key进行计算，计算的结果和V相加得到当前词的注意力信息。</li><li>Transformer中的一个encoder对应Bert中的一个Trm。（输入是所有句子。输出是当前词的注意力特征）。（实际上可以理解一层的encoder是共享的）bert会使用基于所有层中的左右两侧的语句信息。</li><li>bert可以调节的参数 L, H, A. L是网络的层数，Transformer block的数量。A是多头的数量。filter是</li></ol><h2 id=roberta的改进>roberta的改进</h2><ol><li>使用更长的训练时间、 100K-》500K steps</li><li>更大的batch、 从256增加到了8K</li><li>更多的数据</li><li>移除了NSP<ol><li>使用了全句子，不跨文档。</li></ol></li><li>序列更长</li><li>动态调整mask。<ol><li>原本的bert使用静态mask。</li><li>定义了dupe_factor，每个数据复制dupe_factor份，拥有不同的mask，在不同的epoch中使用。</li><li>第三种动态：每次训练的时候，才会随机进行mask。</li></ol></li><li>文字编码<ol><li>使用bytes-level：使用bytes而不是unicode作为subword的基本单位。</li><li>与wordpiece的区别：<ol><li>wordpiece选择提升语言模型概率最大的相邻子词加入此表</li><li>BPE选择聘书最高的相邻子词合并。</li></ol></li><li>好处不会出现unknown，但会增加数据集大小。</li></ol></li></ol><h2 id=词构造算法>词构造算法</h2><ol><li>BPE(roberta使用的)<ol><li>步骤：<ol><li>准备足够大的训练语料，并确定期望的Subword词表大小；</li><li>将单词拆分为成最小单元。比如英文中26个字母加上各种符号，这些作为初始词表；</li><li>在语料上统计单词内相邻单元对的频数，选取频数最高的单元对合并成新的Subword单元；每次合并后词表大小可能出现3种变化：<ol><li>+1，表明加入合并后的新子词，同时原来的2个子词还保留（2个字词分开出现在语料中）。</li><li>+0，表明加入合并后的新子词，同时原来的2个子词中一个保留，一个被消解（一个子词完全随着另一个子词的出现而紧跟着出现）。</li><li>-1，表明加入合并后的新子词，同时原来的2个子词都被消解（2个字词同时连续出现）。</li></ol></li><li>重复第3步直到达到第1步设定的Subword词表大小或下一个最高频数为1.</li></ol></li><li>编码<ol><li>得到Subword词表后，针对每一个单词，我们可以采用如下的方式来进行编码：</li><li>将词典中的所有子词按照长度由大到小进行排序；</li><li>对于单词w，依次遍历排好序的词典。查看当前子词是否是该单词的子字符串，如果是，则输出当前子词，并对剩余单词字符串继续匹配。</li><li>如果遍历完字典后，仍然有子字符串没有匹配，则将剩余字符串替换为特殊符号输出，如”<unk>”。</li><li>单词的表示即为上述所有输出子词。</li></ol></li><li>解码<ol><li>解码过程比较简单，如果相邻子词间没有中止符，则将两子词直接拼接，否则两子词之间添加分隔符。</li></ol></li></ol></li><li>wordpiece（bert使用的）<ol><li>与BPE算法类似，WordPiece算法也是每次从词表中选出两个子词合并成新的子词。与BPE的最大区别在于，如何选择两个子词进行合并：BPE选择频数最高的相邻子词合并（构造词阶段），而WordPiece选择能够提升语言模型概率最大的相邻子词加入词表。</li><li>方法：<ol><li>计算两个词的互信息量，让预料中以相邻方式同时出现的子词进行合并。</li></ol></li></ol></li><li>ULM（Unigram Language Model）<ol><li>ULM是减量法,即先初始化一个大词表，根据评估准则不断丢弃词表，直到满足限定条件。</li><li></li></ol></li></ol><p>上述三种方法的使用
如何使用上述子词算法？一种简便的方法是使用SentencePiece。</p><p>SentencePiece还能支持字符和词级别的分词。更进一步，为了能够处理多语言问题，sentencePiece将句子视为Unicode编码序列，从而子词算法不用依赖于语言的表示。</p><h2 id=参考资料>参考资料</h2><ol><li><a href=https://www.cnblogs.com/ffjsls/p/12257158.html>https://www.cnblogs.com/ffjsls/p/12257158.html</a></li><li>词构造算法 <a href=https://zhuanlan.zhihu.com/p/198964217>https://zhuanlan.zhihu.com/p/198964217</a></li><li><a href=https://zhuanlan.zhihu.com/p/363466672>https://zhuanlan.zhihu.com/p/363466672</a> trm面试题</li><li><a href=https://zhuanlan.zhihu.com/p/151412524>https://zhuanlan.zhihu.com/p/151412524</a> Bert面试题</li><li><a href=https://zhuanlan.zhihu.com/p/95594311>https://zhuanlan.zhihu.com/p/95594311</a> Bert面试题</li><li>trm模型图 <a href=https://zhuanlan.zhihu.com/p/44121378>https://zhuanlan.zhihu.com/p/44121378</a></li></ol></div><footer class=post-footer><div class=addthis_inline_share_toolbox></div><div class=post-nav><div class=article-copyright><div class=article-copyright-img><img src=/img/qq_qrcode.png width=129px height=129px><div style=text-align:center>QQ扫一扫交流</div></div><div class=article-copyright-info><p><span>声明：</span>Transformer和Bert面试题</p><p style=word-break:break-all><span>链接：</span>
https://yanyulinxi.github.io/post/study/deeplearning/transformer%E5%92%8Cbert%E9%9D%A2%E8%AF%95%E9%A2%98/</p><p><span>作者：</span>阳阳</p><p><span>邮箱：</span>yanyulinxi@qq.com</p><p><span>声明： </span>本博客文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/ target=_blank style=text-decoration:underline>CC BY-NC-SA 3.0</a>许可协议，转载请注明出处！</p></div></div><div class=clear></div></div><div class=reward-qr-info><div>创作实属不易，如有帮助，那就打赏博主些许茶钱吧 ^_^</div><button id=rewardButton disable=enable onclick="var qr=document.getElementById('QR');qr.style.display==='none'?qr.style.display='block':qr.style.display='none'">
<span>赏</span></button><div id=QR style=display:none><div id=wechat style=display:inline-block><img id=wechat_qr src=/img/wechat-pay.png alt="WeChat Pay"><p>微信打赏</p></div><div id=alipay style=display:inline-block><img id=alipay_qr src=/img/ali-pay.png alt=Alipay><p>支付宝打赏</p></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=https://yanyulinxi.github.io/post/study/deeplearning/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/ rel=next title=图神经网络面试题><i class="fa fa-chevron-left"></i>图神经网络面试题</a></div><div class="post-nav-prev post-nav-item"><a href=https://yanyulinxi.github.io/post/study/deeplearning/dbscan%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/ rel=prev title=DBSCAN聚类算法>DBSCAN聚类算法
<i class="fa fa-chevron-right"></i></a></div></div><div id=wcomments></div></footer></article></section></div></div><div class=sidebar-toggle><div class=sidebar-toggle-line-wrap><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id=sidebar class=sidebar><div class=sidebar-inner><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image src=/img/linxi_icon.png alt=阳阳><p class=site-author-name itemprop=name>阳阳</p><p class="site-description motion-element" itemprop=description>再平凡的人也有属于他自己的梦想!</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href=/post/><span class=site-state-item-count>112</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>6</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>31</span>
<span class=site-state-item-name>标签</span></a></div></nav><div class="links-of-author motion-element"><span class=links-of-author-item><a href=https://github.com/yanyuLinxi target=_blank title=GitHub><i class="fa fa-fw fa-github"></i>GitHub</a></span>
<span class=links-of-author-item><a href=https://space.bilibili.com/19237450 target=_blank title=哔哩哔哩><i class="fa fa-fw fa-globe"></i>哔哩哔哩</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class=links-of-blogroll-title><i class="fa fa-fw fa-globe"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://www.liaoxuefeng.com/ title=廖雪峰 target=_blank>廖雪峰</a></li></ul></div><div class="tagcloud-of-blogroll motion-element tagcloud-of-blogroll-inline"><div class=tagcloud-of-blogroll-title><i class="fa fa-fw fa-tags"></i>标签云</div><ul class=tagcloud-of-blogroll-list><li class=tagcloud-of-blogroll-item><a href=/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0>论文阅读笔记</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E5%BC%82%E5%B8%B8%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90>异常行为分析</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/python%E7%9B%B8%E5%85%B3%E5%BA%93%E5%AD%A6%E4%B9%A0>Python相关库学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E5%BA%93>机器学习相关库</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/java%E5%AD%A6%E4%B9%A0>Java学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/insider-threat>Insider threat</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/leetcode%E5%AD%A6%E4%B9%A0>Leetcode学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E7%BE%8E%E9%A3%9F%E7%82%B9%E8%AF%84>美食点评</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/rnn>Rnn</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0>科研学习笔记</a></li></ul></div></section></div></aside></div></main><footer id=footer class=footer><div class=footer-inner><div class=copyright><span class=copyright-year>&copy; 2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span><span class=copyright-author>阳阳的人间旅游日记</span></div><div class=powered-info><span class=powered-by>Powered by - <a class=powered-link href=//gohugo.io target=_blank title=hugo>Hugo v0.81.0</a></span>
<span class=separator-line>/</span>
<span class=theme-info>Theme by - <a class=powered-link href=//github.com/elkan1788/hugo-theme-next target=_blank>NexT</a></span></div><div class=vistor-info><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span class=site-uv><i class="fa fa-user"></i><span class=busuanzi-value id=busuanzi_value_site_uv></span></span><span class=separator-line>/</span>
<span class=site-pv><i class="fa fa-eye"></i><span class=busuanzi-value id=busuanzi_value_site_pv></span></span></div><div class=license-info><span class=storage-info>Storage by
<a href style=font-weight:700 target=_blank></a></span><span class=separator-line>/</span>
<span class=license-num><a href target=_blank></a></span></div></div></footer><div class=back-to-top><i class="fa fa-arrow-up"></i><span id=scrollpercent><span>0</span>%</span></div></div><script type=text/javascript src=//cdn.bootcdn.net/ajax/libs/jquery/2.1.4/jquery.min.js></script><script type=text/javascript src=/js/search.js></script><script type=text/javascript src=/js/affix.js></script><script type=text/javascript src=/js/scrollspy.js></script><script type=text/javascript>function detectIE(){var a=window.navigator.userAgent,b=a.indexOf('MSIE '),c=a.indexOf('Trident/'),d=a.indexOf('Edge/');return b>0||c>0||d>0?-1:1}function getCntViewHeight(){var b=$('#content').height(),a=$(window).height(),c=b>a?b-a:$(document).height()-a;return c}function getScrollbarWidth(){var a=$('<div />').addClass('scrollbar-measure').prependTo('body'),b=a[0],c=b.offsetWidth-b.clientWidth;return a.remove(),c}function registerBackTop(){var b=50,a=$('.back-to-top');$(window).on('scroll',function(){var d,e,f,c,g;a.toggleClass('back-to-top-on',window.pageYOffset>b),d=$(window).scrollTop(),e=getCntViewHeight(),f=d/e,c=Math.round(f*100),g=c>100?100:c,$('#scrollpercent>span').html(g)}),a.on('click',function(){$("html,body").animate({scrollTop:0,screenLeft:0},800)})}function initScrollSpy(){var a='.post-toc',d=$(a),b='.active-current';d.on('activate.bs.scrollspy',function(){var b=$(a+' .active').last();c(),b.addClass('active-current')}).on('clear.bs.scrollspy',c),$('body').scrollspy({target:a});function c(){$(a+' '+b).removeClass(b.substring(1))}}function initAffix(){var a=$('.header-inner').height(),b=parseInt($('.main').css('padding-bottom'),10),c=a+10;$('.sidebar-inner').affix({offset:{top:c,bottom:b}}),$(document).on('affixed.bs.affix',function(){updateTOCHeight(document.body.clientHeight-100)})}function initTOCDimension(){var a,b;$(window).on('resize',function(){a&&clearTimeout(a),a=setTimeout(function(){var a=document.body.clientHeight-100;updateTOCHeight(a)},0)}),updateTOCHeight(document.body.clientHeight-100),b=getScrollbarWidth(),$('.post-toc').css('width','calc(100% + '+b+'px)')}function updateTOCHeight(a){a=a||'auto',$('.post-toc').css('max-height',a)}$(function(){var b=$('.header-inner').height()+10,c,d,a,e;$('#sidebar').css({'margin-top':b}).show(),c=parseInt($('#sidebar').css('margin-top')),d=parseInt($('.sidebar-inner').css('height')),a=c+d,e=$('.content-wrap').height(),e<a&&$('.content-wrap').css('min-height',a),$('.site-nav-toggle').on('click',function(){var a=$('.site-nav'),e=$('.toggle'),b='site-nav-on',f='toggle-close',c=a.hasClass(b),g=c?'slideUp':'slideDown',d=c?'removeClass':'addClass';a.stop()[g]('normal',function(){a[d](b),e[d](f)})}),registerBackTop(),initScrollSpy(),initAffix(),initTOCDimension(),$('.sidebar-nav-toc').click(function(){$(this).addClass('sidebar-nav-active'),$(this).next().removeClass('sidebar-nav-active'),$('.'+$(this).next().attr('data-target')).toggle(500),$('.'+$(this).attr('data-target')).toggle(500)}),$('.sidebar-nav-overview').click(function(){$(this).addClass('sidebar-nav-active'),$(this).prev().removeClass('sidebar-nav-active'),$('.'+$(this).prev().attr('data-target')).toggle(500),$('.'+$(this).attr('data-target')).toggle(500)})})</script><script src=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.js></script><script type=text/javascript>$(function(){$('.post-body').viewer()})</script><script type=text/javascript>$(function(){detectIE()>0?$.getScript(document.location.protocol+'//cdn.jsdelivr.net/npm/@waline/client/dist/Waline.min.js',function(){new Waline({el:'#wcomments',visitor:!0,avatar:'wavatar',avatarCDN:'https://sdn.geekzu.org/avatar/',avatarForce:!1,wordLimit:'200',placeholder:' 欢迎留下您的宝贵建议，请填写您的昵称和邮箱便于后续交流. ^_^ ',requiredFields:['nick','mail'],serverURL:"Your WalineSerURL",lang:"zh-cn"})}):$('#wcomments').html('抱歉，Waline插件不支持IE或Edge，建议使用Chrome浏览器。')})</script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=Your%20AddthisId"></script><script>(function(){var a=document.createElement('script'),c=window.location.protocol.split(':')[0],b;c==='https'?a.src='https://zz.bdstatic.com/linksubmit/push.js':a.src='http://push.zhanzhang.baidu.com/push.js',b=document.getElementsByTagName("script")[0],b.parentNode.insertBefore(a,b)})()</script></body></html>