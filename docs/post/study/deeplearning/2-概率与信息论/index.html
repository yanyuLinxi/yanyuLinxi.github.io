<!doctype html><html lang=zh-cn dir=content/zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=content-security-policy content="upgrade-insecure-requests"><title>2 概率与信息论 - 希望至美，永不凋零</title><meta name=keywords content="博客,程序员,思考,读书,笔记,技术,分享"><meta name=author content="烟雨临溪"><meta property="og:title" content="2 概率与信息论"><meta property="og:site_name" content="希望至美，永不凋零"><meta property="og:image" content="/img/author.jpg"><meta name=title content="2 概率与信息论 - 希望至美，永不凋零"><meta name=description content="欢迎来到临溪的博客站，个人主要专注于机器学习、深度学习的相关研究。在这里分享自己的学习心得。"><link rel="shortcut icon" href=/img/favicon.ico><link rel=apple-touch-icon href=/img/apple-touch-icon.png><link rel=apple-touch-icon-precomposed href=/img/apple-touch-icon.png><link href=//cdn.bootcdn.net/ajax/libs/font-awesome/4.6.2/css/font-awesome.min.css rel=stylesheet type=text/css><link href=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.css rel=stylesheet><link href=/css/main.css rel=stylesheet type=text/css><link href=/css/syntax.css rel=stylesheet type=text/css></head><body itemscope itemtype=http://schema.org/WebPage lang=zh-hans><div class="container one-collumn sidebar-position-left page-home"><div class=headband></div><header id=header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle role=button style=opacity:1;top:0><span class=toggle-line></span><span class=toggle-line></span><span class=toggle-line></span></div></div><div class=site-meta><div class=multi-lang-switch><i class="fa fa-fw fa-language" style=margin-right:5px></i><a class=lang-link id=zh-cn href=#>中文</a></div><div class=custom-logo-site-title><a href=/ class=brand rel=start><span class=logo-line-before><i></i></span><span class=site-title>希望至美，永不凋零</span>
<span class=logo-line-after><i></i></span></a></div><p class=site-subtitle>让我们消除隔阂的，不是无所不知的脑袋，而是手拉手，坚决不放弃的那颗心</p></div><div class=site-nav-right><div class="toggle popup-trigger" style=opacity:1;top:0><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul id=menu class=menu><li class=menu-item><a href=/ rel=section><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class=menu-item><a href=/post rel=section><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class=menu-item><a href=/about.html rel=section><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于我</a></li><li class=menu-item><a href=/404.html rel=section><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href=javascript:; class=popup-trigger><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class=site-search><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class=search-icon><i class="fa fa-search"></i></span><span class=popup-btn-close><i class="fa fa-times-circle"></i></span><div class=local-search-input-wrapper><input autocomplete=off placeholder=搜索关键字... spellcheck=false type=text id=local-search-input autocapitalize=none autocorrect=off></div></div><div id=local-search-result></div></div></div></nav></div></header><main id=main class=main><div class=main-inner><div class=content-wrap><div id=content class=content><section id=posts class=posts-expand><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><header class=post-header><h1 class=post-title itemprop="name headline"><a class=post-title-link href=http://next.lisenhui.cn/post/study/deeplearning/2-%E6%A6%82%E7%8E%87%E4%B8%8E%E4%BF%A1%E6%81%AF%E8%AE%BA/ itemprop=url>2 概率与信息论</a></h1><div class=post-meta><span class=post-time><i class="fa fa-calendar-o fa-fw"></i><span class=post-meta-item-text>时间：</span>
<time itemprop=dateCreated datetime=2016-03-22T13:04:35+08:00 content="8080-11-08">8080-11-08</time></span>
<span>|
<i class="fa fa-file-word-o fa-fw"></i><span class=post-meta-item-text>字数：</span>
<span class=leancloud-world-count>5023 字</span></span>
<span>|
<i class="fa fa-eye fa-fw"></i><span class=post-meta-item-text>阅读：</span>
<span class=leancloud-view-count>11分钟</span></span>
<span id=/post/study/deeplearning/2-%E6%A6%82%E7%8E%87%E4%B8%8E%E4%BF%A1%E6%81%AF%E8%AE%BA/ class=leancloud_visitors data-flag-title="2 概率与信息论">|
<i class="fa fa-binoculars fa-fw"></i><span class=post-meta-item-text>阅读次数：</span>
<span class=leancloud-visitors-count></span></span></div></header><div class=post-body itemprop=articleBody><ul><li><a href=#%E6%A6%82%E7%8E%87%E8%AE%BA%E6%84%8F%E4%B9%89>概率论意义</a></li><li><a href=#%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F>随机变量</a></li><li><a href=#%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83>概率分布</a></li><li><a href=#%E8%BE%B9%E7%BC%98%E6%A6%82%E7%8E%87>边缘概率</a></li><li><a href=#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87>条件概率</a></li><li><a href=#%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%E7%9A%84%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99>条件概率的链式法则</a></li><li><a href=#%E7%8B%AC%E7%AB%8B%E6%80%A7%E5%92%8C%E6%9D%A1%E4%BB%B6%E7%8B%AC%E7%AB%8B%E6%80%A7>独立性和条件独立性</a></li><li><a href=#%E6%9C%9F%E6%9C%9B%E6%96%B9%E5%B7%AE%E5%8D%8F%E6%96%B9%E5%B7%AE>期望、方差、协方差</a></li><li><a href=#%E5%B8%B8%E7%94%A8%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83>常用概率分布</a></li><li><a href=#%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E7%9A%84%E6%9C%89%E7%94%A8%E6%80%A7%E8%B4%A8>常用函数的有用性质</a></li><li><a href=#%E8%B4%9D%E5%8F%B6%E6%96%AF%E8%A7%84%E5%88%99>贝叶斯规则</a></li><li><a href=#%E8%BF%9E%E7%BB%AD%E5%9E%8B%E5%8F%98%E9%87%8F%E7%9A%84%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82>连续型变量的技术细节</a></li><li><a href=#%E4%BF%A1%E6%81%AF%E8%AE%BA>信息论</a></li><li><a href=#%E7%BB%93%E6%9E%84%E5%8C%96%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B>结构化概率模型</a></li></ul><p>概率论是用于表示不确定性声明的数学框架。它不仅提供了量化不确定性的方 法，也提供了用于导出新的不确定性声明（statement）的公理</p><p>概率法则告诉我们 AI 系统如何推理。其次，我们可以用概率和统计从 理论上分析我们提出的 AI 系统的行为</p><h1 id=概率论意义>概率论意义</h1><ol><li><strong>几乎所有的活动</strong>都需要一些在<strong>不确定性存在</strong>的情况下进行推理的能力</li><li><strong>不确定性有三种可能的来源</strong><ol><li>被建模系统内在的随机性</li><li>不完全观测。</li><li>不完全建模。当我们使用一些必须舍弃某些观测信息的模型时，舍弃的信息会 导致模型的预测出现不确定性。</li></ol></li><li><strong>使用一些简单而不确定的规则要</strong>比复杂而确定的规则更为实用<ol><li>‘多数鸟儿都会飞’’ 这个简单的规则描述起来很简单很并且使用广泛</li><li>‘除了那些还没学会飞翔的幼鸟，因为生病或是受伤而失去了飞翔能力的 鸟，包括食火鸟 (cassowary)、鸵鸟 (ostrich)、几维 (kiwi，一种新西兰产的无翼鸟)等不会飞的鸟类……以外，鸟儿会飞’’，很难应用、维护和沟通，即使经过这么多的 努力，这个规则还是很脆弱而且容易失效。</li></ol></li><li>频率派概率和 贝叶斯概率<ol><li>当我 们说一个结果发生的概率为 p，这意味着如果我们反复实验 (例如，抽取一手牌) 无限次，有 p 的比例可能会导致这样的结果</li><li>在医生诊断病人的例 子中，我们用概率来表示一种信任度（degree of belief），</li><li>前面那种概率，直接与事件发生的频 率相联系，被称为<strong>频率派概率</strong>（frequentist probability）；</li><li>而后者，涉及到确定性水 平，被称为<strong>贝叶斯概率</strong>（Bayesian probability）</li><li>表征信任度的概率，我们称为贝叶斯概率。</li></ol></li><li>概率可以被看作是用于处理不确定性的逻辑扩展。逻辑提供了一套形式化的规 则，可以在给定某些命题是真或假的假设下，判断另外一些命题是真的还是假的。<strong>概率论提供了一套形式化的规则</strong>，可以在给定一些命题的似然后，计算其他命题为真的似然。<ol><li>似然：likelihood 即文言文版的可能性。</li></ol></li></ol><h1 id=随机变量>随机变量</h1><ol><li><strong>随机变量</strong>（random variable）是可以随机地取不同值的变量<ol><li>例如，x1 和 x2 都是随机变量 x 可能的取值</li><li>向量值变量，我们会将随机变量写成 x</li></ol></li></ol><h1 id=概率分布>概率分布</h1><ol><li><strong>概率分布</strong>（probability distribution）用来描述随机变量或一簇随机变量在每一 个可能取到的状态的可能性大小。我们描述概率分布的方式取决于随机变量是离散 的还是连续的</li><li><strong>离散型变量的概率分布</strong>可以用概率质量函数（probability mass function, PMF）我们通常用大写字母 P 来表示概率质量函数<ol><li>有时为了使得PMF的使用不相互混淆，我们会明确写出随 机变量的名称：P(x = x)。</li><li>有时我们会先定义一个随机变量，然后用 ∼ 符号来说明它遵循的分布：x ∼ P(x)</li></ol></li><li>这种多个变量的概率分布被称 为<strong>联合概率分布</strong>（joint probability distribution）。P(x = x, y = y) 表示 x = x 和 y = y 同时发生的概率</li><li>如果一个函数 P 是随机变量 x 的 PMF，必须满足下面这几个条件：<ol><li>P 的定义域必须是 x 所有可能状态的集合。</li><li>∀x ∈ x, 0 ≤ P(x) ≤ 1. 不可能发生的事件概率为 0，并且不存在比这概率更低 的状态。</li><li>∑ x∈x P(x) = 1. 我们把这条性质称之为归一化的（normalized）。</li></ol></li><li>考虑一个离散型随机变量 x 有 k 个不同的状态。我们可以假设 x 是**均匀 分布（uniform distribution）**的。通常用 x ∼ U(a, b) 表示 x 在 [a, b] 上是均匀分布的。</li><li><strong>连续型变量</strong>和<strong>概率密度函数</strong><ol><li>当我们研究的对象是连续型随机变量时，我们用<strong>概率密度函数</strong>（probability density function, PDF）</li><li>如果一个函数 p 是概率密度函数，必须满足下面这几个条件<ol><li>p 的定义域必须是 x 所有可能状态的集合</li><li>∀x ∈ x, p(x) ≥ 0. 注意，我们并不要求 p(x) ≤ 1。</li><li>∫p(x)dx = 1.</li></ol></li><li>概率密度函数 p(x) 并没有直接对特定的状态给出概率，相对的，它给出了落在 面积为 δx 的无限小的区域内的概率为 p(x)δx。</li></ol></li></ol><h1 id=边缘概率>边缘概率</h1><ol><li>但想要了解其中一个子集的概 率分布。这种定义在子集上的概率分布被称为<strong>边缘概率分布</strong>（marginal probability distribution）。</li></ol><h1 id=条件概率>条件概率</h1><ol><li>在很多情况下，我们感兴趣的是某个事件，在给定其他事件发生时出现的 概率。这种概率叫做条件概率。</li><li>条件概率只在 P(x = x) > 0 时有定义</li><li>计算一个行动的后果被称为干预 查询（intervention query）。<strong>干预查询</strong>属于<strong>因果模型</strong>（causal modeling）的范畴，我
们不会在本书中讨论</li></ol><h1 id=条件概率的链式法则>条件概率的链式法则</h1><ol><li>任何多维随机变量的联合概率分布，都可以分解成只有一个变量的条件概率相 乘的形式： P(x(1), . . . , x(n)) = P(x(1))Πn i=2P(x(i) | x(1), . . . , x(i−1)).</li><li>这个规则被称为<strong>概率的链式法则</strong>（chain rule）或者乘法法则（product rule）。</li></ol><h1 id=独立性和条件独立性>独立性和条件独立性</h1><ol><li>两个随机变量 x 和 y，如果它们的概率分布可以表示成两个因子的乘积形式，并 且一个因子只包含 x 另一个因子只包含 y，我们就称这两个随机变量是相互独立的</li><li>那么这两个随机变量 x 和 y 在给定随机变量 z 时是<strong>条件独立</strong>的（conditionally</li><li>我们可以采用一种简化形式来表示<strong>独立性和条件独立</strong>性：x⊥y 表示 x 和 y 相互 独立，x⊥y | z 表示 x 和 y 在给定 z 时条件独立。</li></ol><h1 id=期望方差协方差>期望、方差、协方差</h1><ol><li>函数 f(x) 关于某分布 P(x) 的<strong>期望</strong>（expectation）或者期望值（expected value）是指，当 x 由 P 产生，f 作用于 x 时，f(x) 的平均值</li><li>我们假设 E[·] 表示对方括号内的所有随机变量的值求平均。 类似的，当没有歧义时，我们还可以省略方括号。</li><li>**方差（variance）**衡量的是当我们对 x 依据它的概率分布进行采样时，随机变 量 x 的函数值会呈现多大的差异：Var(f(x))<ol><li>当方差很小时，f(x) 的值形成的簇比较接近它们的期望值。方差的平方根被称为<strong>标准差</strong>（standard deviation）。</li></ol></li><li>**协方差（covariance）**在某种意义上给出了两个变量线性相关性的强度以及这些 变量的尺度<ol><li><strong>协方差的绝对值如果很大</strong>则意味着变量值变化很大并且它们同时距离各自的均值很 远。</li><li>如果<strong>协方差是正的</strong>，那么两个变量都倾向于同时取得相对较大的值</li><li>如果<strong>协方 差是负的</strong>，那么其中一个变量倾向于取得相对较大的值的同时，另一个变量倾向于取得相对较小的值，</li><li>它们是有联系的，因为 两个变量如果相互独立那么它们的协方差为零，如果两个变量的协方差不为零那么它们一定是相关的</li><li><strong>两个变量相互依赖但具有零协方差是可能的</strong>。例如，假 设我们首先从区间 [−1, 1] 上的均匀分布中采样出一个实数 x。然后我们对一个随机 变量 s 进行采样。s 以 12 的概率值为 1，否则为-1。</li></ol></li></ol><h1 id=常用概率分布>常用概率分布</h1><ol><li><strong>Bernoulli 分布</strong>（Bernoulli distribution）是单个二值随机变量的分布。它由单 个参数 ϕ ∈ [0, 1] 控制，ϕ 给出了随机变量等于 1 的概率。</li><li><strong>Multinoulli 分布</strong>（multinoulli distribution）或者范畴分布（categorical distribution）</li><li>实数上最常用的分布就是<strong>正态分布</strong>（normal distribution），也称为<strong>高斯分布</strong>（Gaussian distribution）<ol><li>正态分布由两个参数控制，µ ∈ R 和 σ ∈ (0,∞)</li><li>当我们由于<strong>缺乏关于某个实 数上分布的先验知识</strong>而不知道该选择怎样的形式时，**正态分布是默认的比较好的选择，**其中有两个原因。<ol><li>第一，我们想要建模的<strong>很多分布的真实情况是比较接近正态分布的</strong>。中心极限 定理（central limit theorem）说明很多独立随机变量的和近似服从正态分布。</li><li>第二，在具有相同方差的所有可能的概率分布中，<strong>正态分布在实数上具有最大 的不确定性</strong>。因此，我们可以认为正态分布是对模型加入的先验知识量最少的分布。</li></ol></li></ol></li><li>我们常常把协方差矩阵固定成一个对角阵。一个更简单的版本是各向同性（isotropic）高斯分布，它的协方差矩阵是一个标量乘以单位阵。</li><li><strong>指数分布和 Laplace 分布</strong><ol><li>我们经常会需要一个在 x = 0 点处取得边界点 (sharp point) 的 分布。为了实现这一目的，我们可以使用指数分布</li><li>一个联系紧密的概率分布是Laplace 分布（Laplace distribution），它允许我们 在任意一点 µ 处设置概率质量的峰值</li></ol></li><li>在一些情况下，我们希望概率分布中的所有质量都集中在一个点上。这可以通 过<strong>Dirac delta 函数</strong>（Dirac delta function）δ(x) 定义概率密度函数来实现。</li><li>通过组合一些简单的概率分布来定义新的概率分布也是很常见的。一种通用的组 合方法是构造混合分布（mixture distribution）。</li><li>一个非常强大且常见的混合模型是<strong>高斯混合模型（Gaussian Mixture Model）</strong>， 它的组件 p(x | c = i) 是高斯分布<ol><li>高斯混合模型的参数指明了给每个组件 i 的<strong>先验概率</strong> （prior probability）αi = P(c = i)。</li><li>‘‘先验’’ 一词表明了在观测到 x 之前传递给模 型关于 c 的信念</li><li><strong>P(c | x) 是后验概率</strong>（posterior probability），因为它 是在观测到 x 之后进行计算的</li></ol></li><li><strong>高斯混合模型</strong>是<strong>概率密度的万能近似器</strong>（universal approximator），在这种意义下，任何平滑的概率密度都可以用具有足够多组件的<strong>高斯混合模型以任意精度来逼近。</strong></li></ol><h1 id=常用函数的有用性质>常用函数的有用性质</h1><ol><li><strong>sigmoid 函数</strong> 在变量取绝对值非常大的正值或负值时会出现饱和（saturate）现象，意味着函数会变得很平，并且对输入的微小改变会变得不敏感</li><li>另外一个经常遇到的函数**softplus **函数（softplus function）<ol><li>softplus 函数名来源于它是另外一个函数的平滑（或 ‘‘软化’’）形式，这个函数是<ol><li>x+ = max(0, x).</li></ol></li></ol></li></ol><h1 id=贝叶斯规则>贝叶斯规则</h1><ol><li>我们经常会需要在已知 P(y | x) 时计算 P(x | y)。幸运的是，如果还知道 P(x)， 我们可以用贝叶斯规则（Bayes’ rule）来实现这一目的：<ol><li>它通常使用 P(y) = 所以我们并不需要事先知道 P(y) 的信息。</li></ol></li></ol><h1 id=连续型变量的技术细节>连续型变量的技术细节</h1><ol><li>对于我们的目的，测度论更多的是用来描述那些适用于 Rn 上的大多数点，却不 适用于一些边界情况的定理。测度论提供了一种严格的方式来描述那些非常微小的点集。这种集合被称为 <strong>“零测度（measure zero）’</strong>’ 的</li><li>另外一个有用的测度论中的术语是 <strong>“几乎处处（almost everywhere）’’</strong>。某个性 质如果是几乎处处都成立的，那么它在整个空间中除了一个测度为零的集合以外都是成立的。</li></ol><h1 id=信息论>信息论</h1><ol><li>信息论的基本想法是<strong>一个不太可能的事件居然发生了，要比一个非常可能的事 件发生，能提供更多的信息</strong>。<ol><li>非常可能发生的事件信息量要比较少，并且极端情况下，<strong>确保能够发生的事件 应该没有信息量</strong>。</li><li><strong>较不可能发生的事件具有更高的信息量。</strong></li><li><strong>独立事件应具有增量的信息</strong>。例如，投掷的硬币两次正面朝上传递的信息量， 应该是投掷一次硬币正面朝上的信息量的两倍。</li></ol></li><li>我们定义一个事件 x = x 的自信息（self-information）<ol><li>I(x) = −logP(x).</li></ol></li><li>自信息只处理单个的输出。我们可以用香农熵（Shannon entropy）来对整个概 率分布中的不确定性总量进行量化<ol><li>当 x 是连续的，香农熵被称为微分熵（differential entropy）。</li><li>说明了<strong>更接近确定性的分布是如何具有较低的香农熵</strong>，而<strong>更 接近均匀分布的分布是如何具有较高的香农熵。</strong></li></ol></li><li>如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可 以使用KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异：<ol><li>KL 散度衡量的是，当我们使用一种被设计成能够使 得概率分布 Q 产生的消息的长度最小的编码</li><li>KL 散度为 0 当且仅当 P 和 Q 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是 ‘‘几乎处处’’ 相同的</li></ol></li><li>个和 KL 散度密切联系的量是交叉熵（cross-entropy）</li></ol><h1 id=结构化概率模型>结构化概率模型</h1><ol><li>由 一些可以通过边互相连接的顶点的集合构成。当我们用图来表示这种概率分布的分 解，我们把它称为<strong>结构化概率模型</strong>（structured probabilistic model）或者<strong>图模型（graphical model）</strong>。</li><li><strong>有向（directed）模型使用带有有向边的图</strong>，它们用条件概率分布来表示分解，</li><li>**无向（undirected）模型使用带有无向边的图，**它们将分解表示成一组函数</li><li>随机变量的联合概率与所有这些因子的乘积成比例（proportional）——意味着因子的值越大则可能性越大</li><li>这些图模型表示的分解仅仅是描述概率分布的一种语言。它们不是互 相排斥的概率分布族。有向或者无向不是概率分布的特性；它是概率分布的一种特
殊描述（description）所具有的特性</li></ol></div><footer class=post-footer><div class=addthis_inline_share_toolbox></div><div class=post-nav><div class=article-copyright><div class=article-copyright-img><img src=/img/qq_qrcode.png width=129px height=129px><div style=text-align:center>QQ扫一扫交流</div></div><div class=article-copyright-info><p><span>声明：</span>2 概率与信息论</p><p><span>链接：</span>http://next.lisenhui.cn/post/study/deeplearning/2-%E6%A6%82%E7%8E%87%E4%B8%8E%E4%BF%A1%E6%81%AF%E8%AE%BA/</p><p><span>作者：</span>烟雨临溪</p><p><span>声明： </span>本博客文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/ target=_blank style=text-decoration:underline>CC BY-NC-SA 3.0</a>许可协议，转载请注明出处！</p></div></div><div class=clear></div></div><div class=reward-qr-info><div>创作实属不易，如有帮助，那就打赏博主些许茶钱吧 ^_^</div><button id=rewardButton disable=enable onclick="var qr=document.getElementById('QR');qr.style.display==='none'?qr.style.display='block':qr.style.display='none'">
<span>赏</span></button><div id=QR style=display:none><div id=wechat style=display:inline-block><img id=wechat_qr src=/img/wechat-pay.png alt="WeChat Pay"><p>微信打赏</p></div><div id=alipay style=display:inline-block><img id=alipay_qr src=/img/ali-pay.png alt=Alipay><p>支付宝打赏</p></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=http://next.lisenhui.cn/post/study/kaggle/kaggle%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E-x%E5%85%89%E6%A3%80%E6%B5%8B/ rel=next title="科大讯飞 X光检测"><i class="fa fa-chevron-left"></i>科大讯飞 X光检测</a></div><div class="post-nav-prev post-nav-item"><a href=http://next.lisenhui.cn/post/study/kaggle/kaggle%E6%80%9D%E8%B7%AF%E6%80%BB%E7%BB%93%E5%92%8C%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/ rel=prev title=Kaggle记录总结>Kaggle记录总结
<i class="fa fa-chevron-right"></i></a></div></div><div id=wcomments></div></footer></article></section></div></div><div class=sidebar-toggle><div class=sidebar-toggle-line-wrap><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id=sidebar class=sidebar><div class=sidebar-inner><section class="site-overview sidebar-panel sidebar-panel-active"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image src=/img/linxi_icon.png alt=烟雨临溪><p class=site-author-name itemprop=name>烟雨临溪</p><p class="site-description motion-element" itemprop=description>再平凡的人也有属于他自己的梦想!</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href=/post/><span class=site-state-item-count>112</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>6</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>31</span>
<span class=site-state-item-name>标签</span></a></div></nav><div class="links-of-author motion-element"><span class=links-of-author-item><a href=https://github.com/yanyuLinxi target=_blank title=GitHub><i class="fa fa-fw fa-github"></i>GitHub</a></span>
<span class=links-of-author-item><a href=https://space.bilibili.com/19237450 target=_blank title=哔哩哔哩><i class="fa fa-fw fa-globe"></i>哔哩哔哩</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class=links-of-blogroll-title><i class="fa fa-fw fa-globe"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://www.liaoxuefeng.com/ title=廖雪峰 target=_blank>廖雪峰</a></li></ul></div><div class="tagcloud-of-blogroll motion-element tagcloud-of-blogroll-inline"><div class=tagcloud-of-blogroll-title><i class="fa fa-fw fa-tags"></i>标签云</div><ul class=tagcloud-of-blogroll-list><li class=tagcloud-of-blogroll-item><a href=/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0>论文阅读笔记</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E5%BC%82%E5%B8%B8%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90>异常行为分析</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/python%E7%9B%B8%E5%85%B3%E5%BA%93%E5%AD%A6%E4%B9%A0>Python相关库学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E5%BA%93>机器学习相关库</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/java%E5%AD%A6%E4%B9%A0>Java学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/insider-threat>Insider threat</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/leetcode%E5%AD%A6%E4%B9%A0>Leetcode学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E7%BE%8E%E9%A3%9F%E7%82%B9%E8%AF%84>美食点评</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/rnn>Rnn</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0>科研学习笔记</a></li></ul></div></section></div></aside></div></main><footer id=footer class=footer><div class=footer-inner><div class=copyright><span class=copyright-year>&copy; 2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span><span class=copyright-author>希望至美，永不凋零</span></div><div class=powered-info><span class=powered-by>Powered by - <a class=powered-link href=//gohugo.io target=_blank title=hugo>Hugo v0.81.0</a></span>
<span class=separator-line>/</span>
<span class=theme-info>Theme by - <a class=powered-link href=//github.com/elkan1788/hugo-theme-next target=_blank>NexT</a></span></div><div class=vistor-info><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span class=site-uv><i class="fa fa-user"></i><span class=busuanzi-value id=busuanzi_value_site_uv></span></span><span class=separator-line>/</span>
<span class=site-pv><i class="fa fa-eye"></i><span class=busuanzi-value id=busuanzi_value_site_pv></span></span></div><div class=license-info><span class=storage-info>Storage by
<a href=https://www.ucloud.cn/ style=font-weight:700 target=_blank>UCloud云存储</a></span>
<span class=separator-line>/</span>
<span class=license-num><a href=http://beian.miit.gov.cn target=_blank>未归档</a></span></div></div></footer><div class=back-to-top><i class="fa fa-arrow-up"></i><span id=scrollpercent><span>0</span>%</span></div></div><script type=text/javascript src=//cdn.bootcdn.net/ajax/libs/jquery/2.1.4/jquery.min.js></script><script type=text/javascript src=/js/search.js></script><script type=text/javascript src=/js/affix.js></script><script type=text/javascript src=/js/scrollspy.js></script><script type=text/javascript>function detectIE(){var a=window.navigator.userAgent,b=a.indexOf('MSIE '),c=a.indexOf('Trident/'),d=a.indexOf('Edge/');return b>0||c>0||d>0?-1:1}function getCntViewHeight(){var b=$('#content').height(),a=$(window).height(),c=b>a?b-a:$(document).height()-a;return c}function getScrollbarWidth(){var a=$('<div />').addClass('scrollbar-measure').prependTo('body'),b=a[0],c=b.offsetWidth-b.clientWidth;return a.remove(),c}function registerBackTop(){var b=50,a=$('.back-to-top');$(window).on('scroll',function(){var d,e,f,c,g;a.toggleClass('back-to-top-on',window.pageYOffset>b),d=$(window).scrollTop(),e=getCntViewHeight(),f=d/e,c=Math.round(f*100),g=c>100?100:c,$('#scrollpercent>span').html(g)}),a.on('click',function(){$("html,body").animate({scrollTop:0,screenLeft:0},800)})}function initScrollSpy(){var a='.post-toc',d=$(a),b='.active-current';d.on('activate.bs.scrollspy',function(){var b=$(a+' .active').last();c(),b.addClass('active-current')}).on('clear.bs.scrollspy',c),$('body').scrollspy({target:a});function c(){$(a+' '+b).removeClass(b.substring(1))}}function initAffix(){var a=$('.header-inner').height(),b=parseInt($('.main').css('padding-bottom'),10),c=a+10;$('.sidebar-inner').affix({offset:{top:c,bottom:b}}),$(document).on('affixed.bs.affix',function(){updateTOCHeight(document.body.clientHeight-100)})}function initTOCDimension(){var a,b;$(window).on('resize',function(){a&&clearTimeout(a),a=setTimeout(function(){var a=document.body.clientHeight-100;updateTOCHeight(a)},0)}),updateTOCHeight(document.body.clientHeight-100),b=getScrollbarWidth(),$('.post-toc').css('width','calc(100% + '+b+'px)')}function updateTOCHeight(a){a=a||'auto',$('.post-toc').css('max-height',a)}$(function(){var b=$('.header-inner').height()+10,c,d,a,e;$('#sidebar').css({'margin-top':b}).show(),c=parseInt($('#sidebar').css('margin-top')),d=parseInt($('.sidebar-inner').css('height')),a=c+d,e=$('.content-wrap').height(),e<a&&$('.content-wrap').css('min-height',a),$('.site-nav-toggle').on('click',function(){var a=$('.site-nav'),e=$('.toggle'),b='site-nav-on',f='toggle-close',c=a.hasClass(b),g=c?'slideUp':'slideDown',d=c?'removeClass':'addClass';a.stop()[g]('normal',function(){a[d](b),e[d](f)})}),registerBackTop(),initScrollSpy(),initAffix(),initTOCDimension(),$('.sidebar-nav-toc').click(function(){$(this).addClass('sidebar-nav-active'),$(this).next().removeClass('sidebar-nav-active'),$('.'+$(this).next().attr('data-target')).toggle(500),$('.'+$(this).attr('data-target')).toggle(500)}),$('.sidebar-nav-overview').click(function(){$(this).addClass('sidebar-nav-active'),$(this).prev().removeClass('sidebar-nav-active'),$('.'+$(this).prev().attr('data-target')).toggle(500),$('.'+$(this).attr('data-target')).toggle(500)})})</script><script src=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.js></script><script type=text/javascript>$(function(){$('.post-body').viewer()})</script><script type=text/javascript>$(function(){detectIE()>0?$.getScript(document.location.protocol+'//cdn.jsdelivr.net/npm/@waline/client/dist/Waline.min.js',function(){new Waline({el:'#wcomments',visitor:!0,avatar:'wavatar',avatarCDN:'https://sdn.geekzu.org/avatar/',avatarForce:!1,wordLimit:'200',placeholder:' 欢迎留下您的宝贵建议，请填写您的昵称和邮箱便于后续交流. ^_^ ',requiredFields:['nick','mail'],serverURL:"Your WalineSerURL",lang:"zh-cn"})}):$('#wcomments').html('抱歉，Waline插件不支持IE或Edge，建议使用Chrome浏览器。')})</script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=Your%20AddthisId"></script><script>(function(){var a=document.createElement('script'),c=window.location.protocol.split(':')[0],b;c==='https'?a.src='https://zz.bdstatic.com/linksubmit/push.js':a.src='http://push.zhanzhang.baidu.com/push.js',b=document.getElementsByTagName("script")[0],b.parentNode.insertBefore(a,b)})()</script></body></html>