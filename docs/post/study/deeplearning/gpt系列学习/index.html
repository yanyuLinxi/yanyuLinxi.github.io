<!doctype html><html lang=zh-cn dir=content/zh-cn><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=content-security-policy content="upgrade-insecure-requests"><title>GPT系列学习 - 阳阳的人间旅游日记</title><meta name=keywords content="博客,程序员,思考,读书,笔记,技术,分享"><meta name=author content="阳阳"><meta property="og:title" content="GPT系列学习"><meta property="og:site_name" content="阳阳的人间旅游日记"><meta property="og:image" content="/img/author.jpg"><meta name=title content="GPT系列学习 - 阳阳的人间旅游日记"><meta name=description content="欢迎来到临溪的博客站，个人主要专注于机器学习、深度学习的相关研究。在这里分享自己的学习心得。"><link rel="shortcut icon" href=/img/favicon.ico><link rel=apple-touch-icon href=/img/apple-touch-icon.png><link rel=apple-touch-icon-precomposed href=/img/apple-touch-icon.png><link href=//cdn.bootcdn.net/ajax/libs/font-awesome/4.6.2/css/font-awesome.min.css rel=stylesheet type=text/css><link href=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.css rel=stylesheet><link href=/css/main.css rel=stylesheet type=text/css><link href=/css/syntax.css rel=stylesheet type=text/css></head><body itemscope itemtype=http://schema.org/WebPage lang=zh-hans><div class="container one-collumn sidebar-position-left page-home"><div class=headband></div><header id=header class=header itemscope itemtype=http://schema.org/WPHeader><div class=header-inner><div class=site-brand-container><div class=site-nav-toggle><div class=toggle role=button style=opacity:1;top:0><span class=toggle-line></span><span class=toggle-line></span><span class=toggle-line></span></div></div><div class=site-meta><div class=multi-lang-switch><i class="fa fa-fw fa-language" style=margin-right:5px></i><a class=lang-link id=zh-cn href=#>中文</a></div><div class=custom-logo-site-title><a href=/ class=brand rel=start><span class=logo-line-before><i></i></span><span class=site-title>阳阳的人间旅游日记</span>
<span class=logo-line-after><i></i></span></a></div><p class=site-subtitle>让我们消除隔阂的，不是无所不知的脑袋，而是手拉手，坚决不放弃的那颗心</p></div><div class=site-nav-right><div class="toggle popup-trigger" style=opacity:1;top:0><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class=site-nav><ul id=menu class=menu><li class=menu-item><a href=/ rel=section><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class=menu-item><a href=/post rel=section><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class=menu-item><a href=/about.html rel=section><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于我</a></li><li class=menu-item><a href=/404.html rel=section><i class="menu-item-icon fa fa-fw fa-heartbeat"></i><br>公益404</a></li><li class="menu-item menu-item-search"><a href=javascript:; class=popup-trigger><i class="menu-item-icon fa fa-search fa-fw"></i><br>搜索</a></li></ul><div class=site-search><div class="popup search-popup local-search-popup"><div class="local-search-header clearfix"><span class=search-icon><i class="fa fa-search"></i></span><span class=popup-btn-close><i class="fa fa-times-circle"></i></span><div class=local-search-input-wrapper><input autocomplete=off placeholder=搜索关键字... spellcheck=false type=text id=local-search-input autocapitalize=none autocorrect=off></div></div><div id=local-search-result></div></div></div></nav></div></header><main id=main class=main><div class=main-inner><div class=content-wrap><div id=content class=content><section id=posts class=posts-expand><article class="post post-type-normal" itemscope itemtype=http://schema.org/Article><header class=post-header><h1 class=post-title itemprop="name headline"><a class=post-title-link href=https://yanyulinxi.github.io/post/study/deeplearning/gpt%E7%B3%BB%E5%88%97%E5%AD%A6%E4%B9%A0/ itemprop=url>GPT系列学习</a></h1><div class=post-meta><span class=post-time><i class="fa fa-calendar-o fa-fw"></i><span class=post-meta-item-text>时间：</span>
<time itemprop=dateCreated datetime=2016-03-22T13:04:35+08:00 content="2022-06-11">2022-06-11</time></span>
<span class=post-category>&nbsp; | &nbsp;
<i class="fa fa-folder-o fa-fw"></i><span class=post-meta-item-text>分类：</span>
<span itemprop=about itemscope itemtype=https://schema.org/Thing><a href=/categories/%E5%AD%A6%E4%B9%A0 itemprop=url rel=index style=text-decoration:underline><span itemprop=name>学习</span></a>
&nbsp;</span></span>
<span>|
<i class="fa fa-file-word-o fa-fw"></i><span class=post-meta-item-text>字数：</span>
<span class=leancloud-world-count>2505 字</span></span>
<span>|
<i class="fa fa-eye fa-fw"></i><span class=post-meta-item-text>阅读：</span>
<span class=leancloud-view-count>5分钟</span></span>
<span id=/post/study/deeplearning/gpt%E7%B3%BB%E5%88%97%E5%AD%A6%E4%B9%A0/ class=leancloud_visitors data-flag-title=GPT系列学习>|
<i class="fa fa-binoculars fa-fw"></i><span class=post-meta-item-text>阅读次数：</span>
<span class=leancloud-visitors-count></span></span></div></header><div class=post-body itemprop=articleBody><h2 id=资料>资料</h2><ol><li><a href="https://www.bilibili.com/video/BV1AF411b7xQ?share_source=copy_web">视频资料李沐(强烈推荐)</a></li><li><a href=https://zhuanlan.zhihu.com/p/412351920>GPT系列论文阅读</a></li><li><a href=https://www.aminer.cn/research_report/6039bfcce8a87f775ad225f4>GPT-3代码范例</a></li><li><a href=https://zhuanlan.zhihu.com/p/419215591>https://zhuanlan.zhihu.com/p/419215591</a> prompt learning 总结。</li><li>prompt <a href=https://finisky.github.io/briefintrotoprompt/>https://finisky.github.io/briefintrotoprompt/</a></li><li>bert gpt对比。bert是双向，gpt是单向。gpt参数量比bert大好多倍。gpt主要就是做生成式的语言模型。bert需要微调，gpt主要在做零样本任务？</li><li>再了解下bert。</li><li>bert和gpt的对比：https://analyticsindiamag.com/gpt-3-vs-bert-for-nlp-tasks/<ol><li><a href=https://analyticsindiamag.com/gpt-3-vs-bert-for-nlp-tasks/>https://analyticsindiamag.com/gpt-3-vs-bert-for-nlp-tasks/</a></li></ol></li><li>GPT-3仅仅公布了API，没有开源模型。换句话说，没法改。<ol><li>gpt-3自己没开源，只有别人的复现版，复现版细节无法保证。自己也没有办法训练。</li><li>2022-5月发布的。</li><li>gpt3最重要的是zero-shot的能力。</li></ol></li></ol><h2 id=简介>简介</h2><h3 id=对比>对比</h3><table><thead><tr><th style=text-align:center></th><th style=text-align:center>Bert</th><th style=text-align:center>GPT（Generative Pre-Training)</th></tr></thead><tbody><tr><td style=text-align:center>时间线</td><td style=text-align:center>bert:2018/10</td><td style=text-align:center>gpt:2018/6, gpt-2 2019/2, gpt-3:2020/5</td></tr><tr><td style=text-align:center>参数量</td><td style=text-align:center>bert：3.5亿</td><td style=text-align:center>gpt:1亿,gpt-2：10亿，gpt-3：1750亿。</td></tr><tr><td style=text-align:center>效果</td><td style=text-align:center>同等参数下， Bert好于GPT</td><td></td></tr><tr><td style=text-align:center>模型</td><td style=text-align:center>Transformer编码器，双向结构</td><td style=text-align:center>Transformer解码器，单向结构</td></tr><tr><td style=text-align:center>用途</td><td style=text-align:center>归纳Inductive</td><td style=text-align:center>生成Generative</td></tr><tr><td style=text-align:center>特点</td><td style=text-align:center>需要微调</td><td style=text-align:center>做zero-shot</td></tr><tr><td style=text-align:center></td><td style=text-align:center></td><td></td></tr></tbody></table><ol><li>参数量对比：gpt:1亿，，gpt-2：10亿，gpt-3：1750亿。</li><li>效果对比：在同等参数量上，bert效果好于GPT。</li></ol><p>为什么Transformer后，预训练的模型就火了。因为Transformer这种模块生成的特征非常稳定，推测是因为学了更多的结构信息。</p><h3 id=模型详解>模型详解</h3><ol><li>目标函数:</li><li>$$L_ { 1 } ( U ) = \sum _ { i } \log P ( u _ { i } | u _ { i - k} , \cdots , u _ { i - 1 } ; \theta )$$<ol><li>在参数为$\theta$的情况下，使用窗口大小为k的文本中找到关系来使得$u_i$的概率最大。</li></ol></li><li>使用的Transformer的解码器。Transformer的解码器和编码器的区别参考:<a href=https://yanyulinxi.github.io/post/study/deeplearning/transformer%E5%92%8Cbert%E9%9D%A2%E8%AF%95%E9%A2%98/>Transformer和Bert讲解</a>。 主要的区别是解码器会使用掩码盖住后面的词的输入，使得模型只能看到当前词之前的输入。Bert是使用一种完形填空的预测方式，使用上下文来预测当前词。GPT是预测未来，Bert是预测连续状态中的某个状态，这是导致Bert效果比GPT好的原因</li><li>模型输入:<ol><li>$$U=(u_{-k},&mldr;,u_{-1})$$</li><li>$$h = UW_e + W_p$$</li><li>$$h_l = transformer_block(h_{l-1})$$</li><li>$$p(u)=softmax(h_{n}W_e^T)$$</li></ol></li><li>微调<ol><li>对于x1,&mldr;,x_m，标号y</li><li>使用最后一层块的x_m的特征计算softmax</li><li>两个目标函数：<ol><li>给序列预测下一个此</li><li>给序列预测目标</li><li>这两个目标函数结合起来最佳。参考论文。</li></ol></li><li>多种微调方式:<ol><li>分类：加入Extract分隔符。使用Extract词根送入线性层</li><li>包含：前一个问题、后一个问题是否包含。</li><li>相似：A和B相似，B也要和A相似。由于GPT是序列模型，所以相似使用两个模型。然后做二分类</li><li>选择：问题和多个答案进行选择。</li></ol></li></ol></li></ol><p>GPT2：</p><ol><li>参数量10亿。</li><li>学习如何写作文。当你参数量翻了倍后，效果还是比不上别人的时候，怎么办？从新角度看问题，找新卖点：zero-shot<ol><li>zero-shot：我训练好一个模型后，在下游模型上，我完全不用下游模型的数据集，也完全不用再训练。</li></ol></li><li>做zero-shot遇到的问题，如果我还想gpt-1一样引入分隔符的话，我需要在下游任务上重新去学习这些任务。这违背了初衷。所以不能引入分隔符。而是引入了prompt（提示）<ol><li>比如一个英语翻译到法语的任务：</li><li>(translate to french, english text, french text)， 前面的translate to french 就是prompt</li><li>比如回答问题</li><li>（answer the question, document, question, answer)。prompt就是"answer the question"。</li><li>这个思路是MAML这篇论文提出来的想法。意思就是如果模型足够强大，我可以让模型去学习理解任务。</li></ol></li></ol><p>GPT3（技术报告）:</p><ol><li>在zero-shot上继续发散。zero-shot是完全的零样本，但是可以考虑提供一点点样本。</li><li>值得注意的点：作者的个数，31个作者，且末尾给出了每个作者明确的贡献。</li><li>提出两种模式：<ol><li>meta-learning。训练一个特别大的模型，比拼泛化性能。完全零样本。</li><li>in-context learning。有部分样本(10-100个样本），但仍然不做梯度下降或者微调。</li></ol></li><li>学习方式：<ol><li>zero-shot:<ol><li>Translate English to French.</li><li>cheese =></li><li>这个=>就是prompt，就是告诉模型，现在该你输出了。</li></ol></li><li>one-shot。相比于zero-shot会提供一个样例。<ol><li>这个案例仍然不参与模型梯度下降，是让模型从上下文中学习到知识，学习需要去干什么。</li></ol></li><li>Few-shot。相比于one-shot多加了些样本。</li><li>这样学习的问题：<ol><li>Few-shot中学习到的知识无法保留。</li><li>我有远超Few-shot样本个数的样本，没法参与这个模型进行训练。</li></ol></li></ol></li><li>模型<ol><li>使用GPT-2的模型，使用了Sparse Transformer类似的改动（还没看，从名字上看是稀疏transformer）</li></ol></li><li>实验<ol><li>注意batch_size大小、模型大小的对比。<ol><li>模型越来越大的时候，过拟合没有那么严重。</li><li>模型小的时候，需要batch_size提供噪音来提升泛化误差。</li></ol></li><li>优化细节有很多东西，但是论文没讲。</li></ol></li><li>局限性：<ol><li>文本生成还有难度。</li><li>gpt只能往前看，不能像bert一样反过来看。</li><li>只看见了文本，没看其他类型的数据。gpt只能均匀的预测下一个词，没法告诉哪些词是重点（没懂）</li><li>一个不确定的事，来一个任务，不知道是不是真的从头学习，还是因为之前的数据中出现过这些任务。</li><li>训练起来非常贵。</li><li>黑盒子。不知道决策怎么做的，不知道哪些起到了作用。</li></ol></li><li>可以看看案例。</li><li>危害性：<ol><li>生成性太强，可能以假乱真。对社会造成危害，比如假新闻，垃圾邮件。</li><li>模型有性别歧视。男性评价词偏低，女性评价词高。</li><li>模型有种族歧视，对种族有非常大的差别。</li><li>有宗教歧视。</li><li>能耗相当夸张。</li></ol></li></ol><h3 id=难点>难点</h3><ol><li>GPT-3没有开源。只提供了API。大家也不是在GPT上做文章。GPT-1，-2，-3三个引用量加起来是bert的一半。</li><li>GPT-3参数量太大。以现在的经验，5亿参数的bert需要两张显卡，GPT-3需要350张显卡联合训练。</li><li>GPT-3的API没有开放中国区API使用。且免费的API有使用量的限制。</li><li>GPT更多关注于zero-shot，Bert则更关注于微调。</li></ol><h2 id=notes>Notes</h2><ol><li>机器学习中，部分有标签，大量无标签的学习叫semi-supervised，半监督。后面的bert、gpt叫自监督。</li><li>论文价值：新颖度<em>有效性</em>问题大小。</li></ol></div><footer class=post-footer><div class=addthis_inline_share_toolbox></div><div class=post-nav><div class=article-copyright><div class=article-copyright-img><img src=/img/qq_qrcode.png width=129px height=129px><div style=text-align:center>QQ扫一扫交流</div></div><div class=article-copyright-info><p><span>声明：</span>GPT系列学习</p><p style=word-break:break-all><span>链接：</span>
https://yanyulinxi.github.io/post/study/deeplearning/gpt%E7%B3%BB%E5%88%97%E5%AD%A6%E4%B9%A0/</p><p><span>作者：</span>阳阳</p><p><span>邮箱：</span>yanyulinxi@qq.com</p><p><span>声明： </span>本博客文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/3.0/ target=_blank style=text-decoration:underline>CC BY-NC-SA 3.0</a>许可协议，转载请注明出处！</p></div></div><div class=clear></div></div><div class=reward-qr-info><div>创作实属不易，如有帮助，那就打赏博主些许茶钱吧 ^_^</div><button id=rewardButton disable=enable onclick="var qr=document.getElementById('QR');qr.style.display==='none'?qr.style.display='block':qr.style.display='none'">
<span>赏</span></button><div id=QR style=display:none><div id=wechat style=display:inline-block><img id=wechat_qr src=/img/wechat-pay.png alt="WeChat Pay"><p>微信打赏</p></div><div id=alipay style=display:inline-block><img id=alipay_qr src=/img/ali-pay.png alt=Alipay><p>支付宝打赏</p></div></div></div><div class=post-nav><div class="post-nav-next post-nav-item"><a href=https://yanyulinxi.github.io/post/study/kaggle/datawhale/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%BB%84%E9%98%9F%E5%AD%A6%E4%B9%A0/%E7%BB%84%E9%98%9F%E6%B5%81%E7%A8%8B/ rel=next title=组队流程><i class="fa fa-chevron-left"></i>组队流程</a></div><div class="post-nav-prev post-nav-item"><a href=https://yanyulinxi.github.io/post/essay/thought/%E8%BF%BD%E5%BF%86%E4%BC%BC%E6%B0%B4%E5%B9%B4%E5%8D%8E/ rel=prev title=追忆似水年华>追忆似水年华
<i class="fa fa-chevron-right"></i></a></div></div><div id=wcomments></div></footer></article></section></div></div><div class=sidebar-toggle><div class=sidebar-toggle-line-wrap><span class="sidebar-toggle-line sidebar-toggle-line-first"></span><span class="sidebar-toggle-line sidebar-toggle-line-middle"></span><span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id=sidebar class=sidebar><div class=sidebar-inner><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target=post-toc-wrap>文章目录</li><li class=sidebar-nav-overview data-target=site-overview>站点概览</li></ul><section class="site-overview sidebar-panel"><div class="site-author motion-element" itemprop=author itemscope itemtype=http://schema.org/Person><img class=site-author-image itemprop=image src=/img/linxi_icon.png alt=阳阳><p class=site-author-name itemprop=name>阳阳</p><p class="site-description motion-element" itemprop=description>再平凡的人也有属于他自己的梦想!</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href=/post/><span class=site-state-item-count>141</span>
<span class=site-state-item-name>日志</span></a></div><div class="site-state-item site-state-categories"><a href=/categories/><span class=site-state-item-count>6</span>
<span class=site-state-item-name>分类</span></a></div><div class="site-state-item site-state-tags"><a href=/tags/><span class=site-state-item-count>35</span>
<span class=site-state-item-name>标签</span></a></div></nav><div class="links-of-author motion-element"><span class=links-of-author-item><a href=https://github.com/yanyuLinxi target=_blank title=GitHub><i class="fa fa-fw fa-github"></i>GitHub</a></span>
<span class=links-of-author-item><a href=https://space.bilibili.com/19237450 target=_blank title=哔哩哔哩><i class="fa fa-fw fa-globe"></i>哔哩哔哩</a></span></div><div class="links-of-blogroll motion-element links-of-blogroll-inline"><div class=links-of-blogroll-title><i class="fa fa-fw fa-globe"></i>友情链接</div><ul class=links-of-blogroll-list><li class=links-of-blogroll-item><a href=https://www.liaoxuefeng.com/ title=廖雪峰 target=_blank>廖雪峰</a></li></ul></div><div class="tagcloud-of-blogroll motion-element tagcloud-of-blogroll-inline"><div class=tagcloud-of-blogroll-title><i class="fa fa-fw fa-tags"></i>标签云</div><ul class=tagcloud-of-blogroll-list><li class=tagcloud-of-blogroll-item><a href=/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0>论文阅读笔记</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/python%E7%9B%B8%E5%85%B3%E5%BA%93%E5%AD%A6%E4%B9%A0>Python相关库学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E5%BC%82%E5%B8%B8%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90>异常行为分析</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%BE%85%E5%8A%A9%E5%B7%A5%E5%85%B7>深度学习辅助工具</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9B%B8%E5%85%B3%E5%BA%93>机器学习相关库</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/kaggle>Kaggle</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/java%E5%AD%A6%E4%B9%A0>Java学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/insider-threat>Insider threat</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/leetcode%E5%AD%A6%E4%B9%A0>Leetcode学习</a></li><li class=tagcloud-of-blogroll-item><a href=/tags/spark>Spark</a></li></ul></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class=post-toc><div class=post-toc-content><nav id=TableOfContents><ul><li><a href=#资料>资料</a></li><li><a href=#简介>简介</a><ul><li><a href=#对比>对比</a></li><li><a href=#模型详解>模型详解</a></li><li><a href=#难点>难点</a></li></ul></li><li><a href=#notes>Notes</a></li></ul></nav></div></div></section></div></aside></div></main><footer id=footer class=footer><div class=footer-inner><div class=copyright><span class=copyright-year>&copy; 2010 - 2022</span>
<span class=with-love><i class="fa fa-heart"></i></span><span class=copyright-author>阳阳的人间旅游日记</span></div><div class=powered-info><span class=powered-by>Powered by - <a class=powered-link href=//gohugo.io target=_blank title=hugo>Hugo v0.81.0</a></span>
<span class=separator-line>/</span>
<span class=theme-info>Theme by - <a class=powered-link href=//github.com/elkan1788/hugo-theme-next target=_blank>NexT</a></span></div><div class=vistor-info><script async src=//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js></script><span class=site-uv><i class="fa fa-user"></i><span class=busuanzi-value id=busuanzi_value_site_uv></span></span><span class=separator-line>/</span>
<span class=site-pv><i class="fa fa-eye"></i><span class=busuanzi-value id=busuanzi_value_site_pv></span></span></div><div class=license-info><span class=storage-info>Storage by
<a href style=font-weight:700 target=_blank></a></span><span class=separator-line>/</span>
<span class=license-num><a href target=_blank></a></span></div></div></footer><div class=back-to-top><i class="fa fa-arrow-up"></i><span id=scrollpercent><span>0</span>%</span></div></div><script type=text/javascript src=//cdn.bootcdn.net/ajax/libs/jquery/2.1.4/jquery.min.js></script><script type=text/javascript src=/js/search.js></script><script type=text/javascript src=/js/affix.js></script><script type=text/javascript src=/js/scrollspy.js></script><script type=text/javascript>function detectIE(){var a=window.navigator.userAgent,b=a.indexOf('MSIE '),c=a.indexOf('Trident/'),d=a.indexOf('Edge/');return b>0||c>0||d>0?-1:1}function getCntViewHeight(){var b=$('#content').height(),a=$(window).height(),c=b>a?b-a:$(document).height()-a;return c}function getScrollbarWidth(){var a=$('<div />').addClass('scrollbar-measure').prependTo('body'),b=a[0],c=b.offsetWidth-b.clientWidth;return a.remove(),c}function registerBackTop(){var b=50,a=$('.back-to-top');$(window).on('scroll',function(){var d,e,f,c,g;a.toggleClass('back-to-top-on',window.pageYOffset>b),d=$(window).scrollTop(),e=getCntViewHeight(),f=d/e,c=Math.round(f*100),g=c>100?100:c,$('#scrollpercent>span').html(g)}),a.on('click',function(){$("html,body").animate({scrollTop:0,screenLeft:0},800)})}function initScrollSpy(){var a='.post-toc',d=$(a),b='.active-current';d.on('activate.bs.scrollspy',function(){var b=$(a+' .active').last();c(),b.addClass('active-current')}).on('clear.bs.scrollspy',c),$('body').scrollspy({target:a});function c(){$(a+' '+b).removeClass(b.substring(1))}}function initAffix(){var a=$('.header-inner').height(),b=parseInt($('.main').css('padding-bottom'),10),c=a+10;$('.sidebar-inner').affix({offset:{top:c,bottom:b}}),$(document).on('affixed.bs.affix',function(){updateTOCHeight(document.body.clientHeight-100)})}function initTOCDimension(){var a,b;$(window).on('resize',function(){a&&clearTimeout(a),a=setTimeout(function(){var a=document.body.clientHeight-100;updateTOCHeight(a)},0)}),updateTOCHeight(document.body.clientHeight-100),b=getScrollbarWidth(),$('.post-toc').css('width','calc(100% + '+b+'px)')}function updateTOCHeight(a){a=a||'auto',$('.post-toc').css('max-height',a)}$(function(){var b=$('.header-inner').height()+10,c,d,a,e;$('#sidebar').css({'margin-top':b}).show(),c=parseInt($('#sidebar').css('margin-top')),d=parseInt($('.sidebar-inner').css('height')),a=c+d,e=$('.content-wrap').height(),e<a&&$('.content-wrap').css('min-height',a),$('.site-nav-toggle').on('click',function(){var a=$('.site-nav'),e=$('.toggle'),b='site-nav-on',f='toggle-close',c=a.hasClass(b),g=c?'slideUp':'slideDown',d=c?'removeClass':'addClass';a.stop()[g]('normal',function(){a[d](b),e[d](f)})}),registerBackTop(),initScrollSpy(),initAffix(),initTOCDimension(),$('.sidebar-nav-toc').click(function(){$(this).addClass('sidebar-nav-active'),$(this).next().removeClass('sidebar-nav-active'),$('.'+$(this).next().attr('data-target')).toggle(500),$('.'+$(this).attr('data-target')).toggle(500)}),$('.sidebar-nav-overview').click(function(){$(this).addClass('sidebar-nav-active'),$(this).prev().removeClass('sidebar-nav-active'),$('.'+$(this).prev().attr('data-target')).toggle(500),$('.'+$(this).attr('data-target')).toggle(500)})})</script><script src=//cdn.bootcdn.net/ajax/libs/imageviewer/0.1.0/viewer.min.js></script><script type=text/javascript>$(function(){$('.post-body').viewer()})</script><script type=text/javascript>$(function(){detectIE()>0?$.getScript(document.location.protocol+'//cdn.jsdelivr.net/npm/@waline/client/dist/Waline.min.js',function(){new Waline({el:'#wcomments',visitor:!0,avatar:'wavatar',avatarCDN:'https://sdn.geekzu.org/avatar/',avatarForce:!1,wordLimit:'200',placeholder:' 欢迎留下您的宝贵建议，请填写您的昵称和邮箱便于后续交流. ^_^ ',requiredFields:['nick','mail'],serverURL:"Your WalineSerURL",lang:"zh-cn"})}):$('#wcomments').html('抱歉，Waline插件不支持IE或Edge，建议使用Chrome浏览器。')})</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_SVG"></script><script type=text/javascript src="//s7.addthis.com/js/300/addthis_widget.js#pubid=Your%20AddthisId"></script><script>(function(){var a=document.createElement('script'),c=window.location.protocol.split(':')[0],b;c==='https'?a.src='https://zz.bdstatic.com/linksubmit/push.js':a.src='http://push.zhanzhang.baidu.com/push.js',b=document.getElementsByTagName("script")[0],b.parentNode.insertBefore(a,b)})()</script></body></html>