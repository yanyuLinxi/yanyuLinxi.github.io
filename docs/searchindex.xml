<?xml version="1.0" encoding="utf-8" standalone="yes"?><search><entry><title>（置顶）机器学习面经总结</title><url>http://next.lisenhui.cn/post/study/deeplearning/%E7%BD%AE%E9%A1%B6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E7%BB%8F%E6%80%BB%E7%BB%93/</url><categories><category>学习</category></categories><tags><tag>机器学习</tag></tags><content type="html"> 机器学习面经知识 笔者是22年参加实习的机器学习方向的学生，自己总结了面经，先开源出来供大家学习。</content></entry><entry><title>Bat脚本常用命令浅学</title><url>http://next.lisenhui.cn/post/study/pcsoftware/bat%E8%84%9A%E6%9C%AC%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%B5%85%E5%AD%A6/</url><categories><category>学习</category></categories><tags/><content type="html"> 基础语法 赋值的时候等号两边没有空格 set a=%b 取值：%a 输出变量：%a% 常用命令 @REM 注释
xcopy 批量复制文件夹
实例： xcopy /e/y/i/f source destination 参数解释： /e 复制所有子目录，即使它们是空的。将/e与/s和/t命令行选项一起使用。 /y 禁止提示确认您要覆盖现有目标文件。 /i 如果Source是目录或包含通配符且Destination不存在，则xcopy假定Destination指定目录名称并创建新目录。然后，xcopy将所有指定的文件复制到新目录中。默认情况下，xcopy会提示您指定Destination是文件还是目录。 /f 复制时显示源文件名和目标文件名。 /? 提供帮助 %cd% 获取当前路径。
判断参数为空
@REM 空格都不能写错 if "%one%"=="" ( set msg="rebuilding site %date%%time%" )else ( set msg="%one%rebuilding site %date%%time%" )</content></entry><entry><title>GraphCodeBert PreTraining Code Representation With Data Flow</title><url>http://next.lisenhui.cn/post/paperreading/graphcodebert-pretraining-code-representation-with-data-flow/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag></tags><content type="html"> 1. 综述翻译 编程语言的预训练模型在代码搜索、代码完成、代码摘要等各种与代码相关的任务上取得了显着的经验改进。然而，现有的预训练模型将代码片段视为一系列标记，同时忽略了代码的固有结构，它提供了关键的代码语义并会增强代码理解过程。我们提出了 GraphCodeBERT，这是一种考虑代码固有结构的编程语言预训练模型。我们没有像抽象语法树（AST）那样采用代码的语法级结构，而是在预训练阶段使用数据流，这是一种代码的语义级结构，编码了“价值来自哪里”的关系”变量之间。这种语义级结构不太复杂，不会带来不必要的 AST 层次结构，其属性使模型更有效。我们基于 Transformer 开发 GraphCodeBERT。除了使用掩码语言建模任务外，我们还引入了两个结构感知预训练任务。一种是预测代码结构边缘，另一种是对齐源代码和代码结构之间的表示。我们使用图形引导的蒙面注意功能以有效的方式实现模型，以结合代码结构。我们在四个任务上评估我们的模型，包括代码搜索、克隆检测、代码翻译和代码细化。结果表明，代码结构和新引入的预训练任务可以提高 GraphCodeBERT 并在四个下游任务上实现最先进的性能。我们进一步表明，该模型在代码搜索任务中更喜欢结构级别的注意力而不是令牌级别的注意力。
1.1 发表于 ICLR 2021
2. Tag bert; code semantic extraction; data flow; ast;
3. 任务描述 代码表征模型。在多个任务上进行训练。 1.code search 2.clone detection 3.code translation 4.code refinement
4. 方法 特点 从AST中提取data flow
data flow是一个图，图上的节点代表变量，边表示这些变量的值来自哪里。不像AST，data flow在相同的源代码的不同的抽象语法下是一样的。AST是提取了代码的syntactic-level结构信息，而data flow提取代码的semantic-level信息。同时，data flow有助于模型考虑离的很远距离但是使用了相同的变量或函数引起的长期依赖关系。例如下图的代码中有好几个地方都有x，通过提取到的data flow后，我们可以知道，x11会更加注意x7和x9而不是x3。 GraphCodeBert采用数据流而不是AST，是考虑到数据流图不像AST这么复杂，也不会带来不必要的深层信息。 输入： 给定源代码C={c1,c2,,Cn},对应的注释W={w1,w2,wm}.相应的数据流图G(C)=(V,E),V={v1,v2,}为变量序列，E={e1,e2,}为边集合，其中每条边代表数据流向。 输入包括以下部分：
源代码C={c1,c2,&hellip;,Cn} 对应的注释W={w1,w2,&hellip;,wm} 变量序列V={v1,v2,&hellip;,vk} 最终输入的序列X为上面3个序列的连接 X={[CLS],W,[SEP] C,[SEP],V}
输入序列X会被转化为向量H^0。包括了token和position embedding。并对变量序列V使用了一种特殊的position embedding来标识它们是数据流图的一个结点。
模型： 在注意力机制中引入了M $$head = s o ftmax ( \frac { Q _ { i } K^T_i}{\sqrt { d _ { k } } } + M ) V _ { i }$$
∣X∣表示输入序列的长度，包括 token序列, 注释序列，变量序列。M 是 Graph-Guided Masked Attention 矩阵
Graph-Guided Masked Attention 这里用
v表示变量序列V第i个变量 c,表示源代码token集合C第i个token E‘定义为，如果变量v与itoken/序列第j个token c;相关联，那么〈v,c〉/〈cj,v〉∈ E’ 为了将图结构引入transformer,这里提出Graph-Guided Masked Attention来过滤不相关signal.graph&ndash;guided masked attention用矩阵M表示。
$M_{ij} = 0 &lt;= if ( q _ { i } \in [ C L S ] , [ S E P ] ) or ( q _ { i } , k _ { j }\in W \cup C ) or ( &lt; k _ { j } , k _ { j }> \in E \cup E' )$
$M_{ij} = -\infty &lt;= otherwise$
负无穷在归一化的时候会归一化成0.所以用这种方式表示他们在dataflow中是有连接的。
预训练任务： 该模型以源代码和注释以及相应的数据流作为输入，并用标准的masked language模型应用在2个结构化预训练任务上：
data flow edges prediction数据流边预测，用来学习代码的结构化表示 摘掉边进行预测。引入了负采样。 variable-alignment across source code and data flow。源代码和数据流之间的变量分配，用于学习数据流结点来自源代码中哪个token。 该任务是为了学习数据流图与源代码之间的对应关系，与边预测不同的是，边预测学习的是变量序列 V VV 中2个结点之间的联系， 而变量分配任务学习的是源代码token序列 C和变量序列 V之间的联系， 也就是学习变量结点 v_i和token c_j对应的关系。 下游任务： natural language code search，代码搜索 clone detection，克隆检测 code translation，代码翻译 code refinement，代码细化 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释</content></entry><entry><title>CodeBert a Pretrained Model for Programming and Natural Languages</title><url>http://next.lisenhui.cn/post/paperreading/codebert-a-pretrained-model-for-programming-and-natural-languages/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag></tags><content type="html"> 1. 综述翻译 我们展示了 CodeBERT，一种用于编程语言 (PL) 和自然语言 (NL) 的双峰预训练模型。 CodeBERT 学习支持下游 NL-PL 应用程序的通用表示，例如自然语言代码搜索、代码文档生成等。我们使用基于 Transformer 的神经架构开发 CodeBERT，并使用包含预训练的混合目标函数对其进行训练替换令牌检测的任务，即检测从生成器中采样的似是而非的替代方案。这使我们能够利用 NL-PL 对的“双峰”数据和“单峰”数据，前者为模型训练提供输入标记，而后者有助于学习更好的生成器。我们通过微调模型参数在两个 NL-PL 应用程序上评估 CodeBERT。结果表明，CodeBERT 在自然语言代码搜索和代码文档生成方面都取得了最先进的性能。此外，为了研究在 CodeBERT 中学习了什么类型的知识，我们构建了一个用于 NL-PL 探测的数据集，并在预训练模型的参数固定的零样本设置中进行评估。结果表明，CodeBERT 在 NL- 上的表现优于以前的预训练模型PL 探测.1
1.1 发表于 EMNLP 2020
2. Tag 代码语义; Bert; Code Semantic; CodeBert; Natural Language; Multi-tasks;
3. 任务描述 特征提取模型，用于提取语义特征，并不针对某个任务。
CodeBERT是一种可处理双模态数据（编程语言PL和自然语言NL）的预训练模型，支持下游 NL-PL 应用程序(如自然语言代码搜索、代码文档生成等)的通用表示。
CodeBERT基于 Bert 的架构开发，混合目标函数结合了替换标记检测的预训练任务。
在两个 NL-PL 下游任务上微调，结果显示在自然语言代码搜索和代码文档生成任务上达到了SOTA。此外，文章还构建了一个用于 NL-PL 探测的数据集，并在预训练模型的参数固定的零样本设置中进行了评估。
4. 方法 输入： [CLS], w1, w2, …wn,[SEP],c1, c2, …, cm,[EOS] w为NL单词序列，c为PL token序列。[CLS]为两个片段前的特殊token。 输出：自然语言和代码中每个token的上下文向量表示，以及[CLS]的表示，作为聚合序列的表示
预训练任务： 数据：
双峰NL-PL 对是指类似下面的自然语言-程序语言对，即带有配对文档的单独函数，语料一般以json行格式文件保存，一行是一个json对象： { "nl": "Increment this vector in this place. con_elem_sep double[] vecElement con_elem_sep double[] weights con_func_sep void add(double)", "code": "public void inc ( ) { this . add ( 1 ) ; }" } 单峰数据是指没有成对自然语言文本的单独函数代码和没有成对代码的自然语言 任务：
MLM任务 (Masked Language Modeling) 对NL-PL双峰数据对应用MLM，即选择随机位置的NL和PL mask，用特殊token [MASK]代替 MLM的目标是预测被mask的token。鉴别器$p^{D_i}$预测第i个单词为masked的token的概率 $L _ { M L M }( \theta ) = \sum _ { i \in m^w \cup m^c } - \log p ^ { D _ { 1 } } ( x _ { i } | w ^ { m a s k e d } , { c ^ { m a s k e d } } )$ 公式意思就是针对被mask的值，$p^D$从一个大的字典库中预测出他们。 RTD任务（Replaced Token Detection) 使用双峰数据。训练时同时使用双峰数据和单峰数据，有两个数据生成器$p^{G_w}, p^{G_c}$，一个生成NL，一个生成PL，用于选择随机掩蔽位置。生成器随机找一个位置使用生成的token进行代替。 鉴别器$p^{D_2}$鉴别是否正好生成了原始单词，即第i个单词为原始单词的概率。 $L _ { R T D } ( \theta ) = \sum _ { i = 1 } ^ { | w | + | c | } ( \delta ( i ) \log p ^ { D_2 } ( x ^ { \operatorname { corrupt} } , i ) + ( 1 - \delta ( i ) ) ( 1 - \log p ^ { D_2 } ( x ^ { \operatorname { corrupt}} , i ))$ 就是针对每个单词判断它为原始词的概率。 两个任务的组合： $$m i n L_{MLM} ( \theta ) + L _ { R T D } ( \theta )$$ 数据处理 1)每个项目应该被至少一个其他项目使用，(2)每个文档被截断到第一段（图中红色表示），(3)小于三个标记的文档被删除，(4)小于三行的函数被删除，(5)带有子字符串“test”的函数名被删除。图13给出了一个数据示例。 实验任务 自然语言代码搜索 微调 不用微调 代码文档生成 微调 不用微调 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释</content></entry><entry><title>OrderMatters Semantic Aware Neural Networks</title><url>http://next.lisenhui.cn/post/paperreading/ordermatters-semantic-aware-neural-networks/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag></tags><content type="html"> 目录： 1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 Related work 1. 综述翻译 二进制代码相似性检测，其目标是在不访问源代码的情况下检测相似的二进制函数，是计算机安全中的一项基本任务。传统方法通常使用图匹配算法，速度慢且不准确。最近，基于神经网络的方法取得了巨大的成就。二元函数首先表示为具有手动选择块特征的控制流图（CFG），然后采用图神经网络（GNN）来计算图嵌入。虽然这些方法有效且高效，但它们无法捕获二进制代码的足够语义信息。在本文中，我们提出语义感知神经网络来提取二进制代码的语义信息。特别是，我们使用 BERT 对一个令牌级任务、一个块级任务和两个图级任务的二进制代码进行预训练。此外，我们发现 CFG 节点的顺序对于图相似性检测很重要，因此我们在邻接矩阵上采用卷积神经网络 (CNN) 来提取顺序信息。我们用四个数据集对两个任务进行实验。结果表明，我们的方法优于执行最先进的模型。
1.1 发表于 AAAI
2. Tag 3. 任务描述 4. 方法 我们提出了一个包含三个组件的整体框架：语义感知建模、结构感知建模和顺序感知建模。
在语义感知建模中，我们使用 NLP 模型来提取二进制代码的语义信息。CFG 块中的标记被视为单词，块被视为句子。
我们采用 BERT (Devlin et al. 2018) 来预训练令牌和块。与 BERT 相同，我们屏蔽标记以对屏蔽语言模型任务 (MLM) 进行预训练，并提取所有相邻块以对邻接节点预测任务 (ANP) 进行预训练。
此外，因为我们的最终目标是生成整个图表示，所以我们添加了两个图级任务。一是判断两个采样的block是否在同一个图中，我们称之为block inside graph task（BIG）。另一个是区分块属于哪个平台/优化，称为图分类任务（GC）
我们发现额外的任务可以帮助提取更多的语义信息并更好地学习块表示。在对块嵌入进行预训练后，我们在图级任务上对其进行微调。
在结构感知建模中，我们使用 MPNN (Gilmer et al. 2017) 和 GRU (Cho et al. 2014) 更新功能。 (Xu et al. 2018) 已经证明，图神经网络可以具有与 Weisfeiler-Lehman 测试一样的区分能力 (Weisfeiler and Lehman 1968)。我们发现在每一步使用 GRU 可以存储比仅使用 tanh 函数更多的信息。
在顺序感知建模中，我们尝试设计一种架构来提取 CFG 的节点顺序信息。即在邻接矩阵上使用 CNN。我们发现只有 3 层 CNN 表现良好。我们进一步探索了其他 CNN 模型，例如 Resnet (He et al. 2016)，并讨论了 CNN 模型可以从邻接矩阵中学到什么。
我们模型的输入是二进制代码函数的 CFG，其中每个块都是具有中间表示的令牌序列。 我们模型的整体结构如图 3 所示。在语义感知组件上，该模型将 CFG 作为输入，并使用 BERT 对令牌嵌入和块嵌入进行预训练。 在结构感知组件上，我们使用带有 GRU 更新功能的 MPNN 来计算图语义和结构嵌入 gss。 在 order-aware 组件上，模型以 CFG 的邻接矩阵作为输入，采用 CNN 计算图 order embedding go。 最后，我们将它们连接起来并使用 MLP 层来计算图嵌入
Bert任务 MLM是一个token级别的任务，它在输入层屏蔽token并在输出层预测它们。邻接节点预测任务（ANP）是一个块级任务 在 ANP 任务中，我们提取一个图上的所有相邻块，并在同一个图中随机抽取几个块来预测两个块是否相邻。
图内块任务（BIG）和图分类任务（GC）。 我们在同一个图中/不在同一个图中随机采样块对，并在 BIG 任务中预测它们。这个任务帮助模型理解块和图之间的关系，
为了使模型能够区分这些差异，我们设计了图分类任务（GC）。 GC 使模型对不同平台、不同架构或不同优化选项中的块进行分类。
5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 Related work 一些研究尝试使用图核方法（Weisfeiler 和 Lehman 1968；Borgwardt 等人 2005）。最近（Xu et al. 2017）提出了一种基于 GNN 的模型，称为 Gemini，
它比以前的方法获得了更好的结果。但它使用手动选择的特征来表示 CFG 块，可能没有足够的语义信息。
(Zuo et al. 2018) 在这项任务上使用 NLP 模型。他们把一个token当作一个词，把一个块当作一个句子，并使用LSTM对句子的语义向量进行编码。为了确保具有相同语义信息的块具有相似的表示，他们使用孪生网络并计算 CFG 对的余弦距离。他们认为从同一段源代码编译的两个基本块是等效的。为了获得基本事实块对，他们修改编译器以添加基本块特殊注释器，该注释器为每个生成的装配块注释一个唯一 ID。</content></entry><entry><title>图神经网络面试题</title><url>http://next.lisenhui.cn/post/study/deeplearning/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories/><tags/><content type="html"> GCN GraphSage GraphSAGE(Graph SAmple and aggreGatE)框架，通过训练聚合节点邻居的函数（卷积层），使GCN扩展成归纳学习任务，对未知节点起到泛化作用。
每次只计算一跳邻居特征，然后通过递归得获得k跳内的邻居信息，极大减少计算复杂度
前向传播 先对邻居随机采样，降低计算复杂度（图中一跳邻居采样数=3，二跳邻居采样数=5） 生成目标节点emebedding：先聚合2跳邻居特征，生成一跳邻居embedding，再聚合一跳邻居embedding，生成目标节点embedding，从而获得二跳邻居信息。（后面具体会讲）。 将embedding作为全连接层的输入，预测目标节点的标签。 聚合函数 聚合邻居信息的时候可以使用不同的聚合函数
平均聚合：先对邻居embedding中每个维度取平均，然后与目标节点embedding拼接后进行非线性转换。 归纳式聚合：直接对目标节点和所有邻居emebdding中每个维度取平均（替换伪代码中第5、6行），后再非线性转换： LSTM聚合：LSTM函数不符合“排序不变量”的性质，需要先对邻居随机排序，然后将随机的邻居序列embedding作为LSTM输入。 Pooling聚合器:先对每个邻居节点上一层embedding进行非线性转换（等价单个全连接层，每一维度代表在某方面的表示（如信用情况）），再按维度应用 max/mean pooling，捕获邻居集上在某方面的突出的／综合的表现 以此表示目标节点embedding。 无监督和有监督损失设定 基于图的无监督损失：希望节点u与“邻居”v的embedding也相似（对应公式第一项），而与“没有交集”的节点 V_n 不相似 有监督损失：无监督损失函数的设定来学习节点embedding 可以供下游多个任务使用，若仅使用在特定某个任务上，则可以替代上述损失函数符合特定任务目标，如交叉熵。 实际运行 对每个节点进行步长为5的50次随机游走</content></entry><entry><title>Transformer和Bert面试题</title><url>http://next.lisenhui.cn/post/study/deeplearning/transformer%E5%92%8Cbert%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories/><tags/><content type="html"> Transformer 和 Bert 面试题 资料 视频资料 https://www.bilibili.com/video/BV1Di4y1c7Zm 面试资料 https://zhuanlan.zhihu.com/p/148656446 讲解 来源 机器翻译 机器翻译常用RNN模型。RNN模型的缺点 RNN模型需要一个一个送入进去，无法并行 RNN模型的梯度信息被近距离梯度信息主导，远距离梯度信息被忽略。 Transformer两列的结构 左边6个（或更多）Encoder。Encoder的结构相同。但每个Encoder参数是不共享的。 右边6个（或更多）Decoder。Decoder和Encoder结构不同。 具体结构 Encoder： 输入->position encoding ->多头注意机制 -> position+多头注意力机制（残差结构）并进行norm -> 送入FF网络 -> ff输出和第三步相加（残差）并进行norm Decoder （解码器前一个时刻的输出）输入->position encoding 送入 masked 多头注意力机制 输出和第一步残差并norm 接收encoder输出和第三步输出送入多头注意力 注意这里接收的encoder输出，是所有encoder计算完后产生的输出，不是同层encoder的输出。 和第3步的残差并norm 送入FF网络 FF输出和第5步残差 Decoder最终输出->线性层->softmax 总结Encoder：可以分为三个部分，输入部分，注意力机制部分，前馈神经网络部分。后面两个部分都有残差和norm的结构。 Encoder和Decoder差别。Decoder中间多了一层交互层，输入为Decoder上一层输出和同层Encoder输出。且Decoder第一层是mask操作。 位置编码 Encoder 输入部分
Embedding 位置编码 为什么需要位置编码
相比于RNN来说，Transformer对于一个句子中的值是一起处理的（一起处理的好处加快了速度，但忽略了序列关系），而不是一个个处理的，所以需要加入位置编码来告诉网络每一个的位置。 位置编码公式
对于一个长度为512的词特征向量。对于词向量中的奇数位置使用sin编码，对偶数位置使用cos编码。得到一个新的512维度向量。 上述两个512维度的向量相加 为什么这样编码有用？
$${ \begin{array} { l } { \sin ( \alpha + \beta ) = \sin \alpha \cos \beta + \cos \alpha \sin \beta } \ { \cos ( \alpha + \beta ) = \cos \alpha \cos \beta - \sin \alpha \sin \beta } \end{array}$$ $${ P E ( p o s + k , 2 i ) = P E ( p o s , 2 i ) * P E ( k , 2 i + 1 ) + P E ( p o s , 2 i + 1 ) * P E ( k , 2 i ) } \ P E ( p o s + k , 2 i + 1 ) = P E ( p o s , 2 i + 1 ) * P E ( k , 2 i +1) - PE(pos, 2i) * PE(k, 2i)$$ 所以对于pos+k的位置向量的某一维度2i或2i+1而言，可以表示为pos位置和k位置的位置向量的2i和2i+1的线性组合。这样的线性组合意味着向量中蕴含了相对位置信息。 注意
这种相对位置信息会在注意力机制中消失。 相对位置信息即$PE_{pos+k}$，可以由PE_{pos}线性表示。但经过点乘后$PE_{pos}W_qW_kPE_{pos+k}$，这两个值就没有相对位置信息了，两个w可以看成两个线性变化（实验得到的）。 多头注意力机制 基础的注意力机制 自注意力机制，见Attention章节更详细的知识。 多头注意力机制。 多头注意力机制 与其做一个单一的注意力函数，不如将整个query映射到低维（使用Linear）。然后使用多个注意力机制进行计算，最后再并起来，送入Linear 两个好处： 原始的单一注意力方式并没有很多可以学习的参数。可学习的参数少，说明网络的容量小。 希望不同的注意力头可以学习到不同的投影方法，去匹配不同的注意力模式。和卷积中多个核很像。 可以类比CNN中同时使用多个滤波器的作用，直观上讲，多头的注意力有助于网络捕捉到更丰富的特征/信息。 残差和layerNorm 输入x1, x2 经过self-attention得到 z1, z2 进行残差 f1= x1+z1, f2=x2+z2 残差为什么起作用，公式一写就明白了。它增加了一条梯度下降的路径。缓解了梯度消失。 小括号中的1表明短路机制可以无损地传播梯度，而另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。https://zhuanlan.zhihu.com/p/31852747 进行layernorm 为什么用layernorm不使用BN BN优点： 解决ICS问题（数据分布） 解决梯度消失问题 BN缺点： batch小的时候效果差。 在RNN中效果比较差（句子的输入是动态的，句子长度不一致，且任意一个词都可以放在第一个位置，所以batchnorm保存的信息不够好。所以BN效果不好） LayerNorm对同一个样本内的所有值做归一化 这种归一化就学习到了句子中的分布 前馈神经网络 接着上一步 z1+Feed Forward, z2+Feed Forward。 FeedForward又叫多层感知器。多层的FC全连接。 加上残差和归一化 前馈神经网络就是全连接+非线性激活函数。然后多层累加。 在实现中，就是单隐藏层的MLP。维度扩大四倍再缩小四倍。实现中，mlp是对每个词做运算得到输出。 Decoder MASK 需要对当前值和之前的值进行mask 因为预测的时候，无法知道当前值后面的值 如何做mask 在decoder端计算注意力时，仍然计算，但在做softmax前，将第t时刻以后的注意力权重设置为极大的负数。这样softmax后这个值就变为了0。 交互层 Encoder的输出（K，V）矩阵。是所有单词输出汇总计算k,v矩阵。这个过程和在Encoder中计算K/v矩阵是一样的。 Decoder生成Q矩阵。 运行阶段 Encoder： 长度为n的句子。输入是n个词的embedding，【我，爱，机器，学习】 计算position encoding。 然后计算多头注意力机制 Decoder: 第二个多头注意力中，key、value来自于编码器输出，query来自上一层masked attention. masked注意力层的输出向量维度仍然是不变的 decoder的每一次输入都是上一个时间状态理应的输出，第一个时间状态下，上一个时间状态使用表示。所以未【】，【, i,】,【, i,love】这样。为了方便，变为【, i,love, machine, learning】，然后在第一个attention 的时候，使用mask。 词映射到向量时的这个embedding和decoder输入的embedding和输出时softmax前的embedding，这些是共享权重，减少训练难度。这个embedding的权重除以了根号d，d为512。为了和position编码相加。 TRM面试题讲解 所有Encoder和所有decoder做交互？ Encoder怎么生成Q矩阵的？ 和RNN的区别。 RNN是使用上一刻的语义信息辅助当前的语义信息。 Attention是关注全局的语义信息（Encoder）提取出来后，再通过mlp映射到相应语义空间。 position embedding？ 正则化 在进入残差和layernorm前都做0.1的dropout。词嵌入、输入也用dropout label smoothing，就是标签的value只用达到0.1的置信度就行。 并行 6个encoder、decoder都是串行。 encoder两个子模块之间是串行。子模块本身是可以并行的。 self-attention的Q、K为什么要乘以一个矩阵 Q、K本身的点积是为了计算相似度。但原空间的相似度不够，为了使模型有更强的表现力，所以进行映射 如果Q、K保持相同。则Q、K的计算拥有自反性，即Q、K计算出来的矩阵是对称的。实际上，我是男孩，“我”对“男孩”的重要性应该低于“男孩”修饰“我”的重要性。所以乘上了矩阵 实际上reformer也提出来，Q、K可以是同一个。Q、K相乘以后再乘一个矩阵。其实还是在做线性映射。 为什么attention会除以$\sqrt{d_k}$ 对于输入[a, 10a, 100a]在输入的数据量级很大的时候，softmax会将几乎所有的概率分布分配给最大值对应的标签。会造成梯度消失为0，参数更新困难。 为什么使用$\sqrt{d_k}$:对于两个向量q、k，均值0，方差1。它们乘积后，均值0，方差为$d_k$。方差越大也就说明，点积的数量级越大（以越大的概率取大值）。那么一个自然的做法就是把方差稳定到1，做法是将点积除以$\sqrt{d_k}$ attention两种形式，加法Add然后映射到tanh，第二种乘法Mul。Add运算中的矩阵乘法，和Attention中的矩阵乘法不同。前者中只有随机变量X和参数矩阵W相乘，但是后者中包含随机变量X和随机变量X之间的乘法。对于Mul来说，如果S和h都分布在[0,1]，在相乘时引入一次对所有位置的∑求和，整体的分布就会扩大到[0,dk], 反过来看Add,右侧是被tanh0钳位后的值，分布在[-1,1]。整体分布和dk没有关系。 https://www.cnblogs.com/hongdoudou/p/12594430.html Bert模型attention部分如何加速。即如何一个矩阵完成多个投影。 多头运算加速 原始特征 batch, seqlen, hidden，映射成hidden/head的特征。则可以优化为： batch, head, seqlen, hidden/head. 这样可以进行多头的快速运算。 注意力的运算： hidden: B, S, Hidden Q, K, V: B, S, Hidden2 Q*K=> S, Hidden2 * Hidden2, S => B, S, S QKV=> S, S* S, Hidden2 = > S, Hidden2 最终C=>B, S, Hidden2 无论多少维，最后运算的时候都是二维的。 attention相比于lstm的优点 attention将任意两个词的距离拉到了1 attention的运算可以并行化。 位置编码技术 RPE(相对位置编码) 多头时，将key绝对位置转为相对query的位置。 复数域函数。 并行化 Transformer的并行化主要体现在self-attention模块，在Encoder端Transformer可以并行处理整个序列，并得到整个输入序列经过Encoder端的输出，但是rnn只能从前到后的执行 Decoder 交互的时候不能并行化，其他的时候可以。 bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？ BERT和transformer的目标不一致，bert是语言的预训练模型，需要充分考虑上下文的关系，而transformer主要考虑句子中第i个元素与前i-1个元素的关系。Transformer主要是用来做机器翻译的。 Bert 简单介绍 bert是用了transformer的encoder侧的网络，作为一个文本编码器，使用大规模数据进行预训练，预训练使用两个loss，一个是mask LM，遮蔽掉源端的一些字，然后根据上下文去预测这些字 可能会被问到mask的具体做法，15%概率mask词，这其中80%用[mask]替换，10%随机替换一个其他字，10%不替换，至于为什么这么做，那就得问问BERT的作者了{捂脸}） 一个是next sentence，判断两个句子是否在文章中互为上下句，然后使用了大规模的语料去预训练。 roberta证明nsp这个loss并没有起到什么作用。 和GPT最大的区别在于它是双向的。
概要 用了双向信息和Transformer来做预测。 以前的预训练模型 基于特征：将学到的特征和输入一起放入网络训练。 基于微调：将训练好的权重，在下一层进行微调。 使用两个任务: 带掩码的语言模型。masked language model.可以从双向看句子。 下一个句子预测。 贡献： 针对于Elmo的改动，Elmo使用了双向的信息，但是使用的是RNN的老旧模型。GPT使用了Transformer的结构，但是使用的是单向的模型。所以把它们结合起来。 使用了大量无标签的数据集进行预训练，比在小批量有标签的数据集上训练效果更好。 模型 预训练 在没有标签上的数据上进行训练 参数量 嵌入层：输入是30K，输出是隐藏层大小H。 自注意力块：多头注意力机制。 投影矩阵：H*H。投影后合并。 拿到输出concat后，再做一次投影，矩阵为H*H MLP层：H * 4H + 4H * H = 8H^2 输入/输出。 输入不再是一个句子，而是一个句子对。 使用wordpiece进行切词。如果一个词根可以表示一个词，则用词根来表示词。 输入进行处理：行首添加[CLS]，两个句子中间添加[SEP]。来表示这是第一个句子还是第二个句子。 在词送入网络时，添加一个嵌入层。 token embedding层。节点、句子的嵌入 segment embedding层。用来标识该词时第一个句子还是第二个句子。cls是第一个句子，sep是第二个句子。输入就是2. 位置编码。和句子长度相关。位置编码的one-hot。对应值为1. 预训练的任务 MLM任务。 对于句子中的词，有15%的词会被替换为[MASK]。 为了避免预训练和微调时数据的不一致性。这15%的词中，80%被替换为[MASK], 10%的概率替换成随机的词源， 10%的概率什么也不干。 预测下一个句子 输入序列中有两个句子，50%样本就是下一个句子，50%的概率是随机选择一个句子。增大QA和语言推理中的信息。 微调 要么使用对应词元做输出。 要么使用[cls]做输出。NSP任务也是用cls的embedding做预测的。 实验 glue：将CLS的输出拿出来接入softmax QA：对于一段话中，找出语句中的开头和结尾。 微调设置 3 epoch， 学习率5e-5, batchsize 32. 增大epoch数量。（epoch太少是没办法达到好的效果） 消融实验 bert微调的特征比直接使用特征进行训练效果要好。（大家都得做微调） 写作贡献：
突出你的主要贡献点。 分析你的贡献点带来的优点和缺点。 bert模型详解 每个词的embedding加上位置编码，加上句子编码。（并不是concat，是直接相加） 乘以Q、K、V三个矩阵得到Q, K, V 使用一个词的Q对其他的key进行计算，计算的结果和V相加得到当前词的注意力信息。 Transformer中的一个encoder对应Bert中的一个Trm。（输入是所有句子。输出是当前词的注意力特征）。（实际上可以理解一层的encoder是共享的）bert会使用基于所有层中的左右两侧的语句信息。 bert可以调节的参数 L, H, A. L是网络的层数，Transformer block的数量。A是多头的数量。filter是 roberta的改进 使用更长的训练时间、 100K-》500K steps 更大的batch、 从256增加到了8K 更多的数据 移除了NSP 使用了全句子，不跨文档。 序列更长 动态调整mask。 原本的bert使用静态mask。 定义了dupe_factor，每个数据复制dupe_factor份，拥有不同的mask，在不同的epoch中使用。 第三种动态：每次训练的时候，才会随机进行mask。 文字编码 使用bytes-level：使用bytes而不是unicode作为subword的基本单位。 与wordpiece的区别： wordpiece选择提升语言模型概率最大的相邻子词加入此表 BPE选择聘书最高的相邻子词合并。 好处不会出现unknown，但会增加数据集大小。 词构造算法 BPE(roberta使用的) 步骤： 准备足够大的训练语料，并确定期望的Subword词表大小； 将单词拆分为成最小单元。比如英文中26个字母加上各种符号，这些作为初始词表； 在语料上统计单词内相邻单元对的频数，选取频数最高的单元对合并成新的Subword单元；每次合并后词表大小可能出现3种变化： +1，表明加入合并后的新子词，同时原来的2个子词还保留（2个字词分开出现在语料中）。 +0，表明加入合并后的新子词，同时原来的2个子词中一个保留，一个被消解（一个子词完全随着另一个子词的出现而紧跟着出现）。 -1，表明加入合并后的新子词，同时原来的2个子词都被消解（2个字词同时连续出现）。 重复第3步直到达到第1步设定的Subword词表大小或下一个最高频数为1. 编码 得到Subword词表后，针对每一个单词，我们可以采用如下的方式来进行编码： 将词典中的所有子词按照长度由大到小进行排序； 对于单词w，依次遍历排好序的词典。查看当前子词是否是该单词的子字符串，如果是，则输出当前子词，并对剩余单词字符串继续匹配。 如果遍历完字典后，仍然有子字符串没有匹配，则将剩余字符串替换为特殊符号输出，如””。 单词的表示即为上述所有输出子词。 解码 解码过程比较简单，如果相邻子词间没有中止符，则将两子词直接拼接，否则两子词之间添加分隔符。 wordpiece（bert使用的） 与BPE算法类似，WordPiece算法也是每次从词表中选出两个子词合并成新的子词。与BPE的最大区别在于，如何选择两个子词进行合并：BPE选择频数最高的相邻子词合并（构造词阶段），而WordPiece选择能够提升语言模型概率最大的相邻子词加入词表。 方法： 计算两个词的互信息量，让预料中以相邻方式同时出现的子词进行合并。 ULM（Unigram Language Model） ULM是减量法,即先初始化一个大词表，根据评估准则不断丢弃词表，直到满足限定条件。 上述三种方法的使用 如何使用上述子词算法？一种简便的方法是使用SentencePiece。
SentencePiece还能支持字符和词级别的分词。更进一步，为了能够处理多语言问题，sentencePiece将句子视为Unicode编码序列，从而子词算法不用依赖于语言的表示。
参考资料 https://www.cnblogs.com/ffjsls/p/12257158.html 词构造算法 https://zhuanlan.zhihu.com/p/198964217 https://zhuanlan.zhihu.com/p/363466672 trm面试题 https://zhuanlan.zhihu.com/p/151412524 Bert面试题 https://zhuanlan.zhihu.com/p/95594311 Bert面试题 trm模型图 https://zhuanlan.zhihu.com/p/44121378</content></entry><entry><title>DBSCAN聚类算法</title><url>http://next.lisenhui.cn/post/study/deeplearning/dbscan%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/</url><categories/><tags/><content type="html"> DBSCAN 定义核心点，获取所有核心点
对于每一个点画一个圈，当周围圈中点的个数大于定义的值的个数，将其设置为核心点。 从随机选择的一个核心点出发，建立一个簇，将核心点周围圈中的核心点加入到簇中
将与核心点接触的非核心点加入到簇中。不能使用非核心点进行扩充。
公式化描述：
初始化核心点对象为空寂。簇个数为k=0， 对于所有的样本，找出核心点： 通过距离度量的方式，找到样本x_j的ϵ-邻域子样本集N(x_j) 对于xj∈D，其ϵ-邻域包含样本集D中与xj的距离不大于ϵ的子样本集 子样本个数大于等于最小的Minpts， 将样本xj加入核心对象样本集合 MinPts描述了某一样本的距离为ϵ的邻域中样本个数的阈值。 随机选择一个核心点，初始化当前核心队列，将邻域内所有的核心点加入到当前核心队列中。 然后将所有核心点队列邻域中的非核心点加入簇中。并将核心队列加入到簇中。构建完一个簇。 重复上述步骤，没有加入到任何簇中的点是异常点。 与传统的K-Means算法相比，DBSCAN最大的不同就是不需要输入类别数k，当然它最大的优势是可以发现任意形状的聚类簇，而不是像K-Means，一般仅仅使用于凸的样本集聚类。同时它在聚类的同时还可以找出异常点。
那么我们什么时候需要用DBSCAN来聚类呢？一般来说，如果数据集是稠密的，并且数据集不是凸的，那么用DBSCAN会比K-Means聚类效果好很多。
轮廓系数 当文本类别未知时，可以选择轮廓系数作为聚类性能的评估指标。
轮廓系数取值范围为[-1,1]，取值越接近1则说明聚类性能越好，相反，取值越接近-1则说明聚类性能越差。
a：某个样本与其所在簇内其他样本的平均距离 b：某个样本与其他簇样本的平均距离
针对某个样本的轮廓系数$s = \frac{b-a}{max(a,b)}$ 聚类总的轮廓系数等于所有样本的罗阔系数的均值。
评估 轮廓系数越接近于1，效果越好。越接近于0，说明类内和类别之间的距离差不多，分界不明显。越接近-1说明类别之间越相似，类别内部反而不相似。
面试题 优缺点： 优点： 可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。 可以在聚类的同时发现异常点，对数据集中的异常点不敏感。 聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。 总结：对任意的稠密数据进行聚类，比K-means适用更广泛。聚类的时候可以发现异常点。初始值影响不大。（K-means三个缺点，k值难以确定，初始化影响大，异常值影响大) 缺点 如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。 调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。 总结：密度算法当密度不均匀时效果差。调参复杂。</content></entry><entry><title>PCA,LDA降维面试题</title><url>http://next.lisenhui.cn/post/study/deeplearning/pcalda%E9%99%8D%E7%BB%B4%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories/><tags/><content type="html"> PCA降维 PCA的理论，旨在找到数据中的主成分分析，利用这些主成分表征原始数据，从而达到降维的目的 在信号处理领域，认为信号拥有较大的方差，噪声拥有较小的方差。信号和噪声之比即为信噪比。信噪比越大说明数据的质量越好。PCA的目标就是找到一个投影向量。最大化投影方差。 公式写在纸上，步骤如下： 对样本进行去中心化处理（将样本减去均值） 对样本求协方差矩阵 对协方差矩阵进行特征值分解，特征值从大到小跑列 选择特征值前d大对应的特征所对应的特征向量w1..wd, 进行映射。x[w1&hellip;wd] 投影后信息量占比:$\mu=\sqrt{\frac{\sum_{i=1}^d \lambda^2}{\sum_{i=1}^n \lambda^2}}$, $\lambda$是协方差矩阵的特征值。 一般情况下，方差是$\sum(x-\mu)^2$, 协方差是$\sum_{i=1}(x_i-\mu_x)^T(y_i-\mu_y)$,方差可视作随机变量x关于其自身的协方差Cov(x, x). LDA线性判别分析 主要为分类服务。 核心就是，让投影之后的向量，同一类的样本更加几种。不同类的样本隔的更远 所以即最大化类间距离，和最小化类内方差。 类间距离即均值的差值。类内距离即各个类别的方差之和。将损失函数定义为类间距离和类内距离的比值。
$max J(w) = \frac{||w(\mu_1-\mu_2)||^2}{D1+D2}$ 定义类间散度为$S_B=(\mu_1-\mu_2)(\mu_1-\mu_2)^T$, 类内散度为$S_w=\sum_i(x_i-\mu_i)(x_i-\mu_i)^T$ 最终结果为 $S_w^{-1}S_Bw=\lambda w$即只要求样本的均值和类内方差，就能计算出最佳投影方向。
多个类的LDA 计算类内样本的均值。总体均值向量 计算类内散度，全局散度，类间散度=全局-类内 对类内散度$S_w^{-1}S_b$进行特征分解，取最大的特征向量前d维进行映射。 面试题 pca的优缺点 优点： 计算方法简单。 各主要成分之间正交，可消除原始数据成分间相互影响的因素 缺点： 方差小的非主成分也可能对样本的差异有重要信息。降维丢弃可能对后续数据处理有影响。 其他降维方法，LDA（线性判别分析），有监督学习下的降维方法。svd（奇异值分解） 无监督的时候使用PCA，有监督的时候使用LDA。</content></entry><entry><title>LSTM网络</title><url>http://next.lisenhui.cn/post/study/deeplearning/lstm%E7%BD%91%E7%BB%9C/</url><categories/><tags/><content type="html"> 循环神经网络 循环神经网络每一层训练采用相同的w和b。它的求解采用BPTT（基于时间的反向传播算法实现）。当把循环神经网络展开成T层的神经网络来理解，就和普通的反向传播没有任何区别了。 BPTT不能成功捕捉长距离关系，这一现象有主要原因是梯度消失： 梯度消失。梯度消失的原因和深度网络中类似，它意味着只有靠近输出的若干层才能起到真正的学习的作用。 所以引入了长短时记忆网络、GRU来弥补梯度消失。 循环神经网络求导： $net=Ux_t+Wh_{t-1}\$ $h_t=f(net)\$ $y=g(Vh_t)$, g为激活函数
LSTM长短时记忆网络 https://zhuanlan.zhihu.com/p/32085405
$Z^{i,o,f}=tanh(w^{i,o,f}(x_t||h_{t-1})$通过sgmoid映射到[0,1]为一个概率值。
面试题 LSTM优缺点
优点： 缓解了梯度消失的问题。通过引入记忆单元c，避免了无休止的连乘。而是边加边乘。 缺点： 计算耗时，三个sigmoid 缓解长距离依赖、梯度消失问题，但是还不够。 循环神经网络能使用relu激活函数嘛？
能，但是要做一定的限制，比如限制权重矩阵为单位阵附近。 $\frac{\partial f}{\partial f_{i-1}}=\frac{\partial f}{\partial g_{i-1}}\frac{\partial g_{i-1}}{\partial f_{i-1}}, \frac{\partial f}{\partial g_{i-1}}=w_{i-1}$ 为什么LSTM使用tanh？
输出在[-1,1]之间，和大多时候，特征分布是以0为中心是吻合的 Tanh在0附近拥有比sigmoid更大的梯度，更容易收敛 最开始的时候就是采用2sigmoid-1这种，后来实验发现tanh效果更好。 sigmoid作为门控，基本是共同选择。在可穿戴设备中，对计算量有要求，此时sigmoid往往会替换为hard gate[0,1]门。只输出0，1 LSTM怎么解决梯度消失的？
LSTM增加了更多回传梯度的路径，只要一条路径没有梯度消失，那么梯度消失的问题就得到了改善。 LSTM为什么有梯度消失？ 连乘效应 总梯度被近距离梯度所影响，远距离梯度信息被忽略。 这个要接着改。 GRU相比于LSTM的优点：
LSTM使用了三个门：z,i,o:记忆门，输入门，输出门。C_t为内部记忆单元 GRU使用了两个门r, z，重置门和更新门。重置门用于控制记忆的保留量，使用了同一个门控z就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）。 lstm三个门：记忆门，遗忘门（信息门），输出门。
输入门、遗忘门、输出门。 遗忘门。接受长期记忆C_{t-1}, 。决定保留和遗忘$C_{t-1}$的哪些部分 记忆门（信息门、输入门）确定当前的输入和隐藏状态中哪些信息应被保留下来加入长期记忆中。 输出门。根据当前状态，确定输出的值，并将信息保存到隐藏状态中去。 LSTM公式
tanh(x)求导=1-tanh(x)^2 $tanh= \frac{ez-e{-z}}{ez+e{-z}}$ 图像是sigmoid扩充到[-1,1]的图像。 和sigmoid一样，z很大或者很小的时候梯度几乎为零。 RNN反向传播公式
https://zhuanlan.zhihu.com/p/33594517 h_t对h_{t-1}矩阵求导的矩阵称为雅可比矩阵，若该雅可比矩阵最大特征值大于1，随着反向传播链的增长，梯度会逐渐的增大。导致梯度消失或者爆炸。</content></entry><entry><title>梯度消失爆炸理论分析</title><url>http://next.lisenhui.cn/post/study/deeplearning/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E7%88%86%E7%82%B8%E7%90%86%E8%AE%BA%E5%88%86%E6%9E%90/</url><categories/><tags/><content type="html"> 梯度消失和爆炸 资料：https://zhuanlan.zhihu.com/p/33006526
由于链式法则和反向传播，在反向传播过程中需要对激活函数进行求导，如果导数大于1，那么随着网络层数的增加梯度更新将会朝着指数爆炸的方式增加，就是梯度爆炸。
举个例子： $f_{i+1}=g(f_i*w_i+b_i)$, g为激活函数，在进行多层之后，我们进行BP反向传播：$w=w-\alpha \nabla w$, $\nabla w = \frac{\partial f_4}{\partial f_3}\frac{\partial f_3}{\partial f_2}\frac{\partial f_2}{\partial f_1}*x$，其中$\frac{\partial f_2}{\partial f_1}$就是激活函数g，这个如果求导之后大于1，则这个梯度会越来越大，造成梯度爆炸，小于1，这个会越来越小。造成梯度消失。
解决方案 预训练+微调。逐层预训练。避免了长距离的反向传播。然后再微调 梯度剪切。当梯度爆炸超出阈值的时候，进行剪切，限制在一定范围内 使用其他的激活函数：relu, leakrelu激活函数，他们求偏导后值是1，这样就不能存在梯度消失、爆炸了。但缺点是，当值为负的时候，梯度为0。整条线都不会训练。且输出分布不是以0为中心的。 不以0为中心的坏处：https://liam.page/2018/04/17/zero-centered-active-function/ 使用batchnorm。每一层将输入限制在一定范围内，就避免了消失或者爆炸的问题。整个链式法则中包括输入x和激活函数，当激活函数位于不敏感区域的时候，梯度下降比较困难。batchnorm将输入归一化到[0,1]附近，将梯度从饱和区拉到了非饱和区。梯度更新就会更加容易。避免了梯度消失或者爆炸。 使用新的模型结构，长短时记忆网络，GRU，残差结构 总结五个：从避免长链式法则、限制梯度范围，限制梯度消失原因（激活函数求导大于1或小于1）、限制输入、新模型五个方面解决。 反向传播的流程 前向传播 定义一层网络 z = wx+b f = g(z) 多层网络前向传播计算得到了最终的输出$f_n$。
则多层网络之后,计算误差 L=Loss(f_n,y) 然后求每一层的w。 比如四层网络求第2层的梯度。 $\frac{\partial f_4}{\partial z_3}\frac{\partial z_3}{\partial f_2}\frac{\partial f_2}{\partial z_2}\frac{\partial z_2}{\partial w_2}$ 这样可以看出来，前面的部分值是可以复用的。所以链式法则减少了很多的运算量。
同时也可以看出这里的$\frac{\partial z_3}{\partial f_2}=w_2$和激活函数的导数都参与了运算。所以可以解释梯度消失和梯度爆炸的原因。
反向传播是损失函数对当前层的权重求导。 然后求下一层的梯度的时候，可以用到本层计算过的部分值。就是反向传播的流程。</content></entry><entry><title>GCN讲解和面试题</title><url>http://next.lisenhui.cn/post/study/deeplearning/gcn%E8%AE%B2%E8%A7%A3%E5%92%8C%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories/><tags/><content type="html"> GCN图卷积神经网络 基于谱的图卷积神经网络 https://www.bilibili.com/video/BV1Vw411R7Fj https://ml-researcher.github.io/file/gcn.pdf
https://www.cnblogs.com/siviltaram/p/graph_neural_network_1.html 数学公式推导，讲的特别好。
卷积是什么 卷积：聚合相邻元素的信息。它相当于滤波器的作用，不同的卷积核保留了部分的信息，舍去了部分的信息。 图卷积就是需要聚合周围邻居节点的加权信息。 由于拓扑图中每个顶点的邻居数量不同。无法找到一个通用的卷积核来进行卷积运算。所以我们的办法是，使用傅里叶变换，对空间域进行转换。转换后的空间域可以执行卷积操作。 具体的：拉普拉斯矩阵是一个很好的矩阵，由于它是是对称矩阵，它可以被分解为$L=D-A=U\wedge U^T$，$Lx=[\sum(x_1-x_j),\sum(x_2-x_j)]$表示了聚合的含义。 谱领域的卷积就是对图做傅里叶变换，转换到一个新域，然后对特征值进行操作，在转换回现在的域。 但是对拉普拉斯矩阵的特征分解复杂度太高。 所以我们限制核函数（对特征值操作的函数）为多项式$w_1x_1+w_2x_2$等等。这样$Ug(\wedge)U^T=g(U\wedge U^T)$,为了避免梯度消失、梯度爆炸。我们采用切比雪夫不等式的前两项$T(x)=2T_1(x)-T_0(x)$,$T_1(x)=x,T_0(x)=1$,切比雪夫的性质$T_n(cos \theta)=n cos \theta$所以值不会爆炸，而会被限制在这个区间范围内。但是这要求定义域是[-1,1] 而$L_{sym}=D^{-\frac{1}{2}}LD^{-\frac{1}{2}}$的特征值的域是[0,2]，所以我们使用$L_{sym}-I$作为我们的傅里叶变换的矩阵。 最后整体的公式就是$f(A)=f(D^{-\frac{1}{2}}(D-A)D^{-\frac{1}{2}}-I)=f(D^{-\frac{1}{2}}AD^{-\frac{1}{2}}x), x=H^{l-1}w$ 基于空间的图卷积神经网络 基于最简单的空域的思想，聚合邻居节点，然后更新当前节点的信息。 消息传递神经网络 三个步骤：
从邻居当中聚合信息（如何聚合） 使用聚合信息更新当前节点（如何更新） 获取整张图的信息（如何获取）图的输出 需要满足输入顺序不影响最终结果。</content></entry><entry><title>注意力机制讲解和面试题</title><url>http://next.lisenhui.cn/post/study/deeplearning/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E8%AE%B2%E8%A7%A3%E5%92%8C%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories/><tags/><content type="html"> 注意力机制 https://zhuanlan.zhihu.com/p/78655043 https://zhuanlan.zhihu.com/p/379033238
注意力机制，用于从大量信息中筛选出有用的信息，就是注意力机制。最经典的图，就是qkv的结构图。分三个步骤 Query与key送入注意力机器计算，得到相关系数 将相关系数送入softmax得到注意力分数。注意力分数描述了value的重要程度。 使用注意力分数对value进行加权求和 整个注意力机制来源于翻译中的Seq2Seq2模型。是为了缓解梯度消失而设立的一种模型 对于seq2seq的输入$h_1&hellip;h_t$,送入rnn模型得到上下文向量s 输出的时候$y=f(y_{i-1}, s_{i-1})$意思就是由上一个输出和上一个隐藏层状态决定了当前的输出。 但有两个缺陷：对齐问题和长距离依赖问题 为了解决长距离，使用状态向量和所有的输入向量进行注意力计算 得到$y=f(y_{i-1}, s_{i-1}, c)$, c由$s_{i-1}和h_1&hellip;h_i$计算得到 根据选取计算的输入向量不同，可以分为soft、global、local、self soft：从h_1到h_i hard：选择一个计算隐藏状态 global：所有的输入 local：窗口内的输入 self: 不通过状态向量进行计算，而直接从source、target之间进行计算。source乘以权重分别得到q,k,v然后和自己进行运算。 self-attention的三个矩阵由当前矩阵得来的 self-attention的注意力系数要除以d_k 公式：三步：送入注意力机器，归一化得到分数，加权求和。 $e_{ij} = a(h_i, h_j)$ $\alpha_{ij} = \frac{e_{ji}}{\sum e_{ji}}$ $c_j=\sum \alpha_{ji}h_i$ 常见的注意力机器: 引入神经网络$a=LeakyReLU(a[h_i||h_j])$ 向量点积。 Transformer中还除以了根号d，防止注意力系数太大。： $a(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$ 为什么可以用点乘？ 点乘反映了一个向量在两一个向量上的投影长度。是一个标量。两个向量越相似，则点乘结果就越大。 cos相似性 多头注意力机器：设置不同的权重矩阵相乘，计算多个注意力特征，通过求平均或者并列得到多头注意力特征。（一般求并列） 原理：将特征映射到多个特征空间。提取了不同层面的注意力信息。 也可以将原有的高维空间转化为多个低维空间并再最后进行拼接，形成同样维度的输出，借此丰富特性信息，降低了计算量 如何介绍Attention 介绍定义。从所有信息中抽取重要的。 介绍三个步骤 介绍注意力机器 介绍soft、hard、local等。 面试题 self-attention 取消了中间的向量，直接在source和target之间进行注意力计算 这里的source和target往往是一个东西。即QKV三个矩阵是由同一个输入特征计算出来的。x*w=q,k,v 注意力机器 注意力的公式 为什么要用多头 原论文中说，多头可以让模型去关注不同方面的信息，类似于CNN中的多个滤波器 为什么注意力机制缩放了d_k 当维度很大时，点积结果会很大，会导致softmax的梯度很小。为了减轻这个影响，对点积进行缩放。</content></entry><entry><title>Lightgbm面试题相关</title><url>http://next.lisenhui.cn/post/study/deeplearning/lightgbm%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%B8%E5%85%B3/</url><categories/><tags/><content type="html"> lightgbm 从 LightGBM 名字我们可以看出其是轻量级（Light）的梯度提升机（GBM），其相对 XGBoost 具有训练速度快、内存占用低的特点。
LightGBM 是为解决GBDT训练速度慢，内存占用大的缺点，此外还提出了：
基于Histogram的决策树算法
单边梯度采样 Gradient-based One-Side Sampling(GOSS)
互斥特征捆绑 Exclusive Feature Bundling(EFB)
带深度限制的Leaf-wise的叶子生长策略
直接支持类别特征(Categorical Feature)
支持高效并行
Cache命中率优化
lgbm特点：
直方图算法 将大规模数据放在直方图中，将数据划分成若干个区间。将每个区间的值更新为箱子的值。 优点：内存占用小，计算代价小。直方图做差加速 相比xgb不需要额外存储预排序，且只保存特征离散化后的值(整型) 相比xgb不需要遍历一个特征值就需要计算一次分裂的增益，只需要计算k次(k为箱子的个数) 一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到，在速度上可以提升一倍 采用leaf-wise算法 XGBoost 采用 Level-wise，策略遍历一次数据可以同时分裂同一层的叶子，容易进行多线程优化 LightGBM采用Leaf-wise的增长策略，该策略每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂 区别： 优点：因此同Level-wise相比，Leaf-wise的优点是：在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度 缺点：可能会长出比较深的决策树，产生过拟合。 单边梯度采样 GOSS算法从减少样本的角度出发，排除大部分小梯度的样本，仅用剩下的样本计算信息增益。 为了不改变数据的总体分布，GOSS对要进行分裂的特征按照绝对值大小进行排序，选取最大的a个数据，在剩下梯度小的数据中选取b个，这b个数据乘以权重(1−a)/b,最后使用这a+b个数据计算信息增益。 a个大梯度+b个小梯度*(1-a)/b 互斥特征捆绑算法(EFB) 捆绑的特征是互斥的：即特征不会同时为非零值，像one-hot 如果两个特征并不是完全互斥（部分情况下两个特征都是非零值），可以用一个指标对特征不互斥程度进行衡量，称之为冲突比率，当这个值较小时，我们可以选择把不完全互斥的两个特征捆绑，而不影响最后的精度 互斥特征捆绑算法（Exclusive Feature Bundling, EFB）指出如果将一些特征进行融合绑定，则可以降低特征数量。 将互斥的特征进行捆绑（one-hot就是典型的互斥特征，选了其中一个，另一个不会被选择） 然后将他们的值域相加，如a的区域[0,10], b的取值[10,20]，则他么合并为[0, 20] 如何选择互斥特征（保证这些特征不同时为非零值。具体选择的算法是一种图着色问题，我还没有来得及弄懂） LightGBM是怎么支持类别特征？（没看懂） 离散特征建立直方图的过程：统计该特征下每一种离散值出现的次数，并从高到低排序，并过滤掉出现次数较少的特征值, 然后为每一个特征值，建立一个bin容器。 分好桶之后，计算分裂阈值的过程 先看该特征下划分出的bin容器的个数，如果bin容器的数量小于4，直接使用one vs other方式, 逐个扫描每一个bin容器，找出最佳分裂点; 对于bin容器较多的情况, 先进行过滤，只让子集合较大的bin容器参加划分阈值计算, 对每一个符合条件的bin容器进行公式计算 $\frac{该bin下样本一阶导之和}{该bin下样本二阶导之和}+正则项$ 得到一个值，根据该值对bin容器从小到大进行排序，然后分从左到右、从右到左进行搜索，得到最优分裂阈值。但是有一点，没有搜索所有的bin容器，而是设定了一个搜索bin容器数量的上限值，程序中设定是32 对于连续特征，划分阈值只有一个，对于离散值可能会有多个划分阈值，每一个划分阈值对应着一个bin容器编号，当使用离散特征进行分裂时，只要数据样本对应的bin容器编号在这些阈值对应的bin集合之中，这条数据就加入分裂后的左子树，否则加入分裂后的右子树。 支持高效并行 资料 https://zhuanlan.zhihu.com/p/99069186 面试题 lgbm优缺点 优点： 直方图降低了时间复杂度。降低了空间复杂度 单边梯度算法减少了计算量 leaf-wise加强了精度，减少了计算量 互斥捆绑算法减少了特征数量，降低了内存消耗 缺点： leaf-wise容易过拟合 对噪点敏感。 2</content></entry><entry><title>算法竞赛知识点</title><url>http://next.lisenhui.cn/post/study/deeplearning/%E7%AE%97%E6%B3%95%E7%AB%9E%E8%B5%9B%E7%9F%A5%E8%AF%86%E7%82%B9/</url><categories/><tags/><content type="html"> 数据初探 对数据进行分析 单变量可视化分析 多变量可视化分析 降维分析 在这个阶段需要注意的点： 数据基本情况 数据大小，字段类型 是否存在时序信息。（时序信息通常需要相关性、趋势性、周期性和异常点的分析） 特征情况 特征的重复值、缺失值、异常值。 特征的冗余情况 特征分布情况（单变量分布，多变量分布 标签 标签分布（类别不平衡）、回归标签的异常值 训练、测试集的分布 变量分析 单变量分析 查看变量的分布情况。 分析标签 目的：尽量去除异常值，转换分布到适合学习的分布（正态分布) 手段：取log 连续性： 查看变量分布和相关性 正相关：一个特征的增加导致另一个特征增加，则为正相关。 负相关：一个特征的增加导致另一个特征的减少，则为负相关。 多重共线性的特征是冗余的，要尽量剔除这些特征。 类别型 找到和标签有强相关的特征，围绕这些特征展开交叉组合。（强相关加强相关，弱相关加弱相关等） 多变量分析 分析多个变量的强相关性，可以选择细化描述（组合房屋位置和房屋评分），来构造更细致的特征 模型分析 模型学习曲线 特征重要性分析 误差分析。 特征工程 数据预处理 缺失值处理 区分缺失值：存在形式：None，空，特殊值（-1，-999） 处理方法： 类别特征：进行填充众数、填充新类 数值特征：填充平均数、中位数、众数、最大值、最小值 有序数据：next、previous 模型预测填充：对含有缺失值的那一列进行建模并预测。 异常值处理 寻找异常值 可视化分析 数据统计来发现异常值。（四分位箱线图） 处理异常值 删除异常值记录 视为缺失值进行处理 不处理。（部分异常值可能是对真实情况的记录，直接挖掘可能会保留最真实的信息） 内存优化 内存优化：清楚不用的变量，使用gc.collect()手动回收 数值优化：数据重新变量，连续性转为整型 特征变换 连续变量无量纲化 归一化（区间缩放，数据不是正态分布时使用）：min-max归一化：$x=\frac{x-x_{min}}{x_{max}-x_{min}}$ 标准化（数据为正态分布时）（又称z-scores, 零均值） 倾斜数据变为正态分布：log变换。 自动正态分布变换：cbox-cox(竞赛不常用) 连续变量离散化 离散化的变量有很强的健壮性。 无监督离散化 等频。每个区间的数量大致相等。将原有分布改为了均匀分布。 等距。从最小到最大，分为N等份。保持了原有的分布 有监督离散化 GBDT+LR训练多棵树，用多棵树的叶子节点作为特征的编码 类别特征转换 自然数编码：类别特征存在顺序关系。可以为每个特征分布一个编号（从小到大） 独热编码。类别特征没有顺序关系。 不规则特征变换 有些特征拥有丰富含义，应当考虑更多操作。如身份证信息。 特征提取 创造更多特征。很多模型，如决策树模型，难以提取到特征之间的组合效应。组合特征可以显示表达这种效应。以这种思想去创造特征。
类别相关特征 目标编码。为目标变量（标签）的统计量对类别特征进行编码。注意不能泄露验证集信息，验证集的信息应该在训练集上完成。 方法：分类问题，统计正负样本个数。回归问题统计目标均值、中位数等。 目标编码对类别数量少（基数较低）的类别特征很有效，相当于引入了特征的增益信息（决策树中的概念）。对基数较高的类别特征，可能会有过拟合的风险。 count、nunique、ratio特征。 类别特征之间的交叉组合 方法：类别特征的笛卡尔积。 方式：从业务逻辑方面入手，（年龄和性别可以组合）。从类别基数判断（类别基数太大，很多类只会出现一次，没必要组合） 连续特征（数值特征）的统计特征 数值特征之间的交叉组合。（如房屋每平方米均价等） 类别特征和数之特征的组合。（某个类别当中，数值特征的统计量） 按行进行统计。统计0、均值、中位数个数等等。 时间特征 细化到年、月、日、小时、分钟等 根据地理源调整 构造时间差异特征。 多值特征 一列特征包含多个属性的情况。 通常稀疏化、向量化。比如文本分词，进一步tf-idf,lda等。 特征选择 从数据中识别并删除不需要、不相关以及冗余特征。
先验特征关联分析 考虑特征和标签的关联，特征和特征之间的关联。 方法： 皮尔逊相关系数、卡方检验、互信息法、信息增益 后验特征重要性分析（决策树中） weight：计算特征再所有树种被选为分裂特征的次数。 gain：平均增益。特征在所有树种作为分裂节点的信息增益之和除以该特征出现的频次 cover：特征对每棵树的覆盖率。即样本分到该特征节点的样本的二阶导数之和。特征度量是平均覆盖率。 技巧： 不会导致过拟合的话，选择重要程度高的特征进行分析、扩展 重要性低的特征，可以考虑移除。 封装方法 比较耗时的特征选择方法 启发式： 前向搜索、后向搜索 递归消除特征 null importance 将特征和正确的标签送入模型训练，得到一个特征重要性分数 将特坏着呢给和打乱后的标签送入，得到一个特征重要性分数。如果前者没有超过后者。则该特征是一个无用特征。 用户画像 给用户打标签 事实类标签。性别年龄等。归纳统计。 规则类标签。如用户行为、用户偏好等。使用逻辑条件进行统计分析。 模型类分析。预测用户状态、用户信用分、划分兴趣人群等。 其他 相似度计算方法 欧氏距离 余弦相似度:$cos\theta = \frac{\sum_{i=1}^{n}(a_i*b_i)}{\sqrt{\sum_{i=1}^{n}a_i^2}*\sqrt{\sum_{i=1}^{n}b_i^2}}$ jaccard相似度:评价两个集和的差异大小。交集/总集 皮尔逊相似系数 修正余弦相似度 汉明距离 曼哈顿距离 莱温斯坦距离。</content></entry><entry><title>RF随机森林面试题和讲解</title><url>http://next.lisenhui.cn/post/study/deeplearning/rf%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E9%9D%A2%E8%AF%95%E9%A2%98%E5%92%8C%E8%AE%B2%E8%A7%A3/</url><categories/><tags/><content type="html"> 随机森林 Random Forest 一种基于树模型的Bagging的优化版本，一棵树的生成肯定还是不如多棵树，因此就有了随机森林，解决决策树泛化能力弱的特点。
描述： 多次随机取样，多次随机取属性，选取最优分割点，构建多个(CART)分类器，投票表决 算法流程 输入为样本集D=(x，y1)，(x2，y2)…(xm，ym)，弱分类器迭代次数T。 对于t=1，2…T 对训练集进行第t次随机有放回采样，共采集m次，得到包含m个样本的采样集Dt 用采样集Dt训练第t个决策树模型Gt(x)，在训练决策树模型的节点的时候，在节点上所有的样本特征M中选择一部分样本特征m, 使得m&laquo;M，在这些随机选择的部分样本特征中选择一个最优的特征来做决策树的左右子树划分 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。 面试题 优缺点： 优点： 训练可以高度并行化，对于大数据时代的大样本训练速度有优势（并行速度快） 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型（在样本维度高，存在缺失值的情况下能维持高效） 对部分特征缺失不敏感，如果有很大一部分的特征遗失，仍可以维持准确度。 由于采用了随机采样，训练出的模型的方差小，泛化能力强。（引入随机性，泛化能力强） 在训练后，可以给出各个特征对于输出的重要性（可以给出特征重要性） 相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。（实现简单） 非常稳定。即使数据中出现了一个新的数据点，也只会对部分决策树，很难影响所有决策树 总结3个：实现简单，并行速度快，引入随机性泛化性能好（减少方差，减少确实特征影响），非常稳定 缺点： 数据小、特征少的情况下，不一定取得好的效果。 在某些噪音比较大的样本集上，RF模型容易陷入过拟合。（观测现象） 取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。 随机森林的随机性体现在哪里？ 多次有放回的随机取样，多次随机选择特征。引入了随机性，提升了泛化性能。 RF为什么不容易过拟合 随机森林中的每一颗树都是过拟合的，拟合到非常小的细节上 随机森林通过引入随机性，使每一颗树拟合的细节不同 所有树组合在一起，过拟合的部分就会自动被消除掉。 为什么RF的训练效率优于bagging？ 因为在个体决策树的构建过程中，Bagging使用的是“确定型”决策树，bagging在选择划分属性时要对每棵树是对所有特征进行考察；而随机森林仅仅考虑一个特征子集。仅仅考虑特征子集，所以效率高。 随机森林需要剪枝吗？ 不需要，后剪枝是为了避免过拟合，随机森林随机选择变量与树的数量，已经避免了过拟合 RF与 GBDT 的区别 随机森林将多棵决策树的结果进行投票后得到最终的结果，对不同的树的训练结果也没有做进一步的优化提升，将其称为Bagging算法。 GBDT用到的是Boosting算法，在迭代的每一步构建弱学习器弥补原有模型的不足。GBDT中的Gradient Boost就是通过每次迭代的时候构建一个沿梯度下降最快的方向的学习器。 RF与Adaboost区别： 相同之处 二者都是Bootstrap自助法选取样本。 二者都是要训练很多棵决策树。 不同之处 Adaboost是基于Boosting的算法，随机森林是基于Bagging的算法。（算法上不同 Adaboost后面树的训练，其在变量抽样选取的时候，对于上一棵树分错的样本，抽中的概率会加大。（样本选择上不同） 在预测新数据时，Adaboost中所有的树加权投票来决定因变量的预测值，每棵树的权重和错误率有关；随机森林按照所有树中少数服从多数树的分类值来决定因变量的预测值（或者求取树预测的平均值）。（预测形式上不同） RF参数调整： **n_estimators:**随机森林建立子树的数量。 较多的子树一般可以让模型有更好的性能，但同时让你的代码变慢。需要选择最佳的随机森林子树数量 **max_features：**随机森林允许单个决策树使用特征的最大数量。 增加max_features一般能提高模型的性能，因为在每个节点上，我们有更多的选择可以考虑。 max_depth： 决策树最大深度 **min_samples_split：**内部节点再划分所需最小样本数 内部节点再划分所需最小样本数 min_samples_leaf： 叶子节点最少样本 max_leaf_nodes： 最大叶子节点数 min_impurity_split： 节点划分最小不纯度 RF自己的参数，主要是前两个决定树的数量和最大特征数。后面的都是基学习器决策树的参数。 特征重要性 袋外数据(OOB)： 大约有1/3的训练实例没有参与第k棵树的生成，它们称为第k棵树的袋外数据样本。 在随机森林中某个特征X的重要性的计算方法如下： 1. 对于随机森林中的每一颗决策树，使用相应的OOB(袋外数据)来计算它的袋外数据误差，记为errOOB1。 2. 随机地对袋外数据OOB所有样本的特征X加入噪声干扰(就可以随机的改变样本在特征X处的值)，再次计算它的袋外数据误差，记为errOOB2。 3. 假设随机森林中有N棵树，那么对于特征X的重要性为(errOOB2−errOOB1/N)，之所以可以用这个表达式来作为相应特征的重要性的度量值是因为：若给某个特征随机加入噪声之后，袋外的准确率大幅度降低，则说明这个特征对于样本的分类结果影响很大，也就是说它的重要程度比较高。</content></entry><entry><title>写在面试字节被拒</title><url>http://next.lisenhui.cn/post/essay/thought/%E5%86%99%E5%9C%A8%E9%9D%A2%E8%AF%95%E5%AD%97%E8%8A%82%E8%A2%AB%E6%8B%92/</url><categories><category>随笔</category></categories><tags/><content type="html"> 写在面试字节被拒的中午 中午收到了字节的感谢信，告诉我面试没有通过。
虽说是感谢信，本该带着愉快的心情接收，可带来的确实让人难过的消息。就这样，毫无征兆的开始了一中午的emo。
最大的感受就是无助加上很累。失败了自然无助。看着大家都能找到好的工作，自己找不到，无助上的无助。累也是真的累。开学以来的半个月里，除了个别几天，几乎每天都是早上8点上班，晚上11点才回去寝室。娱乐就是听听歌，放松就是趴在那睡觉。除了累，还有迷茫。仿佛不知道自己的前路在哪的那种迷茫，不知道该往哪个方向走的迷茫。迷茫的自己，工作是在工作，可却是低效的。自己也不满意自己的低效。不满意的情绪进一步累加。工作时进一步低效。如此循环，直到感谢信到的一刹那，全都爆发了出来。
又重新开始听那首MELANCHOLY，那首在考研时听过无数遍的歌。现在回想考研，仿佛什么都想不起来。那种高压环境下，自己是怎么坚持过来的？
其实我大概知道的。研究生读的这两年来，认识了很多人，知道人都有力不足的时候。挫折也是难免的。得到点好处，就开心的不行。碰到一点挫折，就失落的不行。这样的心态是不会成功的。优秀的人就不会失败嘛？会。那你失败又如何。天不会塌下来。失败太正常了。你自己没有认真去争取。只想着凭运气通过，那怎么会？如果有一天你面试所有题目都答上来了，然后你失败了，那行，那是时运不济，你可以emo。你题目都没答上来，你emo什么？你有什么理由emo呢？别人说你强，你就真的强了嘛？你自己强不强难道你自己不知道嘛？
什么都不会也好意思说自己强。看似整天在这，一半的时间在摸鱼，也好意思说自己付出的多？别笑话了，眼睛只盯着眼前的人，怎么会走的好直线？
认清自己心中真正期望的东西。 不积跬步无以至千里。
送给emo的自己。</content></entry><entry><title>Word2vec讲解和面经</title><url>http://next.lisenhui.cn/post/study/deeplearning/word2vec%E8%AE%B2%E8%A7%A3%E5%92%8C%E9%9D%A2%E7%BB%8F/</url><categories/><tags/><content type="html"> word2vec 简介 讲的非常好：https://zhuanlan.zhihu.com/p/27234078 负采样：https://www.cnblogs.com/pinard/p/7249903.html
介绍 Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型 通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。
它的输入都是one-hot编码，网络层包括两层隐藏层来学习一个滑动窗口内单词的共现关系。输出层是一个概率分布。
详细定义 word2vec是一个3层的神经网络。输入层和输出层都可以看做词汇表的one-hot表示
CBOW: 每个滑动窗口是一个测试用例。输入 层V_1, V_m ，输出层 V_c 。即用中心词 的上下文(不包含 V_c )为输入，最大化预测输出为中心词 V_c的概率。
Skip-Gram: 每个滑动窗口是w个测试用例。输入层 V_c ,输出层 V_i 。即用中心词，最大化预测输出层为上下文词汇的概率。
$$Z ^ { [ 1 ] } = \sum _ { i } ^ { 4 } W _ { i * } x _ { i } + b _ { i }$$
$$h = A ^ { [ 1 ] } = \frac { 1 } { 4 } \sum _ { i } z _ { i } ^ { [ 1 ] }$$
$$Z ^ { [ 2 ] } = h * W ^ { \prime }$$
$$\hat{y} = a = softmax ( Z ^ { [ 2 ] } ) = \frac { e ^ { [ 2 ] } } { \sum _ { k } ^ { [ 2 ] } }$$
再与 W&rsquo;乘积后softmax归一化。因为输出层词汇的one-hot可以对应 y=[0,0..1..0] ，对于softmax输出一般采用最小化负的交叉熵的似然: ,然后使用梯度下降即可更新。
损失函数 $$E = - \log p ( W _ { 0 } | W _ { 1 } ) = - \log \frac { e x p ( u _j ) } { \sum _ { k \in V } e x p ( u _ { k } ) } = \log \sum _ { k \in v } exp(u_k)-u_j$$
这里的j是目标词真实的下标。u_j表示第j个词就是目标词的可能性。就是使目标次出现频率最高的概率的乘积。就是极大似然估计。
CBOW具体前向传播步骤：
输入层: 输入C个单词x： x1k,⋯,xCk，并且每个 x 都是用 One-hot 编码表示，每一个 x 的维度为 V（词表长度）。 输入层到隐层 首先，共享矩阵为 W_{V×N} ，V表示词表长度，W的每一行表示的就是一个N维的向量（训练结束后，W的每一行就表示一个词的词向量）。 然后，我们把所有输入的词转x化为对应词向量，然后取平均值，这样我们就得到了隐层输出值 ( 注意，隐层中无激活函数，也就是说这里是线性组合)。 其中，隐层输出 h 是一个N维的向量 。 $h=\frac{1}{C}W^T(x1+x2+⋯+xc)$ 隐层到输出层：隐层的输出为N维向量 h ， 隐层到输出层的权重矩阵为 W′N×V 。然后，通过矩阵运算我们得到一个 V×1 维向量u=W′T∗h 其中，向量 u 的第 i 行表示词汇表中第 i 个词的可能性，然后我们的目的就是取可能性最高的那个词。因此，在最后的输出层是一个softmax 层获取分数最高的词，那么就有我们的最终输出：$P(wj|context)=yi=\frac{exp(uj)}{\sum_{k∈V}exp(uk)}$ Skip-gram: Skip-Gram与CBOW的方法略有不同，每个词对（中心词，上下文词）都是一个训练样本
步骤 skip-gram处理步骤：
1.确定窗口大小window，对每个词生成2*window个训练样本，(i, i-window)，(i, i-window+1)，&hellip;，(i, i+window-1)，(i, i+window)
2.确定batch_size，注意batch_size的大小必须是2*window的整数倍，这确保每个batch包含了一个词汇对应的所有样本
3.训练算法有两种：层次Softmax和Negative Sampling
4.神经网络迭代训练一定次数，得到输入层到隐藏层的参数矩阵，矩阵中每一行的转置即是对应词的词向量
损失函数 $$E= - log p(w_1, w_2, \cdots, w_C | w_I) \ = - log \prod_{c=1}^C P(w_c|w_i) \ = - log \prod_{c=1}^{C} \frac{exp(u_{c, j})}{\sum_{k=1}^{V} exp(u_{c,k}) } \ = - \sum_{c=1}^C u_{j,c} + C \cdot log \sum_{k=1}^{V} exp(u_k)$$
层次softmax， 哈夫曼树。 https://zhuanlan.zhihu.com/p/59396559
https://zhuanlan.zhihu.com/p/56139075
层次Softmax是一个二叉树结构，每个非叶子节点是个二分类器，每个叶子节点对应词汇表中的单词（ N个词汇，N-1 个二分类器）二分类器是逻辑回归。
二叉树是根据词频构建的哈夫曼树，词频越大的叶子节点距离根节点的路径越短。
softmax层的求和项被一系列二分类器代替，如果当前节点到目标叶子节点往左走，该节点的输出为二分类器的输出$\theta$ ,否则输出为$1-\theta$ 。最终输出为各个二分类器节点输出的乘积。$o ( h * W _ { 1 } ) * ( 1 - o ( h * W _ { 2 } ) ) * ( 1 - o ( h * W _ { 3 } ) )$.我们期望最大化上述路径乘积，即损失Cost 为最小化其负的log梯度。
所以层次Softmax相当于自动完成了归一化操作，故与普通Softmax可以等价
采用Softmax之后，需要需要更新参数的数量从 N降低到 logN ，对于一个大语料来说，这个改进可以说很大了。只有路径上的权重进行了更新。所以更新速度较快。
损失函数 $l^w$项连乘，即从根节点到叶子节点一功有$l^w$个节点，求负对数后，变成连加。
$$L ( w , j ) = ( 1 - d _ { j } ) \cdot \log [ \delta ( x _ { i v } w _ { j - 1 } ) ] + d _ { j } ^ { w } \cdot \log [ 1 - \delta ( xw ) ]$$ 就是按照词频构建二叉树，然后每一个非叶子节点就是而分类器，输出$\theta$,往左走输出$\theta$,往右走输出$1-\theta$，$d_j$表示向左走，即为0， $d_j$表示向右走，表示值为1.
最终计算出来的梯度信息是上下文单词的梯度信息，word2vec直接将梯度信息应用到每个窗口单词上面去。
Negative Sampling(负采样) 负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。 当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新 采用二元逻辑回归来求解模型参数。通过负采样，得到neg个负例。接下来就是采用一个正例和negative负例，来使用逻辑回归求解 $f = w_1x+b$ $f_2 = w_2f+b$ $p = sigmoid(f_2) = \frac{1}{1+e^{-f_2}}$,这是正例的概率，负例的概率就是1-p。然后对$w_1, w_2$进行梯度更新 损失函数为$L=\sum y_i log(\theta(xw))+(1-y_i)log(1-\theta(xw))$ word2vec中负采样 高频词采样率大，低频词采样率少。 公式：$p=\frac{f(w)^{\frac{3}{4}}}{\sum f(w)^{\frac{3}{4}}}$, f(w)为单词出现频率。 长度为1的线段，词频越大，则词对应的长度越大。 最后线段等份成m份。取neg个位置，每个位置对应一个词 cbow 负采样 上下文分别预测窗口词的平均去预测对应词和负采样的词 skip-gram负采样 分别使用当前窗口词去预测对应词和负采样的词。 word2vec 输入是one-hot向量，B，N。N是字典大小。这里的N不是特征大小，如果是NLP的话，就是字典大小。就是整个集和中不重复样本数的个数。映射成one-hot向量 输出是经过两层隐藏层后，输出也是字典大小的向量，表示概率。
word2vec优化 输入的时候不会相乘，而是“查表” word2vec在学习词向量之间的共现关系 优化 对常见单词组合或词组作为单个words来处理 对高频词进行抽样减少训练样本个数 对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关。 出现频率越高，越容易被删除 使用negative sample，这样每个训练样本只会更新一小部分权重。 面试题 word2vec看起来跟自动编码解码器很像，它们两者有什么关联吗？ word2vec和自编码器的差别。 word2vec是最大化共现概率。 自编码器是最小化重构误差。
cbow和skip-gram哪个更好 参数完全相同的情况下，大语料库cbow好。 这里使用skip-gram是速度更慢一些，但效果更好一点，因为中心词训练的次数更多。 skip-gram训练次数更多。所以学到的东西理应更多。
word2vec中softmax的优化
softmax时间消耗主要在指数计算上，指数计算可以采用多项式逼近的方法，还可以采用查表法计算。将输入的值限制到一个范围内，这样查表就能得到高精度值。 softmax求导
$e^{z_j}$为第j个词的概率。对输入z_j求偏导为1，其余的$z_k$对z_j求偏导为0 a = softmax(z), a对z求偏导后=a(1-a) $E = - log , p(w_1, w_2, \cdots, w_C | w_I) \ = - log \prod_{c=1}^C P(w_c|w_i) \ = - log \prod_{c=1}^{C} \frac{exp(u_{c, j})}{\sum_{k=1}^{V} exp(u_{c,k}) } \ = - \sum_{c=1}^C u_{j^c} + C \cdot log \sum{k=1}^{V} exp(u_k)$
随机游走 https://zhuanlan.zhihu.com/p/66836312
DeepWalk DeepWalk提出了“随机游走”的思想，这个思想有点类似搜索算法中的DFS，从某一点出发，以深搜的方式获得一个节点序列。
这个序列即可以用来描述节点。
node2vec node2vec的作者针对DeepWalk不能用到带权图上的问题，提出了概率游走的策略
在一个图中，假设上一步走到的节点是t ，当前处于节点 v ，则下一步游走的概率为$\alpha$满足
$$\alpha = { \begin{array} { l } { \frac { 1 } { p } \quad d = 0 } \ { 1 \quad d = 1 } \ { \frac { 1 } { q } \quad d = 2 } \end{array}$$
d = 0 表示上一步走过的节点。d=1表示与节点t和节点v距离相同的节点（距离都为1）。d=2表示其他节点。
使用p和q控制了游走的宽度优先和深度优先。在调参的效果下可以取得比word2vec更好的效果
第一个节点由于没有上一个节点，所以直接选取游走节点。
metapath2vec 应用于异质图。异质网络比同质网络包含了更多的语义信息，如果仅仅使用DeepWalk和node2vec这类方法，只能将不同的节点进行不同编号，并进行随机游走。但是这种方式忽略了节点的类型信息，而将作者、论文、会议都视作同一类型的节点，丢失了大量语义信息。
元路径可以理解为预定义的节点类型序列，比如APV表示【作者（Author）-论文（Paper）-期刊（Venue）】
通过设计首尾类型相同的元路径，我们可以不断地重复基于相同元路径的游走</content></entry><entry><title>光谷店铺点评</title><url>http://next.lisenhui.cn/post/journey/%E7%BE%8E%E9%A3%9F%E8%AF%84%E5%88%86/%E5%85%89%E8%B0%B7%E5%BA%97%E9%93%BA%E7%82%B9%E8%AF%84/</url><categories><category>阳阳旅游日记</category></categories><tags><tag>美食点评</tag></tags><content type="html"/></entry><entry><title>东西湖店铺点评</title><url>http://next.lisenhui.cn/post/journey/%E7%BE%8E%E9%A3%9F%E8%AF%84%E5%88%86/%E4%B8%9C%E8%A5%BF%E6%B9%96%E5%BA%97%E9%93%BA%E7%82%B9%E8%AF%84/</url><categories><category>阳阳旅游日记</category></categories><tags><tag>美食点评</tag></tags><content type="html"/></entry><entry><title>百弗卡美食点评</title><url>http://next.lisenhui.cn/post/journey/%E7%BE%8E%E9%A3%9F%E8%AF%84%E5%88%86/%E7%99%BE%E5%BC%97%E5%8D%A1%E7%BE%8E%E9%A3%9F%E7%82%B9%E8%AF%84/</url><categories><category>阳阳旅游日记</category></categories><tags><tag>美食点评</tag></tags><content type="html"> 1-5分 1分这啥啊 2分低于预期 3分一般 4分喜欢吃 5分很喜欢吃 1分白送也不吃 2分白送就吃 3分没事不买 4分会再买 5分会推荐给朋友
司康（红茶，蔓越莓）。司康还是不错的。慢慢吃很香。也不噎人。喜欢吃给4分。 肉丁贝果。 味道一般。口感略生硬。没味道。3分。 可颂-好吃，吃起有奶香味道。4分。</content></entry><entry><title>模型调优和面试题</title><url>http://next.lisenhui.cn/post/study/deeplearning/%E6%A8%A1%E5%9E%8B%E8%B0%83%E4%BC%98%E5%92%8C%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories/><tags/><content type="html"/></entry><entry><title>指标讲解和面试题</title><url>http://next.lisenhui.cn/post/study/deeplearning/metrics%E6%8C%87%E6%A0%87%E8%AE%B2%E8%A7%A3%E5%92%8C%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories/><tags/><content type="html"> 指标讲解和面试题 指标 TP, FP, TN, FN tp = 预测为正，实际为正 fp = 预测为正，实际为负 fn = 预测为负，实际为正 tn = 预测为负，实际为负 后一个字母代表预测，前一个字母表示预测是正确还是错误 accuracy准确率: 表示预测能力（预测正、负）的好坏：accuracy=(tp+tn)/(tp+fp+fn+tn) precision: 精确度。所有预测中对了多少个。precision=(tp)/(tp+fp) recall：召回率。度量有多少个正例被正确预测：recall=tp/(tp+fn) f1-score: $\frac{1}{f1}=\frac{1}{2}(\frac{1}{p}+\frac{1}{r})$，得到$f1=\frac{2precisionrecall}{precision+recall}$ f1更多知识看百面机器学习，看https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E7%AC%AC%E4%BA%8C%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md $\frac{1}{f_\beta}=\frac{1}{1+\beta^2}(\frac{1}{p}+\frac{\beta^2}{r})$ $\beta$表示了recall对precision的重要程度。$\beta>1$则，recall有更大的影响。否则precision有更大的影响。 ROC曲线 真正例率：所有正例中被正确预测为正的比例：TPR(tp ratio)=(tp)/(tp+fn) 假正例率：所有负例中被错误的预测为正例：FPR(fp ratio)=(fp)/(fp+tn) 怎么记忆：真正和假正都是看“正”的比例，“真”即真的正例中被正确预测的比例。“假”即所有负例中被错误预测为正的比例。 tp,fp分别做分子。然后一个是所有真的里选。后面的是所有假的里面选。 ROC曲线：横坐标FPR，纵坐标TPR。 FPR为0，TPR为1，则证明所有样本正确分类。即是最好。 ROC曲线的绘制 ROC底下的AUC的计算： ​ AUC是衡量二分类模型优劣的一种评价指标，表示正例排在负例前面的概率。 计算面积。先排序score。然后按照梯形计算公式计算：$Auc=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)*(y_{i+1}+y_i)$ 使用等价的Wilcoxon-Mann-Witney Test法。任意给一个正类样本和负类样本，正样本的score有多大概率大于负样本的score。方法：设正样本为M，负样本为N。总样本数为n。score从大到小排序，然后最大的样本设为n，第二位n-1。$AUC=\frac{\sum_{ispositive}rank_i-\frac{M(1+M)}{2}}{M*N}$. 统计正样本的rank，它代表了排在它下面的所有样本数量。减去和正样本的匹配对，就是和负样本的匹配对。rank求和-正样本和正样本匹配对个数求和就是正样本和比它值小的负样本匹配对个数。除以正样本*负样本就是AUC。 PR曲线 横坐标Precision，纵坐标Recall。 该曲线对应的AUC即为(AP Average Precision) AP 越高，模型性能越好。 PR曲线易受类别分布影响。而ROC曲线不会 注意TPR用到的TP和FN同属P列，FPR用到的FP和TN同属N列，**所以即使P或N的整体数量发生了改变，也不会影响到另一列。**也就是说，即使正例与负例的比例发生了很大变化，ROC曲线也不会产生大的变化，而像Precision使用的TP和FP就分属两列，则易受类别分布改变的影响。 二分类转多分类 在二分类转多分类，或者多个混淆矩阵（多个验证集）时如何计算。 宏平均（macro-average)：分别计算出各个指标后平均。如计算出每一类的precision、recall后再平均得到平均precision,平均recall。再求平均F1。 微平均（micro-average)：先平均TP、FP等参数。再计算各个metrics。 加权平均（weighted-average)：根据类别的样本数量得到权重（该类样本数/总样本数）。然后加权求宏平均。 优缺点比较 宏平均平等看待每个类别，但是它的值会受稀有类别影响。 微平均F1平等考虑所有样本，所以它的值受到常见类别的影响比较大。 加权平均考虑了类别不平衡情况，它的值更容易受到常见类（majority class）的影响。 训练集和测试集分类法 留出法 划分训练集、验证机（验证参数用）、测试集。验证集从训练集中划分，比例为(1/3-1/5) 交叉验证k-fold 将数据集划分为k个大小相似的互斥子集。每次选k-1个作为训练集，1个作为验证集。总共进行k次训练和测试。 自助法：有放回采样 设数据集个数为m。对数据集进行有放回采样m次，得到m个样本的训练集。剩下没有被采样到的数据作为测试集。不被采样到的概率为$lim_{m->+\infty}(1-\frac{1}{m})^m=\frac{1}{e}$.极限情况下有36.8%的数据不被采样到。缺点：这种方式会改变数据集的分布，引入偏差。数据量足够的情况下，不采用这个。 面试题 正确率可以评估分类算法嘛？ 不能。比如正负样本不平衡的数据中。全部预测负也能达到99的正确率。显然这是个无用的分类器。所以仅仅看正确率不行。 如何判别分类器的好坏？ 看ROC曲线。在保证正确率的情况下，提升ROC。 f1-score和ROC的好坏。</content></entry><entry><title>决策树讲解和面试题</title><url>http://next.lisenhui.cn/post/study/deeplearning/%E5%86%B3%E7%AD%96%E6%A0%91%E8%AE%B2%E8%A7%A3%E5%92%8C%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories/><tags/><content type="html"> 决策树 决策树算法采用树形结构，使用层层推理来实现最终的分类。决策树由下面几种元素构成： 根节点：包含样本的全集 内部节点：对应特征属性测试 叶节点：代表决策的结果 预测时，在树的内部节点处用某一属性值进行判断，根据判断结果决定进入哪个分支节点，直到到达叶节点处，得到分类结果。 增益计算 ID3：信息增益 某一个属性的信息熵 $Ent(D) = -\sum_{k=1}^y p_k log p_k$。其中在所有标签y中，第k类标签所占的比例为$p_k$。分类越纯，$p_k$就越大，信息熵就越小。即信息熵越小，则信息量越小，混乱程度越低，置信度越高。 属性a信息增益为:$Gain(D, a)=Ent(D)-\sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)$ 其中属性（特征）a的分类有v种，第v类的分类数量为$D^v$。 信息增益的缺点：偏爱属性分类多的特征。属性分类多的特征，每一分类中的越纯的可能性越大。信息熵就低，信息增益就高。所以偏爱特征分类多的特征。比如按照id来分类，则信息增益最大。 C4.5：信息增益率 为了解决上述缺点。将信息熵除以一个固有值。 $Gain_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$，其中$IV(a)=-\sum_{v=1}^V \frac{|D^v|}{|D|} log \frac{|D^v|}{|D|}$ 固有值为属性a的每一种类所占的样本的比例的熵。 信息增益率偏爱特征分类数量少的特征。所以C4.5的信息选择为，先选择信息增益大于平均值的属性。然后在其中选择信息增益率最大的属性。 CART：基尼系数 由于熵运算非常耗时。所以采用计算更快的基尼系数:$Gain(D)=\sum_{y=1}^Y\sum_{k'\neq k}p_kp_k'=1-\sum_{k=1}^yp_k^2$ 其中$p_k$为标签y中第k类所占的比例。Gini系数表示了随机抽两个样本，它们标签不一致的概率。Gini越小，纯度越高。基尼系数替代信息熵，则是CART计算信息增益的方法。注意：算信息增益时，基尼系数仍然需要乘以$\frac{|D^v|}{|D|}$ 基尼系数约等于熵模型的一阶泰勒展开。 当CART为回归树时，采用MSE平方差代替基尼系数作为增益计算。 连续值的处理 C4.5和cart拥有相同的连续值处理 对于连续值，排序后，选取每两个值的平均值作为候选值。根据增益计算选取最佳增益的候选值进行分裂。详细点说：将连续特征离散化，假设 n 个样本的连续特征 A 有 m 个取值，C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点； 如果当前属性为连续属性，则该属性在后面还可以参与子节点的产生选择过程。 缺失值的处理 C4.5采用将缺失样本同时划分入所有子节点，为每个确实样本赋予不同权重。形式上等价于使用不同概率将缺失样本划入子节点。 属性的选择：首先仅使用非缺失样本进行属性选择。每个属性的计算$Gain(D, a) = \rho * Gain(D, a)$，其中$\rho=\frac{\sum_{x\in \hat{D}}w_x}{\sum_{x\in D} w_x}$，即非缺失值样本所占的比例。 缺失值样本的划分：将缺失值同时划分入左右子节点，样本权值调整为:$w_x=\hat{r_v}*w_x$， 其中$\hat{r_v}$为非缺失值样本中，该分类中，样本所占的比例。即17个样本，2个缺失。15个样本分为5,8,2.则每一类的$\hat{r_v}$比例为$\frac{5}{15},\frac{8}{15},\frac{2}{15}$ CART采用代理测试来估计缺失值 属性的选择：使用没有缺失值的样本进行划分，并进行惩罚。和C4.5类似。缺失为20%则惩罚20%的权重 缺失值样本的划分：为每个属性建立一个代理属性（不管是否缺失都会这么做）。代理属性满足和主分裂类似，且有着正关联。使用代理属性对缺失值进行划分。 剪枝 C4.5剪枝 预剪枝：在分裂时进行剪枝 限制树的节点个数：节点数据比例低于某一阈值 限制树的高度：节点特征都已经分裂。或者树到达一定高度。 利用分类指标：节点划分前准确率比划分后准确率高。 总结：常见树防止过拟合的方法。树高度、节点个数，指标。 预剪枝优点：节省大量的训练开销，降低过拟合。预剪枝缺点：存在欠拟合风险（存在当前性能下降，后续性能大幅提高的情形） 后剪枝： 使用测试集进行验证：对所有非叶子节点，自底向上的进行剪枝：如果这个节点不分裂，效果保持或者不下降，则可以替换这个子树。 优点：后剪枝欠拟合风险小很多，泛化性能会优于预剪枝。缺点：时间开销大。 悲观后剪枝TODO: CART剪枝： 基于代价复杂度剪枝： 这种方法会生成一系列树，每个树都是通过将前面的树的某个或某些子树替换成一个叶节点而得到的，这种方法需要使用一个单独的测试数据集来评估所有的树，根据它们在测试数据集熵的分类性能选出最佳的树。 在所有的子树中，找出那些使用了较多节点，却使错误率下降最低的子树，把这些子树剪裁掉。 指标$R(T)=e(T)*p(T)$，e(T)指该节点的错分率。如二分类，1占13，0占7，则错分率为7/20（因为这个节点被判为1）。p(T)是该节点样本数占样本总数的比例。计算当前节点的R(T)，计算当前节点下的所有叶子节点的R(T)，得到代价复杂度$\alpha=\frac{R(T)-R(T_t)}{N-1}$N为该节点下叶子节点数量。对于这个值，分子越大说明分类效果越好。分母越小说明复杂度越低。整个值越大越好。 代价复杂度剪枝： 循环对代价复杂度参数最小的节点进行剪枝（有多个节点同时取到最小值时取叶子节点最多的节点），直到只剩下根节点，可得到一系列的剪枝数{T0, T1, T2, …, Tm}，其中T0为原始的决策树，Tm为根节点，Ti+1为Ti剪枝后的结果。 当树只有根节点时结束。记录下所有的树，根据实际误差获得最优决策树。 三个树的比较： 划分标准的差异：ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。 使用场景的差异：ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快； 样本数据的差异：ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ； 样本特征的差异：ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征； 剪枝策略的差异：ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。 决策树的优缺点 优点： 决策树易于理解和解释，可以可视化分析，容易提取出规则； 比较适合处理有缺失属性的样本； 测试数据集时，运行速度比较快； 缺点： 容易发生过拟合（随机森林可以很大程度上减少过拟合）； 容易忽略数据集中属性的相互关联； 对于那些各类别样本数量不一致的数据，在决策树中，进行属性划分时，不同的判定准则会带来不同的属性选择倾向； 随机森林 随机森林属于bagging
步骤：
样本选择：一个样本容量为N的样本，有放回的抽取N次，每次抽取1个，最终形成了N个样本。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。 属性选择：当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m &laquo; M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。 决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。 按照步骤1~3建立大量的决策树，这样就构成了随机森林了。 面试题 部分面试题： https://github.com/datawhalechina/daily-interview/tree/master/AI%E7%AE%97%E6%B3%95/machine-learning 给出案例手算信息增益（ID3,C4.5,CART)(面试中遇到过) 参考资料 《机器学习》周志华。（推荐） https://zhuanlan.zhihu.com/p/133838427 https://zhuanlan.zhihu.com/p/85731206</content></entry><entry><title>XGBoost理论推导和面试题</title><url>http://next.lisenhui.cn/post/study/deeplearning/xgboost%E7%90%86%E8%AE%BA%E6%8E%A8%E5%AF%BC%E5%92%8C%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories/><tags/><content type="html"> 停止增长的条件：
叶子节点的分裂gain&lt;=0 叶子节点只有一个样本的时候，就没必要划分了。 限制叶子节点个数 计算出叶子节点的b_j。计算完后叶子节点的权重就为b_j
步骤： 1.
公式：https://zhuanlan.zhihu.com/p/46683728
$$f ( x ) = \sum _ { j = 1 } ^ { J } b _ { j } I ( x \in R _ { j } )$$ 将所有的样本加起来可以表示为： $$\sum _ { i = 1 } ^ { N } f ( x _ { i } ) = \sum _ { j = 1 } ^ { J } \sum _ { x\in R_j} b _ { j }$$ 所以公式推导为： $$L ^ { m } = \sum _ { i = 1 } ^ { N } [g _ { i } f _ { m } ( x _ { i } ) + \frac { 1 } { 2 } h _ { i } f_m^2 ( x _ { i } )] + \gamma J + \frac { 1 } { 2 } \lambda \sum_{j=1}^Jb_j^2$$ xgboost的训练方式和gbdt类似。计算m-1 f(x)的损失函数在训练样本点的一阶导、二阶导。然后用来分裂每个节点，生成树。最后加到模型中去f_{m-1}(x)+f_m(x)。再计算损失函数，拟合下一个。
优化 优化思路：1. 压缩特征数（列采样） 2. 压缩每个特征下，特征的种类数（即特征的分裂点的数量）
预防过拟合 正则化 限制节点数量 限制Gain的增长。当Gain &lt; gamma则不分裂 列采样。随机采样特征 按树随机。每一颗树筛选出来特征采样后，一整颗树的特征都不变 按层随机。每一层叶子节点分裂时，都进行采样。 shrinkage 学习率 加入学习率到基学习器中。$y=y_{m-1}+\eta f(x)$ 有助于防止过拟合。一般取0.1. 速度优化： 近似分裂算法： 使用二阶导代替样本的权重，然后根据权重划分桶。设定一个值$\epsilon$,每个桶的权重值和应该小于$\epsilon$。会得到$\frac{1}{\epsilon}$个切分点。 为什么使用二阶导？二阶导可以看成是样本的权重。同时二阶导越大，说明该值附近变换越快，越应该进行切分。 和采样类似。有全局策略和局部策略。 全局策略。第一次计算得到的分裂点，后面的分裂只会用这几个分裂点。 局部策略。每次分裂节点时，都会重新计算分裂点。优点是可以根据节点中的样本数计算分裂点。 系统设计 核外块运算： 块拆分。将不能存储在内存的数据放在磁盘中。并且拆分成块，以提高磁盘IO率。 块压缩。每个块存储时会压缩存储。读取时解压读取。 开启线程在运算数据的同时对数据进行读取。 分块并行。 预先对所有的特征进行按列预排序，并存储索引。这样每次分裂只需要一次线性扫描就能获得最大的Gain 分布式运行：将所有的特征分成N各块，发送给各个机器执行。最后将执行结果返回给调度中心。 缓存命中率优化。由于特征存储为block结构。所以访问不是顺序访问的。为此在block后面开一个buffer，存储他们的一阶导和二阶导。直接读取一阶导和二阶导。这样命中率就会提高。 对缺失值的处理： 所有的计算对非缺失值进行计算。计算完成后，缺失值整体放入左右，计算增益。设置最大增益方向为缺省（默认）方向放入缺失值。 预测中遇到缺失值，默认放入右节点。 题目 参考：https://mp.weixin.qq.com/s?__biz=MzI1MzY0MzE4Mg==&amp;mid=2247485159&amp;idx=1&amp;sn=d429aac8370ca5127e1e786995d4e8ec&amp;chksm=e9d01626dea79f30043ab80652c4a859760c1ebc0d602e58e13490bf525ad7608a9610495b3d&amp;scene=21#wechat_redirect
https://github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning/XGBoost.md
xgboost优缺点：
XGBoost是大规模、分布式的通用梯度提升(GBDT)库。它是一个加法模型，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。XGBoost对GBDT进行了一系列优化，比如损失函数进行了二阶泰勒展开、目标函数加入正则项、支持并行、默认缺失值处理等，在可扩展性和训练速度上有了巨大的提升，但其核心思想没有大的变化。 优点： 精度更高、灵活性更好（不同的基分类器，不同的损失函数）、正则化、缺失值、训练速度快（可并行） 缺点 和lgbm相比，节点分裂过程中需要遍历所有特征 存储空间消耗较大。 XGBoost特征重要性。
三种方式判断特征重要性。 freq：特定的特征在模型树种发生分裂的次数的百分比 gain：平均增益。特征在所有树种作为分裂节点的信息增益之和除以该特征出现的频次。 cover：节点样本的二阶导数和处理特征出现的频次。平均二阶导数和。二阶导和越大，说明该节点对方向做出了较多的指导。 xgboost一般采用gain。 介绍一下XGB，怎么调参防止过拟合
决策树方面： 决策树节点最小样本权重和的阈值（这里指的是二阶导，官网api这么定义的） 树的最大深度 树节点的分裂阈值（增益阈值） 公式 正则化。目标函数添加正则化项。调整正则化系数 调整学习率 shrinkage 数据方面： 子采样 列抽样 第一类参数：用于直接控制模型的复杂度。包括max_depth，min_child_weight，gamma 等参数 第二类参数：用于增加随机性，从而使得模型在训练时对于噪音不敏感。包括subsample，colsample_by树 还有就是直接减小learning rate，但需要同时增加estimator 参数。
比较LR和GBDT，说说什么情景下GBDT不如LR
在特征高维稀疏的情况下。逻辑回归线性模型控制权重。xgboost正则化控制树的大小。此时xboost只需要一个节点就能分类所有样本。正则化的作用很小。但是逻辑回归的正则化就不容易过拟合 xgboost如何选择分裂点
对特征值进行排序，存储为block结构。方便重复使用 使用近似分裂算法，根据样本的二阶导选择常数个分裂点。 采用特征并行的方法利用多个线程分别计算每个特征的最佳分裂点。进行分裂。 xgboost缺失值
默认使用不缺失的样本进行分裂 缺失样本整体放入左右节点，最大增益节点就为默认的分裂方向 预测值中出现没见过的缺失值，则放入右节点 xgboost不平衡数据
为正负样本设置不同权重。scale_pos_weight参数 xgboost 为什么快？
四点 特征预排序+block缓存，多线程并行 核外块运算 近似分裂算法 命中缓存优化</content></entry><entry><title>GBDT理论推导和面试题</title><url>http://next.lisenhui.cn/post/study/deeplearning/gbdt%E7%90%86%E8%AE%BA%E6%8E%A8%E5%AF%BC%E5%92%8C%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories/><tags/><content type="html"> 加法模型 加法模型事梯度提升树、Adaboost的基础模型
公式: $f(x) = \sum_{m=1}^M \beta_m b(x;\gamma_m) $ 其中$\beta_m$是分类器的权重，$\gamma_m$是分类器的参数。
损失函数：L(y, f(x)) 回归问题，使用MSE平方误差 分类问题：使用指数函数、交叉熵损失函数
训练方式：使用前向分布算法。 不使用梯度下降，是因为M个基学习器的参数太多，复杂度太高，不适合梯度下降。所以分成M步，在训练当前基学习器时，前面的基学习器已经训练完毕，就是前向分布算法。
前向分布算法步骤：
初始化第一个学习器。根据loss function的选择。 for m=1 to M: 极小化损失函数 $(\beta_m, \gamma_m) = argmin_{\beta_m, \gamma_m}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma)$，得到参数$(\beta_m, \gamma_m)$ 更新 $f_m(x)=f_{m-1}(x) + \beta_m b(x;\gamma_m)$ 得到加法模型 $f(x)=f_M(x)=\sum_{m=1}^M\beta_m b(x;\gamma_m)$ Gradient Boosting Decision Tree 梯度提升树。依然采用加法模型和前向分布算法。
特点：
其可以支持设置不同的可微损失函数可以处理各类学习任务（多分类、回归、Ranking等），应用范围大大扩展。 梯度提升算法利用损失函数的负梯度作为残差拟合的方式 整体步骤和加法模型类似。 不一样的点：
可以使用自定义的损失函数 去拟合负梯度。 基学习器选择回归树 步骤：
计算当前损失函数负梯度的表达式 构造新的训练样本$T_m={(x, r_m1), (x_2, r_m2)&hellip;}$ 根据负梯度构造新的训练样本。（即如何拟合负梯度的） 让当前的基学习器去你和上述训练样本，得到$T(x;\theta_m)$ 具体步骤：
初始化第一个学习器。根据loss function的选择。$f_0(x)=argmin_c\sum_{i=1}^NL(y_i, c)$, 对损失函数求导。 for m=1 to M: 计算负梯度：$y_i = - \frac{\partial L(y_i,f_{m-1}(x_i))}{\partial f_{m-1}(x_i)}$ 让基学习器去拟合负梯度 使用line search 确定一个p_m。用来最小化和标签的损失函数L 确定p_m，$p_m = argmin_p \sum_i^N L(y_i, f_{m-1}(x_i)+ph_m(x_i;w_m))$ 来使得L最小 p_m就是最终加法模型当中每个基学习器的权重。 $f_m(x) = f_{m-1}(x) + p_mh_m(x; w_m)$ 当采用回归树做基分类器时，回归树公式可以表达为$T(x;\theta)=\sum_j^J b_j I(x\in R_j)$。上述公式可以进一步优化: $f_m(x) = f_{m-1}(x) + \sum_j^J \gamma_jI(x\in R_jm), 令\gamma_j=p_m b_j$ 此时，第三步训练得到$\gamma$，第四步变成纯加法。
公式推导：
需要公式推导的部分，就是为何拟合负梯度是可行的 带入平方误差、指数损失函数。可以算出初始值、负梯度。 注意平方误差时，负梯度就是残差。 问题 优缺点 优点：1. 可以使用任意的损失函数，只要损失函数连续可导。这使得抗噪音能力较强。
剩一个问题。基学习器如何去拟合负梯度的？ 负梯度作为下一个基学习器的输入。
为何拟合负梯度？ 根据加法模型的损失函数得到。 $Loss=L(y, f_m(x))=L(y, f_{m-1}(x)+h_m(x;w_m))$ 进行一阶泰勒展开:
$Loss=L(y, f_{m-1}(x))+\frac{\partial L(y_i,f_{m-1}(x_i))}{\partial f_{m-1}(x_i)}h_m(x;w_m)$
所以让$h_m$去拟合负梯度。
笔记 GBDT二分类问题是Adaboost的特殊情况。 GBDT如何处理分类任务、回归任务、如何处理负梯度？ 负梯度作为x的标签去拟合。 底层采用cart，所以分类使用基尼系数，回归使用mse</content></entry><entry><title>Adaboost理论推导和面试题</title><url>http://next.lisenhui.cn/post/study/deeplearning/adaboost%E7%90%86%E8%AE%BA%E6%8E%A8%E5%AF%BC%E5%92%8C%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories/><tags/><content type="html"> 集成学习 集成学习通过叠加弱分类器，来训练得到一个强分类器。 根据叠加的方式不同，可以分为Bagging, Boosting, Stacking.
Bagging: 多次采样，并行训练，集体投票，减少方差 Boosting：层层叠加，串行训练，聚焦错误，减少偏差 Stacking：多次采样，并串结合，输出作为最后的输入
Adaboost Adaboost 使用加法模型，将各个弱分类器组成强分类器。
特点:
采用指数函数$y=e^{-f(x)h(x)}$作为损失函数 步骤：
初始化第一个分类器的权重 根据前一个分类器的训练结果，依次调整后面样本的权重。 将弱分类器组成一个强分类器。误差率小的弱分类器占据的权重更大。 具体步骤：
初始化权重 进行t=1&hellip;T轮迭代 选取当前误差最低的弱分类器h作为第t个分类器。使用该分类器去拟合带权重的标签。将带权重的标签作为下一轮拟合的目标。 计算弱分类器在分布D_t的分类误差e_t。 计算误差率。$\alpha_t = \frac{1}{2}ln\frac{1-e_t}{e_t}$ 更新样本的权值分布。 最终按照误差率$\alpha_t$来组合所有的弱分类器。 Notes：
样本权重在决策树计算的时候。和误差乘起来，即增大错误分类的误差权重，减少正确分类的误差权重。通过这种方式做到决策树去拟合误差。 公式推导：
计算为什么可以使用指数函数 计算基分类器的权重 计算下一轮数据的分布。且分析为什么这样设置样本权重。 问题 为什么使用指数函数作为损失函数
adaboost优缺点
优点：可以得到较好的效果 缺点：1. 对异常样本敏感（异常样本会不停的增加权重，影响效果。）。2. 只能采用指数损失函数，做二分类学习任务。</content></entry><entry><title>自暴自弃</title><url>http://next.lisenhui.cn/post/essay/thought/%E8%87%AA%E6%9A%B4%E8%87%AA%E5%BC%83/</url><categories><category>随笔</category></categories><tags/><content type="html"> 自暴自弃 2022-2-16 独处随写
今天有一点感悟。加上看了个视频。遂来写下这段话。主题是自暴自弃。
我是个容易自暴自弃的人。每当任务过多觉得无法完成，或者压力太大觉得前途渺茫，总会自暴自弃起来。此时，什么也不想做，也什么都做不了。时间往往就这么浪费了。
短时间的努力看不到效果，这是很正常的事。正如我现在面临找实习，找工作的困境。还有很多东西要学，很多东西要做。自己掌握的太少，而时间太紧。找工作对我来说太难了。
每当这时，总会后悔以前的自己怎么不努力？后悔以前没有抓紧时间。后悔没有规划好自己的路。其实，人哪能预料到以后，而且当下总有当下的任务，等到处理完这个任务，下一个任务也来了。哪有时间去想以后的事情。
想要解决困境，只能一步步的做。可一步步做，又会时间不够。更难的是，不知道怎么做，像是考研。知识点固定，资料固定。按顺序刷下来就行。可现在的我更像是一个无头苍蝇，不知道往哪飞。拼命煽着翅膀，也只是在打转。
每当此时，我总想告诉自己，正如我经常告诉别人的那样。种一棵树最好是十年前，其次是现在。如果想要做什么，就从现在开始做。但又总是担心自己慢了慢了。别人都做得多好了，可自己还啥也没做好。仿佛慢了一步，整个世界都崩塌了。
可，看看博物杂志的那些奇人。他们是真的找到了自己热爱的事情。然后开始干起。他们没有物质的生活。没有甜美的大餐。但他们有自己热爱的事业。你呢？是，可能是慢了。但世界的节奏不只有一种。慢了，便放弃嘛？你那只是在和其他人比，而不是在为自己拼。
为自己热爱的事情学习。
发挥自己主动能动性的去学习。
长久的坚持的做下去。
世界有很多种生活方式。
也不知道自己在说什么。没想好之前以后不要成文了。
自暴自弃？那就自暴自弃把。
间接性自暴自弃。持续性学习。这样会成为大牛吧？</content></entry><entry><title>DeepReflect</title><url>http://next.lisenhui.cn/post/paperreading/deepreflect/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag></tags><content type="html"> 目录： 1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 特征 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 1. 综述翻译 深度学习继续在恶意软件分类方面显示出可喜的结果。然而，为了识别关键的恶意行为，恶意软件分析师仍然需要使用静态分析工具对未知的恶意软件二进制文件进行逆向工程，这可能需要数小时。尽管机器学习可用于帮助识别二进制文件的重要部分，但由于获取足够大的标记数据集的费用，有监督的方法是不切实际的。为了提高静态（或手动）逆向工程的生产力，我们提出了 DEEPREFLECT：一种用于在恶意二进制文件中本地化和识别恶意软件组件的工具。为了定位恶意软件组件，我们以一种新颖的方式使用无监督深度神经网络，并通过半监督聚类分析对组件进行分类，分析师在日常工作流程中逐步提供标签。该工具是实用的，因为它不需要数据标记来训练定位模型，并且需要最小/无创标记来逐步训练分类器。在我们对超过 26k 的五位恶意软件分析师的评估中在恶意软件样本中，我们发现 DEEPREFLECT 平均可将分析师逆向工程所需的功能数量减少 85%。我们的方法还检测到 80% 的恶意软件组件，而使用基于签名的工具 (CAPA) 时为 43%。此外，使用我们提出的自动编码器，DEEPREFLECT 比 SHAP（一种 AI 解释工具）表现更好。这很重要，因为 SHAP 是最先进的方法，需要一个标记的数据集，而自动编码器不需要。
1.1 发表于 USENIX CCF A 2021
2. Tag 3. 任务描述 4. 方法 DEEPREFLECT 不做这样的假设，而是通过控制流图 (CFG) 功能和 API 调用的组合来识别这些相同的行为。 DEEPREFLECT 的工作原理是了解良性二进制功能的正常外观。因此，任何异常都表明这些功能不会出现在良性二进制文件中，并且可用于促进恶意行为。
DEEPREFLECT 的目标是识别恶意软件二进制文件中的恶意功能。在实践中，它通过定位异常的基本块（感兴趣的区域 - RoI）来识别可能是恶意的功能。然后，分析师必须确定这些功能是否表现出恶意或良性行为。我们的流程中有两个主要步骤，如图 2 所示：（1）RoI 检测和（2）RoI 注释。 RoI 检测是使用自动编码器执行的，而注释是通过对每个函数的所有 RoI 进行聚类并标记这些聚类来执行的。术语。首先，我们定义“恶意行为”的含义。我们基于识别恶意软件源代码的核心组件（例如，拒绝服务功能、垃圾邮件功能、键盘记录功能、命令和控制 (C&amp;C) 功能、利用远程服务等）来生成我们的基本事实。这些很容易被 MITRE ATT&amp;CK 框架 [9] 描述，该框架旨在标准化这些术语和行为描述。然而，当对我们的评估恶意软件二进制文件（即野生恶意软件二进制文件）进行静态逆向工程时，我们有时无法确定将观察到的低级功能归因于这些高级描述。例如，恶意软件可能出于多种不同原因修改注册表项（其中许多可以由 MITRE 描述），但有时很难确定哪个注册表项被修改是出于什么原因，因此只能松散地标记为“防御规避：修改注册表”在 MITRE。甚至像 CAPA [3] 这样的现代工具也可以识别这些类型的模糊标签。因此，在我们的评估中，我们将“恶意行为”表示为可以由 MITRE 框架描述的功能。
投资回报率检测。检测的目标是自动识别恶意软件二进制文件中的恶意区域。例如，我们希望检测 C&amp;C 逻辑的位置，而不是检测该逻辑的特定组件（例如，网络 API 调用 connect()、send() 和 recv()）。 RoI 检测的优势在于，分析师可以快速定位到负责启动和操作其恶意行为的特定代码区域。之前的工作只专注于创建临时签名，这些签名只是将二进制文件识别为恶意软件或仅基于 API 调用的某些功能。这对于分析师扩展他们的工作特别有帮助（即，不单独依赖手动逆向工程和领域专业知识）。投资回报率注释。注释的目标是自动标记包含 RoI 的函数的行为。换句话说，我们管道的这一部分识别了这个恶意功能正在做什么。使这种标签对分析师的工作流程无干扰且可扩展是至关重要的。分析师为标记集群执行的初始工作是长尾分布。也就是说，前期工作量相对较大，但随着他们继续标记每个集群，工作量会减少。此过程的优点很简单：它为分析人员提供了一种自动生成有关未见过样本的报告和见解的方法。例如，如果恶意软件样本的变体包含与以前的恶意软件样本相似的逻辑（但对于分析师来说看起来不同，以至于不熟悉），我们的工具为他们提供了一种更快实现这一点的方法。
特征 当给定一个二进制样本时，我们提取特征以将样本总结为 x。在先前的恶意软件检测工作中使用了许多静态特征（例如，代码段熵、导入的 API 调用等）[29、35、53、61、63]。 然而，为了在二进制文件中定位恶意行为，我们的特征必须一对一地映射回原始样本。因此，我们将每个二进制表示为一个 m×c 矩阵，该矩阵使用 c 特征捕获前 m 个基本块，以总结它们的每个活动。基本块通常是一系列以控制转移指令结束的指令。当然，根据反汇编程序的不同，基本块的表示方式可能有所不同，因此这种严格的定义可能并不适用于所有静态恶意软件分析系统。我们的功能灵感来自先前作品中的功能， 即属性控制流图（ACFG）特征[23, 75]。在这些作品中选择 ACFG 特征来执行二进制相似性是因为它们**假设这些特征（由结构和数字 CFG 特征组成）**将在多个平台和编译器之间保持一致。虽然可以说我们的目标是相似的（即识别二进制文件之间的异同），但我们专门为研究恶意软件定制了这些功能。特别是，我们为自动编码器选择了我们的功能，以捕获更高级别的行为。我们的特征包括每个基本块中指令类型的计数（为 ACFG 特征提取的更详细的形式）、CFG 的结构特征和 API 调用的类别（已用于总结恶意软件程序行为 [18]） .在 DEEPREFLECT 中，我们将 m 设置为前 20k 个基本块。 我们选择这个是因为我们 95% 的数据集样本有 20k 或更少的基本块。我们将 c 设置为 18 个特征，将每个基本块总结如下：
结构特征。我们使用的结构特征是每个基本块的后代数量和中介分数。这些特征可以表示通常用于网络通信（例如，连接、发送、接收）和文件加密（例如，查找文件、打开、读取、加密、写入、关闭）等操作的控制流结构。图 6. 算术指令中可以找到来自实际恶意软件样本的此功能示例。我们使用的算术指令功能是每个基本块中包含的“基本数学”、“逻辑运算”和“位移”指令的数量。这些特征可用于表示如何为更高级别的行为执行数学运算。它们说明了数字是如何与函数交互的（例如，加密函数可能包括许多异或指令，混淆函数可能包括逻辑和位移操作的组合等）。我们从英特尔架构软件开发人员手册 [26] 中检索了这些说明。此外，我们提供了一个恶意软件样本示例，展示了图 9 中的这些类型的功能。 转移说明。我们使用的传输指令特征是每个基本块内的“堆栈操作”、“寄存器操作”和“端口操作”指令的数量。这些特征可用于表示如何为更高级别的行为执行传输操作。 它们说明了提供给函数的参数（以及函数调用的返回值）如何与该函数中的其余数据交互。它可能表示复杂的逻辑和数据操作（例如，去混淆/解密可能涉及更多与移动相关的指令，而 C&amp;C 逻辑将涉及更多与堆栈相关的指令，因为它调用更多的内部/外部函数）。我们同样从英特尔架构软件开发人员手册 [26] 中检索了这些说明。 API 调用类别。我们使用的API调用特性是“文件系统”、“注册表”、“网络”、“DLL”、“对象”、“进程”、“服务”、“同步”、“系统信息”和“时间”的数量" 每个基本块中的相关 API 调用。这些类别的灵感来自恶意软件聚类的先前工作 [18]。这些功能可用于表示执行恶意活动所需的高级库操作，例如网络通信和文件系统、注册表和进程操作。由于这些直接代表高级行为，因此它们对于理解函数的整体行为至关重要。在图 6 和图 8 中可以找到利用这些不同调用类型来执行不同行为的恶意软件功能示例。
我们认为这些功能比经典的 ACFG 功能更适合恶意软件，因为 (1) 它们包含在先前工作中用于恶意软件检测的 API 调用，(2) 指令类别更细粒度，允许在每个基本功能中包含更多上下文 块（如前所述），并且（3）它们不依赖于太容易受到规避攻击的字符串[77]。 当然，给定一个有动机的对手，任何机器学习模型都可能受到攻击并被诱骗产生不正确和意外的输出。 虽然我们的特征和模型也不例外，但我们认为它们足以产生一个可靠的模型（即，它的行为符合预期）并使其变得足够困难，以至于对手必须广泛工作才能产生误导性输入（ 如第 4.7 节所示）。 有关针对我们系统的潜在攻击的讨论，请参阅第 5 节。
5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释</content></entry><entry><title>Person Reidentification via Multi Feature Fusion With Adaptive Graph Learning</title><url>http://next.lisenhui.cn/post/paperreading/person-reidentification-via-multi-feature-fusion-with-adaptive-graph-learning/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag></tags><content type="html"> 目录： 1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 详细 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 1. 综述翻译 人员重新识别 (Re-ID) 的目标是从不重叠的监控摄像头网络中识别给定的行人。大多数现有工作都遵循监督学习范式，该范式需要为每对相机提供成对标记的训练数据。然而，这将它们的可扩展性限制在有大量未标记数据可用的实际应用程序中。为了解决这个问题，我们提出了一种用于无监督 ReID 的自适应图学习模型的多特征融合。我们的模型旨在借助特征描述符的特殊信息对行人的一致图结构进行综合评估。具体来说，我们将多特征字典学习和自适应多特征图学习结合到一个统一的学习模型中，使得学习的字典具有判别性，随后的图结构学习是准确的。开发了一种具有证明收敛性的交替优化算法来解决最终的优化目标。在四个基准数据集上的大量实验证明了所提方法的优越性和有效性。
1.1 发表于 IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS (CCF B)
2. Tag 3. 任务描述 Re-ID 的目标是从不重叠的监控摄像头网络中识别给定的行人。为了解决这些问题，已经探索了许多监督方法，它们主要遵循两条路线，要么学习判别特征，要么学习距离度量
利用实际应用中存在的大量未标记数据[18]-[28]已经引起了很多研究关注。这个方向的进展可以分为三类：基于判别特征的方法、基于度量学习的方法和基于字典学习的方法。基于判别特征的方法试图通过自定义几个特征描述符然后将它们连接在一起来消除对行人外观的各种偏见[18]-[22]。基于度量学习的方法旨在开发一种有利的距离度量学习方案来消除特定的干扰[23]-[25]。
此外，基于字典学习的方法旨在通过流形正则化器[26]-[28]来学习行人的潜在特征。对于这个分支，Kodirov 等人。 [26] 最早尝试引入用于身份匹配的图拉普拉斯正则化项。尽管这些无监督方法具有相对可扩展性，但与有监督方法相比，它们的性能并不能令人满意。主要原因如下：1）缺少标签标注使得模型难以学习行人的关键判别因素；2）现有的无监督方法没有充分有效地利用无标签数据。
在本文中，我们尝试介绍一种用于无监督 Re-ID 的通用多特征融合模型，该模型具有两个潜在优势。一方面，多特征可以从不同的视角捕捉行人的不同信息，例如遮挡、光照和姿势等。这种多视图信息将提供对行人的更自信的估计。另一方面，多特征可以通过各种方式轻松获取，例如手工描述或深度神经网络，无需标签监督。前者可以减少行人外观的不确定性并提高模型的性能，而后者可以使其具有可扩展性。受多视图学习[33]、[34]的启发，我们提出了一种基于图的多特征融合方法来缓解表示行人的歧义。基本思想来自两个观察：1）多特征可以反映行人的不同视图信息，因此更具表现力；2）它们在图像表示方面共享一组对象，并且可能具有内在关联。基于上述观察，我们假设不同的特征描述符应该共享底层的图结构，即使它们代表不同的信息。在此假设下，我们的方法可以借助特征描述符的特殊信息，实现多特征协商对行人图结构的综合评估。所提出的方法由两个关键部分组成。第一个组件基于字典学习技术，它在各自的潜在属性空间中学习判别字典和低维代码向量。详细地说，我们为每个描述符分配一个特定的字典来学习行人的特殊属性。第二个组件是多特征图学习术语，用于就图结构达成共识。特别是，我们采用自适应学习方式来表征图结构，这在经验上更有效。此外，我们引入了一个权重系统来考虑特征描述符的质量，也就是说，我们为每个特征描述符分配一个特定的权重，以区分它们对图学习的贡献。通过将上述两个组件合并到一个统一的学习模型中，当字典和图结构与所提出的优化过程交替优化直到收敛时，我们可以获得判别字典和准确的图结构。
4. 方法 监督重识别方法 例如，Paisitkriangkrai 等人。 [37] 通过将多个低级和高级视觉特征组合到一个框架中，提出了一种有效的基于结构化学习的人员 Re-ID 方法。 郭等人。 [35]通过整合稀疏和协作表示，利用了行人图像的全局性和局部性。 王等人。 [36] 融合了来自色调、饱和度、值 (HSV) 颜色空间、纹理和空间信息的通道特征来表示一个人。
无监督重识别方法 可以进一步划分为三个细粒度的类别：基于判别特征、基于度量学习和基于字典学习的方法。基于判别特征的方法试图设计专门的或连接的特征来解决复杂的 Re-ID 场景 [18]-[22]。例如，法伦泽纳等人。 [18] 提出了一种对称驱动的局部特征累积方法来提取人类外观的三个互补方面，即 HSV 直方图、最大稳定颜色区域和经常出现的高度结构化的补丁。
迁移学习的思想，它借助源数据集的统计信息来缓解目标标签丢失的问题[29]-[32]。一方面，这些方法依赖于源数据集和目标数据集的接近程度；数据集的质量会影响性能。另一方面，由于需要标记，它们不可扩展源样本。
多视图学习 多视图学习的一个吸引人的优点是能够分析隐藏在多个视图中的常见模式，即一致性 [38]，借助来自每个视图的特殊信息，即互补性 [39]。 与单视图学习相比，多视图学习已在许多机器学习任务中证明了其优越性，包括但不限于聚类[40]、检索[41]和识别[42]。 在本文中，我们从多视图学习的角度关注行人的潜在表示学习。 据作者所知，这是多特征融合在无监督 Re-ID 中的首次应用。
详细 在本节中，我们将首先重温基于无监督 Re-ID 的字典学习，这与我们的方法有些相关。 然后，我们介绍了所提出的具有自适应图学习方法的多特征融合，然后介绍了其优化算法和在 Re-ID 中的应用。
A. Dictionary Learning-Based Unsupervised Re-ID 设 X ∈ Rd×n 为输入特征矩阵，每列 xi 对应一个 d 维特征向量，表示第 i 个人物图像特征，n 为样本总数。 传统的字典学习技术可以由几个字典上的线性分解表示
提出了对每一个特征选择一个特征矩阵。然后乘上权重。由于过于多的变量导致其不是一个凸优化问题，所以拆分依次优化每个变量。从原理上确实比较相似。但实现上还是有很大的不同。它需要手动进行调控，我们是端到端的。
conclusion 在本文中，我们提出了一种用于无监督 Re-ID 的自适应图学习方法的多特征融合。通过借助特征描述符的特殊信息来表征受约束的图结构，该方法旨在就最终的图结构达成共识，从而可以很好地捕捉行人的判别因素。具体来说，我们通过将多特征字典学习和自适应多特征图学习结合到一个统一的学习模型中来实现这一点。随后，我们开发了一种具有证明收敛性的交替优化算法来解决所提出的优化目标。在四个数据集上的大量实验表明了所提出方法的优越性和有效性。将来，有几种方法可以扩展这项工作。 首先，这项工作可以扩展到深度网络模型，以更好地捕捉多特征的非线性和同质性。此外，将这项工作进一步扩展到基于视频的人 Re-ID [53]、[54] 可能会很有趣。
5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释</content></entry><entry><title>IntellijIDEA技巧</title><url>http://next.lisenhui.cn/post/study/java/intellijidea%E6%8A%80%E5%B7%A7/</url><categories><category>学习</category></categories><tags><tag>java学习</tag></tags><content type="html"> 快捷键：
一个函数调用完了后，输入.var可以快速将其赋值。 Scala可以输入.val快速赋值。 快捷键 ctrl H 搜索。</content></entry><entry><title>JavaParser学习</title><url>http://next.lisenhui.cn/post/study/java/javaparser%E5%AD%A6%E4%B9%A0/</url><categories><category>学习</category></categories><tags><tag>java学习</tag></tags><content type="html"> Pretty Printing是用来将AST打印成代码的。 NodeText和具体的语法模型 Void foo (int a ){} Type child: void Name child: foo param child: int a body child: {} TreeVisitor。使用树访问模式来访问一个节点和子节点。 通过getClass()获取节点的类。来判断是不是叶子节点。 直接打印Node node，打印出来的是这个节点和子节点的代码。 getChildNodes()获取节点子节点。 getData(DataKey)通过DataKey获取节点数据。 不清楚DataKey是什么。</content></entry><entry><title>Electron学习</title><url>http://next.lisenhui.cn/post/study/ui/electron%E5%AD%A6%E4%B9%A0/</url><categories><category>学习</category></categories><tags><tag>electron</tag></tags><content type="html"> electron打包程序 https://segmentfault.com/a/1190000013924153
electron简单入门</content></entry><entry><title>其他有用有趣的软件记录</title><url>http://next.lisenhui.cn/post/study/pcsoftware/%E5%85%B6%E4%BB%96%E6%9C%89%E7%94%A8%E6%9C%89%E8%B6%A3%E7%9A%84%E8%BD%AF%E4%BB%B6%E8%AE%B0%E5%BD%95/</url><categories><category>学习</category></categories><tags><tag>PC有用的软件</tag></tags><content type="html"> Balsamiq 画图软件。</content></entry><entry><title>Scala学习</title><url>http://next.lisenhui.cn/post/study/java/scala%E5%AD%A6%E4%B9%A0/</url><categories><category>学习</category></categories><tags><tag>java学习</tag></tags><content type="html"> scala学习网站 https://www.w3cschool.cn/scala/scala-index.html
专为spark学习：https://www.bilibili.com/video/BV1oJ411m7z3?p=11
基础知识
变量var常量val。常量不可修改，常量相当于final 引用。常量指向的对象仍然可以修改。 val可以不初始赋值。这种情况需要制定变量类型。 设置val的初衷就是为了保证它可控。 数组： 数组取值 data(6)：取第7个值。 Nil空集合。 array.mkString(",")将array中的值通过,组合起来。相当于python的string.join。 List.tail 数组的尾部。除开第一个值。List(1,2,3,4).tail=[2,3,4] List.init 不包含尾部元素 迭代器 iter 数组转迭代器: list.iterator API 最大值。max。 iter.max 模式匹配 data match{ case a:List[_]=>List case dat=>dat} case后面跟一个对象，可以指定对象的类型用来匹配。 函数定义 无参函数。当一个函数无需输入参数时，我们称这个函数为“0参函数“。 如果你在定义0参函数时加了括号，则在调用时可以加括号或者省略括号； 但当你在定义0参函数时没加括号，则在调用时不能加括号，只能使用函数名。 调用无参数函数时，可以省略括号。 implicit隐式类型。可以通过隐式转换获取一个对象。 符号语法糖：
https://www.zybuluo.com/boothsun/note/1014438 $符号和python中的fstring一样。放在字符串前引用变量。s"asd ${variable}&ldquo;就是取variable的值。 匿名函数： 初始: (num:Int)=>{num*2} 进阶：自简原则（能简单则简单） 逻辑代码只有一行，花括号省略: (num:Int)=>num*2 类型能够推断，类型省略：(num)=>num*2 如果参数只有一个，小括号可以省略：num=>num*2 参数只出现一次，且和参数顺序保持一致，使用下划线代替: _*2 如果有多行，多行语句用分号隔开。 tuple _._n表示取第n个值。n下标从1开始。 语法：
1 until 10: [1,10) for(i&lt;- 1 until 10) 自定义函数 def funName(para1:Type1,para2:Type2):Type = { do some things } Scala函数可以没有return 语句，默认返回最后一个值。 如果函数的参数在函数体内只出现一次，则可以使用下划线代替 def mul=(:Int)*(:Int) 最后一个参数后加上*，则允许参数重复如def prints(args:String*) arg&lt;- args 是说对args中的每个 item遍历。 可以将一个函数赋值给一个变量， val 变量名 = 函数名+空格+_ 这里函数名后面必须要有空格，表明是函数的原型 匿名函数格式： val 变量名 = （参数：类型） => 函数体 高阶函数： 高阶函数其实就是普通函数中的参数进一步推广了，高阶函数的参数可以是一个函数，参数名就是函数名， def valueFor(f:(Double)=>Double,value:Double)=f(value) f:(Double)=>Double 就是输入的类型，输出的类型。 柯里化：柯里化函数是将原来接手两个参数的函数转变成新的接收一个参数的函数过程。def mul(x:Int)=(y:Int)=>x*y。 mul(2)(3) 导入包： import scala.math._ _ 是通配符。等于*</content></entry><entry><title>Java重拾</title><url>http://next.lisenhui.cn/post/study/java/java%E9%87%8D%E6%8B%BE/</url><categories><category>学习</category></categories><tags><tag>java学习</tag></tags><content type="html"> 基本数据类型： 小数默认double。float数值需要额外添加字母f 长整型245L
理解：
申明，基本类型申明的是变量，是深度拷贝，类申明的是引用。Hero b; 申明的都是引用。 实例化后，才是对象。123, new Hero()都是实际的占用内存的值。 类：
继承 public class a extends b 继承会拥有b的属性。 构造函数 public A(){} 类引用，引用的是对象，所以修改对象会修改其他指向这个引用的值。 权限：成员变量有四种修饰符 private 私有的 仅仅自身访问 package/friendly/default 不写。 同包继承访问 protected 受保护的 子类继承访问 public 公共的 所有人继承访问。 静态类、静态方法。推荐类名访问。所有对象共有。 静态初始化块： static{初始化各个值。} 属性声明-》初始化块-》构造方法。按顺序赋值。 单例模式。 自己申明一个私有静态属性指向自己的对象。 申明一个静态方法，返回这个对象。 也可以静态方法时才调用对象。称作懒汉。 懒汉会有线程安全的问题。初始化时间充分，饿汉。否则懒汉。饿汉当有类存在时即加载。 系统API：
Array: Arrays.copyOfRange(array, start, end) Arrays.toString() Arrays.sort(a) Arrays.binarySearch 二分法。使用前先排序。 Arrays.equals(a,b) 比较两个数组是否相同。 Arrays.fill(a, int) 填充数组。 其他API：
enum 类型。public enum season{day1, day2}。 season.values()所有的枚举。 tips:
final类型仅仅可以赋值一次。final可以修饰函数，类。 长路与&amp;，短路与&amp;&amp;。按位与&amp; 有符号右移&raquo; 不带符号右移&raquo;> 一维数组int a[] = new int[] 二维数组int a[][] = new int[2][3] 或者int a[][] = new int[2]; int a[0] = new int[3] 同个package下的包直接调用，不同package下的包需要import</content></entry><entry><title>Spark学习计划</title><url>http://next.lisenhui.cn/post/study/spark/spark%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/</url><categories><category>学习</category></categories><tags><tag>spark</tag></tags><content type="html"> spark学习计划：https://zhuanlan.zhihu.com/p/384903354 Scala学习《Scala实用指南》 然后书籍路线，我目前自己的学习路线是：《Spark权威指南》->《Spark内核设计的艺术 架构设计与实现》->《SparkSQL内核剖析》，也就是先从第一本API表层原理的入门书开始
学习spark2。然后到时候转spark3。先做到会用。 https://blog.csdn.net/chengyuqiang/category_9270040.html
spark中文教程 https://spark-reference-doc-cn.readthedocs.io/zh_CN/latest/deploy-guide/cluster-overview.html
spark生产环境开发环境远程开发环境搭建 https://zhuanlan.zhihu.com/p/55450219
https://cloud.tencent.com/developer/article/1482700 https://blog.csdn.net/lijingjingchn/article/details/83143093 https://developer.aliyun.com/article/108549
教程 可能还不错的： https://sparkbyexamples.com/
速看视频随记 集群模式，主从模式，一个控制端，几个从属端。可以在一个机子上部署。主从之间通过端口7077进行通信。 高可用模式，建立备用的master。当主要的master断开之后，后续的master会顶替上来。 yarn模式。 主要运行模式：编写脚本然后提交运行。 RDD RDD体现了装饰者的设计模式，所有的操作在前面的RDD上进行封装。 装饰者模式，允许向现有对象上添加新的功能，同时不改变其结构。将现有类进行包装。 RDD是一个抽象类，代表弹性、不可变、可分区、里面元素可并行计算的集和 弹性： 存储的弹性：内存、与磁盘的自动切换：内存有限制。所以内存不能装入所有的数据。RDD支持数据在内存和磁盘的自动切换。对上层透明。 容错的弹性：数据丢失可以自动恢复：数据丢失了，可以重新读取数据。 计算的弹性：计算出错重试机制。计算出错后，可以重新计算。容错机制。 分区的弹性：可根据需要重新分片。根据计算资源的需要重新分区。 RDD特性： 分布式：数据存储在大数据集群不同节点上。对上层透明。 数据抽象（数据集）：只封装了计算逻辑，并不保存数据。需要子类实现计算逻辑。 不可变。RDD不可以改变，想要改变只能生成新的RDD。在新RDD里封装计算逻辑。 可分区，可并行计算： 惰性计算。只有在真正需要的时候，才会执行业务逻辑。 五大属性 分区列表。多个分区用来执行并行计算。getPartitions函数 分区逻辑函数：每个分区的计算逻辑。每个分区的计算逻辑相同。computing函数 RDD依赖关系：一个依赖于其他RDD依赖的列表。getDependencies获取依赖。 分区器partitioner。可选的。数据如何分区 首选位置prefer location。可选的。判断计算发送到哪个节点效率最优（计算和数据在同一个集群效率更优） RDD的作用 进行逻辑的封装，并生成Task，发送给Executor节点。 RDD分区和并行度 RDD根据分区来生成Task。 在行动算子触发Task后，是将一个Task所有的逻辑执行完毕后，再执行下一个Task的。 同一分区内执行顺序是有序的。不同分区执行是无序的（并行的） 根据Task和Executor的数量来影响并行度。 Spark执行原理 Spark执行 申请资源。 将数据处理逻辑，分成一个一个的计算任务。 将任务分配到资源的计算节点上。 以Yarn为例的工作原理 启动Yarn集群环境。资源管理器ResourceManager和若干个节点管理器NodeManager。 Spark申请资源创建调度节点Driver和计算节点Executor。 Driver将计算逻辑根据分区划分成不同的任务Task，并将Task组成TaskPool。 Driver主要来做调度。 Driver根据计算节点的状态将TaskPool中的Task发送到对应的计算节点进行计算。 Spark 代码 准备环境 基础登录设置 val sparkConf = new SparkConf().setMaster(&ldquo;local&rdquo;).setAppName("") # 设置本地环境。 &ldquo;local"单线程单核。local[*]多线程模拟多集群。 val sc = new SparkContext(sparkConf) 关闭环境：sc.stop() RDD相关 RDD的内部方法将计算逻辑发送到Executor端进行执行。所以将RDD方法称为算子。 RDD创建 从内存创建 sc.parallelize(data=数据集合, numSlices=Option[n]) parallelize意思是并行。 numSlices表示分区的数量。可以缺省。缺省的话即使用默认的并行度defaultParallelism。 defaultParallelism默认值来自scheduler.conf.getInt(&ldquo;spark.default.parallelism&rdquo;, totalCores) 默认情况下，从配置对象conf中获取配置参数spark.default.parallelism 如果取不到，会因为设置为local[*]为当前环境的最大可用核数。 当numSlices不可以整分的时候。通过读源码可以看到。不是均匀划分。是依次划分。 sc.makeRDD(数据集合) 调用同parallelize函数。 这个函数实现时调用了parallelize函数。 从文件中创建。 sc.textFile(path="",minPartitions)。以行为单位读取数据 path可以是文件名、可以是目录名 path以当前环境的根路径为基准（项目路径）。可以写绝对路径。也可以写相对路径。 path中可以使用通配符 path=&ldquo;dataste/1*.txt&rdquo; minPartitions为最小的分区数量=math.min(defaultParallelism, 2)。在默认最小分区下，可以往上增加分区，同下。 如果不使用默认的分区数量，可以指定。spark读取文件，底层使用的hadoop的方式，统计字节数（注意换行算一个字节），字节数除以分区数，得到一个余数，如果余数大于百分之十，则加一个分区，否则就不加分区。 例子：7个字节，分两个分区。7（字节）/2（分区）=3（字节/分区）。7/3（字节/分区）=2&hellip;1分区。余出来的分区书来给你超过了百分之10，加一个分区。 总结。以文件字节数为单位统计分区。以行为单位读取文件并存入分区。 sc.wholeTextFiles 以文件为单位读取文件。会显示数据来源。 RDD存储 saveAsTextFile(path) 存成文本文件。 RDD逻辑相关方法：RDD算子（解决问题就是将问题的状态进行改变。算子就是改变问题状态的操作） 分为两大类（转换和行动） 转换算子：将一个旧的RDD包装为新的RDD。可根据参数类型分为：value类型，double Value类型，key-value类型。默认窄依赖，宽依赖标注。 value类型 map(转换函数=>Int) 使用转换函数将RDD中所有的元素进行转换。 mapPartitions(转换函数=>Iter) 以分区为单位进行map。获取Iter。注意**需要返回Iter。**但不要求迭代器的数量保持一致。 注意：容易内存溢出 会将分区的数据加载再内存中进行引用。Task没执行完时，处理完的数据不会被释放掉，会在内存中，因为存在对象的引用。所以当数据量很大的时候容易溢出。会长时间占用内存。 当数据量大时，建议使用map mapValues(f) 只对每个值的values做map。 mapPartitionsWithIndex((index, Iter)=> Iter) 获取分区和对应下标。 flatMap(f:T=>TraversableOnce[U]): RDD[U] 扁平化操作。 将数据map后转为一个集合。Map函数中传入为元素，传出为数组，一个元素对应一个数组。将处理的数据进行扁平化后再进行映射处理。所以算子也称作为扁平映射 flatMap就是先按照传入的函数对每个元素进行map，然后再将map后的元素一起扁平化。 flatMap 的传入是一个函数对象。参数是集和的元素。返回值是一个可迭代的集和。 样例：[[1,2],[3,4]]=>[1,2,3,4]。flatmap接收的参数是数组，flat将多个数组所有元素聚合。 只有flatmap，没有flat 是一个窄依赖。就是当前分区内的数据进行操作。 glom() 无参数=> RDD[Array[T]] 逆扁平化操作。 将一个分区所有数据转为一个集合。将同一个分区的数据转换为相同类型的内存数组进行处理。分区不变。可以理解为flat的反面。将一个分区组成一个数组返回。 样例：[1,2,3,4] 两个分区 => [[1,2],[3,4]] 将同一个分区的内容聚集在一起。 groupByK => RDD[(K, Iterable[T])] 宽依赖 根据f返回的key进行分组。这个操作会对数据shuffle。 将数组根据制定规则进行分组。分区默认不变，但是数据会被打乱重新组合。这样的操作称作shuffle。极限情况下，数据可能被分为同一个分区中。一个组的数据在一个分区中，并不是一个分区只有一个组。 分组和分区没有必然逻辑 filter(f:T=>Boolean):RDD[T] 将数据根据f进行筛选，符合规则的留下。区内filter 筛选过后，分区不变。可能会出现数据倾斜。 sample(withReplacement:Boolean, fraction:Double, seed:Long=Utils.random.nextLong)=>RDD[T] 抽样选取部分样本 withReplacement是否放回。fraction选取每一个数的概率为多少，不放回时，fraction最高1。有放回fraction大于1时，则是每个值可能被抽到的次数。seed种子。抽取数据特殊场合有用。 不放回：伯努利算法 有放回：泊松算法 distinct(Option[numPartitions:Int]):=>RDD[T] 根据传入的参数去重。numPartitions分区数量 底层实现时，需要考虑数据所在的分区，根据分区去重。 coalesce(numPartitions:Int, shuffle:Boolean=false, partitionCoalescer:Option[PartitionCoalescer]=Option.empty)=>RDD[T] 宽依赖 缩减分区，用于大数据过滤后，提高小数据集的执行效率。使用这个函数减少分区个数。numPartitions 分区数量。 默认不会将分区的数据打乱重新组合。一个分区的数据会整个加入另一个分区。这种情况下的缩减分区会导致数据不均衡。 如果想要数据均衡。可以设置shuffle为True。但是会打乱分区。 repartition(numPartitions:Int) 宽依赖 扩大分区。底层调用coalesce，默认shuffle为true。 sortBy[K](f:(T)=>K, ascending=True) 宽依赖 根据key进行排序。但默认不改变分区，但存在shuffle操作。ascending为升序。 doubule-value类型。双值函数。两个RDD。以下都是shuffle 交集 intersection 用法 rdd1.intersection(rdd2)。 要求两个数据源数据类型保持一致。 差集 subtract 并集 union 并集会将两个rdd合成一起。不会去重交集后合并。 示例:[[1,2]].union([[3,4]])=>[[1,2],[3,4]] 拉链 zip 将对应位置对应值打包成tuple。 数据源要求分区数量，分组中元素的数量保持一致。 key-value类型。要求RDD是键值类型的PairRDD：RDD[K, V]。RDD[(K, V)]会隐式转换为PairRDD[K, V]。以下的操作大部分按照Key进行 partitionBy(partitioner:Partitioner):RDD[(K, V)] 将数据按照传入的Partitioner重新进行分区。 Spark有一个默认分区器为HashPartitioner(numPartition=n)获取分区数量和数据对应分区。 reduceByKey(f:(V,V)=>V, Option[numPartitions]):RDD[(K, V)] 宽依赖 将相同的数据按照相同的key对value进行聚合。然后根据f对value依次操作。 当一个key只有一个元素时，不会参与训练。 groupByKey(Option[numPartitions], Option[[partitioner])=>RDD[(K, Iterable[V])] 宽依赖 根据key 组合数据的value为List。注意没有f。 和groupBy的区别就是groupByKey会将value组合在一起。和前一个不会。 和reduceBy的对比见下面。落盘前预聚合。 aggregateByKey(zeroValu e:U)(seqOp:(U, V)) => U, combOp(U, U)=> U):RDD[(K, U)] 将数据根据不同的规则进行分区内计算和分区间计算。传参是两个函数。一共两个要传两个参数列表 第一个参数(括号)为初始值。制定分区内计算时，碰到的第一个值和初始值如何操作。就是分区内reduce的初始值。 第二个参数(括号)为函数列表。第一个函数区间内计算，对象是分区内每个元素，第二个函数分区间计算，对象是每个分区提取到的每个元素。 注意每个函数的参数类型，初始值参数类型为U。最终返回类型和初始值保持一致。 foldByKey(zeroValue:U)(combOp(U, U)=> U):RDD[(K, U)] aggregateByKey的简化版本。当分区内和分区间操作相同的时候，只用一个函数就可以了。 combineByKey[C]( createCombiner: V => C, mergeValue: (C, V) => C,mergeCombiners: (C, C) => C): RDD[(K, C)] 三个参数 第一个参数：将相同的第一个数据进行结构转换。实现操作。 第二个参数：分区内的计算规则 第三个参数：分区间的计算规则 join[W](other: RDD[(K, W)]):RDD[(K, (V, W))] 在类型(K, V)和(K, W)的RDD上调用，返回一个相同key对应的所有元素连接在一起的(K, (V, W))的RDD。类似于内连接。 如果两个rdd中的key没有匹配上，则数据不会出现在结果中。 如果一个源中存在多个相同key，则另一个源会和这个源的每个相同key连接。类似笛卡尔积。 使用 rdd1.join(rdd2)。数据经过join会几何增长。会影响shuffle的性能。不推荐使用。 leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))] 左外连接。在右表中不存在的key会用None显示。 rightOuterJoin 右外连接。在左表中不存在的key会用None显示。 cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))] 和外连接类似。但会先在RDD内聚合后再进行外连接。先group再外连接。等于connect+group. 样例：[(&ldquo;a&rdquo;,1),(&ldquo;b&rdquo;,1)][(&ldquo;a&rdquo;,1),(&ldquo;a&rdquo;, 2)] = >[(&ldquo;a&rdquo;, (1), (1,2)), (&ldquo;b&rdquo;, (1),())]。就是RDD内先按照key聚合，聚合的值放在一起。然后rdd间按照key进行外连接。没有的值为空。 行动算子：出发任务的调度和作业的执行。 解释。行动算子会触发作业的执行。底层调用的是sc.runJob方法。创建ActiveJob 并执行 reduce(f: (T, T) => T): T 聚合元素并输出。 collect(): Array[T] collect收集所有的数据并打印。 count(): Long 返回数据源中的个数。 first()T: 返回第一个 take(num: Int): Array[T]： 获取num个数据 takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] 将RDD排序后返回前n个元素。默认升序。 降序takeOrdered(n)(降序) aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U 初始值、分区内、分区间的计算规则。 会直接返回结果。不需要考虑键值类型，普通类型就能用。 和aggergateByKey的不同 aggergateByKey仅参与分区内的运算 aggregate会参与分区内和分区间的运算。 fold(zeroValue: T)(op: (T, T) => T): T 分区内、分区间的规则相同时，使用fold。其他没有不同 countByKey(): Map[K, Long] 统计key-value RDD中key出现的次数 countByValue(): 获取每个value和其对应的出现次数 foreach executor端对分区里的数据执行foreach。不是按照executor顺序打印。 collect后foreach是先收集数据并返回到driver，按照分区顺序采集executor并打印。 由于RDD需要发送到executor端。所以foreach中用到的对象都需要序列化。样例类可以自动序列化。 RDD算子传递的函数是会包含闭包操作，就会进行检测功能。会检测是否能序列化。 这个不太懂，算了。 分区器： 分区器有两个需要实现的。numPartitions, getPartition numPartitions为分区数量。 getPartition根据Key获取该Key所在的分区。 分区器有HashPartitioner, RangePartitioner, PythonPartitioner. RDD序列化 算子以外的代码都是在Driver 端执行, 算子里面的代码都是在 Executor 端执行。那么在 scala 的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就 形成了闭包的效果，如果使用的算子外的数据无法序列化，就意味着无法传值给 Executor 端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。即检测RDD操作内部（RDD函数执行的操作）是否使用的都是能序列化的值。 rdd.filter(this.query) 这个时候因为使用了this,所以会检测this所代表的对象能否序列化 scala中类的构造参数是类的属性。构造参数需要进行闭包检测.类也需要进行闭包检测 可以使用kryo序列化操作来序列化，减少字节传输量。 RDD依赖关系 直接依赖关系称为依赖。间接依赖关系为血缘关系 每个RDD保存血缘关系。当某一个RDD出现运算错误的时候，该RDD可以通过血缘关系从数据源重新获取数据。 rdd.dependencies 打印依赖关系 新RDD依赖于旧RDD。当不存在shuffle时，是窄依赖（一对一）依赖，指分区一对一的依赖。（OneToOneDependency) 当存在shuffle的时候，是宽依赖。叫shuffle依赖。(ShuffleDependency) rdd.toDebugString ：打印血缘关系。没有括号注意。 当存在shuffle操作的时候，血缘关系会产生缩进的效果 (2) ShuffledRDD[2] at reduceByKey at :25 [] +-(2) MapPartitionsRDD[1] at map at :25 [] | ParallelCollectionRDD[0] at makeRDD at :25 [] 其中(2)表示的是分区数量。+-表示shuffle操作。 一个分区的多个连续窄依赖会集成成一个Task后再执行。宽依赖会等多个分区的窄依赖Task运行结束后执行。上述两个操作是分阶段进行的。先进行窄依赖Task阶段。再进行宽依赖Task阶段。 阶段：根据DAG（Directed Acyclic Graph)有向无环图组成拓扑图，构建转换过程和任务阶段。shuffle操作产生一个新阶段。 RDD的阶段划分： 每存在一个shuffleRDD时，阶段会自动增加一个。阶段数量=shuffle依赖的数量+1.ResultStage只会出现一次，在最后一次执行。 RDD的任务划分： RDD任务切分中间分为：Application、Job、Stage 和 Task ⚫ Application：初始化一个 SparkContext 即生成一个Application； ⚫ Job：一个Action 算子行动算子就会生成一个 Job； ⚫ Stage：Stage 等于宽依赖(ShuffleDependency)的个数加 1； ⚫ Task：一个Stage 阶段中，最后一个RDD的分区个数就是 Task 的个数。 注意：Application->Job->Stage->Task 每一层都是 1 对 n 的关系。 阶段名称和任务名称相同，如ShuffleMaskStage对应任务ShuffleMaskTask RDD持久化 如果多个行动算子存在相同的前部分操作，则RDD对象可以重用。但由于RDD中不存储数据，所以数据无法重用，每个行动算子会从头获取数据并计算。 如果为了性能需要重用中间数据，我们应该持久化存储中间存储。可以存在内存中（速度快不安全）或者磁盘中（慢但安全）。 rdd.persist()则这个rdd会持久化存储。后面重用这个rdd对象的时候，会从cache中读取数据，而不是重复计算。默认保存到内存中，如果需要保存到磁盘中，需要传入参数StorageLevel.*** ***可以为：DISK_ONLY, MEMORY_ONLY, MEMORY_AND_DISK等等。 persist会在血缘关系中添加新的依赖。如果出现问题，可以重头读取数据。cache一样。 代码rdd.cache() 调用了persist()。将数据保存在内存中。 注意cache持久化操作，会在行动算子执行时才触发。 cache会保存在临时文件夹下。作业结束后，会被删除。 rdd.checkpoint() 检查点操作。会将算子落盘。一般保存hdfs中。且checkpoint操作会独立执行作业。 sc.setCheckpointDir(&ldquo;cp&rdquo;)设置检查点目录。 checkpoint执行过程中，由于从磁盘中读取数据，会切断血缘关系，重新建立从磁盘中读取数据的血缘关系。等同于改变数据源。 持久化区别 cache: 将数据临时存储在内存中进行数据重用。 persist：将数据临时存在磁盘中进行重用。涉及IO性能较低。作业执行完毕。临时存储的文件就会丢失。 checkpoint：将数据长久的保存在磁盘中。性能较低。为了保证数据安全。一般情况下，会独立执行一个作业（额外的作业），产生数据文件来长期存储。一般情况和cache联合使用。 解释：checkpoint()调用的时候，会触发作业执行操作。相当于一个行动算子。如果在这之前调用了cache()，则会从cache中取出数据，而避免重复运算。 持久化操作的应用： rdd被多个行动算子重复使用。 单个rdd走的路程太长，为了避免出错，或者数据比较重要的场合。可以持久化中间步骤。 RDD分区器。 分区相关API： partitionBy，根据传入的自定义分区器进行分区。coalesce重新设置数据分区。repartition同coalesce。 自定义分区器。 继承类： extends Partitioner{} 重写两个方法： override def numPartitions: Int。 分区数 override def getPartition(key:Any): Int=??? 根据数据的key返回数据所在的分区索引（从0开始）。 Spark 目前支持Hash 分区和Range 分区，和用户自定义分区。Hash 分区为当前的默认 分区。分区器直接决定了RDD中分区的个数、RDD中每条数据经过 Shuffle 后进入哪个分区，进而决定了Reduce 的个数。 只有Key-Value 类型的RDD才有分区器，非Key-Value 类型的RDD分区的值是None 每个RDD的分区 ID 范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的 Hash 分区：对于给定的 key，计算其 hashCode,并除以分区个数取余 Range 分区：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而 且分区间有序 RDD文件读取和保存： Spark 的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。 文件格式分为：text 文件、csv 文件、sequence 文件以及Object 文件；文件系统分为：本地文件系统、HDFS、HBASE 以及数据库。 存储： RDD.saveAsTextFile(path) RDD.saveAsObjectFile(path) RDD.saveAsSequenceFile(path) 要求必须是key-value RDD 读取 sc.textFile(&ldquo;output&rdquo;) sc.sequenceFileInt,Int sc.objectFile[Int](&ldquo;output&rdquo; 三大数据结构： RDD：弹性分布式数据集 累加器：分布式共享只写变量 广播变量：分布式共享只读变量 累加器 为什么需要累加器？ spark分布式框架，相当于一个多线程的task。而且是发送到执行端executor执行。而且executor执行的task没有返回操作。无法修改同时修改本地的数据。累加器的作用就是在task执行完毕后，从task返回到driver端。 累加器用来把 Executor 端变量信息聚合到Driver 端。在Driver 程序中定义的变量，在 Executor 端的每个 Task都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后，传回Driver 端进行merge。 使用 sc.longAccumulator(name=&ldquo;sum&rdquo;) 整型累加器。 ac.add ac.value sc.doubleAccumulator sc.collectionAccumulator List类型的累加器。 自定义累加器： 继承： extends AccumulatorV2[In, Out] 定义泛型In :累加器输入的数据类型 Out: 累加器返回的数据类型。 如: extends AccumulatorV2[String, mutable.Map[String, Long]] 重写方法： isZero: Boolean 判断是否是初始状态。 add(In): 获取累加器需要计算的值 copy(): 复制一个新的累加器 reset(): 重置累加器 merge(): 合并多个累加器 value(): Out 返回累加器的结果 使用步骤： 创建累加器对象 向spark注册。sc.register(累加器，name=&ldquo;xx&rdquo;) 累加器的问题： 少加。如果转换算子中调用累加器。如果没有行动算子，那么不会执行。 多加。如果多次调用行动算子，则会多次执行，导致多次累加。 所以一般情况下，累加器会在行动算子中进行操作。即写在行动算子中。 多个Task的累加器是不能相互访问的。 广播变量 **广播变量用来高效分发较大的对象。**向所有工作节点发送一个较大的只读值，以供一个 或多个 Spark 操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表， 广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark 会为每个任务分别发送。 闭包数据，都是以task为单位发送的，每个任务中包含闭包数据，这样可能导致一个executor中含有大量重复的数据，并且占用大量的内存。Executor就是一个JVM，启动时会分配内存。完全可以将闭包数据放在executor中，达到共享的目的。这些放在内存的数据称为广播变量。广播变量不能被更改。分布式的只读变量。 使用： 初始化bc = sc.boardcast(variable) 方位bc.value 就是variable spark性能优化相关名词解释 shuffle 不同分区内的数据需要聚合。这种操作即宽依赖操作。即是shuffle操作。 针对同一个数据的多次shuffle，spark会在底层优化缓存。 落盘 当涉及shuffle操作的时候，即宽依赖的时候，不能在内存中存储数据，会容易溢出。必须落盘处理。即将符合条件的数据存储到一个文件中。再从文件中读取。所以shuffle的性能非常低。 预聚合功能 在落盘之前，在分区内预聚合。有效的而减少shuffle时落盘的数据量。 架构 Java的架构： MVC: Model View Controller 架构。 Model，模型：业务模型和数据模型（封装）。 View视图是数据展示 Controller是控制层。控制数据的流转
大数据中一般不需要View。所以不用MVC模式。改用三层架构： Controller(控制层), service(服务层), dao(持久层)。 Application=》controller进行调度=》service(启动服务)=>dao(持久层)访问数据。 common 一些共同的代码放在这。 util 一些工具类放在这。哪里都能用的工具。 bean 实体类 application 应用类 controller 控制类 service 服务类 dao 持久层，操作数据。
问题：
reduceByKey和groupByKey的区别 性能上： reduceByKey会在区内进行聚合，然后再落盘。所以数据量会小很多。比groupByKey快很多，性能更高。就是所谓的预聚合功能 功能上： groupByKey只分组，不聚合。reduceByKey分组且聚合。 reduceByKey, aggregateByKey, foldByKey, combineByKey底层调用都是combineByKeyWithTags(createCombiner, mergeValue, mergeCombines).设定初始值，然后分区内聚合，然后分区间聚合。 题目：（做过的题目都得记录，不然很容易忘）
创建数据
sc.makeRDD(List(1,2,3,4), numSlices=2) 分区内最大值，分区间最大值求和。
使用mapPartitions求出分区最大值，然后collect。 rdd.mapPartitions(list=>Iterator(list.max)).collect().sum 这里的list是val的名称。Iterator是获得一个Iterator对象。 使用glom求出分区，然后map取最大值，然后collect求和。 rdd.glom().map(_.max).collect() 当使用key、value组成对时即List((&ldquo;a&rdquo;, 1), (&ldquo;a&rdquo;, 2), (&ldquo;a&rdquo;, 3), (&ldquo;a&rdquo;, 4))这样子。可用aggregateByKey rdd.aggregateByKey(0).(math.max(,), +).collect() wordCount
数据 sc.makeRDD(List(&ldquo;Hello spark&rdquo;, &ldquo;Hello scala&rdquo;), numSlices=2).flatMap(_.split(&rdquo; &ldquo;)) 方法： 使用groupBy 转换为iter后统计数量 rdd.groupBy(word=>word).mapValues(iter=>iter.size) 使用map转为key, value RDD，使用reduceByKey, aggregateByKey, foldByKey中的一个 rdd.map((, 1)).foldByKey(0)(+_).collect() 双值countByKey countValue单值。 累加器实现。 相同Key的平均值
数据List((&ldquo;a&rdquo;, 1), (&ldquo;b&rdquo;, 3),(&ldquo;b&rdquo;, 4), (&ldquo;a&rdquo;, 3), (&ldquo;a&rdquo;, 3),(&ldquo;b&rdquo;, 4)) 统计Key的总和和出现的次数 rdd.aggregrateByKey((0,0))((t, v)=>(t._1+v, t._2+1), (t1, t2)=>(t1._1+t2._1, t1._2+t2._2)).map(a=>a._1, a._2._1/a._2._2) rdd.combineByKey(v=>(v,1), (t:(Int, Int), n:Int)=>(t._1+n, t._2+1), (t1:(Int, Int), t2:(Int, Int))=>(t1._1+t2._1, t1._2+t2._2)) 这个操作是对相同key的value做聚合。所以key是不会传进来的。 这里需要注意Scala的语法。由于tuple是中途产生的，所以scala并不理解t的类型是什么。scala是静态语言，这样没法编译。所以我们需要标注类型，才能让它正常的编译。 统计每一个省份每个广告被点击数量排行的Top3
数据： xxxx 河北 北京 张三 广告A 思路，首先仅提取需要的特征 河北、广告A。思路按照省分类统计广告的点击次数，然后再按照省份进行归纳。你的思路是首先按照省分类，再统计广告次数。为什么这么做不好？不好写 首先用map创造数据，使用split分割。分割后的数据 List((省份,广告),1) 然后按照 省份-广告 作为key 进行聚合 再根据 省份 作为key 进行聚合。 rdd.map(data=>{val datas=data.split(&rdquo; &ldquo;);((datas(1), datas(4)),1)})
rdd.reduceByKey(+)
rdd.map{case ((prv, ad), sum) =>{(prv, (ad, sum))}}
rdd.groupByKey().mapValues(iter=>iter.toList.sortBy(_._2)(Ordering.Int.reverse).take(3))
top10热门种类。
需求：按照点击数排名。靠前的就排名高。再比较点击数、下单数、支付数。 实现一： 首先filter出点击数量等分类存在的值=》进行map为(种类，1)=>reduceByKey。统计每个种类的个数=>三个种类进行连接。(种类ID，(点击数量)，(下单数量),(支付数量)) 这里有多种连接方式：join,zip, leftOuterJoin, cogroup. join不行，因为可能存在三个种类一个不存在。leftOuterJoin不行，同样必须外连接。zip不行，zip连接不会按照key连接，而是按照分区内数据的数量和位置连接。cogroup是外连接。所以可行: a.cogroup(b,c) 连接起来后，将同id的几个种类值相加 a.mapValues() 进行排序a.sortBy(_._2, ascending=false).take(10)。按照第二个就是元组进行排序。先排元组第一个值，再排第二个值。 优化一： 多个种类调用读取数据textFile。所以cache这个RDD。 cogroup是shuffle操作，性能较低。 map数据 => (品类ID，(点击数量,0,0))。将所有数据转换为相同格式后相加。 然后a.union(b).union(c) 再排序等等。 优化二： 存在大量的reduceByKey操作。 将数据转换结构：（品类ID，（1，0，0）），（品类ID，（0，1，0）） 完整代码： val flatRDD=actionRDD.flatMap( action=>{ val datas=action.split("_"); if(datas(6)!="-1") {List((datas(6),(1,0,0)))} else if (datas(8)!="null") {datas(8).split(",").map(id=>(id, (0,1,0)))} else if(datas(10)!="null"){datas(10).split(",").map(id=>(id,(0,0,1)))} else{Nil}}) flatRDD.reduceByKey((t1,t2)=>(t1._1+t2._1,t1._2+t2._2,t1._3+t2._3)).sortBy(_._2,false).take(10) 优化三： 不用shuffle操作。使用累加器。 case class HotCategory(cid:String, clickCnt:Int, OrderCnt:Int, payCnt:Int) class HotCategoryAccumulator extends AccumulatorV2[(String,String),mutable.Map[String,HotCategory]]{ // IN:(品类ID，行为类型) // out: mutable.Map[String, HotCategory] } top10品类中，sessionID top10统计。 需求分析：top10热门品类中，每个品类点击Top10的session。可以先统计出top10的热门品类。然后统计session。将项目映射为 (品类id, session, 1)。然后按照品类、session进行相加。再按照品类id进行聚合起来。然后再进行排序取前10个。思路和各省份广告点击前10相同。 首先map((省份，广告)，1)。然后按照省份广告相加。然后按照省份进行聚合。然后按照次数进行排序。 页面单跳转换率统计。 需求分析：数据： session id， 访问页面， 时间。 目标是求 A->B 的次数/ A的次数。所以首先统计A的次数。直接wordcount。然后统计A->B的次数。按照session id分组groupBy，然后按照时间排列。然后拉链 形成 A->B。然后再wordCount. 优化: 不用统计所有页面。只用统计部分页面。所以我们对部分页面过滤一下就行。</content></entry><entry><title>为什么要努力</title><url>http://next.lisenhui.cn/post/essay/thought/%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%8A%AA%E5%8A%9B/</url><categories><category>随笔</category></categories><tags/><content type="html"> 看看那些为考研拼搏的人吧，哪个不是拼劲全力？哪个不是竭尽全力？
为什么要努力，想想那些考研的日子的拼搏付出，不忍心辜负努力的自己。</content></entry><entry><title>Boost复习</title><url>http://next.lisenhui.cn/post/study/deeplearning/xueguo%E5%AD%A6%E8%BF%87%E7%9F%A5%E8%AF%86%E7%9A%84%E5%A4%8D%E4%B9%A0/</url><categories/><tags/><content type="html"> Boost boost是一种加法模型，通过串联的叠加多个弱学习器，每个学习器来学习上一个学习器的偏差，来得到一个强学习器。
boost和bagging的区别 bagging的弱学习器使用部分特征。boost使用全部特征 bagging可以并行训练。boost只能串行训练 bagging解决方差，boost解决偏差。 提问： boost描述不精准（方法，目的） boost通过层层叠加多个基分类器，聚焦错误的样本，来减少偏差。 bagging使用多个基分类器，多次采样集体投票，减小方差。
boost、bagging区别 从头想到尾， 输入上，boost使用全部的数据集。bagging使用部分数据集 运行上，boost串联运行，bagging支持并行 运行方式上，bagging均匀采样样本比例相同，boost对错误的样本赋予更大的权重。 结果（基学习器组合）bagging每个基学习器权重相等。boost分类误差小的基学习器拥有更大的权重。 目的上：bagging关注于降低方差。boost关注于降低偏差
adaboost 将若干个弱学习器通过加法模型连接起来，每一个弱学习器中，加大判断错误的样本的权重，减少判断正确的样本的权重。采用指数函数：$f(x) = e^{-yh(x)}$作为损失函数。
算法流程： 1). 初始化第一个基学习器 2). 对于1-T个基学习器 3). 计算误差率$\epsilon$ 4). 计算基学习器在加法模型中的权重 5). 计算样本的分布 6). end For 7). $f(x) = sign(\sum \alpha h(x))$
为什么采用指数函数 指数函数是-1到1的替代性函数。 指数函数拥有连续可微等优秀的特性。
如何分配基学习器的权重？ $\alpha = \frac{1}{2}ln\frac{1-\epsilon}{\epsilon}$ 通过公式$f(x) = e^{-y \alpha h(x)}$求得。
如何分配样本的权重？ $\D_t = \frac{\D_{t-1}exp(-y \alpha_t h_t(x))}{Z_t}$ 其中$Z_t$是归一化系数，值为$\sum exp(-y \alpha_t h_t(x))$
提问： 优缺点 优缺点就是boost的优缺点。 优点：1. 能够叠加弱学习器迅速构造出强学习器。 缺点：1. 异常样本的权重会不停的增加，影响最终的表现。对异常样本敏感。容易过拟合。 2. 无法并行运行，运行比较慢。
描述
GBDT GBDT就是梯度提升树，和adaboost一样，通过加法模型串联多个弱学习器达到强学习器。不同的是，GBDT通过拟合负梯度来纠正偏差。
算法步骤：
初始化第一个基学习器f_0。初始化方式根据损失函数来决定。 for 1 to T: 计算f_{t-1}的负梯度 f_{t-1}' $\alpha = min_\alpha L(f_{t-1}', f_t(\alpha))$ $\rho = min_\rho L(y, f_{t-1} + \rho f_t)$ 更新$f(x) = f_{t-1} + \rho f_t(\alpha)$ end For 为什么要拟合负梯度 根据提升树的公式 $L(y, f(x)) = L(y, f_{t-1}(x) + f_t(x))$.对f_{t-1}(x)进行泰勒展开。即 $L(y, f(x)) = L(y, f_{t-1}(x)) + (梯度）f_t(x)$所以可以看到这里完全可以让f_t(x)去拟合负梯度。就可以降低损失，来达到训练效果。负梯度中错误的样本梯度大，正确的样本梯度小，所以也是在去加大错误样本的权重。
负梯度的引入，可以支持更多的损失函数。
残差是梯度的特殊情况，拟合负梯度效果更好。
$\alpha是什么 \rho是什么$ $\alpha$是基学习器拟合负梯度的参数。$\rho$是基学习器的步长。负梯度仅仅指明了方向，但是没有指明移动多长距离。
如何优化这个算法？ 对于基学习器$f_t$,其可以表示为叶子节点的值$f_t = \sum_{x\in R_j} b_j$, $b_j$为叶子节点学习的值，在CART中一般为叶子节点的均值。
所以$\rho = min_\rho L(y, f_{t-1}+ \sum_j \rho b_j$
另$\gamma_j = \rho b_j$ 就可以一次最小运算得到$\gamma$
最终公式变为 $f(x) = f_{t-1} + \sum_j \gamma_j$
提问 描述
优缺点
举例子
如果需要举例子，可以使用对数损失函数L = log(1+e^{-2yF})来举例。
XGBoost xgboost属于一种梯度提升树，相比于GBDT，它在损失函数中引入了二阶导和正则化。并且使用了新的增益计算函数。在运行过程中，通过预排序和多线程特征并行查找来加快速度，引入了近似分裂算法来减少决策树特征选择的复杂度，引入了列抽样、shrinkage等避免过拟合的方法。
算法流程
如何使用二阶导信息的？ 对于提升树的损失函数$L(y, f) = L(y, f_{t-1} + f_t) + \Omega(f_t)$其中$\Omega(f_t) = \gamma j + \frac{1}{2} \lambda \sum b_j ^2$，就是通过正则化方式，尽量减少叶子结点的数量和叶子节点的权重的L2正则化。其中$b_j$就是叶子节点的预测值。对损失函数进行f_{t-1}的二阶泰勒展开。
$$ L(y, f) = \sum_i^{N}(L(y, f_{t-1}) + g_i f_t + \frac{1}{2} h f_t^2) + \gamma j + \frac{1}{2} \lambda \sum b_j ^2 \
L(y, f) = \sum_j L(y, f_{t-1}) + \sum_{x\in R_j}g_i b_j + \frac{1}{2} \sum_{x\in R_j}h_i b_j^2 + \gamma j + \frac{1}{2} \lambda \sum b_j ^2 \
L(y, f) = \sum_j L(y, f_{t-1}) + G_j b_j + \frac{1}{2}(H_j+\lambda) b_j^2 + \gamma j $$ 其中g为一阶导，h为二阶导。将公式对$b_j$求导得到$b_j = -\sum_j \frac{G_j}{H_j+\lambda}$。这个值为叶子节点的权重。
将$b_j$带回式子得到损失函数$L = \sum_j -\frac{1}{2}\frac{G_j^2}{H_j+\lambda} +\gamma j$ 所以信息增益计算方式为 $$ Gain = L - (L_{left} +L_{right}) \
= \frac{G_{left}^2}{H_{left}+\lambda} + \frac{G_{right}^2}{H_{right}+\lambda} - \frac{(G_{left}+G_{right})^2}{H_{left}+H_{right}+\lambda} - \gamma $$
为何引入二阶导信息？ 二阶导可以让结果更加精准。一阶导只提供了梯度方向，没有提供下降多少，二阶导提供了一阶导的变化趋势。可以使下降结果更加精确 二阶导的引入，更方便的支持扩展性。只要二阶可导的函数都可以作为损失函数。 新的增益如何计算的？ 使用Gain = L - L_left - L_right. $L = \frac{G^2}{H+\lambda}$ 如何剪枝 后剪枝。生长到指定的max_depth，然后从底到上进行剪枝，当某个节点以后的增益都小于gamma，则剪掉。一定程度上避免了欠拟合 达到max_depth停止。 当叶子权重小于最小的叶子权重时，避免分裂。 如何避免过拟合 增加正则化项。控制复杂度。 修改min_child_weight，分裂gamma等 进行列抽样。 shrinkage。缩小当前叶子节点的权重，为后面的树留出训练空间。 子采样。每轮计算不适用全部样本。 调参:
调整控制模型复杂度的参数：max_depth, min_child_weight, gamma
调整随机性的参数，subsample。colsample（列采样）
调整模型学习率，基学习器的数量等。
如何处理缺失值
1） 训练时，使用没有缺失值的部分进行训练。缺失值则依次放入左右叶子节点，计算最大增益。作为默认的缺省方向。 2） 预测时出现缺失值，默认送入右节点。 树模型对缺失值敏感度低。样本缺失不影响最优特征的选择，所以树模型对缺失值敏感度低
优点 （思考时和gbdt做对比）
二阶导，精准度更高。且可扩展性更好
正则化可以更好的避免过拟合。降低泛化误差。shrinkage缩减，削弱每棵树的影响，让后面有更大的学习空间。
支持列抽样
多线程特征查找速度更快。将特征预排序存为block。然后对每个特征的最有特征的查找使用多线程。
近似分裂算法，降低特征查找时的时间复杂度。提供了可并行的近似算法，高效的生成候选分割点。
缺失值的处理。
缺点
预排序消耗的时间空间大。
在寻找分裂点的时候，仍然需要遍历所有的数据。
对比
多线程特征并行查找如何实现的？ 对特征进行预排序，然后存为blcok结构。每次分裂的时候。启用多个线程对多个特征的最优分裂点进行查找。
提问 LightGBM LightGBM是GBDT的一个算法框架。为了解决GBDT速度慢、内存占用大的缺点。
有以下的改进： 1）基于Histogram的决策树算法 基于直方图的算法。特征划分多个区间，每个区间成为一个箱子。区间中的值更新为箱子的值。 优点：
内存占用小 计算代价小。相比xgb不需要遍历一个特征值就需要计算一次分裂的增益，只需要计算k次(k为箱子的个数) 子直方图可以用父直方图做差得到。速度上加快。 2）单边梯度采样 Gradient-based One-Side Sampling(GOSS)
减少样本的角度出发。排除大部分小梯度的样本。为了保证分布，按照梯度大小排序，选取最大的a个数，和小特征中的b个数。b个数乘以权重(1-a)/b。然后用a+b来计算信息增益。就减少了1-a-b个数。
3）互斥特征捆绑 Exclusive Feature Bundling(EFB)
如果将部分特征进行捆绑，可以降低特征数量。将互斥的特征进行捆绑（互斥即不同时为0，这样两个值可以叠加。如果同时为0）。且允许一小部分的冲突。可以得到更少的绑定特征。特征如何捆绑，A和B的取值空间叠加。
4）带深度限制的Leaf-wise的叶子生长策略
level-wise每次分裂一层的叶子。不容易过拟合，但是比较低效。 leaf-wise，找到分裂增益最大的叶子节点进行分裂，降低的误差更多，但是更容易过拟合。所以限制高度。
5）直接支持类别特征(Categorical Feature)
实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，通过 one-hot 编码，转化到多维的0/1特征，降低了空间和时间的效率。
但我们知道对于决策树来说并不推荐使用 one-hot 编码，尤其当类别特征中类别个数很多的情况下，会存在以下问题 1） 会产生样本切分不平衡问题，导致切分增益非常小（即浪费了这个特征）。即这个特征没有用。 2） 会影响决策树的学习。因为就算可以对这个类别特征进行切分，独热编码也会把数据切分到很多零散的小空间上，
LightGBM是第一个直接支持类别特征的GBDT工具。
6）支持高效并行
特征并行、数据并行、投票并行
7）Cache命中率优化
大概知道。并不很了解。
优点： 速度更快:
速度更快。比如使用直方图减少内存消耗。降低时间复杂度。 使用单边梯度算法，减少了大量的计算。GOSS在进行数据采样的时候只保留了梯度较大的数据 基于leaf-wise算法的增长策略。减少了很多不必要的计算量 采用优化后的特征并行，数据并行方法加速计算。还可以使用投票并行 对缓存进行了优化。 内存更小： 使用直方图将特征转为bin，减少了内存消耗 缺点：
可能决策树较深，产生过拟合。在leaf_wise熵增加了一个最大深度限制，防止过拟合 不断降低偏差，所以对噪点铭感。 在寻找最优解时，依据的是最优切分变量，没有将最优解是全部特征的综合这一理念考虑进去； 决策树 ID3 C4.5 CART
决策树根是树形结构的判决树，来完成分类。拥有以下特点：
根节点拥有所有的样本
叶节点包含决策结果
每个节点包含样本集，根据属性测试的结果划分到左右子树中。
划分测试： ID3 信息增益。 信息熵：所含的信息量。信息熵越大，信息量越多，混乱程度越高。公式$=-\sum p log p$。p就是第k类样本（标签）所占比例。 信息增益就是父节点的信息熵减去（左右子节点的信息熵乘以节点权重）。由于是多分类问题，每个节点拥有的样本数量不同，所以乘以这个权重。
信息增益的缺陷就是容易偏爱特征种数多的特征。当一个特征种数等于样本个数，则每种特征都是纯的，那这个信息增益会非常大。
信息增益率 = 信息增益 / 固有值。固有值为每一类样本个数所占比例的熵。公式$=-\sum p log p, p=\frac{D^v}{D}$ 信息增益率让特征选择更偏向种类少的特征。
基尼系数 $Gini(D) = 1-\sum p^2$ p为第k类样本所占比例。 基尼指数 = $\sum \frac{D^v}{D}Gini(D^v)$
连续值的处理。对于属性a，在a上出现了n个不同的取值，排序，然后取每两个相邻元素的中位点进行划分。进行二分。二分后的特征后面可以继续划分。
缺失值
剪枝
SVM 公式：$L = \frac{1}{2}||w||^2 + \sum\alpha_i(1-y_i(W^TX+b))$
怎么求解？先转为对偶问题，先求w，b再求$\alpha$。两个参数W，b求偏导，得到的值带入能求出$\alpha$, $W = \sum_i \alpha y_i x_i$
软间隔 $L = min (1/2 ||w||^2 + C\sum_i^n\mu)$ C为松弛变量。
$\mu > max(0, 1-y_i(WX+b))$ 这个就是合页损失。
逻辑回归 k-means 随机森林 其他 各个部分面经</content></entry><entry><title>Kmeans</title><url>http://next.lisenhui.cn/post/study/deeplearning/kmeans/</url><categories/><tags/><content type="html"> 聚类算法（clustering Algorithms）介绍 聚类是一种无监督学习—对大量未知标注的数据集，按数据的内在相似性将数据集划分为多个类别，使类别内的数据相似度较大而类别间的数据相似度较小。
聚类算法可以分为原型聚类（k均值算法（K-means）、学习向量量化、（Learning Vector Quantization -LVQ）、高斯混合聚类（Mixture-of-Gaussian），密度聚类（DBSCAN），层次聚类（AGNES）等。
原型聚类：，就是通过参考一个模板向量或模板分布的方式来完成聚类的过程
kmeans原理详解 kmean步骤：
随机初始化k个簇中心坐标 或者随机选k个点做为簇坐标。 计算数据集内所有点到k个簇中心的距离，并将数据点划分近最近的簇 更新簇中心坐标为当前簇内节点的坐标平均值 重复2、3步骤直到簇中心坐标不再改变（收敛了） 改进 缺点 改进 描述 k值的确定 ISODATA 当属于某个簇的样本数过少时把这个簇去除， 当属于某个簇的样本数过多、分散程度较大时把这个簇分为两个子簇 对奇异点敏感 k-median 中位数代替平均值作为簇中心 只能找到球状群 GMM 以高斯分布考虑簇内数据点的分布 分群结果不稳定 K-means++ 初始的聚类中心之间的相互距离要尽可能的远 k值的选取 簇内误差和：簇内每个样本到簇中心的距离的平方和，一定程度代表了聚类效果的好坏。
绘制K和合簇内误差和的图，将拐点确定为k。
K-means算法要求事先知道数据集能分为几群，主要有两种方法定义k。
手肘法：通过绘制k和损失函数的关系图，选拐点处的k值。
经验选取人工据经验先定几个k，多次随机初始化中心选经验上最适合的。
通常都是以经验选取，因为实际操作中拐点不明显，且手肘法效率不高。
注意 K-means算法中初始点的选择对最终结果的影响 K-means选择的初始点不同获得的最终分类结果也可能不同，随机选择的中心会导致K-means陷入局部最优解。
为什么在计算K-means之前要将数据点在各维度上归一化 确保各个维度量纲相同。
K-means不适用哪些数据 数据特征极强相关的数据集，因为会很难收敛（损失函数是非凸函数），一般要用kernal K-means，将数据点映射到更高维度再分群。
数据集可分出来的簇密度不一，或有很多离群值（outliers），这时候考虑使用密度聚类。
K-means 中常用的距离度量 K-means中比较常用的距离度量是欧几里得距离和余弦相似度。
K-means是否会一直陷入选择质心的循环停不下来（为什么迭代次数后会收敛）？ 从K-means的第三步我们可以看出，每回迭代都会用簇内点的平均值去更新簇中心，所以最终簇内的平方误差和（SSE, sum of squared error）一定最小
聚类和分类区别 产生的结果相同（将数据进行分类） 聚类事先没有给出标签（无监督学习）
如何对K-means聚类效果进行评估 回到聚类的定义，我们希望得到簇内数据相似度尽可能地大，而簇间相似度尽可能地小。
聚类算法几乎没有统一的评估指标，可能还需要根据聚类目标想评估方式
这个自己记得不是很清楚。 主要记得 簇内平方和。（每个点和质心的欧几里得距离平方平均） 簇间平方和。（质心的平方平均距离）
面试题 优缺点 优点： 实现容易，收敛速度快 聚类效果优良 缺点： k难以确定，依赖于初始值的选择。 对噪音和异常点比较的敏感，聚出来的类密度相差很大。</content></entry><entry><title>KnnK最近邻居</title><url>http://next.lisenhui.cn/post/study/deeplearning/knnk%E6%9C%80%E8%BF%91%E9%82%BB%E5%B1%85/</url><categories/><tags/><content type="html"> KNN步骤。 一般来说，KNN分类算法的计算过程：
1）计算待分类点与已知类别的点之间的距离
2）按照距离递增次序排序
3）选取与待分类点距离最小的K个点
4）确定前K个点所在类别的出现次数
5）返回前K个点出现次数最高的类别作为待分类点的预测分类
关键点 算法超参数K 距离度量，特征空间中样本点的距离是样本点间相似程度的反映 分类决策规则，少数服从多数。
距离：
欧式距离 曼哈顿距离。 闵可夫斯基距离。 $(\sum(x_i-x_j) ^p) ^1/p$ notes： k一般选择奇数。 优缺点 优点：
1）算法简单，理论成熟，既可以用来做分类也可以用来做回归。
2）可用于非线性分类。
4）由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属的类别，因此对于类域的交叉或重叠较多的待分类样本集来说，KNN方法较其他方法更为适合。
5）该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量比较小的类域采用这种算法比较容易产生误分类情况。
6）预测时间复杂度为O(n)，训练时，排序时间复杂度为O(nlogN)
缺点：
1）需要算每个测试点与训练集的距离，当训练集较大时，计算量相当大，时间复杂度高，特别是特征数量比较大的时候。
2）需要大量的内存，空间复杂度高。
3）样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少），对稀有类别的预测准确度低。
4）预测时速度比起逻辑回归之类的算法慢。</content></entry><entry><title>逻辑回归lr</title><url>http://next.lisenhui.cn/post/study/deeplearning/lr%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url><categories/><tags/><content type="html"> 逻辑回归 个人理解 y = f(wx+b) f 为 sigmoid激活函数 $1/(1+e^{-z})$
公式推导: https://zhuanlan.zhihu.com/p/44591359 https://zhuanlan.zhihu.com/p/46928319
极大似然估计 概率 在知道分布规律和具体参数的情况下，可以计算出某个事件发生的概率。 举例：抛硬币10次，5次正面朝上的概率。分布规律为二项分布。参数为正面朝上的概率为1/2。随意概率为$c^5_{10}*0.5^5*(1-0.5)^5=0.25$ 估计 不知道分布规律和具体参数，进行猜测。即为估计。 极大似然估计 似然函数（联合概率）公式：将多个概率值相乘。$L=\prod_{i=1}^n p{x=k_i|p}$，k_i为事件。P_k为概率。 优化目标：$\hat{p}=argmax_p L$. 求一个参数使得出现现有样本可能性最大。 方法：取对数 $lnL$然后拟牛顿法，或者梯度下降。 步骤： 确定分布/模型 进行多组实验并观察客观现象 写出似然概率（联合概率） 求解似然函数最大的凸优化问题，求的参数（梯度下降） 梯度下降 原理：将参数沿着梯度相反的方向前进一个步长，就可以实现目标函数（loss function）的下降。 方法： 使用损失函数，对每个参数（变量）求偏导数 将值带入，求下降方向$J(\theta)$ 根据步长$\alpha$，进行一步梯度下降 $\theta &lt;= \theta-\alpha J(\theta)$ 交叉熵损失 公式$f(w)=-\frac{1}{m}\sum_{i=1}^N(y^ilogf_w(x^i)+(1-y^i)log(1-x^i))$ 推导： 对于每一个样本，设它为1的概率为p，则为0的概率为1-p 即 $$ P(y|x)= \begin{cases} p&amp; \text{y=1}\
1-p&amp; \text{y=0} \end{cases} $$ 上述式子不方便计算。其等价于$p(y_i|x_i)=p^{y_i}(1-p)^{1-y_i}$。 即当$y_i=1$时，该值为p,否则为1-p 其最大似然估计为$L=\prod_{i=1}^n p^{y_n}(1-p)^{1-y_n}$。在逻辑回归中，这里面只有一个参数就在p里面。 上个式子不好计算，所以我们取对数$F ( w ) = \ln ( P _ { 总} ) \= \ln ( \prod _ { n = 1 } ^ { N } p ^ { y _ { n } }( 1 - p ) ^ { 1 - y _ { n } } ) \= \sum _ { n = 1 } ^ { N } ( y _ { n } \ln ( p ) + ( 1 - y _ { n } ) \ln ( 1 - p ) )$。 最终为求使得这个值最大的w。添加一个符号即为求最小值。即可以用梯度下降。 注意：在使用softmax归一化后，极大似然函数和交叉熵损失函数等价。 凸函数判定：二阶导存在且为正。则为凸函数。凸函数意思是在凸集中局部最优等于全局最优。 二阶导存在且为正，则一阶导有负有正，则原函数局部最小就是全局最小。 线性回归 线性回归(Linear Regression)是利用称为线性回归方程对线性问题进行建模。 公式：$f ( x ) = \theta _ { 0 } x _ { 0 } + \theta _ { 1 } x _ { 1 } + \theta _ { 2 } x _ { 2 }+&hellip; + \theta _ { n } x _ { n } = \theta ^ { T }$
损失函数。
损失函数一般使用最小二乘法。$MSE=\frac{1}{2m} \sum(y-f(x))^2$ Lasso回归
目的：解决线性回归出现的过拟合的请况。本质：约束(限制)要优化的参数 Lasso回归加入L1正则化（$\lambda$*绝对值和) 岭回归加入L2正则化$\lambda$*平方和。
ElasticNet 回归：$\lambda(pL1+(1-p)L2)$
LWR( 局部加权)回归:
局部加权线性回归是在线性回归的基础上对每一个测试样本（训练的时候就是每一个训练样本）在其已有的样本进行一个加权拟合，权重的确定可以通过一个核来计算，常用的有高斯核（离测试样本越近，权重越大，反之越小） 具体的查资料 面试题： 线性回归要求因变量服从正态分布吗 前提假设是因变量服从正态分布，此时效果达到最好。 公式推导 背景知识 回归：人们在测量事物的时候因为客观条件所限，求得的都是测量值，而不是事物真实的值，为了能够得到真实值，无限次的进行测量，最后通过这些测量数据计算回归到真实值，这就是回归的由来。通过有监督的学习，学习到由x到y的映射h，利用该映射关系对未知的数据进行预估，因为y为连续值，所以是回归问题。 逻辑回归使用回归函数，来解决分类问题，线性回归的结果Y带入一个非线性变换的Sigmoid函数中，得到[0,1]之间取值范围的数S，S可以把它看成是一个概率值，如果我们设置概率阈值为0.5，那么S大于0.5可以看成是正样本，小于0.5看成是负样本，就可以进行分类了。 线性回归的损失函数。一般使用最小二乘法，则MSE误差平方为$(yi−hθ(xi))^2$找到合适的参数 逻辑回归 逻辑回归=线性回归+sigmoid函数 公式: $z=wx+b, y=\frac{1}{1+e^{-z}}$。判别式是$f(x)=sign(y)$ 损失函数（交叉熵损失函数）:$L = - \sum_i^n (ylogp+(1-y)log(1-p))$。 前面的y是标签，后面的p是预测出来的分布。 损失函数求导。损失函数使用最大似然估计:$L=- \sum _ { n = 1 } ^ { N } ( y _ { n } \ln ( p ) + ( 1 - y _ { n } ) \ln ( 1 - p ) )$,这里的$p=\frac{1}{1+e^{-z}}$就是这个值的概率。 $\frac{\partial f(x)}{\partial z}=\frac{1}{1+e^{-z}}*\frac{e^{-z}}{1+e^{-z}}=p(1-p)$ 最终求导结果为$\frac{\partial L}{\partial w}= -\frac{1}{n}\sum_{n=1}^{N}(y_i-\frac{1}{1+e^{-z}})x$ 案例： p(Y=1|x)=p(x) p(y=0|x)=1-p(x).p就是函数 极大似然估计 推导W、b的梯度下降步骤。
当label={-1, +1}下的公式推导 https://github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning
在label={-1,+1},则$\left. \begin{array} { l } { p ( y = 1 | x ) = h _ { w } ( x ) } \ { p ( y = - 1 | x ) = 1 - h _ { w } ( x ) } \end{array} \right.$ 由于sigmoid函数的特性： $\left. \begin{array} { l } { h ( - x ) = 1 - h ( x ) } \ 综上{ p ( y | x ) = h _ { w } ( y x ) } \end{array} \right.$ 仍然使用极大似然函数，$L=\prod p(y|x;w)=\prod h(yx)=\prod_{ i = 1 } ^ { m } \frac{ 1 }{ 1 + e ^ { - y _ { i } w x _ { i } }}$ 再求导计算即可。 面试相关 如何多分类：
一对一：N个类别两两配对 一对多：每次将一个例作为正例。其他作为反例。 多对多。多个类正类，多个类反类。需要进一步的设置。 逻辑回归优缺点：
优点 LR的可解释性强、特征的权重可以看到不同特征的影响。模型效果不错，运行速度快（仅和特征数目有关）。方便调整阈值来做分类。 缺点： 特征工程复杂。需要较多的特征工程和归一化。 准确率不足（简单的线性回归，和SVM相比） 难以处理不平衡的数据。（没有不平衡数据的处理方案） 使用场景：大规模数据线性分类。 记忆：LR简单，解释性强，适用于大规模数据的线性分类。但由于简单所以准确率不足，对特征工程要求较高。 逻辑回归训练技巧
特征离散化 逻辑斯特回归为什么要对特征进行离散化 （ 对特征离散化，让特征里没有连续值）
逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合 稀疏向量内积乘法运算速度快计算结果方便存储，容易扩展 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。 线性回归与逻辑回归的区别
线性回归主要解决回归任务，逻辑回归主要解决分类问题。 线性回归的输出一半是连续的，逻辑回归的输出一般是离散的。 线性回归的损失函数是MSE,逻辑回归中，采用的是负对数损失函数 为什么逻辑回归比线性回归要好？
在特征到结果的映射中加入了一层sigmoid函数（非线性）映射，即先把特征线性求和， 另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在0,1间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。 逻辑回归的损失函数让它只有靠近决策取阈的样本影响大，所以鲁棒性更好。 总结：1. 加入非线性，2靠近决策区域影响更大，效果更好。3. 它在0-1进行预测，比线性回归解空间小。 LR为什么使用sigmoid函数？,为什么不使用MSE
而Sigmoid能够把它映射到[0,1]之间。正好这个是概率的范围。 Sigmoid是连续光滑的。 Sigmoid也让逻辑回归的损失函数成为凸函数，这也是很好的性质。可以寻找到一个全局最优点进行下降。 使用MSE，则不是一个凸函数。根据损失函数可以得到。 逻辑回归为什么不使用最小二乘法，而使用最大似然估计。注意区分，这是损失函数，上一个问题是激活函数的问题。
在最小二乘法下，逻辑回归损失函数非凸。 非凸判定：二阶导存在且为正。则为凸函数。意思是在凸集中局部最优等于全局最优。 LR和SVM有什么不同吗
相同 两个方法都可以增加不同的正则化项 LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题 不同： LR是参数模型，SVM是非参数模型。参数模型即需要知道数据的分布参数。 从目标函数来看，区别在于逻辑回归采用的是交叉熵损失函数，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。 SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。 LR在特别是大规模线性分类时比较方便。**SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,**这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。 LR依赖数据分布。LR和所有的数据相关。SVM不依赖。 LR能做的 SVM能做，但可能在准确率上有问题，SVM能做的LR有的做不了。 总结。模型类型，从损失函数，支持向量上面进行分析。 sigmoid作为激活函数的缺点（难算，非中心化，梯度消失）</content></entry><entry><title>百面机器学习baimian</title><url>http://next.lisenhui.cn/post/study/deeplearning/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0baimian/</url><categories/><tags/><content type="html"> 为了实习而计划的学习顺序 特征工程 模型评估部分 经典算法部分 非监督学习部分 优化算法部分 采样部分 前向神经网络部分 循环神经网络部分 集成学习部分
注重工程共性问题。其他问题先放放
特征工程 归一化: 线性函数归一化，数据集中时比较适用。 零均值归一化。 特征编码 常用的分为序号编码和one-hot编码 当数据有严格的偏序关系时，建议使用序号编码。就是对数据分类，比如编号为2的类，整体上比编号为1的类要大。这样有助于减少数据带来的噪声。比如19和16相差只有三岁，63和60也相差3岁。但是成年和未成年带来的意义就完全不一样。 优化指标 roc曲线相比于pr曲线更加的不受数据集分布的影响。
分类和聚类的区别 区别：
从训练目标来看 分类是根据一些给定的已知类别标号的样本，训练某种学习机器（即得到某种目标函数），使它能够对未知类别的样本进行分类。这属于supervised learning（监督学习）。 聚类指事先并不知道任何样本的类别标号，希望通过某种算法来把一组未知类别的样本划分成若干类别，聚类的时候，我们并不关心某一类是什么，我们需要实现的目标只是把相似的东西聚到一起，这在机器学习中被称作 unsupervised learning （无监督学习） 分类，根据目标来学习特征。 聚类根据数据来训练。组内的对象相互之间时相似的（相关的），而不同组中的对象是不同的（不相关的）。组内的相似性越大，组间差别越大，聚类就越好。 从数据来看 分类知道目标类，讲记录分到具体的类 聚类没有训练条件下将样本划分到若干类 分类对数据要求没那么高，尤其是深度学习。可以学习特征的表达。 聚类对特征要求很高。 熵函数 各种熵 https://zhuanlan.zhihu.com/p/35423404
自己读一下的话，信息熵和交叉熵是差不多的公式。\sum p log p。 只不过交叉熵的时候。后面的p是预测的概率分布。所以是\sum p log q.
分类算法： K近邻（KNN） 决策树 朴素贝叶斯 逻辑回归 支持向量机 随机森林
聚类算法： K均值（K-means） DBSCAN DPEAK Mediods Canopy</content></entry><entry><title>Matrix矩阵补全</title><url>http://next.lisenhui.cn/post/study/deeplearning/matrix%E7%9F%A9%E9%98%B5%E8%A1%A5%E5%85%A8/</url><categories/><tags/><content type="html"> 默认矩阵式低秩的。</content></entry><entry><title>SVM讲解</title><url>http://next.lisenhui.cn/post/study/deeplearning/svm%E8%AE%B2%E8%A7%A3%E5%92%8C%E9%9D%A2%E8%AF%95%E9%A2%98/</url><categories/><tags/><content type="html"> SVM https://www.bilibili.com/video/BV1Y7411P7nd 讲的非常好。
概念:
超平面（最优分割面）能使支持向量和分割线最小距离最大的平面。 点到直线距离=$\frac{Ax+By+b}{\sqrt{A^2+B^2}}$。 直线到直线的距离=$\frac{b1-b2}{\sqrt{A^2+B^2}}$ 正确分类样本点：$y_i(w^Tx_i+b)>=1, i=1,2,&hellip;,m$。$y_i$取+1，-1， 从二维扩展到多维空间中时，将 D0和D1完全正确地划分开的 就成了一个超平面。为了使这个超平面更具鲁棒性，我们会去找最佳超平面，以最大间隔把两类样本分开的超平面，也称之为最大间隔超平面。两类样本分别分割在该超平面的两侧；两侧距离超平面最近的样本点到超平面的距离被最大化了。
样本中距离超平面最近的一些点，这些点叫做支持向量。
在决定最佳超平面时只有支持向量起作用，而其他数据点并不起作用
目标函数： 在这个条件下 $yi(W^T x_i +b )>= 1$ 使得 $d = \frac{|W^T x +b|}{||w||}$最大，就是||w||最小。$\gamma=\frac{2}{||w||}$被称为间隔。这个w是直线的参数。同时使得$y_i(w^Tx_i+b)>=1, i=1,2,&hellip;,m$
这个不好求解。所以我们需要引入拉格朗日乘子法，对每条约束添加拉格朗日乘子$\alpha_i>=0$：
（SVM核心数学问题）：$L(w,b,\alpha) = \frac{1}{2} ||w||^2 - \sum_{i=0}^N \alpha_iy_i(wx_i+b-1)$ 使得距离最大，则w最小，使得L最小，则后面一项最高为0.所以求W最小值和距离最大问题等价。 对偶问题 $min_{w, b}max_{\alpha}L = max_{\alpha}min_{w,b}$
拉格朗日乘子法 综合可知，在相切点，圆的梯度向量和曲线的梯度向量平行： 联立就是拉格朗日乘子法。 在某个条件下，求某个函数的最小值。则两个函数的梯度是平行的。
拉格朗日函数：求某个函数在某个约束条件下的极值。则这个函数和约束条件的法线平行。可以构造方程组求导$（f=\lambda g）$ , g(x,y) = 0
将约束条件乘以一个常数，与原函数求和得到一个新的函数，来求极值。 构造拉格朗日函数 L = fx + $\lambda g(x)$ 再求偏函数。 限制条件再一次来到了要求lambda 乘以g(x)等于0.
对偶问题 对偶问题更易求解，由下文知对偶问题只需优化一个变量$\alpha$且约束条件更简单； 能更加自然地引入核函数，进而推广到非线性问题
拉格朗日函数 综合可知，在相切点，圆的梯度向量和曲线的梯度向量平行： 联立就是拉格朗日乘子法。
线性不可分 我们刚刚讨论的硬间隔和软间隔都是在说样本的完全线性可分或者大部分样本点的线性可分。
但我们可能会碰到的一种情况是样本点不是线性可分的.
我们用 x 表示原来的样本点，用 $\theta(x)$ 表示 x 映射到特征新的特征空间后到新向量。
优点 有严格的数学理论支持，可解释性强，不依靠统计方法，从而简化了通常的分类和回归问题；能找出对任务至关重要的关键样本（即：支持向量）；采用核技巧之后，可以处理非线性分类/回归任务；最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”。
缺点 训练时间长。当采用 SMO 算法时，由于每次都需要挑选一对参数，因此时间复杂度为 ，其中 N 为训练样本的数量；当采用核技巧时，如果需要存储核矩阵，则空间复杂度为 ；模型预测时，预测时间与支持向量的个数成正比。当支持向量的数量较大时，预测计算复杂度较高。
面经：
SVM 原理 SVM 是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器
当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机； 当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机； 当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。 SVM自我总结 超平面：n维欧式空间中，n-1维的线性子空间。是空间中的直线的推广。 SVM的目标就是找到一个超平面，使得两个平行的分割数据的超平面距离最大。这样健壮性最好。两条直线的距离为$1/||W||$，等价于$1/2||w||^2$最小。约束条件是$1-y(x^Tw+b)&lt;0$ 使用拉格朗日函数，转换为求等式的最值：$min_{w,b} max_{\lambda} L = 1/2||w||^2 + \sum \lambda [1-y(x^Tw+b)]$. $min_{w,b} max_{\lambda} L>=max_{\lambda}min_{w,b} L$。当L为凸优化问题时，这个问题转为强对偶问题。 在三个条件下转为强对偶问题。 原问题凸函数 原问题X受线性约束 满足KKT条件,对于不等式 $\lambda h(x)$： $\lambda$大于等于0， h(x)小于等于0. $\lambda h(x)=0$ L对每个x的偏导为0. 所以转换为强对偶问题。此时求偏导得到 $W=\sum_i^n \lambda_i y_i x_i$ 重点 $0 = -\sum_i^n \lambda_i y_i$ 重点 解出$b=y_k-\sum_{i=1}^{N}\lambda_iy_ix_i^Tx_k$ 将这个值带入原始。得到最终的优化方程： $L(w,b,\lambda) = \sum_i^n\lambda_i-1/2\sum_i^m\sum_j\lambda_i y_i \lambda_j y_j x_i x_j$ 重点 这里为什么出现了i，j因为上一步的W是对所有的n。这里的sum也是对所有的n两个n不是相同的取值。 转换为了$\lambda$的最大值。 由KKT条件可以知道，这个时候要么$\lambda=0$要么$1-y(x^Tw+b)=0$所以，解只和支持向量有关。和$1-y(x^Tw+b)&lt;0$的点无关。 这个问题的求解是一个二次规划问题。 就是对多参数的一个求解。这里有m个$\lambda$参数。 对于常见的二次规划问题，我们采用SMO（启发式）固定剩下所有参数求一个参数。由于b的公式限制了$\sum\lambda_iy_i=0$所以我们固定n-2个，来求两个值 最终得到决策函数： $f = \sum_i^n\lambda_iy_iX_i^TX+b$ 软间隔。为了提高泛化性，引入软间隔。允许少量样本不满足约束 修改约束条件$y(Wx+b)>1-\mu$ 损失函数使用hinge损失（合页损失）$l_{hinge} = max(0, 1-z)$ $L = min (1/2 ||w||^2 + C\sum_i^n\xi)， s.t. \xi>=0, y_i(W^Tx_i+b)>=1-\xi$ $\mu > max(0, 1-y_i(WX+b))$ 这个就是合页损失。 C为松弛变量。 当c越大的时候，损失越大，越在意这个异常值。所以越不平滑 C越小，损失越小，对异常值越不在乎，所以越平滑。 将约束条件根据拉格朗日定理带入，可以重新求得解。和原解不同的是$\lambda$的取值范围。具体可以参考西瓜书。 核函数 $K(x_1, x_2) = K(x_1)K(x_2) = K(x_1x_2)$ 因此我们不需要显式地定义映射K(x)是什么而只需事先定义核函数K(x_1*x_2)即可。 决策函数 $f = \sum_i^n\lambda_iy_i K(x^T,x_i)+b$ 常用核函数： 线性核 $K(x_i, x_j)=x_i^Tx_j$ 高斯核 $K(x_i, x_j)=exp(-\frac{||x_i-x_j||^2}{2\theta^2})$ 优点： 理论清晰。逻辑完善 仅支持向量影响数据，减少运算，无需依赖全部数据。泛化能力强。 核函数可以支持非线性问题。 SVM是凸优化问题。求得的解是全局最优。 缺点： 二次规划将涉及M阶矩阵运算，M为样本个数，不适合大数据集。SMO缓解这个问题 SVM只适用于二分类。 SVM对缺失值敏感。 面试题 优缺点： 优点： 可以解决小样本下机器学习的问题。（支持向量就可以做出决策，不需要全部样本） 提高泛化性能。（软间隔） 解决高维、非线性问题（核技巧） 缺点： 对缺失数据敏感（缺失数据如果是支持向量，则会影响决策） 这里说的缺失数据是指缺失某些特征数据，向量数据不完整。SVM 没有处理缺失值的策略。 内存消耗大（支持向量计算复杂） 运行调参麻烦（支持向量计算复杂 SVM的支持向量是什么 超平面（最优分割面）能使支持向量和分割线最小距离最大的平面。 样本中距离超平面最近的一些点，这些点叫做支持向量。 线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强 SVM核函数和意义 当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。而引入这样的映射后，所要求解的对偶问题的求解中，无需求解真正的映射函数，而只需要知道其核函数。即不需要知道K(x1)* K(x2)，只需要知到K(x1* x2) 的值。所以不用定义非线性的映射函数。 结论：一方面数据变成了高维空间中线性可分的数据，另一方面不需要求解具体的映射函数，只需要给定具体的核函数即可，这样使得求解的难度大大降低。 SVM 核函数之间的区别 SVM 核函数一般选择线性核和高斯核(RBF 核)。 线性核：主要用于线性可分的情形，参数少，速度快，对于一般数据，分类效果已经很理想了。 RBF 核：主要用于线性不可分的情形，参数多，分类结果非常依赖于参数。 如果 Feature 的数量很大，跟样本数量差不多，这时候选用线性核的 SVM。 如果 Feature 的数量比较小，样本数量一般，不算大也不算小，选用高斯核的 SVM。 为什么转换为对偶问题 对偶问题往往更易求解， 可以自然引入核函数，进而推广到非线性分类问题。 处理多分类 一对多：就是对每个类都训练出一个分类器，设定为目标类为一类，其余类为另外一类。这样针对k个类可以训练出k个分类器，当有一个新的样本来的时候，用这k个分类器来测试，那个分类器的概率高，那么这个样本就属于哪一类。 一对一：任意两个类训练出一个分类器，如果有k类，一共训练出C(2,k) 个分类器，这样当有一个新的样本要来的时候，用这C(2,k)个分类器来测试，每当被判定属于某一类的时候，该类就加一，最后票数最多的类别被认定为该样本的类。 多对多：了解不多。 SVM可以用梯度下降嘛？ 可以。在优化后的对偶问题中，是一个最小化损失函数的目标。但SVM是凸优化问题，用SMO效果会更好。 SVM SMO是什么算法？ 这里简单介绍。具体请看视频 解决这样一个有多变量（n个αi）的优化问题确实比较困难，但是如果能多次迭代，每次选择两个变量优化，同时把其他变量看做是固定的常数，这样“分而治之”的话，问题似乎就容易多了。 到这里，SMO算法的大致介绍总算是说完了。我把它的步骤概括式的总结一下： 初始化αα，一般情况下令初始的αiαi全部为0； 选取优化变量α1α1和α2α2，执行相关的优化计算，得到更新后的α1,α2α1,α2； 开始新的一轮迭代，重复执行上面的第2步，知道全部的αiαi满足公式(14)的KKT条件以及公式(1)中的约束条件； 总结：每次优化两个变量，固定其他变量为常数。</content></entry><entry><title>图结构特征的使用</title><url>http://next.lisenhui.cn/post/study/deeplearning/%E5%9B%BE%E7%BB%93%E6%9E%84%E7%89%B9%E5%BE%81%E7%9A%84%E4%BD%BF%E7%94%A8/</url><categories/><tags/><content type="html"> 图特征提取出来之后怎么使用 从神经网络的角度上，图神经网络怎么使用的：
图：邻接矩阵、结点（结点属性）。
使用图神经网络
图结构特征和图神经网络是天然契合的。 使用其他网络
CNN网络需要二维特征，这个和图结构中的邻接矩阵也是契合的。 RNN网络需要的是序列化的特征，所以这里面一般会使用图嵌入技术，将图结构降维成一维特征。 深度优先遍历序列化。将图结构序列化成一维特征 随机游走。另一种方式的序列化，会根据邻居节点生成多条序列化队列 Graph2Vec 一种无监督的方法，通过graph的结构将Graph映射为一维结构 Node2Vec 随机游走的改进版。 提取了一维的特征后，就可以用一维特征去做其他训练。有监督、无监督都可以。
有监督学习算法
上面的方法基本都可以使用有监督学习算法 无监督学习算法
神经网络中的无监督学习算法并不多。比较典型的应用是重建损失。 AutoEncoder 比较典型 AE需要一维特征，先降维再恢复。是比较典型的无监督算法。对于图的使用和图嵌入无异 并不很看好无监督。无监督在学习数据分布，但并没有学习到针对目标的条件概率。 自监督学习
使用结构自身的信息作为标签，从而提取特征。 GNN是能做自监督学习的。比如说图对比学习。通过学习两个图的联系，来提取特征。 其他神经网络的自监督学习，也有很多，也是通过图结构之间的内在联系来学习这种分布特征。原理可以参考word2vec。上面的Graph2vec、Node2Vec也是从此而来 无监督学习在学习一种有指导性的概率分布，比纯粹的概率分布来说，可以提取到更有指向性的特征，效果上看比无监督要好很多。 建议：
搜索Graph embedding 图嵌入相关知识 知乎上很多 搜索code vulnerability 相关知识。 code vulnerability是在源码上运行的。只不过目标换成了漏洞检测，搜索这方面的论文可以对方法有个大概的了解。 推荐一篇。《Software Vulnerability Detection Using Deep Neural Networks: A Survey》</content></entry><entry><title>TitanicSolutionGold1</title><url>http://next.lisenhui.cn/post/study/kaggle/kaggle%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90/%E7%89%B9%E5%BE%81%E5%88%86%E6%9E%90feature/titanicsolutiongold1/</url><categories/><tags/><content type="html"> 解决思路流程：
工作流程阶段 竞赛解决方案工作流程经历了数据科学解决方案书中描述的七个阶段。
问题或问题定义。 获取训练和测试数据。 整理、准备、清理数据。 分析、识别模式并探索数据。 建模、预测和解决问题。 可视化、报告和呈现问题解决步骤和最终解决方案。 提供或提交结果。 工作流指示每个阶段如何跟随另一个阶段的一般顺序。 但是，也有例外的用例。 我们可以结合多个工作流程阶段。 我们可以通过可视化数据进行分析。 执行比指示更早的阶段。 我们可能会在争论前后分析数据。 在我们的工作流程中多次执行一个阶段。 可视化阶段可以多次使用。 完全放弃一个阶段。 我们可能不需要供应阶段来生产或为我们的数据集提供服务以进行竞争。
问题和问题定义 Kaggle 等竞赛网站定义要解决的问题或要提出的问题，同时提供用于训练数据科学模型的数据集，并针对测试数据集测试模型结果。在 Kaggle 中描述了泰坦尼克号生存竞赛的问题或问题定义。
从列出在泰坦尼克号灾难中幸存或没能幸存的乘客的训练样本集中知道，我们的模型能否基于给定的不包含生存信息的测试数据集确定测试数据集中的这些乘客是否幸存下来。
我们可能还想对我们的问题领域有一些早期的了解。这在此处的 Kaggle 比赛描述页面上有描述。以下是要注意的重点。
1912 年 4 月 15 日，在她的处女航期间，泰坦尼克号在与冰山相撞后沉没，2224 名乘客和船员中有 1502 人遇难。翻译成 32% 的存活率。 沉船造成如此大的生命损失的原因之一是没有足够的救生艇供乘客和船员使用。 尽管在沉没中幸存下来有一些运气因素，但某些人群比其他人群更有可能幸存下来，例如妇女、儿童和上层阶级 工作流目标 数据科学解决方案工作流解决了七个主要目标。
分类。我们可能想要对我们的样本进行分类或分类别。我们可能还想了解不同类别与我们的解决方案目标的含义或相关性。
相关性分析。人们可以根据训练数据集中的可用特征来解决问题。数据集中的哪些特征对我们的解决方案目标有重大贡献？从统计上讲，特征和解决方案目标之间是否存在相关性？随着特征值的变化，解决方案的状态也会发生变化，反之亦然？这可以针对给定数据集中的数值和分类特征进行测试。我们可能还想确定后续目标和工作流程阶段的生存以外的特征之间的相关性。关联某些特征可能有助于创建、完成或修正特征。
数据转换。对于建模阶段，需要准备数据。根据模型算法的选择，可能需要将所有特征转换为数值等效值。例如，将文本分类值转换为数值。
数据补全。数据准备还可能需要我们估计特征中的任何缺失值。当没有缺失值时，模型算法可能效果最好。
数据纠正。我们还可以分析给定训练数据集的错误或特征中可能不准确的值，并尝试校正这些值或排除包含错误的样本。一种方法是检测我们的样本或特征中的任何异常值。如果某个特征对分析没有贡献或可能显着扭曲结果，我们也可能会完全丢弃该特征。
数据创造。我们是否可以基于现有特征或一组特征创建新特征，以便新特征遵循相关性、转换、完整性目标。
数据制图。如何根据数据的性质和解决方案目标选择正确的可视化图和图表。
从7个步骤对数据进行分析。
通过描述数据进行分析 Which features are available in the dataset?
Which features are categorical?
这些值将样本分类为相似样本的集合。 分类特征中的值是基于名义、有序、比率还是基于区间？ 除其他外，这有助于我们选择合适的图表进行可视化。
分类： 幸存，性别，和登船。 序数：Pclass。
Which features are numerical?
哪些特征是数字的？ 这些值因样本而异。 在数值特征中，数值是离散的、连续的还是基于时间序列的？ 除其他外，这有助于我们选择合适的图表进行可视化。 连续：年龄，票价。 离散：SibSp、Parch。
Which features are mixed data types? 同一特征中的数字、字母数字数据。 这些是待纠正的特征。
Ticket是数字和字母数字数据类型的混合。 Cabin 是字母数字的。
Which features may contain errors or typos?
哪些功能可能包含错误或拼写错误？ 这对于大型数据集来说更难审查，但是从较小的数据集中审查一些样本可能会直接告诉我们哪些特征可能需要更正。
名称特征可能包含错误或拼写错误，因为有多种方法可以用来描述名称，包括标题、圆括号和用于替代名称或简称名称的引号
Which features contain blank, null or empty values?
What are the data types for various features?
在转换目标期间帮助我们。
七个特征是整数或浮点数。 六个在测试数据集的情况下。 五个特征是字符串（对象）。
组合有关联的特征为新特征（孩子、伴侣组成家庭）。对部分特征提取关键信息（名字提取姓氏）。将强相关的分类特征单独拎出来。弱相关的特征进行组合。弱相关的特征组合成强特征。可以删除弱特征，来强化特征表达力。
由于决策树的性质。尽量减少特征的类别。可以增强树的判断力。
本身强相关的信息不用组合？embarked和
从现有特征中挖掘和goal相关联的信息。比如名称中挖掘title和goal相关。
什么时候需要one-hot，什么时候可以用类别数字代替？ 总体来说，没有严格的大小偏序关系的，使用类别。还跟具体的使用方法有关。像决策树中，0，1是两种类别。但深度神经网络中，0，1所带来的权重影响不如one-hot所带来的更多。
通过关联分析补全缺失值。相关联的值的中位数等等补全值。 补齐缺失值后，使用band区域是一种比较好的方式来降低噪音。同时band也确实比具体值更优秀在某些场景下。 对于连续值band特征会比连续特征效果更好。
使用年龄段为什么比年龄更有效？ 23岁和17岁之间相差可能不大，但一个属于成年，一个未成年，很可能结果上的区分更明确一点。年龄这种我们人认为的年龄段比具体的年龄更容易对结果造成影响。 票价范围也是一样的，具体的数字并没有票价分段来的更有效。
名称是否有年龄的相关性？
每执行一步，测试一次效果。</content></entry><entry><title>Pandas学习笔记</title><url>http://next.lisenhui.cn/post/study/python%E5%BA%93/pandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url><categories><category>学习</category></categories><tags><tag>python相关库学习</tag><tag>机器学习相关库</tag></tags><content type="html"> 资料 https://github.com/datawhalechina/joyful-pandas
pandas 临时杂碎知识点记录 .null()检测是否有空值 .sum()对pandas数值进行求和 .unique()检测pandas中的独一值。 train_df.info()查看dataframe的信息。 train_df.describe(include=[&lsquo;O&rsquo;])会计算离散变量的统计特征。 df.groupby(&ldquo;字段&rdquo;) 这个操作会返回一个df对象，后续的agg,apply等操作可以基于这个对象进行操作。 df.sort_values(by=&ldquo;字段&rdquo;, ascending=False)降序排列。 字符串处理 df.字符串字段.str.extract(&lsquo;正则化语言&rsquo;) df.replace 替换字段值。 df.map() 通过字典进行map df.fillna(0)将na 填为0. pd.cut 对数据进行分割。</content></entry><entry><title>-其他推荐书籍资料</title><url>http://next.lisenhui.cn/post/study/deeplearning/-%E5%85%B6%E4%BB%96%E6%89%80%E6%9C%89%E6%8E%A8%E8%8D%90%E9%98%85%E8%AF%BB%E7%9A%84%E4%B9%A6%E7%B1%8D%E8%B5%84%E6%96%99/</url><categories/><tags/><content type="html"> 待读博客 代码补全漫谈： https://www.twblogs.net/a/5d7e6695bd9eee541c346d43
网站 最全资料 https://github.com/Mikoto10032/DeepLearning
书籍 周志华 机器学习 深度学习 deep learning 李航 统计学方法 机器学习实战 how to lie with statistics
视频 吴恩达机器学习
吴恩达深度学习。
NLP入门 https://github.com/datawhalechina/team-learning-nlp</content></entry><entry><title>Batchnorm讲解</title><url>http://next.lisenhui.cn/post/study/deeplearning/batchnorm%E8%AE%B2%E8%A7%A3/</url><categories/><tags/><content type="html"> 引用 https://mp.weixin.qq.com/s/QFpolIXvOQjUsPAngqvqmg
论文： Correct Normalization Matters: Understanding the Effect of Normalization On Deep Neural Network Models For Click-Through Rate Prediction：https://arxiv.org/pdf/2006.12753.pdf
介绍 谷歌在2015年就提出了Batch Normalization(BN)，该方法对每个mini-batch都进行normalize，会把mini-batch中的数据正规化到均值为0，标准差为1，同时还引入了两个可以学的参数，分别为scale和shift，让模型学习其适合的分布。
那么为什么在做过正规化后，又要scale和shift呢？ 因为scale和shift是模型自动学习的，神经网络可以自己琢磨前面的正规化有没有起到优化作用，没有的话就"反"正规化，抵消之前的正规化操作带来的影响。
让特征分布有一定的自由度，可以调整到对下一层更好的分布。
BatchNormalization是对一批样本进行处理, 对一批样本的每个特征分别进行归一化
LayerNormalization是对一个样本进行处理，对一个样本的所有特征进行归一化，乍一看很没有道理，因为如果对身高体重和年龄一起求一个均值方差，都不知道这些值有什么含义，但存在一些场景却非常有效果——NLP领域。 在NLP中，N个特征都可能表示不同的词，这个时候我们仍然采用BatchNormalization的话，对第一个词进行操作，很显然意义就不是非常大了，因为任何一个词都可以放在第一个位置，而且很多时候词序对于我们对于句子的影响没那么大，而此时我们对N个词进行Normalization等操作可以很好地反映句子的分布。(LN一般用在第三维度，[batchsize, seq_len,dims])，因为该维度特征的量纲是相同的，所以并没有太多区别。
为什么要用Normalization？ 解决梯度消失 拿sigmoid激活函数距离，从图中，我们很容易知道，数据值越靠近0梯度越大，越远离0梯度越接近0，我们通过BN改变数据分布到0附近，从而解决梯度消失问题。
解决了ICS internal Covariate Shift. 由于训练过程中参数的变化，每一层的更新，导致上层的输入数据分布发生巨大变化，每一次高层需要重新学习去适应新的分布。神经网络就要学习新的分布，随着层数的加深，学习过程就变的愈加困难，要解决这个问题需要使用较低的学习率，由此又产生收敛速度慢，因此引入BN可以很有效的解决这个问题。数据分布变化很大
加速了模型收敛速度。
和对原始特征做归一化类似，BN使得每一维数据对结果的影响是相同的，由此就能加速模型的收敛速度。
引入了部分噪声。提升了泛化能力。 解析 我们发现 Normalization有效的最大一个原因在于方差的影响而不是均值。
特征Embedding上加入Normalization是否有效？ 从上面的实验中,我们发现,在特征Embedding层加入Normalization都是有效的,而且LayerNorm以及相关的变种是效果相对稳定以及最好的;
normalization公式： $\frac{x-\mu}{\theta}$, $\theta$是标准差。减去平均值除以标准差。
$$y = \frac { x - E [ x ] } { \sqrt {Var [ x ] + \xi } } * \gamma + \beta$$
问题： Batch Norm的描述： 该方法对每个mini-batch都进行normalize，会把mini-batch中的数据正规化到均值为0，标准差为1，同时还引入了两个可以学的参数，分别为scale和shift，让模型学习其适合的分布。 Normalization的优点： 四条： 梯度消失 加快网络训练 引入噪声，提高泛化 解决ICS（内部协变量偏移问题） 为什么引入噪声能提升泛化？加入噪声可以认为是在增加网络训练的难度，可以达到一定的正则效果。提升模型的鲁棒能力。 缺点：1. 当Batch小的时候，均值和方差不稳定 和归一化的不同 BatchNormalization层和正规化/归一化不同，BatchNormalization层是在mini-batch中计算均值方差，因此会带来一些较小的噪声，在神经网络中添加随机噪声可以带来正则化的效果。 batchnorm一般放在那里 一般放在线性层乘以权重参数后，放在激活函数前。 BN是为了让输入进入激活函数的非饱和区，所以这样效果更好。 为什么在做过正规化后，又要scale和shift呢 将分布拉扯到均值0，方差1附近，会丧失部分激活函数的非线性功能。比如sigmoid。所以添加scale和shift来偏移它们 normalization相当于把上一层作出的努力磨平了，神经网络 可以自己琢磨前面的正规化W有没有起到优化作用，没有的话就"反"正规化，抵消之前的正规化操作带来的影响。 给定一个张量[N, C, W]，batchnorm，和layernorm分别在哪层做归一化 画出图，认出哪个是一个样本，batchnorm就是对所有样本的一个维度做。layernorm对一个样本做 batch norm: N, W $\gamma, \beta$大小为c layer norm：C, W layernorm LayerNormalization是对一个样本进行处理，对一个样本的所有特征进行归一化，乍一看很没有道理，因为如果对身高体重和年龄一起求一个均值方差， 而此时我们对N个词进行Normalization等操作可以很好地反映句子的分布。 nlp使用layernorm的理由 bn不适用：1. 第一个位置是什么词都可以。2. 句子长度不一致 layernorm的优点：可以很好的反映句子的分布。</content></entry><entry><title>SimCLR a Simple Framework for Contrastive Learning of Visual Representations</title><url>http://next.lisenhui.cn/post/paperreading/simclr-a-simple-framework-for-contrastive-learning-of-visual-representations/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag></tags><content type="html"> 目录： 1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 1. 综述翻译 本文介绍了 SimCLR：一个用于视觉表示对比学习的简单框架。我们简化了最近提出的对比自监督学习算法，而不需要专门的架构或存储库。为了了解是什么使对比预测任务能够学习有用的表示，我们系统地研究了我们框架的主要组成部分。我们表明（1）数据增强的组合在定义有效的预测任务中起着关键作用，（2）在表示和对比损失之间引入可学习的非线性转换大大提高了学习表示的质量，以及（3）对比学习与监督学习相比，它受益于更大的批量和更多的训练步骤。通过结合这些发现，我们能够大大优于以前在 ImageNet 上进行自监督和半监督学习的方法。在 SimCLR 学习的自监督表示上训练的线性分类器实现了 76.5% 的 top-1 准确率，相对于之前的最新技术水平提高了 7%，与监督 ResNet-50 的性能相匹配。当仅对 1% 的标签进行微调时，我们实现了 85.8% 的 top-5 准确率，在标签数量减少 100 倍的情况下优于 AlexNet。
1.1 发表于 ICML 2020.
分析文章： https://mp.weixin.qq.com/s/tV5eSx73fzMovq0d7Jvu9Q https://zhuanlan.zhihu.com/p/258958247
2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释</content></entry><entry><title>Bagging和Boosting,adboost,xgboost</title><url>http://next.lisenhui.cn/post/study/deeplearning/bagging%E5%92%8Cboostingadaboostxgboost/</url><categories/><tags/><content type="html"> https://cloud.tencent.com/developer/news/393218
Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个性能更加强大的分类器，更准确的说这是一种分类算法的组装方法。即将弱分类器组装成强分类器的方法。
https://zhuanlan.zhihu.com/p/37730184 https://github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning/EnsembleLearning.md
集成学习 什么是集成学习算法？ 集成学习算法是一种优化手段或者策略，将多个较弱的模型集成模型组，一般的弱分类器可以是决策树，SVM，KNN等构成。其中的模型可以单独进行训练，并且它们的预测能以某种方式结合起来去做出一个总体预测。 集成学习主要有哪几种框架？ 集成学习从集成思想的架构分为Bagging，Boosting，Stacking三种。 简单介绍一下bagging，常用bagging算法有哪些？ Bagging 多次采样，训练多个分类器，集体投票，旨在减小方差， 基于数据随机重抽样的分类器构建方法。从训练集中进行子抽样组成每个基模型所需要的子训练集，对所有基模型预测的结果进行综合产生最终的预测结果。 算法流程： 输入为样本集D=(x1，y1)，(x2，y2)…(xm，ym)，弱学习器算法，弱分类器迭代次数T。 输出为最终的强分类器f(x) 对于t=1，2…T 对训练集进行第t次随机采样，共采集T次，得到包含T个样本的采样集Dt 用采样集Dt训练第t个弱学习器Gt(x) 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。 常用bagging算法：随机森林算法 简单介绍一下boosting，常用boosting算法有哪些？ Boosting 基分类器层层叠加，聚焦分错的样本，旨在减小偏差 训练过程为阶梯状，基模型按次序进行训练（实现上可以做到并行），基模型的训练集按照某种策略每次都进行一定的转化，每次都是提高前一次分错了的数据集的权值，最后对所有基模型预测的结果进行线性组合产生最终的预测结果。 算法流程： 给定初始训练数据，由此训练出第一个基学习器； 根据基学习器的表现对样本进行调整，在之前学习器做错的样本上投入更多关注； 用调整后的样本，训练下一个基学习器； 重复上述过程T次，将T个学习器加权结合。 常用boosting算法 Adaboost GBDT XGBoost 数学表达式： $f(x)=w_0+\sum_{m=1}^Mw_mϕ_m(x)$ 就是给弱分类器拟合权重，来训练强分类器。 简单介绍一下stacking 多次采样，训练多个分类器，将输出作为最后的输入特征 将训练好的所有基模型对训练集进行预测，第个i基模型对第i个训练样本的预测值将作为新的训练集中第i个样本的第i个特征值，**最后基于新的训练集进行训练。**同理，预测的过程也要先经过所有基模型的预测形成新的测试集，最后再对测试集进行预测。 stacking常见的使用方式： 由k-NN、随机森林和朴素贝叶斯基础分类器组成，它的预测结果由作为元分类器的逻回归组合。 使用场合 应对高方差：（就是提升泛化性） 使用正则化技术 使用可变重要性图表中的前n个特征。 bagging 常用的基分类器是什么？ 最常用的基分类器是决策树,原因: 决策树可以较为方便地将样本的权重整合到训练过程中，而不需要使用过采样的方法来调整样本权重 决策树的表达能力和泛化能力，可以通过调节树的层数来做折中。 数据样本的扰动对于决策树的影响较大，因此不同子样本集合生成的决策树基分类器随机性较大，这样的“不稳定学习器”更适合作为基分类器。此外，在决策树节点分裂的时候，随机地选择一个特征子集，从中找出最优分裂属性，很好地引入了随机性。 首先介绍Bootstraping，即自助法：它是一种有放回的抽样方法（可能抽到重复的样本）。
Bagging (bootstrap aggregating) Bagging即套袋法，其算法过程如下：
从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）
对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）
具体流程： 每次使用一份训练集训练一个模型，k 个训练集共得到 k 个基模型 利用这k个基模型对测试集进行预测，将k个预测结果进行聚合。
特点 可并行的集成方法。每个基模型可以分别、独立、互不影响地生成。
主要降低 Variance，对 Bias 无明显作用。因此，适用于 High Variance &amp; Low Bias 的模型。
Boosting 助推法 其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。
关于Boosting的两个核心问题：
2.1 在每一轮如何改变训练数据的权值或概率分布？
通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。
2.2 通过什么方式来组合弱分类器？
通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。
而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。
Bagging，Boosting二者之间的区别 Bagging和Boosting的区别：
1）样本选择上：
Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。
2）样例权重：
Bagging：使用均匀取样，每个样例的权重相等
Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。
3）预测函数：
Bagging：所有预测函数的权重相等。
Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。
4）并行计算：
Bagging：各个预测函数可以并行生成
Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。
5）其他： Bagging主要关注降低方差，而Boosting主要关注降低偏差 Boosting是一族算法，其主要目标为将弱学习器“提升”为强学习器，大部分Boosting算法都是根据前一个学习器的训练效果对样本分布进行调整，再根据新的样本分布训练下一个学习器，如此迭代M次，最后将一系列弱学习器组合成一个强学习器。 而这些Boosting算法的不同点则主要体现在每轮样本分布的调整方式上
总结 这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。
下面是将决策树与这些算法框架进行结合所得到的新的算法：
Bagging + 决策树 = 随机森林
AdaBoost + 决策树 = 提升树
Gradient Boosting + 决策树 = GBDT
AdaBoost https://github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning/Adaboost.md
adaboost 步骤 初始化训练样本的权值分布，每个训练样本的权值应该相等（如果一共有N个样本，则每个样本的权值为1N） 依次构造训练集并训练弱分类器。如果一个样本被准确分类，那么它的权值在下一个训练集中就会降低；相反，如果它被分类错误，那么它在下个训练集中的权值就会提高。权值更新过后的训练集会用于训练下一个分类器。 将训练好的弱分类器集成为一个强分类器，误差率小的弱分类器会在最终的强分类器里占据更大的权重，否则较小。 步骤算法 初始化权重 进行t=1&hellip;T轮迭代 选取当前误差最低的弱分类器h作为第t个分类器。计算弱分类器在分布D_t的分类误差e_t。 计算该分类器在最终分类器中所占权重。$\alpha_t = 1/2ln\frac{1-e_t}{e_t}$ 更新权值分布。 最终按照$\alpha_t$来组合所有的弱分类器。 https://www.cnblogs.com/willnote/p/6801496.html https://zhuanlan.zhihu.com/p/37358517
AdaBoost是一个具有里程碑意义的算法，因为其是第一个具有适应性的算法，即能适应弱学习器各自的训练误差率，这也是其名称的由来（Ada为Adaptive的简写）。
AdaBoost的具体流程为先对每个样本赋予相同的初始权重，每一轮学习器训练过后都会根据其表现对每个样本的权重进行调整，增加分错样本的权重，这样先前做错的样本在后续就能得到更多关注，按这样的过程重复训练出M个学习器，最后进行加权组合，
加法模型，第m次迭代中，前m-1个基学习器是固定的。 AdaBoost最后得到的强学习器是由一系列的弱学习器的线性组合，此即加法模型。这个是说训练第二个基学习器的时候，用的是第一个基学习器的结果。
这里有两个关键问题：
每轮训练过后如何调整样本权重 w ？ 如何确定最后各学习器的权重 $\alpha$ ？
=>
前一轮正确分类的样本减少权重。 线性组合后，准确率越高的学习器赋予较大的系数。
adaboost损失函数 adaboost 表达式 $f(x) = sign(\sum_m^M \alpha_mG_m(x))$
adaboost采用指数损失函数 e^{-yf(x)}求出\alpha_m G_m
AdaBoost采用指数损失的原因 这说明指数损失函数是分类任务原本0-1损失函数的一致性替代函数。由于这个替代函数是单调连续可微函数，因此用它代替0-1损失函数作为优化目标。
其他 Weight Trimming weight trimming不是正则化的方法，其主要目的是提高训练速度。在AdaBoost的每一轮基学习器训练过程中，只有小部分样本的权重较大，因而能产生较大的影响，而其他大部分权重小的样本则对训练影响甚微。
只用高权重样本进行训练。具体是设定一个阈值 (比如90%或99%)，再将所有样本按权重排序，计算权重的累积和，累积和大于阈值的权重 (样本) 被舍弃，不会用于训练。
注意每一轮训练完成后所有样本的权重依然会被重新计算，这意味着之前被舍弃的样本在之后的迭代中如果权重增加，可能会重新用于训练。
adboost优缺点： 优点： 能够基于泛化性能相当弱的的学习器构建出很强的集成，不容易发生过拟合。 缺点：
对异常样本比较敏感，异常样本在迭代过程中会获得较高的权值，影响最终学习器的性能表现。 只能处理采用指数损失函数的二分类学习任务。 这个优缺点和boost的优缺点没什么区别。
Gradient Boosting https://www.cnblogs.com/willnote/p/6801496.html https://zhuanlan.zhihu.com/p/38329631 https://zhuanlan.zhihu.com/p/81016622
adaboost缺点： 上一篇介绍了AdaBoost算法，在AdaBoost中每一轮基学习器训练过后都会更新样本权重，再训练下一个学习器，最后将所有的基学习器加权组合。AdaBoost使用的是指数损失，这个损失函数的缺点是对于异常点非常敏感，。因而通常在噪音比较多的数据集上表现不佳。Gradient Boosting在这方面进行了改进，使得可以使用任何损失函数(只要损失函数是连续可导的)，这样一些比较robust的损失函数就能得以应用，使模型抗噪音能力更强。 注意 Boosting的基本思想是通过某种方式使得每一轮基学习器在训练过程中更加关注上一轮学习错误的样本，区别在于是采用何种方式.AdaBoost采用的是增加上一轮学习错误样本的权重的策略，而在Gradient Boosting中则将负梯度作为上一轮基学习器犯错的衡量指标，在下一轮学习中通过拟合负梯度来纠正上一轮犯的错误。
对于分类问题。转换为回归问题进行计算。
为什么通过拟合负梯度就能纠正上一轮的错误了？Gradient Boosting的发明者给出的答案是：函数空间的梯度下降。
Gradient Boosting 采用和AdaBoost同样的加法模型，在第m次迭代中，前m-1个基学习器都是固定的。
负梯度也被称为“响应 (response)”或“伪残差 (pseudo residual)”，从名字可以看出是一个与残差接近的概念。直觉上来看，残差 r=y-f(x) 越大，表明前一轮学习器 f(x)的结果与真实值 y 相差较大，那么下一轮学习器通过拟合残差或负梯度，就能纠正之前的学习器犯错较大的地方。
就是说让h_m(x)去拟合f(x)和h_{m-1}(x)的差值。
在Gradient Boosting框架中，最常用的基学习器是决策树 (一般是CART)，二者结合就成了著名的梯度提升树 (Gradient Boosting Decision Tree, GBDT)。注意GBDT不论是用于回归还是分类，其基学习器 (即单颗决策树) 都是回归树，即使是分类问题也是将最后的预测值映射为概率。
算法步骤 初始化第一个学习器。根据loss function的选择。 for m=1 to M: 计算负梯度：y_i = - \frac{偏导L(y_i,f_{m-1}(x_i))}{偏导f_{m-1}(x_i)} 通过最小化平方误差，用基学习器来拟合y_i。(这个是回归树叶子决策时的公式。) 使用line search 确定一个p_m。用来最小化和标签的损失函数L 确定p_m，$p_m = argmin_p \sum_i^N L(y_i, f_{m-1}(x_i)+ph_m(x_i;w_m))$ 来使得L最小 p_m就是最终加法模型当中每个基学习器的权重。 f_m(x) = f_{m-1}(x) + p_mh_m(x; w_m) gbdt公式 依然采用前向分步算法，过程如下： $$f_{0}(x)=0 \
f_{m}(x)=f_{m-1}(x)+T(x;\Theta), m=1,2,&hellip;,M\
f_{M}(x)=\sum_{m=1}^{M}T(x;\Theta_{m})$$ 第一个式子首先定义初始提升树$f_{0}(x)=0$；之后第m步的模型即为第二个式子，其中$T(x;\Theta)$表示决策树，$\Theta$为决策树的参数；第三个式子表示GBDM的最终模型，其中M为树的个数。
在前向分步算法的第m步，给定当前模型$f_{m-1}(x)$，需求解 $$\hat{\Theta}_{m}=\underset{\Theta_{m}}{arg\ min}\sum_{i=1}^{N}L(y_{i},f_{m-1}(x_{i})+T(x_{i};\Theta_{m}))$$ 得到的$\hat{\Theta}_{m}$即为第m颗树的参数。当采用平方误差作为损失函数时： $$L(y,f(x))=(y-f(x))^{2}$$ 带入上式中，则其损失函数变为： $$L(y,f_{m-1}(x)+T(x;\Theta_{m}))\
=[y-f_{m-1}(x)-T(x;\Theta_{m})]^{2}\
=[r-T(x;\Theta_{m})]^{2}$$ 这里 $$r=y-f_{m-1}(x)$$ 是当前模型拟合数据的残差。所以，对于回归问题的提升树算法来说，只需简单地拟合当前模型的残差。即每一轮产生的残差作为下一轮回归树的输入，下一轮的回归树的目的就是尽可能的拟合这个输入残差。
可供选择的损失函数 常用的损失函数为平方损失 (squared loss)， 绝对值损失 (absolute loss)， Huber损失 (huber loss)
gbdt正则化： 设立一个步长。防止过快产生过拟合 Early stopping 在训练过程中不断检查在测试集上的表现，如果测试集上的准确率下降到一定阈值之下，则停止训练，选用当前的迭代次数M 限制树的复杂度 Subsampling 借用bootstrap的思想，每一轮训练时只使用一部分样本，不同点是这里的采样是无放回抽样， 面试相关 RF和GBDT的区别（bagging和boosting的区别） 相同点 都是由多棵树组成，最终的结果都是由多棵树一起决定。 不同点： 集成学习：RF属于Bagging思想，而GBDT是Boosting思想 偏差-方差权衡：RF不断的降低模型的方差，而GBDT不断的降低模型的偏差 并行性：RF的树可以并行生成，而GBDT只能顺序生成(需要等上一棵树完全生成) 最终结果：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合 数据敏感性：RF对异常值不敏感，而GBDT对异常值比较敏感 泛化能力：RF不易过拟合，而GBDT容易过拟合 比较LR和GBDT，说说什么情景下GBDT不如LR 先说说LR和GBDT的区别： LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程 GBDT 是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合； 高维稀疏特征的场景下，LR的效果一般会比GBDT好 带正则化的线性模型不容易对稀疏特征过拟合。 GBDT的拟合值残差为什么用负梯度代替，而不是直接拟合残差 使用残差拟合只是考虑到损失函数为平方损失的特殊情况，负梯度是更加广义上的拟合项，更具普适性。 代价函数除了loss还有正则项，梯度的本质也是一种方向导数，综合了各个方向（参数）的变化，选择了一个总是最优（下降最快）的方向； 最后目标函数可表达为由梯度构成，所以说成是拟合梯度，也好像不是不行 总结： 通过拟合上一个基学习器的负梯度来训练当前的基本学习器。负梯度就表示了哪些地方需要加大权重。常用损失函数为平方损失。 然后为每一个基学习器赋予一个权重 都是回归树。 分类问题就用softmax 而adaboost是为错误较大的样本赋予更多的权重。 XGBOOST https://zhuanlan.zhihu.com/p/46683728 https://cloud.tencent.com/developer/article/1500914 https://github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning/XGBoost.md
公式推导：https://zhuanlan.zhihu.com/p/92837676
xgboost面经 https://mp.weixin.qq.com/s?__biz=MzI1MzY0MzE4Mg==&amp;mid=2247485159&amp;idx=1&amp;sn=d429aac8370ca5127e1e786995d4e8ec&amp;chksm=e9d01626dea79f30043ab80652c4a859760c1ebc0d602e58e13490bf525ad7608a9610495b3d&amp;scene=21#wechat_redirect
eXtreme Gradient Boosting
注意：
loss函数和F函数不一样。F就是机器学习学习出来的值。loss是一个函数用来求最小的值。 排序对单个特征进行排序。因为是二分类，所以可以用这种递增的方式进行求和。找出每个特征的最佳分裂。找出所有特征的最好分裂点（分裂后增益最大的特征及特征值） 排序就是将特征排好序，不用每次都排序。 和GBDT的不同 基分类器：XGBoost的基分类器不仅支持CART决策树，还支持线性分类器，此时XGBoost相当于带L1和L2正则化项的LR回归（分类问题）或者线性回归（回归问题）。 导数信息：XGBoost对损失函数做了二阶泰勒展开，可以更为精准的逼近真实的损失函数，GBDT只用了一阶导数信息，并且XGBoost还支持自定义损失函数，只要损失函数一阶、二阶可导。 正则项：XGBoost的目标函数加了正则项， 相当于预剪枝，使得学习出来的模型更加不容易过拟合。 列抽样：XGBoost支持列采样，与随机森林类似，用于防止过拟合。 缺失值处理：对树中的每个非叶子结点，XGBoost可以自动学习出它的默认分裂方向。如果某个样本该特征值缺失，会将其划入默认分支。 并行化：注意不是树维度的并行，而是特征维度的并行。XGBoost预先将每个特征按特征值排好序，存储为块结构，分裂结点时可以采用多线程并行查找每个特征的最佳分割点，极大提升训练速度。 可扩展性：损失函数支持自定义，只需要新的损失函数二阶可导。还可以支持其他弱分类器。如LR。
xgboost 并行训练 每个特征按特征值对样本进行预排序，并存储为block结构，在后面查找特征分割点时可以重复使用 而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。 xgboost 快 分块并行 block 处理优化 CPU cache 命中优化。我不太懂。但我知道这个。 xgboost 如何处理过拟合 模型方面： 增加正则 设置树深度 训练方面 设置增益阈值 设置样本权重和的阈值 设置步长 数据方面： 列抽样 子抽样 调参:
直接控制模型复杂度：包括max_depth，min_child_weight，gamma 等参数 控制随机性：从而使得模型在训练时对于噪音不敏感。包括subsample，colsample_by树 减少learning rate，增加estimator参数。还有就是直接减小learning rate，但需要同时增加estimator 参数。 xgboost 处理缺失值 XGBoost 在构建树的节点过程中只考虑非缺失值的数据遍历。而为每个节点增加了一个缺省方向，分别枚举特征缺省的样本归为左右分支后的增益，选择增益最大的枚举项即为最优缺省方向 如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子结点。 树模型对缺失值的敏感度低，大部分时候可以在数据缺失时时使用。原因就是：一**棵树中每个结点在分裂时，寻找的是某个特征的最佳分裂点（特征值），**完全可以不考虑存在特征值缺失的样本 XGBoost​的优缺点 优点
精度更高： GBDT 只用到一阶泰勒展开，而 XGBoost 对损失函数进行了二阶泰勒展开。XGBoost 引入二阶导一方面是为了增加精度，另一方面也是为了能够自定义损失函数，二阶泰勒展开可以近似大量损失函数 灵活性更强： GBDT 以 CART 作为基分类器，XGBoost 不仅支持 CART 还支持线性分类器，使用线性分类器的 XGBoost 相当于带 和 正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。 正则化 **Shrinkage（缩减）**相当于学习速率。XGBoost 在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。 列抽样 缺失值处理： XGBoost工具支持并行 可并行的近似算法： 所以XGBoost还提出了一种可并行的近似算法，用于高效地生成候选的分割点。常数个数的候选位置作为候选分裂点。 XGBoost使用二阶泰勒展开的目的和优势 XGBoost是以MSE为基础推导出来的，在MSE的情况下，XGBoost的目标函数展开就是一阶项+二阶项的形式。只要二阶可导即可，增强了模型的扩展性。而不需要对损失函数做出过高的要求。 二阶信息能够让梯度收敛的更快，拟牛顿法比SGD收敛更快，一阶信息描述梯度变化方向，二阶信息可以描述梯度变化方向是如何变化的。所以二阶信息可以对一阶信息做出指引 xgboost 损失函数 $Obj^t = \sum_i^n[g_if_t(x_i) + 1/2h_i(f_t(x_i))^2] + \omega(cart)$ \omega是正则项。
体来看，XGBoost 在原理方面的改进主要就是在损失函数上作文章。 一是在原损失函数的基础上添加了正则化项产生了新的目标函数，这类似于对每棵树进行了剪枝并限制了叶结点上的分数来防止过拟合。 二是对目标函数进行二阶泰勒展开，以类似牛顿法的方式来进行优化（事实上早在 Friedman, J., Hastie, T. and Tibshirani, R., 1999 中就已有类似方案，即利用二阶导信息来最小化目标函数，陈天奇在论文中也提到了这一点）。 XGBoost 之所以快的一大原因是在工程上实现了Column Block 方法，使得并行训练成为了可能。
由于已经预先保存为block 结构，所以在对叶结点进行分裂时，每个特征的增益计算就可以开多线程进行，训练速度也由此提升了很多。而且这种 block 结构也支持列抽样，只要每次从所有 block 特征中选择一个子集作为候选分裂特征就可以了，据我的使用经验，列抽样大部分时候都比行抽样的效果好。
最后总结一下 XGBoost 与传统 GBDT 的不同之处：
传统 GBDT 在优化时只用到一阶导数信息，XGBoost 则对目标函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。另外 XGBoost 工具支持自定义损失函数，只要函数可一阶和二阶求导。 XGBoost 在损失函数中加入了正则化项，用于控制模型的复杂度，防止过拟合，从而提高模型的泛化能力。 传统 GBDT 采用的是均方误差作为内部分裂的增益计算指标（因为用的都是回归树），而 XGBoost 使用的是经过优化推导后的式子，即式 [公式] 。 XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算量，这也是 XGBoost 异于传统 GBDT 的一个特性。 XGBoost 添加了对稀疏数据的支持，**在计算分裂增益时不会考虑带有缺失值的样本，这样就减少了时间开销。**在分裂点确定了之后，将带有缺失值的样本分别放在左子树和右子树，比较两者分裂增益，选择增益较大的那一边作为默认分裂方向。 并行化处理：由于 Boosting 本身的特性，无法像随机森林那样树与树之间的并行化。**XGBoost 的并行主要体现在特征粒度上，在对结点进行分裂时，由于已预先对特征排序并保存为block 结构，每个特征的增益计算就可以开多线程进行，**极大提升了训练速度。 传统 GBDT 在损失不再减少时会停止分裂，这是一种预剪枝的贪心策略，容易欠拟合。XGBoost采用的是后剪枝的策略，先分裂到指定的最大深度 (max_depth) 再进行剪枝。而且和一般的后剪枝不同， XGBoost 的后剪枝是不需要验证集的。 不过我并不觉得这是“纯粹”的后剪枝，因为一般还是要预先限制最大深度的呵呵。 ​
说了这么多 XGBoost 的优点，其当然也有不完美之处，因为要在训练之前先对每个特征进行预排序并将结果存储起来，对于空间消耗较大。另外虽然相比传统的 GBDT 速度是快了很多，但和后来的 LightGBM 比起来还是慢了不少，不知以后还会不会出现更加快的 Boosting 实现。
xgboost优点：
传统 GBDT 在优化时只用到一阶导数信息，XGBoost 则对目标函数进行了二阶泰勒展开，同时用到了一阶和二阶导数 XGBoost 在损失函数中加入了正则化项，用于控制模型的复杂度，防止过拟合， XGBoost 的并行主要体现在特征粒度上，在对结点进行分裂时，由于已预先对特征排序并保存为block 结构，每个特征的增益计算就可以开多线程进行，极大提升了训练速度。 空间消耗很大。
lightgbm https://zhuanlan.zhihu.com/p/99069186
LightGBM（Light Gradient Boosting Machine）是一个实现GBDT算法的框架，支持高效率的并行训练，并且具有更快的训练速度、更低的内存消耗、更好的准确率、支持分布式可以快速处理海量数据等优点。
XGBoost的缺点 它是基于预排序方法的决策树算法。这种构建决策树的算法基本思想是：首先，对所有特征都按照特征的数值进行预排序。
首先，空间消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如，为了后续快速的计算分割点，保存了排序后的索引），这就需要消耗训练数据两倍的内存。 其次，时间上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。 最后，对cache优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。 LightGBM的优化 基于Histogram的决策树算法。 单边梯度采样 Gradient-based One-Side Sampling(GOSS)：使用GOSS可以减少大量只具有小梯度的数据实例，这样在计算信息增益的时候只利用剩下的具有高梯度的数据就可以了，相比XGBoost遍历所有特征值节省了不少时间和空间上的开销。 互斥特征捆绑 Exclusive Feature Bundling(EFB)：使用EFB可以将许多互斥的特征绑定为一个特征，这样达到了降维的目的。 带深度限制的Leaf-wise的叶子生长策略：大多数GBDT工具使用低效的按层生长 (level-wise) 的决策树生长策略，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销。实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。LightGBM使用了带有深度限制的按叶子生长 (leaf-wise) 算法。 直接支持类别特征(Categorical Feature) 支持高效并行 Cache命中率优化 优点： 速度更快:
速度更快。比如使用直方图减少内存消耗。降低时间复杂度。 使用单边梯度算法，减少了大量的计算。GOSS在进行数据采样的时候只保留了梯度较大的数据 基于leaf-wise算法的增长策略。减少了很多不必要的计算量 采用优化后的特征并行，数据并行方法加速计算。还可以使用投票并行 对缓存进行了优化。 内存更小： 使用直方图将特征转为bin，减少了内存消耗 缺点：
可能决策树较深，产生过拟合。在leaf_wise熵增加了一个最大深度限制，防止过拟合 不断降低偏差，所以对噪点铭感。 在寻找最优解时，依据的是最优切分变量，没有将最优解是全部特征的综合这一理念考虑进去； xgboost和lgbm的对比 树生长策略 XGB采用level-wise的分裂策略：XGB对每一层所有节点做无差别分裂。这一层的节点都分裂。 LGB采用leaf-wise的分裂策略：Leaf-wise是在所有叶子节点中选取分裂收益最大的节点进行的，但是很容易出现过拟合问题，所以需要对最大深度做限制 。 分割点查找算法 XGB使用特征预排序算法，LGB使用基于直方图的切分点算法， 减少内存占用， 计算效率提高 直方图算法 XGB 在每一层都动态构建直方图 LGB中对每个特征都有一个直方图， 支持离散变量 XGB无法直接输入类别型变量因此需要事先对类别型变量进行编码（例如独热编码）， LGB可以直接处理类别型变量。 缓存命中率 XGB用block结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的，这将导致非连续的内存访问， LGB是基于直方图分裂特征的，梯度信息都存储在一个个bin中，所以访问梯度是连续的，缓存命中率高。 并行策略-特征并行 XGB每个worker节点中仅有部分的列数据，也就是垂直切分，每个worker寻找局部最佳切分点，worker之间相互通信 LGB特征并行的前提是每个worker留有一份完整的数据集，但是每个worker仅在特征子集上进行最佳切分点的寻找 并行策略-数据并行 LGB中先对数据水平切分，每个worker上的数据先建立起局部的直方图，然后合并成全局的直方图，采用直方图相减的方式，先计算样本量少的节点的样本索引，然后直接相减得到另一子节点的样本索引，这个直方图算法使得worker间的通信成本降低一倍，因为只用通信以此样本量少的节点 最后两个了解不多。只知道通过并行策略，可以减少线程之间的通信成本。 重学各个公式 adaboost: 西瓜书 https://zhuanlan.zhihu.com/p/274517564 https://zhuanlan.zhihu.com/p/105515064
gbdt： https://zhuanlan.zhihu.com/p/107751279 https://zhuanlan.zhihu.com/p/38329631</content></entry><entry><title>随即森林和决策树</title><url>http://next.lisenhui.cn/post/study/deeplearning/jcs%E9%9A%8F%E5%8D%B3%E6%A3%AE%E6%9E%97%E5%92%8C%E5%86%B3%E7%AD%96%E6%A0%91/</url><categories/><tags/><content type="html"> https://easyai.tech/ai-definition/decision-tree/ https://zhuanlan.zhihu.com/p/85731206 决策树更详细点。 https://blog.csdn.net/zjsghww/article/details/51638126 c4.5
什么是决策树？ 决策树算法采用树形结构，使用层层推理来实现最终的分类。决策树由下面几种元素构成：
根节点：包含样本的全集 内部节点：对应特征属性测试 叶节点：代表决策的结果
预测时，在树的内部节点处用某一属性值进行判断，根据判断结果决定进入哪个分支节点，直到到达叶节点处，得到分类结果。
决策树学习的 3 个步骤 特征选择。 特征选择决定了使用哪些特征来做判断。在训练数据集中，每个样本的属性可能有很多个，不同属性的作用有大有小。因而特征选择的作用就是筛选出跟分类结果相关性较高的特征，也就是分类能力较强的特征。在特征选择中通常使用的准则是：信息增益。
决策树生成 选择好特征后，就从根节点触发，对节点计算所有特征的信息增益，选择信息增益最大的特征作为节点特征，根据该特征的不同取值建立子节点；对每个子节点使用相同的方式生成新的子节点，直到信息增益很小或者没有特征可以选择为止。
决策树剪枝 剪枝的主要目的是对抗「过拟合」，通过主动去掉部分分支来降低过拟合的风险。
3 种典型的决策树算法 ID3 算法 ID3 是最早提出的决策树算法，他就是利用信息增益来选择特征的。 ID3 计算信息增益来对特征进行划分。信息增益基于信息熵。如果选取了这个特征，针对分类的信息熵减小了，则认为这个信息增益增大了。
ID3 步骤：
初始化特征集合、数据集合。 计算信息熵和所有特征的条件熵（在选取这个特征进行分类后，每一个子树的信息熵）。信息增益=信息熵-条件熵。选取特征。 信息熵：熵越大，信息越多熵越大。 $H(x) = -\sum plogp$ -plogp的图像，想象成一个0-1向上凸的半圆。 熵越大，混乱程度越高。最典型的，二个类个占一半。则混乱度max。 信息增益=信息熵-选择这个特征后的条件熵。 根据信息增益对子节点进行划分 重复2-3直至子集包含纯的标签。则为分支叶子节点。 ID3弊端：
只能处理离散值。 更倾向于选择特征种类多的特征。特征种类多的特征，信息增益会更大。特征种类多，分类就多，每个分类数据就越少，分类标签的纯度就容易越高。信息熵显然会低，信息增益会更大。 无法处理缺失值 没有剪枝，容易过拟合。 c4.5 我自己总结一下。c4.5是为了解决ID3的一些弊端：
对于连续值，进行离散化后排序，取相邻值进行平均并进行分节点。将连续特征离散化，假设 n 个样本的连续特征 A 有 m 个取值，C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点； 选择使用信息增益率来代替信息增益。信息增益率=信息增益/信息内在信息。信息内在信息是指信息本身所拥有的信息。信息熵越大，信息越不确定。所以信息增益率越低。越不选它。 当叶子节点是纯的时候，就停止划分叶子节点。 引入信息增益率，可以减少对多分类特征的偏爱。 对于缺失值。 在缺失特征下选择样本？使用没有缺失的样本子集 对于缺失特征的样本，按不同概率划分到不同的子节点。 剪枝采用悲观后剪枝。 后剪枝： 训练完成后，进行剪枝，如果这个减去这个子树，效果保持或者不下降，则可以替换这个子树。后剪枝欠拟合风险小很多，泛化性能会优于预剪枝。但是时间开销大。 悲观后剪枝更麻烦点，优势就是只使用训练集。不使用测试集。 预剪枝：三种 节点数据比例低于某一阈值 节点特征都已经分裂。 节点划分前准确率比划分后准确率高。 预剪枝可以减少过拟合，减少训练时间。但容易带来欠拟合。 C4.5 算法最大的特点是克服了 ID3 对特征数目的偏重这一缺点
C4.5 缺点：
c4.5包括ID3用的都是多叉树。 c4.5只能用于分类。 对于连续值，需要排序。对内存需求很大。数据不足以放进内存的时候，无法进行运行。 熵模型有大量的耗时的对数运算。 CART CART针对前面的缺点的提升：
采用二叉树运算快 正是因为CART树是二叉树，所以对于样本的有N>=3个取值的离散特征的处理时也只能有两个分支，这就要通过组合人为的创建二取值序列并取GiniGain最小者作为树分叉决策点 所以cart是不停的二分离散特征。若没有停止准则，则树会一直增长。 叶子节点的标签也可能有多个标签分类。 CART可以用于分类或者回归 CART使用Gini作为变量的不纯度量。减少对数运算。CART采用Gini系数来代替熵模型。基尼指数约等于熵模型的一阶泰勒展开。公式Gini = 1 - \sum p^2。 p为选择这个节点后的，标签选择概率的分类。 Gini越小，不纯度越低。 CART 采用代理测试来估计缺失值，而 C4.5 以不同概率划分到不同节点中； 如何在特征值缺失的情况下进行划分特征的选择？ 一开始只使用没有缺失的数据。后来如果缺失是20％，则惩罚20%的权重 选定该划分特征，模型对于缺失该特征值的样本该进行怎样处理？ CART 算法的机制是为树的每个节点都找到代理分裂器。无论在训练数据上得到的树是否有缺失值都会这样做。在代理分裂器中，特征的分值必须超过默认规则的性能才有资格作为代理。 代理分裂： 一个分裂使用另一个预测属性。满足与主分裂最相似，切有着正的关联一簇额度量。来对缺失值分到哪一类进行判别。 CART 采用“基于代价复杂度剪枝”方法进行剪枝，而 C4.5 采用悲观剪枝方法。 采用一种“基于代价复杂度的剪枝”方法进行后剪枝，这种方法会生成一系列树，每个树都是通过将前面的树的某个或某些子树替换成一个叶节点而得到的，这种方法需要使用一个单独的测试数据集来评估所有的树，根据它们在测试数据集熵的分类性能选出最佳的树。 在所有的子树中，找出那些使用了较多节点，却使错误率下降最低的子树，把这些子树剪裁掉。 对于连续值，与C4.5思想相同，都是将连续的特征离散化。区别在选择划分点时，C4.5是信息增益率，CART是基尼系数。。注意的是，与ID3、C4.5处理离散属性不同的是，如果当前节点为连续属性，则该属性在后面还可以参与子节点的产生选择过程。 回归模型中，采用均方差。 预测： 回归采用叶子均值或者中位数 分类，采用概率最大的类别。 运行过程：
分裂：分裂过程是一个二叉递归划分过程，其输入和预测特征既可以是连续型的也可以是离散型的，CART 没有停止准则，会一直生长下去； 一般有预剪枝的停止条件 控制深度、当前的节点数、分裂对测试集的准确度提升大小 限制树的高度，可以利用交叉验证选择 利用分类指标，如果下一次切分没有降低误差，则停止切分 限制树的节点个数，比如某个节点小于100个样本，停止对该节点切分 剪枝：采用代价复杂度剪枝，从最大树开始，每次选择训练数据熵对整体性能贡献最小的那个分裂节点作为下一个剪枝对象，直到只剩下根节点。CART 会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树； 树选择：用单独的测试集评估每棵剪枝树的预测性能（也可以用交叉验证）。 优点: 1.
CART（Classification and Regression Tree）
这种算法即可以用于分类，也可以用于回归问题。CART 算法使用了基尼系数取代了信息熵模型。 CART是“Classification and Regression Tree”的缩写。
cart小总结 cart既是分类也是回归树 cart分类时，采用基尼值。是回归时，采用最小方差作为分类依据。 回归时左右两子树的MSE和最小即为划分点 预测时，回归树为均值，分类树为概率最大的类别。 依然是二叉树。 基尼值:GINI = 1-\sum p^2. 节点不纯（0,1标签相同，则gini最大为1/2.如果只有一个类别就是0。选择jini值作为分裂依据。
方差：方差越大，表示该节点的数据越分散，预测的效果就越差。如果一个节点的所有数据都相同，那么方差就为0,此时可以很肯定得认为该节点的输出值；如果节点的数据相差很大，那么输出的值有很大的可能与实际值相差较大。因此，无论是分类树还是回归树，CART都要选择使子节点的GIN值或者回归方差最小的属性作为分裂的方案。
https://www.bilibili.com/video/BV1Pk4y1C7m9
类别不平衡 CART 的一大优势在于：无论训练数据集有多失衡，它都可以将其子冻消除不需要建模人员采取其他操作。 这是进行剪枝的技术。通过子节点数据的分类核根节点数据的分类，就可以变相的对节点特征进行加权。确保每个类别的概率是1/k。 先验影响的是每个节点的类别赋值和树生长过程中分裂的选择。
决策树的优缺点 优点 决策树易于理解和解释，可以可视化分析，容易提取出规则； 可以同时处理标称型和数值型数据； 比较适合处理有缺失属性的样本； 能够处理不相关的特征； 测试数据集时，运行速度比较快； 在相对短的时间内能够对大型数据源做出可行且效果良好的结果。
缺点 容易发生过拟合（随机森林可以很大程度上减少过拟合）； 容易忽略数据集中属性的相互关联； 对于那些各类别样本数量不一致的数据，在决策树中，进行属性划分时，不同的判定准则会带来不同的属性选择倾向；信息增益准则对可取数目较多的属性有所偏好（典型代表ID3算法），而增益率准则（CART）则对可取数目较少的属性有所偏好，但CART进行属性划分时候不再简单地直接利用增益率尽心划分，而是采用一种启发式规则）（只要是使用了信息增益，都有这个缺点，如RF）。 ID3算法计算信息增益时结果偏向数值比较多的特征。
除了之前列出来的划分标准、剪枝策略、连续值确实值处理方式等之外，我再介绍一些其他差异：
划分标准的差异：ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。 使用场景的差异：ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快； 样本数据的差异：ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ； 样本特征的差异：ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征； 剪枝策略的差异：ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。 总结 一些问题记录
ID3算法—>C4.5算法—> CART算法 ID3 ID3算法没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。 ID3算法采用信息增益大的特征优先建立决策树的节点，偏向于取值比较多的特征 ID3算法对于缺失值的情况没有做考虑 ID3算法没有考虑过拟合的问题
C4.5在ID3算法上面的改进 连续的特征离散化 使用信息增益比 通过剪枝算法解决过拟合
C4.5的不足： C4.5生成的是多叉树 C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。 C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算
CART算法 可以做回归，也可以做分类， 使用基尼系数来代替信息增益比 CART分类树离散值的处理问题，采用的思路是不停的二分离散特征。
决策树的目标函数是什么？ $C_α(T)=\sum_{t=1}^{|T|}N_tH_t(T)+a|T|$ $H_t(T)=−\sum_k\frac{N_{tk}}{N_t}log\frac{N_{tk}}{Nt}$
$H_t$是经验熵。就是信息熵、香农熵。当随机变量的取值越多，状态数越多，混乱度越高，纯度就学小。
防止过拟合 做bagging 对于决策树进行约束：根据情况来选择或组合 设置每个叶子节点的最小样本数，可以避免某个特征类别只适用于极少数的样本。
设置每个节点的最小样本数，从根节点开始避免过度拟合。
设置树的最大深度，避免无限往下划分。
设置叶子节点的最大数量，避免出现无限多次划分类别。
设置评估分割数据是的最大特征数量，避免每次都考虑所有特征为求“最佳”，而采取随机选择的方式避免过度拟合。
预剪枝(提前停止)：控制深度、当前的节点数、分裂对测试集的准确度提升大小 限制树的高度，可以利用交叉验证选择 利用分类指标，如果下一次切分没有降低误差，则停止切分 限制树的节点个数，比如某个节点小于100个样本，停止对该节点切分
后剪枝(自底而上)：生成决策树、交叉验证剪枝：子树删除，节点代替子树、测试集准确率判断决定剪枝 在决策树构建完成之后，根据加上正则项的结构风险最小化自下向上进行的剪枝操作. 剪枝的目的就是防止过拟合，是模型在测试数据上变现良好，更加鲁棒。
如果特征很多，决策树中最后没有用到的特征一定是无用吗？ 不是无用的，从两个角度考虑：
特征替代性，如果可以已经使用的特征A和特征B可以提点特征C，特征C可能就没有被使用，但是如果把特征C单独拿出来进行训练，依然有效
决策树的每一条路径就是计算条件概率的条件，前面的条件如果包含了后面的条件，只是这个条件在这棵树中是无用的，如果把这个条件拿出来也是可以帮助分析数据.
优缺点 优点:
简单直观，生成的决策树很直观。 基本不需要预处理，不需要提前归一化，处理缺失值。 既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。 可以处理多维度输出的分类问题。 相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释 可以交叉验证的剪枝来选择模型，从而提高泛化能力。 对于异常点的容错能力好，健壮性高。 用白盒模型，可清洗观察每个步骤，对大数据量的处理性能较好，更贴近人类思维。
简单直接，理论性强，有很好的解释性 可以处理离散值、连续值 对异常值、缺失值的容错能力好 测试数据集时，运行速度比较快； 缺点:
决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。 决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。 寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。 有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。 如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。
容易过拟合。 容易陷入局部最优。因为最优决策树是NP难问题。 不能处理较为复杂的关系，忽略特征的相互关联，比如异或。 不同的判定准则会带来不同的属性选择倾向 随机森林 随机森林属于bagging
构造随机森林的 4 个步骤 一个样本容量为N的样本，有放回的抽取N次，每次抽取1个，最终形成了N个样本。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。 当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m &laquo; M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。 决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。 按照步骤1~3建立大量的决策树，这样就构成了随机森林了。</content></entry><entry><title>比赛学习网址或工具</title><url>http://next.lisenhui.cn/post/study/kaggle/kaggle%E6%AF%94%E8%B5%9B%E5%AD%A6%E4%B9%A0%E7%BD%91%E5%9D%80%E6%88%96%E5%B7%A5%E5%85%B7/</url><categories/><tags/><content type="html"> 怎么学习？ Kaggle怎么学？比赛驱动。技术驱动。 我们按照这样的路径： 三方面驱动。
比赛驱动。 然后记录比赛中的相关技术，去学习相关技术。 并学习相关的writeup deadline 网址 https://iphysresearch.github.io/DataSciComp/hostby?sub=PF,AC
比赛网站： https://aistudio.baidu.com/aistudio/index
常用工具。 自动化训练工具 AutoX. 自动化EDA工具 pandas_profiling EDA可视化工具 plotly 待学习的kaggle 恶意软件的 https://www.kaggle.com/c/malware-classification/
泰坦尼克。</content></entry><entry><title>_NLP案例目录</title><url>http://next.lisenhui.cn/post/study/kaggle/kaggle%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90/nlp%E7%9B%B8%E5%85%B3%E6%AF%94%E8%B5%9B/_nlp%E6%A1%88%E4%BE%8B%E7%9B%AE%E5%BD%95/</url><categories/><tags/><content type="html"> NLP比赛目录 https://github.com/zhpmatrix/nlp-competitions-list-review
适合初学者学习的NLP开源项目有哪些？ - 武博文的回答 - 知乎 https://www.zhihu.com/question/264352009/answer/386628568</content></entry><entry><title>NLP总结1</title><url>http://next.lisenhui.cn/post/study/kaggle/kaggle%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90/nlp%E7%9B%B8%E5%85%B3%E6%AF%94%E8%B5%9B/nlp%E6%80%BB%E7%BB%931/</url><categories/><tags/><content type="html"> 新手入门 Kaggle NLP类比赛总结 - jiazhuamh的文章 - 知乎 https://zhuanlan.zhihu.com/p/109992475</content></entry><entry><title>Shap学习笔记</title><url>http://next.lisenhui.cn/post/study/python%E5%BA%93/shap%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url><categories><category>学习</category></categories><tags><tag>python相关库学习</tag><tag>机器学习相关库</tag></tags><content type="html"> api 随笔记 shap.force_plot这个函数应该是打印对于这个样本，哪些权重做出了贡献。</content></entry><entry><title>4 机器学习基础</title><url>http://next.lisenhui.cn/post/study/deeplearning/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/</url><categories/><tags/><content type="html"> 深度学习是机器学习的一个特定分支。我们要想充分理解深度学习，必须对机器 学习的基本原理有深刻的理解。
机器学习本质上属于应用统计学，更多地关注于如何用 计算机统计地估计复杂函数，不太关注为这些函数提供置信区间。因此我们会探讨 两种统计学的主要方法：频率派估计和贝叶斯推断。
学习算法 我们所谓的 ‘‘学习’’ 是什 么意思呢？Mitchell (1997) 提供了一个简洁的定义：‘‘对于某类任务 T 和性能度量 P，一个计算机程序被认为可以从经验 E中学习是指，通过经验 E 改进后，它在任务 T 上由性能度量 P 衡量的性能有所提升。 样本是 指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的特征（feature）的集合 一些非常常见的机器学习任务列举如下 分类 学习算法通常会返回一个函数 f : Rn → {1, . . . , k}。当 y = f(x) 时，模型将向量 x 所代表的输入分类到数字码 y 所代表的类别。 输入缺失分类 学习算法只需要定义一个从输入向量映射到输出类别的函数。当一些输入可能丢失时，学习算法必须学习一组函数，而不是 单个分类函数。 有 效地定义这样一个大集合函数的方法是学习所有相关变量的概率分布，然后通 过边缘化缺失变量来解决分类任务。使用 n 个输入变量，我们现在可以获得每 个可能的缺失输入集合所需的所有 2n 个不同的分类函数，但是计算机程序仅需要学习一个描述联合概率分布的函数。 回归 在这类任务中，计算机程序需要对给定输入预测数值。学习算法需要输出函数 f : Rn → R。除了返回结果的形式不一样外，这类 问题和分类问题是很像的 转录 机器学习系统观测一些相对非结构化表示的数据，并转 录信息为离散的文本形式。 深度学习是现代语音识别系统的重要组成部分 机器翻译 在机器翻译任务中，输入是一种语言的符号序列，计算机程序必须 将其转化成另一种语言的符号序列 结构化输出 结构化输出任务的输出是向量或者其他包含多个值的数据结构， 并且构成输出的这些不同元素间具有重要关系 这类任务被称为结构化输出任务是因为输出值之 间内部紧密相关。例如，为图片添加标题的程序输出的单词必须组合成一个通顺的句子。 异常检测 计算机程序在一组事件或对象中筛选，并标记不正 常或非典型的个体。 异常检测任务的一个示例是信用卡欺诈检测。 合成和采样 机器学习程序生成一些和训练数据相似的新样本。 通过机器学习，合成和采样可能在媒体应用中非常有用 例如，视频游戏可以自动生成大型物体或风景 的纹理，而不是让艺术家手动标记每个像素 (Luo et al., 2013)。 我们提供书写的句子，要求程序输出这个句子语音的音频 波形。这是一类结构化输出任务，但是多了每个输入并非只有一个正确输出的 条件，并且我们明确希望输出有很多变化，这可以使结果看上去更加自然和真实 缺失值填补： 在这类任务中，机器学习算法给定一个新样本 x ∈ Rn，x 中某些 元素 xi 缺失。算法必须填补这些缺失值。 去噪： 干净样本 x ∈ Rn 经过未知损 坏过程后得到的损坏样本 ˜x ∈ Rn。 密度估计或概率质量函数估计： 在密度估计问题中，机器学习算法学习函数 pmodel : Rn → R，其中 pmodel(x) 可以解释成样本采样空间的概率密度函数（如果 x 是连续的）或者概率质量函数（如果 x 是离散的）。 算法必须知道什么情况下样本聚集出现。例如，如果我们通过密度估计得到了概率分布 p(x)， 我们可以用该分布解决缺失值填补任务。如果 xi 的值是缺失的，但是其他的变量值 x−i 已知，那么我们可以得到条件概率分布 p(xi | x−i)。因为在很多情况下 p(x) 是难以计算的。 性能度量p 性能度量 P。我们通常度量模型的准确率（accuracy）。们也可以通过错误率（error rate）得到相同的信息 我们通常把错 误率称为 0 − 1损失的期望。在一个特定的样本上，如果结果是对的，那么 0 − 1损失是 0；否则是 1。 但是对于密度估计这类任务而言，度量准确率，错误率或者其他 类型的 0 − 1损失是没有意义的。最常用的方法是输出模型在一些样本上概率对 数的平均值 在某些情况下，这是因为很难确定应该度量什么。这些设计的选择取决 于应用.在这些情况下，我们必须设计 一个仍然对应于设计对象的替代标准，或者设计一个理想标准的良好近似。 经验E 机器学习算法可以大致分类为无监督（unsupervised）算法和监督（supervised）算法。 本书中的大部分学习算法可以被理解为在整个数据集（dataset）上获取经验。有时我们也将样本称为数 据点（data point） 无监督学习算法。我们通常要学习生成数据集的整个概率分布，显式地，比如密度估计，或是隐式地，比如合成或去噪。还有一些其他类型的无监督学习任务，例如聚类，将数据集分成相似样本的集合。 无监督学习算法的三个特性： 没有标签 没有目的 无法量化效果。 监督学习算法。训练含有很多特征的数据集，不过数据集中的样本都有一个标签（label）或目标（target）。 大致说来，无监督学习涉及到观察随机向量 x 的好几个样本，试图显式或隐式 地学习出概率分布 p(x)。而监督学习包含观察随 机向量 x 及其相关联的值或向量 y，然后从 x 预测 y，通常是估计 p(y | x) 无监督学习和监督学习不是严格定义的术语。例如，概率的链式法则表明对于向量 x ∈ Rn， 联合分布可以分解成p(x) = ∏n i=1p(xi | x1, . . . , xi−1).该分解意味着我们可以将其拆分成 n 个监督学习问题 尽管无监督学习和监督学习并非完全没有交集的正式概念，传统地，人们将回归、分类或者结构化输出问题称为监督学习。支持其他任务的密度估计通常被称为无监督学习。 有些机器学习算法并不是训练于一个固定的数据集上。例如，**强化学习（reinforcement learning）**算法会和环境进行交互，所以学习系统和它的训练过程会有反馈回路。 在所有的情况下，数据集都是样本的集合，而样本是特征的集合。 示例：线性回归 理解如何进行梯度下降的。对于一个目标值y，建立函数y=f(x)=ax+b。其中a、b为权重。每次得到一个loss值。比如loss = min (y-y_label)^2计算得到。则根据loss求每一个权重的偏导数，并进行梯度下降。为的是知道，在权重取什么值的时候，y能够取到最小值。 在更复杂的情况下，我们最简单的方法是使用牛顿法来计算得到最小点。如果f不是真正二次，而是局部近似为正定二次，则需要多次迭代应用式子。使用梯度下降是一阶优化算法。使用Hessian矩阵是二阶优化算法。如牛顿法。 容量、过拟合和欠拟合 机器学习的主要挑战就是能够在先前未观测的新输入上表现良好。在先前未观测的输入上表现良好称为泛化。 机器学习的目的希望泛化误差（也被称为测试误差）很低。 统计学理论可以让我们通过训练集影响测试集。 训练集和测试被数据生成过程的概率分布生成。 会进行独立同分布的假设。该假设是说，每个数据集中的样本都是彼此相互独立的（independent），并且训练集和测试集是同分布的（identically distributed）， 我们将这个共享的潜在 分布称为数据生成分布（data generating distribution），记作 pdata。 我们能观察到训练误差和测试误差之间的直接联系是，随机模型训练误差的期 望和该模型测试误差的期望是一样的。假设我们有概率分布 p(x, y)， 以下是决定机器 学习算法效果是否好的因素： . 降低训练误差。 缩小训练误差和测试误差的差距。 这两个因素对应机器学习的两个主要挑战：欠拟合（underfitting）和过拟合 （overfitting）。欠拟合是指模型不能在训练集上获得足够低的误差。而过拟合是指训练误差和和测试误差之间的差距太大。 通过调整模型的容量（capacity），我们可以控制模型是否偏向于过拟合或者欠 拟合。 通俗地，模型的容量是指其拟合各种函数的能力。容量低的模型可能很难拟 合训练集。容量高的模型可能会过拟合，因为记住了不适用于测试集的训练集性质。 一种控制训练算法容量的方法是选择假设空间（hypothesis space），即学习算 法可以选择为解决方案的函数集 我们探讨了通过改变输入特征的数目和加入这些特征对应的参数，改 变模型的容量 模型增加特征是：增加特征的维数。这里从x变成了x+x^2学习参数是，y=wx+b不变，通过学习w,b来学习y。增加特征是增加x来拟合y，学习参数是学习权重来拟合y 网上查到的解释： 参数模型，对目标函数有一个假设，如y=ax+b。通过最小二乘法来拟合目标函数的参数 非参数模型：不对目标函数有一个确定的假设。通过训练来拟合学习某种函数。 容量不足的模型不能解决复杂任务。容量高的模型能够解决 复杂的任务，但是当其容量高于任务所需时，有可能会过拟合。 模型规定了调整参数降低训练目标时，学习算法可以从哪些函数族中选择函数。这被称为模型的表示容量（representational capacity）. 额外的限制因素，比如 优化算法的不完美，意味着学习算法的**有效容量（effective capacity）**可能小于模型族的表示容量 现在广泛被称为**奥卡姆剃刀（Occam’s razor）（c. 1287-1387）。**该原则指出，在同样能够解释已知观测现象的假设中，我们应该挑选 ‘‘最简单’’ 的那一个。 统计学习理论提供了量化模型容量的不同方法。在这些中，最有名的是VapnikChervonenkis 维度（Vapnik-Chervonenkis dimension, VC）。 VC维定义为该分类器能够分类的训练样本的最大数目。假设存在 m 个 不同 x 点的训练集，分类器可以任意地标记该 m 个不同的 x 点，VC维被定义为 m的最大可能值。 统计学习理论中最重要 的结论阐述了训练误差和泛化误差之间差异的上界随着模型容量增长而增长，但随着训练样本增多而下降 但是它们很少应用于实际中的深度学习算法。 一部分原因是边界太松， 另一部分原因是很难确定深度学习算法的容量。 由于有效容量受限于优化算法的能力， 确定深度学习模型容量的问题特别困难。而且对于深度学习中的一般非凸优化问题，我们只有很少的理论分析 为考虑容量任意高的极端情况，我们介绍非参数（non-parametric）模型的概 念。 参数模型学习的函数在观测到新 数据前，参数向量的分量个数是有限且固定的。非参数模型没有这些限制 非参数模型典例：决策树。最近邻回归。 理想模型假设我们能够预先知道生成数据的真实概率分布。然而这样的模型仍 然会在很多问题上发生一些错误，因为分布中仍然会有一些噪声。从预先知道的真实分布 p(x, y) 预测而出现的误差被称为贝叶斯误差（Bayes error）。知道测试集的分布，仍然产生的误差就是贝叶斯误差。 训练误差和泛化误差会随训练集的大小发生变化。 泛化误差的期望从不会因训 练样本数目的增加而增加。 对于非参数模型而言，更多的数据会得到更好的泛化能 力，直到达到最佳可能的泛化误差。 任何模型容量小于最优容量的固定参数模型会渐近到大于贝叶斯误差的误差值。 归纳推理，或是从一组有限的样本中推断一般的规则，在 逻辑上不是很有效。为了逻辑地推断一个规则去描述集合中的元素，我们必须具有集合中每个元素的信息。上升到哲学。 机器学习保证找到一个在所关注的大多数样本上可能正 确的规则 机器学习的没有免费午餐定理（no free lunch theorem）表明 (Wolpert, 1996)，每 一个分类算法在未事先观测的点上都有相同的错误率。换言之，在某种意义上，没有一个机器学习算法总是比其他的要好。我们能够设想的最先进的算法和简单地将所有点归为同一类的简单算法有着相同的平均性能（在所有可能的任务上）。 反之，我们的目标是理解什么样的分布与人工智能获取经验的 ‘‘真实世界’’ 相 关，什么样的学习算法在我们关注的数据生成分布上效果最好。 最小二乘法：h = X$\theta$。损失函数定义为h = 1/2(X$\theta$-Y)^T(X$\theta$-Y) 和最小均方差公式很类似。。 正则化 没有免费午餐定理暗示我们必须在特定任务上设计性能良好的机器学习算法。没有一个设计可满足所有的任务。 算法的效果不仅很大程度上受影响于假设空间的函数数量，也取决于这些函数 的具体形式。在假设空间中，相比于某一个学习算法，我们可能更偏好另一个学习算法。只有非偏好函数比偏 好函数在训练数据集上效果明显好很多时，我们才会考虑非偏好函数。 例如，我们可以加入权重衰减（weight decay）来修改线性回归的训练标准。带权重衰减的线性回归最小化训练集上的均方误差和正则项的和 J(w)，其偏好于平方L2 范数较小的权重 J(w) = MSE_{train} + λw⊤w, 其中 λ 是提前挑选的值，控制我们偏好小范数权重的程度。越大的 λ偏好范数越小的权重.或是将权重放在较少 的特征上。 常用的惩罚项是所有权重的平方乘以一个衰减常量之和。其用来惩罚大的权值。 所以W^T W 就是权重的乘积。表现为如果权重大的话，这个值也大。损失函数的目的让这个数变小。所以加入这个可以惩罚权重大的值。 更一般地，正则化一个学习函数 f(x; θ) 的模型，我们可以给代价函数添加被称为正则化项（regularizer）的惩罚。在权重衰减的例子中，正则化项是 Ω(w) = w⊤w。 正则化是指我们 修改学习算法，使其降低泛化误差而非训练误差。正则化是机器学习领域的中心问题之一，只有优化能够与其重要性相媲。这些不同的方法都被称为正则化（regularization）。 超参数和验证集 有时一个选项被设为学习算法不用学习的超参数，是因为它太难优化了。更多 的情况是，该选项必须是超参数，因为它不适合在训练集上学习 如果在训练集上学习超参数，这些超参数总是趋向于最大可 能的模型容量，导致过拟合。例如，相比低次多项式和正的权重衰减 设定，更高次的多项式和权重衰减参数设定 λ = 0 总能在训练集上更好地拟合。 其重点在于测试样本不能以任何形式参与到 模型的选择中，包括设定超参数。测试集中的样本不能用于验证集。 因此，我们总是从训练数据中构建验证集。 特别地，我们将训练数据分成两个不相 交的子集。其中一个用于学习参数。另一个作为验证集，用于挑选超参数的数据子集被称为验 证集（validation set）。通常，80% 的训练数据用于训练，20% 用于验证。 交叉验证。这些过程是基于在原始数据上随机采样或分离出的不同数据集上重复训练和 测试的想法。最常见的是 k-折交叉验证过程， 估计、偏差和方差 Notes: 估计这一章理论性较强，可以说你只看懂了概念，其他都没看懂。
参数估计、偏差和方差，用于正式的刻画泛化、欠拟合和过拟合。
点估计、函数估计，就是利用已知的样本结果信息，反推导致这些样本结果出现的模型参数值！通过已知的数据分布推断样本出现结果的模型的参数。
点估计。 点估计试图为一些感兴趣的量提供单个 ‘‘最优’’ 预测 为了区分参数估计和真实值，我们习惯将参数 θ 的点估计表示为 ˆθ。 令 {x(1), . . . , x(m)} 是 m 个独立同分布（i.i.d.）的数据点。点估计（point estimator）或统计量（statistics）是这些数据的任意函数： 点估计也可以指输入和目标变量之间关系的估计。我们将这种类型的点估计称 为函数估计。 函数估计 有时我们会关注函数估计（或函数近似）。我们假设有一个函数 f(x) 表示 y 和 x 之间的近似关系。例如，我们可能 假设 y = f(x) + ϵ，其中 ϵ 是 y 中未能从 x 预测的一部分。 理解统计量和真实值。统计量是统计出来的，和真实的值是存在差异的。真实值\theta, 统计量就是\theta的计算公式的出来的值，这两个值是有差距的。估计量是从统计量中计算得到的。 函数估计，就是通过输入向量算出来的值。基于观测数据推断一个已知量的的估计值。如y=f(x)+e, 用模型估计去近似f。 无偏估计就是，函数估计的均值减去这个值为0.那这个估计就是无偏的。那这个估计量是优良的。通过这个估计评价的估计函数是没有理论上的偏差的。 偏差 我们通过偏差来衡量一个估计是否合理。
估计的偏差被定义为：bias( ˆθm) = E( ˆθm) − θ,如果bias( ˆθm) = 0，那么估计量 ˆθm 被称为是无偏 （unbiased），这意味着 E( ˆθm) = θ。如果 limm→∞ bias( ˆθm) = 0，那么估计量 ˆθm 被 称为是渐近无偏（asymptotically unbiased），这意味着 limm→∞ E( ˆθm) = θ。 均值的高斯分布估计是无偏的。高斯分布方差估计是有偏估计。无偏样本方差（unbiased sample variance）估计是无偏的。 一个是有偏的，另一个是无偏的。尽管无偏估计显然是令 人满意的，但它并不总是 ‘‘最好’’ 的估计。 估计量的**方差（variance）就是一个方差Var(ˆθ)方差的平方根被称为标准差（**standard error），记作 SE(ˆθ)。 样本方差的平方根和 方差无偏估计的平方根都不是标准差的无偏估计。这两种计算方法都倾向于低估真实的标准差，但仍用于实际中。相较而言，方差无偏估计的平方根较少被低估。 均值的标准差在机器学习实验中非常有用。测试集中样本的数量决定了这个估计的精确度。中心极限定理告 诉我们均值会接近一个高斯分布，我们可以用标准差计算出真实期望落在选定区间 的概率。例如，以均值 ˆµm 为中心的 95% 置信区间是。在机器学习实验中，我们通 常说算法 A 比算法 B 好，是指算法 A 的误差的 95% 置信区间的上界小于算法 B的误差的 95% 置信区间的下界。 均方误差 偏差和方差度量着估计量的两个不同误差来源。偏差度量着偏离真实函数或参 数的误差期望。而方差度量着数据上任意特定采样可能导致的估计期望的偏差 当我们可以在一个偏差更大的估计和一个方差更大的估计中进行选择时，会发 生什么呢？我们该如何选择？判断这种权衡最常用的方法是交叉验证。 另外，我们也可以比较这些估计的均方误差。MSE度量着估计和真实参数 θ 之间平方误差的总体期望偏差。MSE=E(Var(x)) 方差的总体期望偏差。 使用MSE度量泛化误差时，增加容量会增加方差，降低偏差。 最大似然估计 概率函数 f(x|o)，已知o，来预测x。看x出现的概率是多少 似然函数 f(x|o), 已知x，来预测o。在知道数据分布的情况下，推断数据分布o。对不同的o，x出现的概率是多少。
似然函数和条件概率函数从不同的视角来看的。 最大似然估计，就是在所有可能的o中取一个值，使得出现x这种分布的可能性最大。即最大似然估计。
考虑一组含有 m 个样本的数据集 X = {x(1), . . . , x(m)}，独立地由未知的真实数 据生成分布 pdata(x) 生成。令 pmodel(x; θ) 是一族由 θ 确定在相同空间上的概率分布。 对 θ 的最大似然估计被定义为。$\theta = argmax_{\theta} p_{model}(X;\theta)=argmax_{\theta}\prod_{i=1}^m p_{model}(x^i;\theta)$ 一种解释最大似然估计的观点是将它看作最小化训练集上的经验分布 ˆpdata 和模型分布之间的差异。就是通过模型参数训练出来的分布和真实分布之间的差异，通过最大似然估计来减少这个差异。 两者之间的差异程度可以通过 KL 散度度量。KL 散度被定义为：分布交叉熵的均值。 任何一个由负对数似然组成的损失都是定义在训练集上的经验分布和定义在模型上的概率 分布之间的交叉熵。 最大似然估计很容易扩展到估计条件概率 P(y | x; θ)，从而给定 x 预测 y。实 际上这是最常见的情况，因为这构成了大多数监督学习的基础。 最大似然估计最吸引人的地方在于，它被证明当样本数目 m → ∞ 时，就收敛 率而言是最好的渐近估计。 因为这些原因（一致性和统计效率），最大似然通常是机器学习中的首选估计。 当样本数目小到会发生过拟合时，正则化策略如权重衰减可用于获得训练数据有限时方差较小的最大似然有偏版本。 贝叶斯统计 至此我们已经讨论了频率派统计（frequentist statistics）方法和基于估计单一 值 θ 的方法，然后基于该估计作所有的预测。另一种方法是在做预测时会考虑所有可能的 θ。后者属于贝叶斯统计（Bayesian statistics）的范畴。 频率派的视角是真实参数 θ是未知的定值，而点估计 θˆ是考虑数据集上函数（可以看作是随机的）的随机变量。贝叶斯统计的视角完全不同。贝叶斯用概率反映知识状态的确定性程度 我们将 θ 的已知知识表示成先验概率分布。在贝叶斯估计常用的情景下，先验开始是相对均匀的分布或高熵的高斯分布，观测 数据通常会使后验的熵下降，并集中在参数的几个可能性很高的值。 相对于最大似然估计，贝叶斯估计有两个重要区别。 第一，不像最大似然方法预 测时使用 θ 的点估计，贝叶斯方法使用 θ 的全分布。 贝叶斯方法和最大似然方法的第二个最大区别是由贝叶斯先验分布造成的。先 验能够影响概率质量密度朝参数空间中偏好先验的区域偏移。 当训练数据很有限时，贝叶斯方法通常泛化得更好，但是当训练样本数目很大 时，通常会有很大的计算代价。 检查此后验分布可以让我们获得贝叶斯推断效果的一些直觉。。我们不能将贝叶斯学习过程初始化为一个无限宽的 w 先验。更重 要的区别是贝叶斯估计会给出一个协方差矩阵，表示 w 所有不同值的可能范围，而不仅是估计 µm。 最大后验估计MAP 希望使用点估计的一个常见原因是，对于大多数有意义的模型而 言，大多数涉及到贝叶斯后验的计算是非常棘手的，点估计提供了一个可行的近似解。我们仍然可以让先验影响点估计的选择来利用贝叶斯方法的优点，而不是简单 地回到最大似然估计。一种能够做到这一点的合理方式是选择最大后验点估计 监督学习算法 在许多情况下，输出 y 很难自动收集，必须由人来 提供 ‘‘监督’’，不过该术语仍然适用于训练集目标可以被自动收集的情况。 本书的大部分监督学习算法都是基于估计概率分布 p(y | x) 的。我们可以使用最 大似然估计找到对于有参分布族p(y | x; θ) 最好的参数向量 θ。 逻辑回归（logistic regression），一种方法是使用 logistic sigmoid 函数将线性函数的输出压缩进区间 (0, 1)。该值可以解释为概率。 支持向量机（support vector machine, SVM）是监督学习中最有影响力的方法 之一 支持向量机不输出概率，只输 出类别。**当 w⊤x+ b 为正时，支持向量机预测属于正类。**类似地，当 w⊤x+ b 为负时，支持向量机预测属于负类。 支持向量机二分类。 支持向量机的一个重要创新是核技巧（kernel trick）。 学习算法重写为这种形式允许我们将 x 替 换为特征函数 ϕ(x) 的输出，点积替换为被称为核函数（kernel function）的函数k(x, x(i)) = ϕ(x) · ϕ(x(i))。就是说通过将点积替换为核函数可以让这种算法能过够有非线性的表达能力。核函数完全等价于用 ϕ(x) 预处理所有的输入，然后在新的转换空间学习线性模 型。 核技巧十分强大有两个原因 首先，它使我们能够使用保证有效收敛的凸优化 技术来学习非线性模型（关于 x 的函数） 其二，核 函数 k 的实现方法通常有比直接构建 ϕ(x) 再算点积高效很多 在很多情况下，即使 ϕ(x) 是难算的k(x, x′) 却会是一个关于 x 非线性的、易算的函数.假设这个映射返回一个由开头 x 个 1，随后是无限个 0 的向量。我们可以写一个核函数 k(x, x(i)) = min(x, x(i))，完全等价于对应的无限维点积。 这种简单的比喻非常容易理解为什么自定义核函数可以比点积更有效。就是因为核函数可以自定义，根据特征的性值对函数进行自定义。 最常用的核函数是高斯核（Gaussian kernel）， 我们可以认为高斯核在执行一种模板匹配(template matching)。训练标签 y 相 关的训练样本 x 变成了类别 y 的模版。当测试点 x′ 到 x 的欧几里得距离很小，对应的高斯核响应很大时，表明 x′ 和模版 x 非常相似。 。使用核技巧的算法类别被称为核机器（kernel machine） 或核方法（kernel method） 核机器的一个主要缺点是计算决策函数的成本关于训练样本的数目是线性的。 因为第 i 个样本贡献 αik(x, x(i)) 到决策函数。 支持向量机能够通过学习主要包含零 的向量 α，以缓和这个缺点。那么判断新样本的类别仅需要计算非零 αi 对应的训练样本的核函数。这些训练样本被称为支持向量 带通用核的核机器致力于泛化得更好 当前深度学习的复兴始于 Hinton et al. (2006b) 表明神经网络能够在 MNIST 基准数据上胜过 RBF 核的支持向量机。 其他简单的监督学习算法 更一般地，k-最 近邻是一类可用于分类或回归的技术 训练过程：反之，在 测试阶段我们希望在新的测试输入 x 上产生 y，我们需要在训练数据 X上找到 x 的k-最近邻。然后我们返回训练集上对应的 y 值的平均值 k-最近邻的高容 量使其在训练样本数目大时能够获取较高的精度。然而，它的计算成本很高，另外 在训练集较小时泛化能力很差。k-最近邻的一个弱点是它不能学习出哪一个特征比其他更具识别力。 决策树（decision tree）及其变种是另一类将输入空间分成不同的区域，每个区 域有独立参数的算法 (Breiman et al., 1984) 无监督学习算法 无监督算法只处理 &ldquo;特征&rdquo;，不操作监督信号。无监督学习的大多数尝试是指从不需要人为注释的样 本的分布中抽取信息。该术语通常与密度估计相关。一个经典的无监督学习任务是找到数据的 **‘‘最佳’’**表示。但是一般来说，是指该表示在比本身表示的信息更简单或更易访问而受到一 些惩罚或限制的情况下，尽可能地保存关于 x 更多的信息。 最常见的三种包括低维表示、稀疏表示和独立 表示。低维表示尝试将 x 中的信息尽可能压缩在一个较小的表示中。稀疏表示通常用于需要增加表示维数的情况，使得 大部分为零的表示不会丢失很多信息。独立表示试图分开数据分布中变化的来源，使得表示的维 度是统计独立的。 主成分分析 PCA通过线性变换找到一个 Var[z] 是对角矩阵的表示 z =W⊤x。 主成分也可以通过奇异值分解 (SVD) 得 到。假设 W是奇异值分解 X= UΣW⊤ 的右奇异向量。以W作为特征向量基，我们可以得到原来的特征向量。以上分析指明当我们通过线性变换 W将数据 x 投影到 z 时，得到的数据表示 的协方差矩阵是对角的（即 Σ2），立刻可得 z 中的元素是彼此无关的。 见p157。 PCA这种将数据变换为元素之间彼此不相关表示的能力是PCA的一个重要性 质。它是消除数据中未知变化因素的简单表示示例
K-均值据类 k-均值聚类提供的 one-hot 编码也是一种稀疏表示,因为每个输入的表示中大 部分元素为零。one-hot 编码是稀疏表示的一个极端示例，丢失 了很多分布式表示的优点。one-hot 编码仍然有一些统计优点（自然地传达了相同聚类中的样本彼此相似的观点）,也具有计算上的优势，因为整个表示可以用一个单独 的整数表示. k-均值聚类初始化 k 个不同的中心点 {µ(1), . . . , µ(k)}，然后迭代交换两个不同 的步骤直到收敛。步骤一，每个训练样本分配到最近的中心点 µ(i) 所代表的聚类 i。步骤二，每一个中心点 µ(i) 更新为聚类 i 中所有训练样本 x(j) 的均值。 关于聚类的一个问题是聚类问题本身是病态的。这是说没有单一的标准去度量 聚类的数据在真实世界中效果如何。 然而我们不知道聚类的性质是否很好地对应到真实世界的性质.我们可能希望找到和 一个特征相关的聚类，但是得到了一个和任务无关的，同样是合理的不同聚类。我们只知道它们是不同的。 这些问题说明了一些我们可能更偏好于分布式表示（相对于 one-hot 表示而言） 的原因。分布式表示可以对每个车辆赋予两个属性——一个表示它颜色，一个表示它是汽车还是卡车。但是多个属性减少了算法去猜我们关心哪一个属性的负担. 随机梯度下降 几乎所有的深度学习算法都用到了一个非常重要的算法：随机梯度下降 （stochastic gradient descent, SGD）。 机器学习中反复出现的一个问题是好的泛化需要大的训练集，但大的训练集的 计算代价也更大。机器学习算法中的代价函数通常可以分解成每个样本的代价函数的总和。. 这个运算的计算代价是 O(m)。随着训练集规模增长为数十亿的样本，计算一步梯度 也会消耗相当长的时间。 随机梯度下降的核心是，梯度是期望。期望可使用小规模的样本近似估计.在算法的每一步，我们从训练集中均匀抽出一小批量（minibatch）样本.使用来自小批量 B 的样本。然后，随机梯度下降算法使用如下的梯度下降估计：θ ← θ − ϵg,其中，ϵ 是学习率。 它是在大规模数据上训练大 型线性模型的主要方法。对于固定大小的模型，每一步随机梯度下降更新的计算量不取决于训练集的大小 m。而，当 m 趋向于无穷大时，该模型最终会在随机梯度下降抽样完训练集上的所有样本之前收敛到可能的最优测试误差。我们可以认为用SGD训练模型的渐近代价 是关于 m 的函数的 O(1) 级别。 在深度学习兴起之前，学习非线性模型的主要方法是结合核技巧的线性模型。 很多核学习算法需要构建一个 m×m 的矩阵. 深度学习从 2006 年开始受到关注的原因是，在数以万计样本的中等规模数据集上， 深度学习在新样本上比当时很多热门算法泛化得更好。不久后，深度学习在工业界受到了更多的关注，因为其提供了一种训练大数据集上的非线性模型的可扩展方式。</content></entry><entry><title>3 数值统计</title><url>http://next.lisenhui.cn/post/study/deeplearning/3-%E6%95%B0%E5%80%BC%E7%BB%9F%E8%AE%A1/</url><categories/><tags/><content type="html"> 机器学习算法通常需要大量的数值计算。这通常是指通过迭代过程更新解的估 计值来解决数学问题的算法，而不是通过解析过程推导出公式来提供正确解的方法
上溢下溢 连续数学在数字计算机上的根本困难是，我们需要通过有限数量的位模式来表 示无限多的实数。意味着我们在计算机中表示实数时，几乎总会引入一些近似误 差。 一种极具毁灭性的舍入误差是下溢（underflow）。当接近零的数被四舍五入为 零时发生下溢。 另一个极具破坏力的数值错误形式是上溢（overflow）。当大量级的数被近似为 ∞ 或 −∞ 时发生上溢。进一步的运算通常会导致这些无限值变为非数字。 上溢和下溢进行数值稳定的一个例子是softmax 函数 Theano (Bergstra et al., 2010a; Bastien et al., 2012a) 就是这样软件包的一个例子，它能自动检测并稳定深度学习中许多常见的数值不稳定的表达式。 病态条件 考虑函数 f(x) = A−1x。当 A ∈ Rn×n 具有特征值分解时，其条件数为$max_{i,j} |\frac{\lambda_i}{\lambda_j}|$。这是最大和最小特征值的模之比1。当**该数很大**时，矩阵求逆对输入的**误差特别敏感**。 基于梯度的优化方法 我们把要最小化或最大化的函数称为目标函数（objective function）或准则 （criterion）。
当我们对其进行最小化时，我们也把它称为代价函数（cost function）、损失函数（loss function）或误差函数（error function）。 我们通常使用一个上标 ∗表示最小化或最大化函数的 x 值。如我们记 x∗ = arg min f(x)。 这个函数的导数（derivative） 记为 f′(x) 或 dy/dx。导数 f′(x) 代表 f(x) 在点 x 处的斜率。
因此导数对于最小化一个函数很有用，因为它告诉我们如何更改 x 来略微地改 善 y。这种技术被称为梯度下降 （gradient descent）(Cauchy, 1847)。
f′(x) = 0 的点称为临界 点（critical point）或驻点（stationary point）。
f′(x) = 0 的点称为临界 点（critical point）或驻点（stationary point）。 f′(x) = 0 的点称为临界 点（critical point）或驻点（stationary point）。 有些临界点既不是最小点也不是最大点。这些点被称为鞍点（saddle point）。 因此，我们通常寻找使 f 非常小的 点，但这在任何形式意义下并不一定是最小。
我们需要用到偏导数（partial derivative）的概念
**梯度（gradient）**是相对一个向量求导的导数:f 的导数是包含所有偏导数的向量，记为 ∇xf(x)。梯度的第i 个元素是 f 关于 xi 的偏导数。
在 u（单位向量）方向的方向导数（directional derivative）是函数 f 在 u 方向 的斜率。
负梯度向量指向下坡。我们在负梯度方向上移动可以减小 f。这被称为最速下降法 (method of steepest descent) 或梯度下降（gradient descent）。
最速下降建议新的点为 x′ = x− ϵ∇xf(x) (4.5)其中 ϵ 为学习率（learning rate），是一个确定步长大小的正标量
最速下降在梯度的每一个元素为零时收敛（或在实践中，很接近零时） 但不断向更好的情况移动一小 步（即近似最佳的小移动）的一般概念可以推广到离散空间。递增带有离散参数的目标函数被称为爬山（hill climbing）算法 (Russel and Norvig, 2003) 有时我们需要计算输入和输出都为向量的函数的所有偏导数。包含所有这样的 偏导数的矩阵被称为 Jacobian 矩阵
二阶导数告诉我们，一阶导数将如何随着输入 的变化而改变。它表示只基于梯度信息的梯度下降步骤是否会产生如我们预期的那样大的改善，因此它是重要的。我们可以认为，二阶导数是对曲率的衡量 如果这样的函数具有零二阶导数，那就没有曲率。也就是一条完全 平坦的线，仅用梯度就可以预测它的值。我们使用沿负梯度方向大小为 ϵ 的下降步， 当该梯度是 1 时，代价函数将下降 ϵ。如果二阶导数是负的，函数曲线向下凹陷 (向 上凸出)，因此代价函数将下降的比 ϵ 多。如果二阶导数是正的，函数曲线是向上凹陷 (向下凸出)，因此代价函数将下降的比 ϵ 少 当我们的函数具有多维输入时，二阶导数也有很多。我们可以将这些导数合并 成一个矩阵，称为 Hessian 矩阵
Hessian 等价于梯度的 Jacobian 矩阵。
微分算子在任何二阶偏导连续的点处可交换，也就是它们的顺序可以互换：这意味着 Hi,j = Hj,i，因此 Hessian 矩阵在这些点上是对称的。因为 Hessian 矩阵是实对 称的，我们可以将其分解成一组实特征值和一组特征向量的正交基。在特定方向 d上的二阶导数可以写成 d⊤Hd。这个方向的二阶导 数就是对应的特征值。对于其他的方向 d，方向二阶导数是所有特征值的加权平均， 权重在 0 和 1 之间，且与 d 夹角越小的特征向量的权重越大。最大特征值确定最大二阶导数，最小特征值确定最小二阶导数。
Hessian 的特征值决定了学 习率的量级 当 f′(x) = 0 且 f′′(x) > 0 时，x 是一个局 部极小点。同样，当 f′(x) = 0 且 f′′(x) &lt; 0 时，x 是一个局部极大点。这就是所谓的二阶导数测试（second derivative test）。
在临界点处（∇xf(x) = 0），我们通 过检测 Hessian 的特征值来判断该临界点是一个局部极大点、局部极小点还是鞍点。 当 Hessian 是正定的（所有特征值都是正的），则该临界点是局部极小点。因为方 向二阶导数在任意方向都是正的，参考单变量的二阶导数测试就能得出此结论。同 样的，当 Hessian 是负定的（所有特征值都是负的），这个点就是局部极大点。在多 维情况下，实际上我们可以找到确定该点是否为鞍点的积极迹象（某些情况下）。如 果 Hessian 的特征值中至少一个是正的且至少一个是负的，那么 x 是 f 某个横截面 的局部极大点，却是另一个横截面的局部极小点。见图4.5中的例子。最后，多维二 阶导数测试可能像单变量版本那样是不确定的。当所有非零特征值是同号的且至少 有一个特征值是 0 时，这个检测就是不确定的。这是因为单变量的二阶导数测试在零特征值对应的横截面上是不确定的。
维度多于一个时，鞍点不一定要具有 0 特征值：仅需要同时具有正特征值和负特征值。我 们可以想象这样一个鞍点（具有正负特征值）在一个横截面内是局部极大点，而在另一个横截面内是局部极小点。
中期稍微总结一下：
jacobian矩阵是梯度矩阵，表明了下一步权重下降的方向。Hessian衡量梯度下降的速率。如果hessian值正定，则权重是局部极小点。如果hessian负定，则权重是局部极大点。当涉及多维的时候条件更苛刻一点。有非零特征值时，这个检测是不确定的。根据梯度下降的方向进行下降，就是梯度下降。证明需要看书。大约就是二阶导，高中学的那一套。 在多维情况下，单个点在每个方向上的二阶导数时不同的，Hessian 的条件数衡量 这些二阶导数的变化范围。当 Hessian 的条件数很差时，梯度下降法也会表现得很差。这是因为一个方向上的导数增加得很快，而在另一个方向上增加得很慢。
梯度下降把时间浪费于在峡谷壁反复下 降，因为它们是最陡峭的特征。由于步长有点大，有超过函数底部的趋势，因此需要在下一次迭代时在对面的峡谷壁下降 我们可以使用 Hessian 矩阵的信息来指导搜索，以解决这个问题。其中最简单 的方法是牛顿法（Newton’s method）。
当 f 是一个正定二次函数时，牛顿法只要应用一次式(4.12)就能直接跳到函数的最 小点。如果 f 不是一个真正二次但能在局部近似为正定二次，牛顿法则需要多次迭代应用式。 如式(8.2.3)所讨论的，当附近的临界点是最小点（Hessian 的所有特征值 都是正的）时牛顿法才适用，而梯度下降不会被吸引到鞍点(除非梯度指向鞍点)。 仅使用梯度信息的优化算法被称为一阶优化算法 (first-order optimization algorithms)，如梯度下降。使用 Hessian 矩阵的优化算法被称为二阶最优化算法(second-order optimization algorithms)(Nocedal and Wright, 2006)，如牛顿法
在深度学习的背景下，限制函数满足Lipschitz 连续（Lipschitz continuous）或 其导数Lipschitz连续可以获得一些保证。最成功的特定优化领域或许是凸优化（Convex optimization）。凸优化通过更强 的限制提供更多的保证。凸优化算法只对凸函数适用，即 Hessian 处处半正定的函数。
约束优化 在 x 的所有可能值下最大化或最小化一个函数 f(x) 不是我们所希望 的。相反，我们可能希望在 x 的某些集合 S 中找 f(x) 的最大值或最小值。这被称为约束优化（constrained optimization）。集合 S 内的点 x 被称可行（feasible）点。 我们常常希望找到在某种意义上小的解。 约束优化的一个简单方法是将约束考虑在内后简单地对梯度下降进行修改。 如 果我们**使用一个小的恒定步长 ϵ，**我们可以先取梯度下降的单步结果，然后将结果投影回 S。 如果我们使用线搜索，我们只能在步长为 ϵ 范围内搜索可行的新 x 点 或者 我们可以将线上的每个点投影到约束区域。 在梯度下降或线搜索前 将梯度投影到可行域的切空间会更高效 (Rosen, 1960)。 一个更复杂的方法是设计一个不同的、无约束的优化问题，其解可以转化成原 始约束优化问题的解**Karush–Kuhn–Tucker（KKT）**方法2是针对约束优化非常通用的解决方案。 我们引入一个称为广义 Lagrangian（generalized Lagrangian） 或广义 Lagrange 函数（generalized Lagrange function）的新函数。 那么 S 可以表示为 S = {x | ∀i, g(i)(x) = 0 and ∀j, h(j)(x) ≤ 0}。其中涉及 g(i) 的等式称为等式约束（equality constraint），涉及 h(j) 的不等式称为不等式约束（inequality constraint）。 这是因为当约束满足时，广义lagrangian函数的值为f(x)的值。 当约束不满足时，广义lagrangian为无穷大。 我们可以使用一组简单的性质来描述约束优化问题的最优点。这些性质称 为Karush–Kuhn–Tucker（KKT）条件 广义 Lagrangian 的梯度为零 所有关于 x 和 KKT 乘子的约束都满足。 不等式约束显示的 ‘‘互补松弛性’’：α⊙ h(x) = 0。 实例分析：线性最小二乘法 没看懂</content></entry><entry><title>Mysql_21mins_tutorial</title><url>http://next.lisenhui.cn/post/study/server%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9B%B8%E5%85%B3/mysql/mysql_21mins_tutorial/</url><categories><category>学习</category></categories><tags><tag>mysql</tag></tags><content type="html"> 快速入门 MySQL 为关系型数据库(Relational Database Management System)，一个关系型数据库由一个或数个表格组成
表头(header): 每一列的名称; 列(col): 具有相同数据类型的数据的集合; 行(row): 每一行用来描述某个人/物的具体信息; 值(value): 行的具体信息, 每个值必须与该列的数据类型相同; 键(key): 表中用来识别某个特定的人\物的方法, 键的值在当前列中具有唯一性。
登录MySQL mysql -h 127.0.0.1 -u 用户名 -p mysql -D 所选择的数据库名 -h 主机名 -u 用户名 -p mysql> exit # 退出 使用 “quit;” 或 “\q;” 一样的效果 mysql> status; # 显示当前mysql的version的各种信息 mysql> select version(); # 显示当前mysql的version信息 mysql> show global variables like &lsquo;port&rsquo;; # 查看MySQL端口号
创建数据库 &ndash; 创建一个名为 samp_db 的数据库，数据库字符编码指定为 gbk create database samp_db character set gbk; drop database samp_db; &ndash; 删除 库名为samp_db的库 show databases; &ndash; 显示数据库列表。 use samp_db; &ndash; 选择创建的数据库samp_db show tables; &ndash; 显示samp_db下面所有的表名字 describe 表名; &ndash; 显示数据表的结构 delete from 表名; &ndash; 清空表中记录
创建数据库表 使用 create table 语句可完成对表的创建, create table 的常见形式: 语法：create table 表名称(列声明);
&ndash; 如果数据库中存在user_accounts表，就把它从数据库中drop掉 DROP TABLE IF EXISTS user_accounts; CREATE TABLE user_accounts ( id int(100) unsigned NOT NULL AUTO_INCREMENT primary key, password varchar(32) NOT NULL DEFAULT '' COMMENT &lsquo;用户密码&rsquo;, reset_password tinyint(32) NOT NULL DEFAULT 0 COMMENT &lsquo;用户类型：0－不需要重置密码；1-需要重置密码&rsquo;, mobile varchar(20) NOT NULL DEFAULT '' COMMENT &lsquo;手机&rsquo;, create_at timestamp(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6), update_at timestamp(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6), &ndash; 创建唯一索引，不允许重复 UNIQUE INDEX idx_user_mobile(mobile) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=&lsquo;用户表信息&rsquo;;
数据类型的属性解释: NULL：数据列可包含NULL值； NOT NULL：数据列不允许包含NULL值； DEFAULT：默认值； PRIMARY KEY：主键； AUTO_INCREMENT：自动递增，适用于整数类型； UNSIGNED：是指数值类型只能为正数； CHARACTER SET name：指定一个字符集； COMMENT：对表或者字段说明；
增删改查 SELECT SELECT 语句用于从表中选取数据。 语法：SELECT 列名称 FROM 表名称 语法：SELECT * FROM 表名称
&ndash; 表station取个别名叫s，表station中不包含 字段id=13或者14 的，并且id不等于4的 查询出来，只显示id SELECT s.id from station s WHERE id in (13,14) and id not in (4);
&ndash; 从表 Persons 选取 LastName 列的数据 SELECT LastName FROM Persons
&ndash; 从表 users 选取 id=3 的数据，并只拉一条数据(据说能优化性能) SELECT * FROM users where id=3 limit 1
&ndash; 结果集中会自动去重复数据 SELECT DISTINCT Company FROM Orders &ndash; 表 Persons 字段 Id_P 等于 Orders 字段 Id_P 的值， &ndash; 结果集显示 Persons表的 LastName、FirstName字段，Orders表的OrderNo字段 SELECT p.LastName, p.FirstName, o.OrderNo FROM Persons p, Orders o WHERE p.Id_P = o.Id_P
&ndash; gbk 和 utf8 中英文混合排序最简单的办法 &ndash; ci是 case insensitive, 即 “大小写不敏感” SELECT tag, COUNT(tag) from news GROUP BY tag order by convert(tag using gbk) collate gbk_chinese_ci; SELECT tag, COUNT(tag) from news GROUP BY tag order by convert(tag using utf8) collate utf8_unicode_ci;
UPDATE Update 语句用于修改表中的数据。 语法：UPDATE 表名称 SET 列名称 = 新值 WHERE 列名称 = 某值
&ndash; update语句设置字段值为另一个结果取出来的字段 update user set name = (select name from user1 where user1 .id = 1 ) where id = (select id from user2 where user2 .name=&lsquo;小苏&rsquo;); &ndash; 更新表 orders 中 id=1 的那一行数据更新它的 title 字段 UPDATE orders set title=&lsquo;这里是标题&rsquo; WHERE id=1;
Insert INSERT INTO 语句用于向表格中插入新的行。 语法：INSERT INTO 表名称 VALUES (值1, 值2,&hellip;.) 语法：INSERT INTO 表名称 (列1, 列2,&hellip;) VALUES (值1, 值2,&hellip;.)
&ndash; 向表 Persons 插入一条字段 LastName = JSLite 字段 Address = shanghai INSERT INTO Persons (LastName, Address) VALUES (&lsquo;JSLite&rsquo;, &lsquo;shanghai&rsquo;); &ndash; 向表 meeting 插入 字段 a=1 和字段 b=2 INSERT INTO meeting SET a=1,b=2; &ndash; SQL实现将一个表的数据插入到另外一个表的代码 &ndash; 如果只希望导入指定字段，可以用这种方法： &ndash; INSERT INTO 目标表 (字段1, 字段2, &hellip;) SELECT 字段1, 字段2, &hellip; FROM 来源表; INSERT INTO orders (user_account_id, title) SELECT m.user_id, m.title FROM meeting m where m.id=1;
&ndash; 向表 charger 插入一条数据，已存在就对表 charger 更新 type,update_at 字段； INSERT INTO charger (id,type,create_at,update_at) VALUES (3,2,&lsquo;2017-05-18 11:06:17&rsquo;,&lsquo;2017-05-18 11:06:17&rsquo;) ON DUPLICATE KEY UPDATE id=VALUES(id), type=VALUES(type), update_at=VALUES(update_at);
DELETE DELETE 语句用于删除表中的行。 语法：DELETE FROM 表名称 WHERE 列名称 = 值
&ndash; 在不删除table_name表的情况下删除所有的行，清空表。 DELETE FROM table_name &ndash; 或者 DELETE * FROM table_name &ndash; 删除 Person表字段 LastName = &lsquo;JSLite&rsquo; DELETE FROM Person WHERE LastName = &lsquo;JSLite&rsquo; &ndash; 删除 表meeting id 为2和3的两条数据 DELETE from meeting where id in (2,3);
AND 和 OR AND - 如果第一个条件和第二个条件都成立； OR - 如果第一个条件和第二个条件中只要有一个成立；
AND &ndash; 删除 meeting 表字段 &ndash; id=2 并且 user_id=5 的数据 和 &ndash; id=3 并且 user_id=6 的数据 DELETE from meeting where id in (2,3) and user_id in (5,6);
&ndash; 使用 AND 来显示所有姓为 &ldquo;Carter&rdquo; 并且名为 &ldquo;Thomas&rdquo; 的人： SELECT * FROM Persons WHERE FirstName=&lsquo;Thomas&rsquo; AND LastName=&lsquo;Carter&rsquo;; OR &ndash; 使用 OR 来显示所有姓为 &ldquo;Carter&rdquo; 或者名为 &ldquo;Thomas&rdquo; 的人： SELECT * FROM Persons WHERE firstname=&lsquo;Thomas&rsquo; OR lastname=&lsquo;Carter&rsquo;
ORDER BY 语句默认按照升序对记录进行排序。 ORDER BY - 语句用于根据指定的列对结果集进行排序。 DESC - 按照降序对记录进行排序。 ASC - 按照顺序对记录进行排序。
&ndash; Company在表Orders中为字母，则会以字母顺序显示公司名称 SELECT Company, OrderNumber FROM Orders ORDER BY Company
&ndash; 后面跟上 DESC 则为降序显示 SELECT Company, OrderNumber FROM Orders ORDER BY Company DESC
&ndash; Company以降序显示公司名称，并OrderNumber以顺序显示 SELECT Company, OrderNumber FROM Orders ORDER BY Company DESC, OrderNumber ASC
IN IN - 操作符允许我们在 WHERE 子句中规定多个值。 IN - 操作符用来指定范围，范围中的每一条，都进行匹配。IN取值规律，由逗号分割，全部放置括号中。 语法：SELECT &ldquo;字段名"FROM &ldquo;表格名"WHERE &ldquo;字段名&rdquo; IN (&lsquo;值一&rsquo;, &lsquo;值二&rsquo;, &hellip;);
&ndash; 从表 Persons 选取 字段 LastName 等于 Adams、Carter SELECT * FROM Persons WHERE LastName IN (&lsquo;Adams&rsquo;,&lsquo;Carter&rsquo;)
NOT NOT - 操作符总是与其他操作符一起使用，用在要过滤的前面。
SELECT vend_id, prod_name FROM Products WHERE NOT vend_id = &lsquo;DLL01&rsquo; ORDER BY prod_name;
UNION UNION - 操作符用于合并两个或多个 SELECT 语句的结果集。
&ndash; 列出所有在中国表（Employees_China）和美国（Employees_USA）的不同的雇员名 SELECT E_Name FROM Employees_China UNION SELECT E_Name FROM Employees_USA
&ndash; 列出 meeting 表中的 pic_url， &ndash; station 表中的 number_station 别名设置成 pic_url 避免字段不一样报错 &ndash; 按更新时间排序 SELECT id,pic_url FROM meeting UNION ALL SELECT id,number_station AS pic_url FROM station ORDER BY update_at; &ndash; 通过 UNION 语法同时查询了 products 表 和 comments 表的总记录数，并且按照 count 排序 SELECT &lsquo;product&rsquo; AS type, count() as count FROM products union select &lsquo;comment&rsquo; as type, count() as count FROM comments order by count;
AS as - 可理解为：用作、当成，作为；别名 一般是重命名列名或者表名。 语法：select column_1 as 列1,column_2 as 列2 from table as 表
SELECT * FROM Employee AS emp &ndash; 这句意思是查找所有Employee 表里面的数据，并把Employee表格命名为 emp。 &ndash; 当你命名一个表之后，你可以在下面用 emp 代替 Employee. &ndash; 例如 SELECT * FROM emp.
SELECT MAX(OrderPrice) AS LargestOrderPrice FROM Orders &ndash; 列出表 Orders 字段 OrderPrice 列最大值， &ndash; 结果集列不显示 OrderPrice 显示 LargestOrderPrice
&ndash; 显示表 users_profile 中的 name 列 SELECT t.name from (SELECT * from users_profile a) AS t;
&ndash; 表 user_accounts 命名别名 ua，表 users_profile 命名别名 up &ndash; 满足条件 表 user_accounts 字段 id 等于 表 users_profile 字段 user_id &ndash; 结果集只显示mobile、name两列 SELECT ua.mobile,up.name FROM user_accounts as ua INNER JOIN users_profile as up ON ua.id = up.use
JOIN 用于根据两个或多个表中的列之间的关系，从这些表中查询数据。
JOIN: 如果表中有至少一个匹配，则返回行 INNER JOIN:在表中存在至少一个匹配时，INNER JOIN 关键字返回行。 LEFT JOIN: 即使右表中没有匹配，也从左表返回所有的行 RIGHT JOIN: 即使左表中没有匹配，也从右表返回所有的行 FULL JOIN: 只要其中一个表中存在匹配，就返回行(MySQL 是不支持的，通过 LEFT JOIN + UNION + RIGHT JOIN 的方式 来实现) SELECT Persons.LastName, Persons.FirstName, Orders.OrderNo FROM Persons INNER JOIN Orders ON Persons.Id_P = Orders.Id_P ORDER BY Persons.LastName;</content></entry><entry><title>西安攻略</title><url>http://next.lisenhui.cn/post/journey/%E8%A5%BF%E5%AE%89/%E8%A5%BF%E5%AE%89%E6%94%BB%E7%95%A51/</url><categories><category>阳阳旅游日记</category></categories><tags><tag>旅游攻略</tag></tags><content type="html"> 建议看原文，然后进行总结。
作者：美途旅行 链接：https://zhuanlan.zhihu.com/p/367113622 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
因为工作的原因，我前前后后的原因去了五次西安，哈哈，这是有多爱西安，但是也踩了不少的坑，今天我要总结出来，送给想来西安的小伙伴们一些建议，记得点赞收藏哦！去景点建议1、陕西历史博物馆！免费票需要提前3-5天在gzh“陕西历史博物馆”预订，晚了根本预约不到免费票，我们当时买的30元票，亏了亏了!注意周一闭馆，不要白跑一趟、陕博人很多，不允许带三脚架和自拍杆2、千万不要错过3月20日-11月3日的长恨歌表演，长恨歌的表演观赏性非常震撼，强烈推荐，一定注意开放时间哦，今年是3月20日-11月3日表,其他时间不演出，别错过3、大唐不夜城千万不要白天去，白天就是一个步行街啊，不夜城只有晚上才能感受到大唐盛世，有一种穿越的感觉，而且还可以偶遇不倒翁小姐姐4、如果要逛西安城墙，一定要骑车，当时我们因为骑车交租金那里人太多，就想着走到下一个门直接出去了。我们应该走了一个小时，累死人了5、回民街只适合逛，不适合吃!回民街，一个我超级想吐槽的地方，里面基本都是清真，吃不惯的别买，我们是晚上去的，人很多，本地人不会去吃的，专坑外地人6、西安碑林博物馆，里面有很多石碑、墓志等，我们对这个没兴趣，就是来打打卡，也没请讲解，就很无聊!所以千万要请讲解呀!7、建议去看18:00场的大慈恩寺北广场音乐喷泉，是亚洲最大的室外喷泉，要早点去，提前30分钟，抢占三排，拥有全世界规模最大的音响组合的纪录，晚上看效果最好，注意!周二检修不开放!8、西安大唐芙蓉园预约流程:搜“大唐芙蓉园”gzh进行预约，记得预约的时候选好出行日期哦(可预约三日内的门票，每人每天仅可预约一次）唐代贵妃的皇家园林，而晚上有大型水幕灯光表演秀，还可以划船，穿着古装瞬间穿越啊9、永兴坊的摔碗酒，真是排队三小时，摔碗5秒钟，人真的超级多，不建议去体验10、不绕路攻略，能在一起玩的景点!古城墙，钟楼鼓楼，回民街，小雁塔一线比较集中，建议花一天游玩;大雁塔，陕西历史博物馆，芙蓉园一线也比较集中;兵马俑，华清宫，骊山一条线可以安排在一起、离市区比较远11、这些景点不建议去乾陵不建议去，没有特殊之处！华清宫不推荐,贵妃洗澡的池子并没什么好看的西安秦岭地宫展览馆和西安半坡遗址博物馆，建议对历史特别感兴趣的可以去不感兴趣不建议去12、小雁塔皮影戏表演和博物馆门口的幻影成像可看性都不太高。敲钟要收费，旅游纪念品都明显偏贵一些，想买的书院门都有；大雁塔位于大慈恩寺内，门票50元，登塔30元，学生半价，大雁塔是唐玄奘法师的地盘，几个取经打扮的人邀请你一起拍照，千万要拒绝!ps-省钱小妙招 -如果你有西安旅游的计划，我可以推荐一个导游小姐姐不管是报团还是自由行，她都会给你一对一耐心指导和介绍的，我之前踩过一些坑，后面找到这位小姐姐后，把行程安排非常好，避了很多坑，下面就是她的联系方式，现在暑假他们家的结伴自由行有很大的优惠福利，可以加她微信领取点击添加佳佳微信免费咨询详细行程：13201682435 （复制打开微信添加）点击跳转至第三方点击链接，添加本地导游微信，即可了解更多详情​lvyou.sltjiancai.com/xian交通上的建议13、千万不要买到西安南站的票!因为西安南站距离市区很远且交通不是很方便，建议买西安站或者西安北站的票出站就是地铁和公交站，非常方便14、别坐黑车！一下高铁站火车机场外面，外面有很多揽客的人，那些都是黑车，专坑外地人，千万别去坐,直接去地铁站或公交站15、不建议乘坐西安公交！西安虽然不大,但是西安的交通真的很堵，坐公交车容易堵车，出行尽量选择地铁，坐公交车容易浪费时间、优选地铁，不堵车，在ZFB搜索“长安电子卡”扫码就可以乘坐啦16、去兵马俑可以选择包车，价格不等，但注意询问清楚司机有没有额外费用;在火车站东广场公交车乘坐5路，可以直接到达(首班七点，未班19点)在旅行中的建议17、一定要提前准备好健康码，特殊时期，不要不做准备就直接去西安提前准备好`“西安绿码”，在VX/zFB搜索“西安市民一码通，地址填写你订的酒店或者民宿的地址就可18、不要穿高跟鞋，双脚全废，古都议带双平底鞋，虽然西安的路都比较平，但是旅游每天走2-3万步，舒适的鞋子会让你西安之旅更愉快19、有学生证的宝宝一定要带上，可以给你剩一半的钱，西安学生证可以享受景区半价，一定要带哟20、一定要提前预定民宿，不是节假日100多就可以享受在家的温暖住宿，首选钟楼、大雁塔附近的酒店，交通方便，离各个景点都比较近去吃美食的建议21、回民街的羊肉泡，拿了两个馍给我们自己掰，过程挺有趣的。然后再给老板帮我们做好，味道怪怪的，我们几个人都吃不来，是可以体验一下，建议几个人吃一份试试，喜欢吃再多买22、回民街的凉皮，我不知道是因为麻酱还是什么原因，吃起来酸酸的，还有点辣舌头，我反正不喜欢，总体来说在回民街踩的坑比较多23、满街上都写着老米家泡馍老店,傻傻分不清楚，进去一定会踩雷、到现在我也没吃到过特别好吃的羊肉泡馍，尝试一下就行了，因为是回民吃的，一般人可能吃不惯24、西安的大多数餐馆都不提供餐纸，大家要自己带好纸巾，不然去餐馆买会很贵，西安的服务态度比南方要差纪多千万不要对服务有很高的期望25、西安的美食分量真的超级大，特别是面食，女孩子2人吃一份才是正确方式，不然都得吃半碗剩下半碗，太浪费了26、不要被西安的辣椒颜色而影响，看着辣椒很多,实际一点都不辣，他们的辣子只是香，如果你是川渝贵或者湖南，能吃辣的可以挑战他们的最辣27、别去装修很高大上的地方吃陕菜陕菜很多，千篇一律的唐风，我吃了几次还是觉得钟楼附近的三根电杆陕菜馆，地道的陕菜味，吃的是儿时的回忆，推荐葫芦鸡28、可以去回民街逛、但是不建议去吃小吃，价格贵，味道还一般，回民街都是给外地人吃的，怕踩雷就去洒金桥、大皮院和北广济街等街区行程推荐Day1 ：抵达古都西安，接站入住,自由活动。 Day2：寻古之旅：兵马俑—华清宫（华清池）—骊山Day3：探险之旅：五岳之一——西岳华山一整天Day4：文化之旅：西安古城墙—钟鼓楼广场—大雁塔广场—回民街—下午自由活动Day5：回归温暖的小家</content></entry><entry><title>案例目录</title><url>http://next.lisenhui.cn/post/study/kaggle/kaggle%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90/_%E6%A1%88%E4%BE%8B%E7%9B%AE%E5%BD%95/</url><categories/><tags/><content type="html"> 科大讯飞 X光检测 https://mp.weixin.qq.com/s/H0NMXOAj6A7jpdpyFsKt_A
水下目标检测 https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&amp;mid=2247498164&amp;idx=3&amp;sn=918d151c748c123e27812ef8ece23bb6&amp;chksm=f9a18b3bced6022d9a7a0c1b9164acfebea33c437656dee662bc6fbb447cd7f902e5976e5c72&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;sharer_sharetime=1587119860977&amp;sharer_shareid=42a896371dfe6ebe8cc4cd474d9b747c&amp;exportkey=AfuYjLAdEIfOR%2FIuYRD8dpM%3D&amp;pass_ticket=781k4y6q5ReAogsulOWaRaDoMxACZW%2F%2FDMiqguAJb7T0aup7%2BNgkdVxCQ9fTKGHF#rd
详细思路介绍kaggle https://zhuanlan.zhihu.com/p/25742261
Transformer 典型应用 https://mp.weixin.qq.com/s/KFVN9PmYMMvV60aM1_QQzw
ATEC比赛
这两个比赛writeup
https://www.zhihu.com/search?q=atec&amp;type=content https://mp.weixin.qq.com/s/F9GLj7NtnXLBWTVdpXlTAQ https://github.com/minghaochen/ATEC2021-Track1/blob/master/train.py 代码</content></entry><entry><title>科大讯飞 X光检测</title><url>http://next.lisenhui.cn/post/study/kaggle/kaggle%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E-x%E5%85%89%E6%A3%80%E6%B5%8B/</url><categories/><tags/><content type="html"> 本篇地址 https://mp.weixin.qq.com/s/H0NMXOAj6A7jpdpyFsKt_A
一、赛题背景 X光安检机是目前我国使用最广泛的安检技术手段，广泛应用于城市轨交、铁路、机场、重点场馆、物流寄递等场景。使用人工智能技术，辅助一线安检员进行X光安检判图，可以有效降低因为人员疲劳或注意力不集中带来的漏报等问题。但在实际场景中，因物品的多样性、成像角度、遮挡等问题，为算法的开发带来了一定的挑战。
http://challenge.xfyun.cn/topic/info?type=Xray-2021
赛题内容 赛题数据组成
初赛：
1）带标注的训练数据，即待识别物品在包裹中的X光图像及其标注文件；
2）不带标注的测试数据；
复赛：
1）无标注训练数据即包裹X光图像（其中有的包裹包含待识别物品）；
2）部分待识别物品X光图像（无背景）；
目标类别：
刀、剪刀、尖锐工具、甩棍、小玻璃瓶、电棍、塑料饮料瓶、带喷嘴塑料瓶、电子设备、电池、公章、伞， 共12类。
模型评价指标
wAP50，即各个类别的AP50按照权重进行加权的结果。
其中各类别权重为： 刀1、剪刀1、尖锐工具1、甩棍1、小玻璃瓶1、电棍1、塑料饮料瓶0.7、带喷嘴塑料瓶0.7、电子设备0.7、电池0.7、公章0.7、伞0.7。
模型大小600M以内
分析 赛题数据中，提供了大量的无标注数据，利用好这些无标注数据进行半监督学习是关键。 数据可视化发现数据背景较复杂且差异较大，设计合适的数据增强方法是关键。 模型评价指标为AP50，因此更关注于模型的分类效果。 在模型大小范围内，允许进行一定的模型融合。 数据增强 数据均衡
发现数据严重不平衡。所以需要对数据进行平衡。
思想：让每一类样本数尽量一样
方法：数量多的不变，数量少的多采样几次
操作：
统计每个类别bbox数量，做归一化得到n 类别采样次数= max（1，oversample_thr/n） 类别样本量足够多时，则oversample_thr/n是小于1的，则原封不动选取 不足时，多采样几次 图片实际的采样次数等于图片中最大的类别采样次数 数据增强
对图像数据进行变换，增强数据量，增强泛化性
方法： 随机反转RandomFlip、随机90°旋转RandomRotate。
几何层面的数据增强一般都能提升模型性能，比较稳定。X光图像对于色彩比较敏感，因此常见的color层面的数据增强经测试基本没什么效果。
MixUpObject
可供训练的数据只有带标签的训练数据，为了提升模型对前景的识别能力，在训练期间，从训练集中随机选取一张图片的目标bbox，通过mixup的方式粘贴到正在训练的样本上。
复赛阶段：官方提供了一些目标的patch，因此训练时可以直接将目标patch给mixup到正在训练的样本上。
mixup效果如图所示（看起来贴的一般，但训起来好啊图片）
资料：
mixup 利用多目标检测方案。将一些图片贴到上面。相当于增强学习能力。 mixup邻域分布可以被理解为一种数据增强方式，它令模型在处理样本和样本之间的区域时表现为线性。我们认为，这种线性建模减少了在预测训练样本以外的数据时的不适应性。 mixup常在alpha层进行粘贴，loss可以加权或者拼接。拼接更简单。 FixScaleResize
结合X光安检图像的成像特点，由于设备限制，其光源到物品的目标的距离是在一定范围内的，因此同一类别的目标的尺寸差异不会特别大。这一点和自然场景下的目标有较大不同，自然场景下的目标是有近大远小的情况的，也就是同一目标在不同距离的成像上，尺度可能会有非常大的差异。
因此，在进行多尺度训练时，首先需要统计数据集中同一类别目标的面积差异分布，然后据此设计出大致的缩放范围，再进行消融实验找到最佳的缩放尺度。
基本步骤：
首先，以图片原始大小为基准，设置缩放比例范围为(1.5, 3.5)进行图片和目标的缩放；
然后，设置最大缩放面积，对于缩放后超出最大面积的图片，使用最大面积进行截断处理。
StackImage
在比赛后期，由于提供了大量的无标签数据，因此自然想到为无标签数据生成伪标签来进行半监督学习，为此我们开发了StackImage数据增强方法，其目的在于：
a. 增强样本多样性
b. 学习无标签数据上的前景和背景信息
c. 通过拼接强监督信息和弱监督信息，达到弱化伪标签中噪音数据的目的。
基本步骤：
a. 同时取一张带标签的图片和一张无标签的图片，无标注图片使用半监督标注信息
b. 将两张图像以水平或垂直的方式进行stack，方向按照面积最小的原则
c. 无法完全对齐的地方用255填充
模型选择和训练 模型选择
检测框架：mmdetection
检测模型：
虽然目前swin-transformer很火，但是由于对其不是非常熟悉，另外transformer系列模型训练一般都需要较长时间和较大的GPU显存，因此选用二阶段经典网络faster-rcnn作为基线模型。
这里没有使用去年冠军方案使用的cascade-rcnn作为检测模型基线，主要是考虑到以下几点：
a. cascade-rcnn主要是对与gt的iou大于0.5的bbox的进一步优化坐标，对AP50的提升贡献较小。
b. cascade-rcnn模型较大，不利于后期模型融合策略的使用。
c. cascade-rcnn模型占用显存较大，且需要更长的训练时间。
backbone选择：
res2net101，不解释了，又强又快。
模型选择小技巧：根据经验，coco检测模型预训练相比于imagenet分类预训练有更好的效果，因此优先选择mmdetection中有coco检测预训练权重的模型。
总结：
尽量使用框架来训练。减少编码负担。 对赛题进行分析选择baseline，不是拿到手就用。 Tricks
基本调参，根据对赛题的分析和对赛题的理解，对模型内相关参数尽量调整，比如学习率、rcnn正负样本采样数量、学习率衰减策略等。
模型组件&ndash;FPN，增强对小目标的识别能力，一般mmdetection实现的fasterrcnn有自带。
模型组件&ndash;DCN，DCN一般都会有比较好的效果，但是也会增加较长训练时间，可以考虑在比赛的后期再加上
模型组件&ndash;GC，有类似于空间注意力机制的作用。
训练策略&ndash;Class Loss Weight，由于AP50的指标更看重模型的分类性能，因此可以适当调大分类损失的权重，经过实验由1调整至1.25效果较好。
预训练权重&ndash;CocoPretrain，相比较于仅使用ImageNet的分类模型预训练，使用COCO的检测模型预训练能稳定涨点。
模型压缩&ndash;FP16，本次比赛有模型大小限制，因此在模型训练之后将其权重由FP32转变为FP16，其大小能降低一半。
半监督学习 本次比赛提供了大量的无标签数据，如表所示，可以看到有四批数据共15000张无标签数据，仅4000张有标签数据，因此如何使用半监督学习的方式利用好这15000张半监督数据样本，也是本次比赛的关键。
基本流程有以下步骤： a. 首先使用有标签数据，训练出一个较好的模型，然后在无标签数据上推理，得到伪标签，并使用阈值进行过滤掉分数低的目标框。 b. 然后重新训练模型，将有标签的样本和伪标签的样本使用StackImage的方式进行拼接，然后送入到模型进行训练。 c. 训练后的模型，在测试集上效果变的更好了，再使用这个模型重新生成无标注数据的伪标签并进行阈值过滤，然后再重复上述训练过程。 d. 直至模型在测试集上的分数不再上升为止。 模型融合 模型内部融合
模型内部融合我们采取的策略是结合图像尺度和数据增强，其实也就是TTA。(Test-Time Augmentation) 训练时数据增强。
在尺度方面，在训练时设置的(1.5, 3.5)范围内选择多个尺度进行消融实验，最终确定使用2.0, 2.5, 3.0的缩放比例，然后分别进行模型推理。
在数据增强方面，在不同的尺度下，进行verticalFlip、Rotate90、Horizontal Flip数据增强，然后进行模型推理。
最终将得到的多尺度多数据增强的推理结果进行融合。 // 多尺度 多数据增强 的结果融合。
模型之间的融合
比赛最后发现，仅使用res2net101-fasterrcnn单模型TTA就已经能够稳坐第一的位置，但是为了能有更好的成绩，我们选择在比赛限制内，融合更多的模型。
由于比赛限制模型大小600M，因此在这个范围内，经过了FP16的压缩，可以进行以下三个模型的融合：
a. res2net101-fasterrcnn
b. resnext101_32x4d-fasterrcnn
c. resnext101_64x4d-fasterrcnn
使用这个三个模型，分别完成上述的模型训练过程，然后各自进行模型内部融合，将各自的融合结果进行模型之间的融合。
模型融合方法
模型融合采用WBF融合策略，如下图所示。 // WBF 加权融合。 将bbox的坐标根据置信度加权平均，将置信度平均。
注意：针对不同的任务，WBF的过程需要一定程度的调参实验，可得到较好的效果。
总结 模型选择
基线模型：Faster-RCNN
BackBone：Res2Net101、Resnext101_32×4d、Resnext101_64×4d
检测框架：MMDetection
主要技术
数据增强：数据平衡、StackImage、 MixupObject、FixScale
Tricks：半监督学习、DCN、 Global Context、COCO预训练、TTA、 WBF、ClsLossWeight
思考 充分理解赛题和评价指标，充分理解经典模型的原理及其适用场景，以便更好地进行模型选型。 关注新技术，决赛时发现大部分选手都使用了swin-transformer，说明应该还是很强的。 充分理解数据，决赛时才知道domain5、domain6是来自两个不同X光机器的图片，如果更细致去分析其中的特点进行相关工作，可能会有更好的效果。 充分理解评价指标，可以看到计算AP50时，不同类别的权重是不一样的，这一点暂时没想好更好的办法利用这一点得到更好的结果。 备注知识 bbox。在目标检测里，我们通常使用边界框（bounding box）来描述目标位置。 WBF 加权融合。 将bbox的坐标根据置信度加权平均，将置信度平均。 TTA (Test-Time Augmentation)。 通过训练的时候，对数据进行增强，来强化效果。 总结 基线模型并不复杂。但是数据清理很重要。包括对数据进行增强也很重要。 使用Tricks等，帮助模型提高效果。 思路很直接，发现问题，穷追问题，解决问题。 数据类别不均衡。=》平衡类别 样本少+常用tricks =》 mixup贴图到前景图片上 目标有近大远小的特点，但X光大小较位相同。但不完全一样 =》 所以统计同一类目标的差异分布，再找到最佳的缩放尺度。 又有了大量的无标签数据。 =》 寻找方式进行stack。根据多目标检测的特征，将无标签数据和标签数据stack在一张图里 避免采用不熟悉的算法，对算法熟悉才能使用更好的算法 = 》 选用res2net 模型调参，知道哪些参数对当前任务效果贡献更大。 =》进一步调参 又来了无标签数据 => 伪标签 模型融合 =》 内部融合、模型之间融合、融合方法的确定 你自己总结一边就知道，全部都是对症下药。发现问题解决问题。这才是为什么能拿高分的思路。 所以首先对这种问题足够熟悉，第二对相关技术、模型熟悉，第三才能发现问题，针对问题进行改进。</content></entry><entry><title>2 概率与信息论</title><url>http://next.lisenhui.cn/post/study/deeplearning/2-%E6%A6%82%E7%8E%87%E4%B8%8E%E4%BF%A1%E6%81%AF%E8%AE%BA/</url><categories/><tags/><content type="html"> 概率论意义 随机变量 概率分布 边缘概率 条件概率 条件概率的链式法则 独立性和条件独立性 期望、方差、协方差 常用概率分布 常用函数的有用性质 贝叶斯规则 连续型变量的技术细节 信息论 结构化概率模型 概率论是用于表示不确定性声明的数学框架。它不仅提供了量化不确定性的方 法，也提供了用于导出新的不确定性声明（statement）的公理
概率法则告诉我们 AI 系统如何推理。其次，我们可以用概率和统计从 理论上分析我们提出的 AI 系统的行为
概率论意义 几乎所有的活动都需要一些在不确定性存在的情况下进行推理的能力 不确定性有三种可能的来源 被建模系统内在的随机性 不完全观测。 不完全建模。当我们使用一些必须舍弃某些观测信息的模型时，舍弃的信息会 导致模型的预测出现不确定性。 使用一些简单而不确定的规则要比复杂而确定的规则更为实用 ‘多数鸟儿都会飞’’ 这个简单的规则描述起来很简单很并且使用广泛 ‘除了那些还没学会飞翔的幼鸟，因为生病或是受伤而失去了飞翔能力的 鸟，包括食火鸟 (cassowary)、鸵鸟 (ostrich)、几维 (kiwi，一种新西兰产的无翼鸟)等不会飞的鸟类……以外，鸟儿会飞’’，很难应用、维护和沟通，即使经过这么多的 努力，这个规则还是很脆弱而且容易失效。 频率派概率和 贝叶斯概率 当我 们说一个结果发生的概率为 p，这意味着如果我们反复实验 (例如，抽取一手牌) 无限次，有 p 的比例可能会导致这样的结果 在医生诊断病人的例 子中，我们用概率来表示一种信任度（degree of belief）， 前面那种概率，直接与事件发生的频 率相联系，被称为频率派概率（frequentist probability）； 而后者，涉及到确定性水 平，被称为贝叶斯概率（Bayesian probability） 表征信任度的概率，我们称为贝叶斯概率。 概率可以被看作是用于处理不确定性的逻辑扩展。逻辑提供了一套形式化的规 则，可以在给定某些命题是真或假的假设下，判断另外一些命题是真的还是假的。概率论提供了一套形式化的规则，可以在给定一些命题的似然后，计算其他命题为真的似然。 似然：likelihood 即文言文版的可能性。 随机变量 随机变量（random variable）是可以随机地取不同值的变量 例如，x1 和 x2 都是随机变量 x 可能的取值 向量值变量，我们会将随机变量写成 x 概率分布 概率分布（probability distribution）用来描述随机变量或一簇随机变量在每一 个可能取到的状态的可能性大小。我们描述概率分布的方式取决于随机变量是离散 的还是连续的 离散型变量的概率分布可以用概率质量函数（probability mass function, PMF）我们通常用大写字母 P 来表示概率质量函数 有时为了使得PMF的使用不相互混淆，我们会明确写出随 机变量的名称：P(x = x)。 有时我们会先定义一个随机变量，然后用 ∼ 符号来说明它遵循的分布：x ∼ P(x) 这种多个变量的概率分布被称 为联合概率分布（joint probability distribution）。P(x = x, y = y) 表示 x = x 和 y = y 同时发生的概率 如果一个函数 P 是随机变量 x 的 PMF，必须满足下面这几个条件： P 的定义域必须是 x 所有可能状态的集合。 ∀x ∈ x, 0 ≤ P(x) ≤ 1. 不可能发生的事件概率为 0，并且不存在比这概率更低 的状态。 ∑ x∈x P(x) = 1. 我们把这条性质称之为归一化的（normalized）。 考虑一个离散型随机变量 x 有 k 个不同的状态。我们可以假设 x 是**均匀 分布（uniform distribution）**的。通常用 x ∼ U(a, b) 表示 x 在 [a, b] 上是均匀分布的。 连续型变量和概率密度函数 当我们研究的对象是连续型随机变量时，我们用概率密度函数（probability density function, PDF） 如果一个函数 p 是概率密度函数，必须满足下面这几个条件 p 的定义域必须是 x 所有可能状态的集合 ∀x ∈ x, p(x) ≥ 0. 注意，我们并不要求 p(x) ≤ 1。 ∫p(x)dx = 1. 概率密度函数 p(x) 并没有直接对特定的状态给出概率，相对的，它给出了落在 面积为 δx 的无限小的区域内的概率为 p(x)δx。 边缘概率 但想要了解其中一个子集的概 率分布。这种定义在子集上的概率分布被称为边缘概率分布（marginal probability distribution）。 条件概率 在很多情况下，我们感兴趣的是某个事件，在给定其他事件发生时出现的 概率。这种概率叫做条件概率。 条件概率只在 P(x = x) > 0 时有定义 计算一个行动的后果被称为干预 查询（intervention query）。干预查询属于因果模型（causal modeling）的范畴，我 们不会在本书中讨论 条件概率的链式法则 任何多维随机变量的联合概率分布，都可以分解成只有一个变量的条件概率相 乘的形式： P(x(1), . . . , x(n)) = P(x(1))Πn i=2P(x(i) | x(1), . . . , x(i−1)). 这个规则被称为概率的链式法则（chain rule）或者乘法法则（product rule）。 独立性和条件独立性 两个随机变量 x 和 y，如果它们的概率分布可以表示成两个因子的乘积形式，并 且一个因子只包含 x 另一个因子只包含 y，我们就称这两个随机变量是相互独立的 那么这两个随机变量 x 和 y 在给定随机变量 z 时是条件独立的（conditionally 我们可以采用一种简化形式来表示独立性和条件独立性：x⊥y 表示 x 和 y 相互 独立，x⊥y | z 表示 x 和 y 在给定 z 时条件独立。 期望、方差、协方差 函数 f(x) 关于某分布 P(x) 的期望（expectation）或者期望值（expected value）是指，当 x 由 P 产生，f 作用于 x 时，f(x) 的平均值 我们假设 E[·] 表示对方括号内的所有随机变量的值求平均。 类似的，当没有歧义时，我们还可以省略方括号。 **方差（variance）**衡量的是当我们对 x 依据它的概率分布进行采样时，随机变 量 x 的函数值会呈现多大的差异：Var(f(x)) 当方差很小时，f(x) 的值形成的簇比较接近它们的期望值。方差的平方根被称为标准差（standard deviation）。 **协方差（covariance）**在某种意义上给出了两个变量线性相关性的强度以及这些 变量的尺度 协方差的绝对值如果很大则意味着变量值变化很大并且它们同时距离各自的均值很 远。 如果协方差是正的，那么两个变量都倾向于同时取得相对较大的值 如果协方 差是负的，那么其中一个变量倾向于取得相对较大的值的同时，另一个变量倾向于取得相对较小的值， 它们是有联系的，因为 两个变量如果相互独立那么它们的协方差为零，如果两个变量的协方差不为零那么它们一定是相关的 两个变量相互依赖但具有零协方差是可能的。例如，假 设我们首先从区间 [−1, 1] 上的均匀分布中采样出一个实数 x。然后我们对一个随机 变量 s 进行采样。s 以 12 的概率值为 1，否则为-1。 常用概率分布 Bernoulli 分布（Bernoulli distribution）是单个二值随机变量的分布。它由单 个参数 ϕ ∈ [0, 1] 控制，ϕ 给出了随机变量等于 1 的概率。 Multinoulli 分布（multinoulli distribution）或者范畴分布（categorical distribution） 实数上最常用的分布就是正态分布（normal distribution），也称为高斯分布（Gaussian distribution） 正态分布由两个参数控制，µ ∈ R 和 σ ∈ (0,∞) 当我们由于缺乏关于某个实 数上分布的先验知识而不知道该选择怎样的形式时，**正态分布是默认的比较好的选择，**其中有两个原因。 第一，我们想要建模的很多分布的真实情况是比较接近正态分布的。中心极限 定理（central limit theorem）说明很多独立随机变量的和近似服从正态分布。 第二，在具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大 的不确定性。因此，我们可以认为正态分布是对模型加入的先验知识量最少的分布。 我们常常把协方差矩阵固定成一个对角阵。一个更简单的版本是各向同性（isotropic）高斯分布，它的协方差矩阵是一个标量乘以单位阵。 指数分布和 Laplace 分布 我们经常会需要一个在 x = 0 点处取得边界点 (sharp point) 的 分布。为了实现这一目的，我们可以使用指数分布 一个联系紧密的概率分布是Laplace 分布（Laplace distribution），它允许我们 在任意一点 µ 处设置概率质量的峰值 在一些情况下，我们希望概率分布中的所有质量都集中在一个点上。这可以通 过Dirac delta 函数（Dirac delta function）δ(x) 定义概率密度函数来实现。 通过组合一些简单的概率分布来定义新的概率分布也是很常见的。一种通用的组 合方法是构造混合分布（mixture distribution）。 一个非常强大且常见的混合模型是高斯混合模型（Gaussian Mixture Model）， 它的组件 p(x | c = i) 是高斯分布 高斯混合模型的参数指明了给每个组件 i 的先验概率 （prior probability）αi = P(c = i)。 ‘‘先验’’ 一词表明了在观测到 x 之前传递给模 型关于 c 的信念 P(c | x) 是后验概率（posterior probability），因为它 是在观测到 x 之后进行计算的 高斯混合模型是概率密度的万能近似器（universal approximator），在这种意义下，任何平滑的概率密度都可以用具有足够多组件的高斯混合模型以任意精度来逼近。 常用函数的有用性质 sigmoid 函数 在变量取绝对值非常大的正值或负值时会出现饱和（saturate）现象，意味着函数会变得很平，并且对输入的微小改变会变得不敏感 另外一个经常遇到的函数**softplus **函数（softplus function） softplus 函数名来源于它是另外一个函数的平滑（或 ‘‘软化’’）形式，这个函数是 x+ = max(0, x). 贝叶斯规则 我们经常会需要在已知 P(y | x) 时计算 P(x | y)。幸运的是，如果还知道 P(x)， 我们可以用贝叶斯规则（Bayes’ rule）来实现这一目的： 它通常使用 P(y) = 所以我们并不需要事先知道 P(y) 的信息。 连续型变量的技术细节 对于我们的目的，测度论更多的是用来描述那些适用于 Rn 上的大多数点，却不 适用于一些边界情况的定理。测度论提供了一种严格的方式来描述那些非常微小的点集。这种集合被称为 “零测度（measure zero）’’ 的 另外一个有用的测度论中的术语是 “几乎处处（almost everywhere）’’。某个性 质如果是几乎处处都成立的，那么它在整个空间中除了一个测度为零的集合以外都是成立的。 信息论 信息论的基本想法是一个不太可能的事件居然发生了，要比一个非常可能的事 件发生，能提供更多的信息。 非常可能发生的事件信息量要比较少，并且极端情况下，确保能够发生的事件 应该没有信息量。 较不可能发生的事件具有更高的信息量。 独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量， 应该是投掷一次硬币正面朝上的信息量的两倍。 我们定义一个事件 x = x 的自信息（self-information） I(x) = −logP(x). 自信息只处理单个的输出。我们可以用香农熵（Shannon entropy）来对整个概 率分布中的不确定性总量进行量化 当 x 是连续的，香农熵被称为微分熵（differential entropy）。 说明了更接近确定性的分布是如何具有较低的香农熵，而更 接近均匀分布的分布是如何具有较高的香农熵。 如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可 以使用KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异： KL 散度衡量的是，当我们使用一种被设计成能够使 得概率分布 Q 产生的消息的长度最小的编码 KL 散度为 0 当且仅当 P 和 Q 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是 ‘‘几乎处处’’ 相同的 个和 KL 散度密切联系的量是交叉熵（cross-entropy） 结构化概率模型 由 一些可以通过边互相连接的顶点的集合构成。当我们用图来表示这种概率分布的分 解，我们把它称为结构化概率模型（structured probabilistic model）或者图模型（graphical model）。 有向（directed）模型使用带有有向边的图，它们用条件概率分布来表示分解， **无向（undirected）模型使用带有无向边的图，**它们将分解表示成一组函数 随机变量的联合概率与所有这些因子的乘积成比例（proportional）——意味着因子的值越大则可能性越大 这些图模型表示的分解仅仅是描述概率分布的一种语言。它们不是互 相排斥的概率分布族。有向或者无向不是概率分布的特性；它是概率分布的一种特 殊描述（description）所具有的特性</content></entry><entry><title>Kaggle记录总结</title><url>http://next.lisenhui.cn/post/study/kaggle/kaggle%E6%80%9D%E8%B7%AF%E6%80%BB%E7%BB%93%E5%92%8C%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/</url><categories/><tags/><content type="html"> 学习路线 kaggle要怎么学？
kaggle相关技术。当空闲时确定一个kaggle技术进行学习 kaggle比赛总结。 kaggle比赛尝试。 kaggle资料 Kaggle 竞赛复盘汇总： https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzIwNDA5NDYzNA==&amp;action=getalbum&amp;album_id=1380279189986787330&amp;scene=173&amp;from_msgid=2247495639&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirect 经验贴 https://zhuanlan.zhihu.com/p/25742261 一般流程总结 流程：
首先最开始的时候对数据进行处理： 将数据处理成pandas（详细学习使用pandas） 查看是否有NAN值 查看是否有异常值 查看数据的众数 画图看数据的分布。使用log查看分布差异较大的数。 查看数据和label的相关性 // 重要，因为数据和label紧密相关，就应该好好对待。 针对不同的数据使用不同的算法： 特征数据： 变换为图像，然后使用CV相关思路。1d CNN, Resnet 1d等 文本数据 使用bert对数据进行预训练。 预训练思路有很多，包括使用masked在文本上微调。 或者使用文本分类，直接运算。 使用tf-idf+svd分析文本中的词频。 使用ldm对文本主题进行分析。 归一化处理 很多树模型不需要归一化，但是神经网络模型大多必需归一化。归一化也有很多，高斯归一化等等，了解他们之间的差异。 然后快速确定一个baseline 根据评分公式计算评分。方便后面评价模型 对数据进行训练集、测试集、验证集的分类。 不要觉得这样分类，减少了数据集会影响分类，其实不是的。可以帮助前期自己对模型的验证。 常用的树模型：lgbm、catboost、xgboost。等快速的api可以帮助快速出一个api 常用的神经网络模型。MLP等。 使用常用的ML框架：Pycaret等。 在baseline上进行改进 前期脑洞大开，后期小心谨慎 包括ms等方法只能提升微小且不可忽略的进步，但不能依靠这个就大幅度增长分数。前期必须要脑洞大开。后期必须小心谨慎 双塔模型。 双塔最初用在推荐系统上，将人的行为特征和商品的行为特征进行匹配。在这里我们可以分别使用不同的模型对不同类型的特征进行建模。这是十分重要的。不同类型的数据显然不应该放在一起进行训练、测试。 对特征进行重新提取、筛选。 伪标签技术。 使用正负置信度为0.9-0.99的标签二次加入训练 使用所有标签加入训练，但是为伪标签使用不同的权重矩阵。权重取决于它们的loss函数。 后期优化： 使用meta_stacking对模型进行搭建。 这个时候可以用上之前的验证集了。使用验证集训练元学习器。 对选定的模型进行调参。使用GridSearch或者使用贝叶斯调参 整体注意事项 需要动手去写的东西并不多，更多的是思路。题目一样的，解法一样，如何取得更好的分数呢？ 数据特征往往很重要，是训练的基石。 不要全身心投入去做一件事。多件事情交叉，让自己的思路打开。 不要在程序开始就进行优化。边写脚本，边对模块进行集成。在写模块时，尽量和现有的框架的api进行匹配，减少记忆负担。对看到比较好的方法进行记录。 Notes:
学好pandas 学好seaborn 学好bert模型。对transform系统的学习，能够手撸代码。 以后学习不能不求甚解。对不了解的一定要了解清楚。刨根问底。如果一个知识点足够重要，就开一个md专门写。 多少数据维度是大维度？心里没概念。多看看别人的经验贴，总结能够明白的信息。总结不能明白的信息。要多多调参。 失败总结经验：
bert没用熟，很多框架第一次上手，完全不知道怎么弄。 不清楚nn网络能有这么大的效果。 以为不是靠调参解决的，实际上就是靠调参解决的。 调参经验不足。就是经验太少。 多少数据维度是大维度？心里没概念。多看看别人的经验贴，总结能够明白的信息。总结不能明白的信息。 工具记录 时序工具。 https://mp.weixin.qq.com/s/sO-Od9x_QH27zJOg6e_FKg</content></entry><entry><title>Kaggle祖传代码</title><url>http://next.lisenhui.cn/post/study/kaggle/kaggle%E7%A5%96%E4%BC%A0%E4%BB%A3%E7%A0%81/</url><categories/><tags/><content type="html"> 这里给出祖传代码list 使用高斯归一化 使用贝叶斯搜寻参数 常用的utils。包括归一化函数、pytorch训练等。 使用bert进行masked 预训练。 编写祖传代码 尽量高类聚，低耦合的代码。提高可复用性。意思就是，我是用这个函数，可以快速适配。</content></entry><entry><title>Kaggle前人经验</title><url>http://next.lisenhui.cn/post/study/kaggle/kaggle%E5%89%8D%E4%BA%BA%E7%BB%8F%E9%AA%8C/</url><categories/><tags/><content type="html"> 大佬经验总结 一位master： 网址:https://mp.weixin.qq.com/s?__biz=Mzk0NDE5Nzg1Ng==&amp;mid=2247490499&amp;idx=1&amp;sn=b9b21c447a31c9532b1ce66961ae0b20&amp;chksm=c329024cf45e8b5a2834a0be9c50420c6933ffe5bc3e348b29a0bd7a1417a0aa7056fc1a0a97&amp;scene=21#wechat_redirect
【起初，我的挑战是我不是很好。我没想到最后会进入前1%，但我喜欢进步。这有助于我每天继续工作。如果我明确地想进入前1%，我可能在到达前就放弃了。这是一个如此艰难的目标，我会放弃我永远无法实现的想法。】
【我认为先学很多理论，然后再开始做项目是错误的。我看到一些人花了很多年时间成为数据科学家，但他们仍然不太了解在实践中这些是如何工作的。 相反，我更倾向于学习最起码的你需要尝试一个项目，如Kaggle竞赛。在你有了实践经验之后，再学习更多的理论来理解理论的适用性。 另外，您绝对需要学习如何使用Git以及如何与其他人协作。 最后，学会用好Pandas。 大多数数据科学家花在处理和清理数据上的时间是使用花在算法的10倍。深入学习也许很有趣，但Pandas更实用。】
学会使用pandas。 学会seaborn 目标是进步，而不是某个成绩。追求卓越，成功自然追着你跑。 不要陷入理论。应该实践和理论相结合。 DOTA 朱宇翔 “只定目标，不做计划” 一个连自己情绪都控制不了的人，又能在未来走多远呢？
如果你没有项目的磨炼，那你应该有着扎实的基本功，并且在基础知识之上你应该有些自己的思考，大多数面试官是在发现你的优点，这也是由你简历入手去了解你的一个过程，初始分50分，每次回答与对问题的理解，为你加减分，所以扎扎实实，拿出你最好的一面，展示自己。最后，好好刷LeetCode！LeetCode！LeetCode！
工作与学习并行 工作习惯的养成 人生目标的设定 战斗意志-逢敌亮剑 拥抱变化 志同道合的小伙伴。</content></entry><entry><title>Gensim学习笔记</title><url>http://next.lisenhui.cn/post/study/python%E5%BA%93/gensim%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url><categories><category>学习</category></categories><tags><tag>python相关库学习</tag></tags><content type="html"> 使用腾讯中文语料库 腾讯词向量使用方法： https://www.jianshu.com/p/65a29663130a
语料库地址： https://ai.tencent.com/ailab/nlp/zh/embedding.html
语料库 chinese-word-vectors: https://github.com/Embedding/Chinese-Word-Vectors 300维 腾讯词向量： https://www.jianshu.com/p/65a29663130a 200维，800万词。 其他多语言语料库 https://sites.google.com/site/rmyeid/projects/polyglot 使用gensim 来读取预训练语料库</content></entry><entry><title>ATEC科技精英赛</title><url>http://next.lisenhui.cn/post/study/kaggle/kaggle%E6%AF%94%E8%B5%9B%E8%AE%B0%E5%BD%95/atec%E7%A7%91%E6%8A%80%E7%B2%BE%E8%8B%B1%E8%B5%9B/</url><categories/><tags/><content type="html"> 结束复盘 2021 ATEC科技精英赛（网络欺诈举报定性）比赛分享 - 机器学习小谈的文章 - 知乎 https://zhuanlan.zhihu.com/p/434432485 https://jishuin.proginn.com/p/763bfbd6d31e
比赛题目 第一道题：
赛题从当前社会中高发的电信网络欺诈识别场景入手，提供模拟的“用户”投诉欺诈信息，要求选手识别投诉中的欺诈风险。
本赛道将选取工业应用中常见的、由于“数据源差异”、“数据维度特征缺失”而导致的、模型应用困难的问题， 考察AI模型如何通过多源数据的有效应用以及半监督学习技术，实现有限数据下的模型决策，从而思考如何减少AI对数据依赖的问题。 赛题从当前社会中高发的电信网络欺诈识别场景入手，提供模拟的“用户”投诉欺诈信息，要求选手识别投诉中的欺诈风险。
11.5 10am 截止。从现在开始满打满算15天。
官方网址: https://www.atecup.cn/competitionIntroduction
第一道题网址：https://www.atecup.cn/trackDetail/1
评分 参赛选手提交模型并对榜单数据进行打分（对样本进行欺诈概率预测），后台通过对打 分结果测算出精确率(Precision)、召回率(Recall)曲线，从中推算出：到达精确率为90% 的模型分数阈值下对应的召回率、到达精确率为85%的模型分数阈值下对应的召回率、 到达精确率为80%的模型分数阈值下对应的召回率，三项召回率进行权重分别为0.4、 0.3和0.3加权融合，作为比赛榜单分数。
Score: 0.4 x Recall precision = 90 + 0.3 x Recall precision = 85 + 0.3 x Recall precision = 80
服务器使用规则 本地服务器配置及使用规则：CPU:8 core；Memory:16GB；GPU:无；Storage:256G ，不限时使用。
公有池服务器配置及使用规则：模型训练可使用含GPU的公有池服务器资源，训练任务占用资源为CPU:6 core，memory:16g； GPU 1卡(v100), Storage:20G。每次任务最长使用2小时。 每队每周公有池服务器资源使用上限30小时，不作累积。
比赛期间排行榜显示A榜成绩排名，比赛结束后 24小时内 展示B榜成绩排名。比赛结束倒计时24小时内允许提交B榜验证任务；提交后，首次运行成功所得分数即为B榜成绩，之后不能再提交打分任务。如需支持，可联络“咨询中心”。 最终比赛成绩以B榜单为准（如B榜成绩持平，则A榜成绩高者排名在前）。 另，为了鼓励更切合实际应用的模型及方案，A/B榜模型提交的打分任务均将限制在 10分钟内 完成。
关于“周”的说明
线上赛合计21天，每7*24小时为1周，共3周。
第一周 指2021年 10月15日 10:00 AM – 10月22日 10:00 AM
第二周 指2021年 10月22日 10:00 AM – 10月29日 10:00 AM
第三周 指2021年 10月29日 10:00 AM – 11月5日 10:00 AM
A榜验证打分每队每24小时最多 10 次（自2021年10月15日10:00AM起算），剩余次数不做累计。 选手可以通过Jupyternotebook进行编码开发。
数据 数据为模拟生成的用户支付宝欺诈投诉举报数据，
标签1代表欺诈案件， 标签0代表非欺诈案件， 标签-1代表未知， 另，测试数据不含-1标签。 数据包含481个特征，其中480个为结构化特征，1个为非结构化的特征。结构化特征包含：欺诈投诉举报案件中主被动双方的相关风控特征，非结构化特征为举报描述信息。无具体特征含义说明。
本赛道所有相关数据（包括但不限于训练数据集）不得以任何形式下载，仅限在主办方提供的本地服务器及含GPU的公有池服务器上、以比赛为目的使用， 违者将被视作“获取未授权数据”，将依照大赛规则，作禁赛处理。
比赛全程只允许使用主办方提供的本地服务器以及含GPU的公有池服务器资源。
如模型训练非必要用到GPU资源，建议优先使用每队不限时的本地服务器资源，以节约每队公有池服务器用时限额。
数据分析 第一维为id，&ldquo;x0&rdquo;-&ldquo;x479"为特征。特征不清楚定义。memo_polish为文本特征 id没有重复用来输出这个样本是否是欺诈案件。 方案 480维度特征，按列进行归一化。 最后一维度特征首先使用word2vec，编码为特征 对数据特征进行降维：PCA, t-SVD，dimension * 0.8-0.9 特征增强(nlp)概念 使用神经网络。autoencoder。CNN、transformer 使用combination，联合多个模型输出test评分。 神经网络三种方式：
深度神经网络。如Transformer CVPR最好的网络 标签为1的半监督 用模型推标签 无标签的自监督学习。 神经网络方法备注:
lgbm方法调研 问题：
特征和nlp特征分别处理再合起来 中文文本处理是否需要用TF-IDF这种编码。处理词频的？ 中文文本处理的问题 预训练语料库太大。腾讯的这个有6.3g 先进行中文分词。然后进行语料库的pretrain导入。 朋飞学长资料： 使用jieba进行分词 使用语料库读取pre-train模型 对训练集数据的词进行统计，然后给出词列表。然后读取embedding然后存储。 资源上传大小限制2G 文本中有中文有数字。 2个小时，是指提交到服务器运行就算时间。不管用不用gpu 时间安排 出炉以天为单位的任务安排 每两天对接一次情况 任务安排： 首先摸清楚数据集的情况 摸清楚实验环境、熟悉实验环境。 完成初步的神经网络搭建 搭建一个使用gpu训练的方式。 在jupyter 中完成归一化、降维、使用autoencoder。 弄清楚整个gpu的训练流程 再弄清楚整个训练流程、方式。 调研： transformer, light transformer 自监督学习 训练的trick 数据测试tta 统计gpu服务器上的数据 将整个步骤打通。 安装朋飞学长依赖库 放到csv中。 统计有没有缺失 如果是使用csv处理的话，肯定不好。因为gpu服务器的数据不行，你不能转成csv再处理。那样比较费时间吧。 搭建投票系统 按照lgbm调参指南来训练 还没发给我 根据评分来计算表格 添加伪标签 过滤掉冗余特征 对过度重合的特征进行筛选 登录服务器步骤 登录信息 ssh -p 60022 6brdcr99q8@onyiwjcofa-public.bastionhost.aliyuncs.com
passwd zIxH5KfSXl3_XIfjqM
堡垒机
账号 6brdcr99q8 密码 zIxH5KfSXl3_XIfjqM 本地服务器： 账号： 2bv211k1vq 密码 Dko2502eZNGmA0mk0_
Jupyter:
账号 2bv211k1vq 密码 Dko2502eZNGmA0mk0_ 登录后常用操作 激活环境 source ~/atec_project/train/your_name_env/bin/activate 提交一次训练 // 将所有文件放在train目录下，通过run.sh来 python train.py // vtag每次不一样。 cd ~/atec_project docker build -f Dockerfile.train &ndash;network=host -t atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com/zark/atec-228:vtag .
// 获取临时用户名密码 adabench_cli auth get-docker-token
// 登录 docker login &ndash;username=cr_temp_user atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com
// 提交 docker push atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com/zark/atec-228:vtag
// 运行任务 -t 指定任务类型，trian为训练，rank_a/rank_b分别刷a/b榜。 adabench_cli task run -i atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com/zark/atec-228:vtag -t train // 返回job id
v_ttw_1
// 查看任务 adabench_cli task status -j
adabench_cli task status -j job_id
// 查看日志 adabench_cli task logs -j
adabench_cli task logs -j job_id
// 下载产出的模型(只有训练) 只保存24个小时 adabench_cli task download &ndash;path /home/2bv211k1vq/atec_project/trained_models -j job_id // 解压产出的打包模型 tar xzf ***.tar.gz
// 任务停止 adabench_cli task stop &ndash;j [任务id]
============================================================= // 打榜 // 生成镜像 cp -af ~/atec_project/train/your_name_env ~/atec_project/rank/
docker build -f Dockerfile.rank &ndash;network=host -t atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com/zark/atec-228:vtag .
adabench_cli auth get-docker-token
docker login &ndash;username=cr_temp_user atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com
docker push atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com/zark/atec-228:vtag
// 运行:rank_a/rank_b分别刷a/b榜。 比赛结束倒计时24小时内允许提交B榜验证任务；提交后，首次运行成功所得分数即为B榜成绩，之后不能再提交打分任务 adabench_cli task run -i atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com/zark/atec-228:vtag -t rank_a // 返回job_id
adabench_cli task status -j adabench_cli task status -j job_id
第一次提交 {'__metrics': {&lsquo;code&rsquo;: 0, &lsquo;score&rsquo;: 0.5302481389578164, &lsquo;msg&rsquo;: &lsquo;success&rsquo;}
注意事项 赛道一服务器操作指南： https://www.atecup.cn/machineGuide
使用jupyter开发的话，记得保持jupyter的环境和your_env_name环境一致。 每次提交rank的时候复制实验环境到rank 本地ECS机器训练数据集存在于本机的/mnt/atec/train.jsonl 服务器端数据： 本机 /mnt/atec/train.jsonl train数据 /home/admin/workspace/job/input/train.jsonl 模型输出数据存放 /home/admin/workspace/job/output/ 关键信息存放 /home/admin/workspace/job/output/result.json 测试数据 /home/admin/workspace/job/input/test.jsonl 预测结果放到： /home/admin/workspace/job/output/predictions.jsonl 创建docker相关命令 docker images 查看所有镜像 镜像制作： cd ~/atec_project docker build -f Dockerfile.train &ndash;network=host -t atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com/zark/atec-228:v1 . v1为tag名。每次tag不一样。 docker登录。每次传输前获取镜像密码 adabench_cli auth get-docker-token 账号：cr_temp_user docker login &ndash;username=cr_temp_user atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com 登录后提交： docker push atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com/zark/atec-228:v1 大赛触发任务执行： adabench_cli task run -i atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com/zark/atec-228:v1 -t train 会返回一个job_id 就是后面的task id adabench_cli task status -j job_id adabench_cli task logs -j job_id adabench_cli task download -j job_id 训练说明： 训练和刷榜任务的输出文件必须放置在/home/admin/workspace/job/output/目录下 如果为刷榜任务：打分的结果文件必须输出到/home/admin/workspace/job/output/predictions.jsonl，平台会使用这个结果文件进行打分 数据集格式： train {&ldquo;id&rdquo;: &ldquo;1&rdquo;, &ldquo;input_feat1&rdquo;: &ldquo;xx&rdquo;, &ldquo;label&rdquo;: 0} {&ldquo;id&rdquo;: &ldquo;2&rdquo;, &ldquo;input_feat1&rdquo;: &ldquo;xx&rdquo;, &ldquo;label&rdquo;: 1} test {&ldquo;id&rdquo;: &ldquo;1&rdquo;, &ldquo;input_feat1&rdquo;: &ldquo;xx&rdquo;} {&ldquo;id&rdquo;: &ldquo;2&rdquo;, &ldquo;input_feat1&rdquo;: &ldquo;xx&rdquo;} predictions.jsonl {&ldquo;id&rdquo;: &ldquo;1&rdquo;, &ldquo;label&rdquo;: 0.4} {&ldquo;id&rdquo;: &ldquo;2&rdquo;, &ldquo;label&rdquo;: 0.8} 数据统计 首先统计各个数值平均数、众数、最大值、最小值 看看标签为 1 的label长什么样子。 相关资料 一些其他比赛的资料。 https://mp.weixin.qq.com/s/vSVhearDrZB3OXW4CXaDGQ 我们最后进行赛后自我总结 如果后面有人分享方案的话，时刻保持关注一下，分析一下前三的每个人的方案。
然后我们开始分析下自己的方案进行总结：
流程：
首先最开始的时候对数据进行处理： 将数据处理成pandas（详细学习使用pandas） 查看是否有NAN值 查看是否有异常值 查看数据的众数 画图看数据的分布。使用log查看分布差异较大的数。 针对不同的数据使用不同的算法： 特征数据： 变换为图像，然后使用CV相关思路。1d CNN, Resnet 1d等 文本数据 使用bert对数据进行预训练。 预训练思路有很多，包括使用masked在文本上微调。 或者使用文本分类，直接运算。 使用tf-idf+svd分析文本中的词频。 使用ldm对文本主题进行分析。 归一化处理 很多树模型不需要归一化，但是神经网络模型大多必需归一化。归一化也有很多，高斯归一化等等，了解他们之间的差异。 然后快速确定一个baseline 根据评分公式计算评分。方便后面评价模型 对数据进行训练集、测试集、验证集的分类。 不要觉得这样分类，减少了数据集会影响分类，其实不是的。可以帮助前期自己对模型的验证。 常用的树模型：lgbm、catboost、xgboost。等快速的api可以帮助快速出一个api 常用的神经网络模型。MLP等。 使用常用的ML框架：Pycaret等。 在baseline上进行改进 前期脑洞大开，后期小心谨慎 包括ms等方法只能提升微小且不可忽略的进步，但不能依靠这个就大幅度增长分数。前期必须要脑洞大开。后期必须小心谨慎 双塔模型。 双塔最初用在推荐系统上，将人的行为特征和商品的行为特征进行匹配。在这里我们可以分别使用不同的模型对不同类型的特征进行建模。这是十分重要的。不同类型的数据显然不应该放在一起进行训练、测试。 对特征进行重新提取、筛选。 伪标签技术。 使用正负置信度为0.9-0.99的标签二次加入训练 使用所有标签加入训练，但是为伪标签使用不同的权重矩阵。权重取决于它们的loss函数。 后期优化： 使用meta_stacking对模型进行搭建。 这个时候可以用上之前的验证集了。使用验证集训练元学习器。 对选定的模型进行调参。使用GridSearch或者使用贝叶斯调参 整体注意事项 需要动手去写的东西并不多，更多的是思路。题目一样的，解法一样，如何取得更好的分数呢？ 不要全身心投入去做一件事。多件事情交叉，让自己的思路打开。 不要在程序开始就进行优化。边写脚本，边对模块进行集成。在写模块时，尽量和现有的框架的api进行匹配，减少记忆负担。对看到比较好的方法进行记录。 Notes:
学好pandas 学好seaborn 学好bert模型。 以后学习不能不求甚解。对不了解的一定要了解清楚。刨根问底。如果一个知识点足够重要，就开一个md专门写。 对transform系统的学习，能够手撸代码。</content></entry><entry><title>Leetcode3_hash表</title><url>http://next.lisenhui.cn/post/study/leetcode/leetcode3_hash%E8%A1%A8/</url><categories><category>学习</category></categories><tags><tag>leetcode学习</tag></tags><content type="html"> 基础知识 hash将通过hash函数将某个东西转为数值，然后存在哈希表上。这个哈希表可能是数组，也可能是红黑树等等。
如果hashCode的数值大于哈希表了，通常会有操作如取模将值限制在哈希表内。
哈希碰撞仍然不可避免，解决方案：
拉链法。在碰撞的地方建立链表。碰撞发生后依次查找。 线性探测法：碰撞后依次往后寻找空位。需要保证dataSize&lt;hashTableSize. hash表一般在语言中分为三种：array, set, map
总体来说，有set, multiset, unordered_set这三种。
具体区别参考网址：https://github.com/youngyangyang04/leetcode-master/blob/master/problems/%E5%93%88%E5%B8%8C%E8%A1%A8%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80.md
总的来说，set、multiset用的是红黑树。set不允许重复。multiset允许key重复。unordered_set用的是哈希表。key是无序的，也不能重复。
有先使用Unordered_set或者_map，然后有序要求用set、map。要重复则multi_set, multi_map
1002 查找常用字符 这是个hash表的问题。它要找重复字符。有一种方法是所有的字符串共用一个hash表。这是统计所有字符串中字符出现的次数
还有一种方法就是每个字符串一个hash表。这是统计该字符串中字符的出现次数
结合具体情况具体分析。</content></entry><entry><title>视频剪辑计划</title><url>http://next.lisenhui.cn/post/study/%E8%A7%86%E9%A2%91%E5%89%AA%E8%BE%91/%E8%A7%86%E9%A2%91%E5%89%AA%E8%BE%91%E8%AE%A1%E5%88%92/</url><categories><category>学习</category></categories><tags><tag>视频剪辑</tag></tags><content type="html"> 视频剪辑计划 泰坦陨落2 视频剪辑音乐</content></entry><entry><title>0 引言</title><url>http://next.lisenhui.cn/post/study/deeplearning/0-%E5%BC%95%E8%A8%80%E5%92%8C%E7%AC%A6%E5%8F%B7%E8%A1%A8/</url><categories/><tags/><content type="html"> 引言 计算机可以处理形式化的语言。人工智能的一大难题就是将如何将非形式化的传达给计算机。 人工智能AI系统需要具备自己获取知识的能力，即从原始数据中提取模式的能力，这种高能力被称为机器学习。 简单机器学习的性能很大程度上取决于给定数据的表示，表示数据的选择对机器学习性能产生很大的影响。例子：笛卡尔坐标系和极坐标系下的数据表示会有不同的划分途径。p3 使用机器学习发掘表示本身，称为表示学习(representation learning)，经典例子：AutoEncoder。 设计特征、设计用于学习的特征的算法时，目标通常是分理处额能解释观察数据的变差因素(factors of variation)，这些因素时不可观察到的量或者力，会影响到可观测的量。人工智能中的困难主要源于多个变差因素同时影响着我们能观察到的数据。 从原始数据提取高层次、抽象的特征是比较困难的。深度学习(deep learning)通过其他比较简单的表示来表示复杂表示。解决了表示学习中的核心问题。输入展示在可见层，一系列从图像中提取到的越来越多的特征叫隐藏层。例子：多层感知机。 深度学习两个度量视角，一个是提取数据的正确表示。另一个是多步骤的计算机程序，多顺序指令的执行帮助计算机理解输入。 维恩图：deeplearning->Represenation learning->Machine Learning->AI 深度学习的趋势。最早有控制论、联结主义。控制论很类似傅里叶变换，使用不同的函数和不同的参数来表示一个函数。 第一次高潮、退潮
控制论：使用一组n个输入，并将它们与一个输出y相关联。这个模型希望学习到一个权重。使得$f(x,w)=x_1w_1+&hellip;+x_nw_n$。显然模型需要正确设置权重之后才能使用。 感知机是第一个能根据每个类别的输入样本来学习权重的模型。 但诸如上面所讲的线性模型，无法学习最简单的异或。这引起了神经网络的第一次退潮。 第二次高潮退潮
神经科学被认为是深度学习的一个重要灵感来源。但已经不再是该领域的主要指导。我们连大脑最简单的部分还远没有理解。 小故事：将雪貂的视觉神经和大脑听觉区域相连，它们可以学会取用听觉区域去“看”。这暗示这大多数的哺乳动物的大脑能够使用单一的算法就可以解决其大脑可以解决的大部分不同的任务。拥有这个假设，深度学习团队同时研究多个领域是很常见的。 新认知机受哺乳动物的视觉系统结构的启发，后来成为CNN的基础。目前大多数神经网络都基于整流线性单元模型。 神经科学是神经网络的重要灵感来源。但线代深度学习从许多其他领域获取灵感：线性代数、信息论、数值优化等等。 第二次浪潮：联结主义、并行分布处理。联结主义思想：当网络将大量的简单的饿计算单元连接在一起时可以实现智能行为。联结主义重要的成就就是反向传播算法的普及。 分布式表示：每一个输入都应该由多个特征表示，并且每一个特征都应该参与到多个可能的输入的表示。 第三次浪潮：
深度信念网络使用一种贪婪逐层预训练的策略来有效的训练。同样的策略可以用来训练其他许多类型的深度网络。深度学习的第三次浪潮才开始。 虽然已开始着眼于无监督学习技术，但目前更多的兴趣点仍是比较传统的监督学习算法和深度模型充分利用大型标注数据的能力。 与日俱增的数据量， 联结主义的主要见解之一是，当动物的需多神经元在一起工作的时候，会变得聪明。单独神经元或者小集合的神经元不是特别有用。几十年内，我们的机器学习模型中的每个神经元的连接数量已经和哺乳动物的大脑在同一数量级上。自从引入隐藏层后，神经元数量每2.4年增加一倍。21世纪50年代，人工智能的神经元可以达到与人脑相同的数量级。目前的网络，相对原始的脊椎动物如青蛙还小。 强化学习，在没有人类指导的情况下，通过试错来学习执行任务。深度学习爷显著改善了机器人强化学习的性能。 符号表 $\odot$ hadamard乘积。 逐元素相乘</content></entry><entry><title>线性代数</title><url>http://next.lisenhui.cn/post/study/deeplearning/1-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/</url><categories/><tags/><content type="html"> 常见概念 特征分解 奇异值分解 常见概念 标量 向量 矩阵 张量
矩阵加法：C = A + b。表示向量b和矩阵的每一行相加。这种隐式复制的方式，称为广播。boardcasting
元素对应乘积： A $\odot$ B hadamard 乘积
由一组解线性组合而成的解称为 生成子空间。表示原始向量线性组合后能抵达的点的集合。
矩阵列向量线性相关的矩阵，称为奇异的。
范数： 形式上 $L^p 定义为 ||x||_p = (\sum_i|x_i|^p)^{\frac{1}{p}}$ 。 满足的性质：
$f(x) = 0 -> x=0$ $f(x+y) \leq f(x) + f(y)$ (三角不等式) $\forall \alpha \in R, f(\alpha x) = |\alpha|f(x)$ 当p=2的时候，$L^2$就是欧几里得范数。表示从原点到x确定的点的欧几里得距离。经常表示为$||x||$。平方$L^2$范数对x中每个元素的导数取决于对应的元素。而$L^2$范数对每个元素的导数却和整个向量相关。但是平方L2范数在原点附近增长的十分缓慢。L1范数在0附近增长则正相关。
最大范数$L^{\infty} => ||x||_\infty = max|x_i|$
Frobenius范数，衡量矩阵的大小。
特征分解 ***对角矩阵（diagonal matrix）***只在主对角线上含有非零元素，其他位置都是零。我们用 diag(v) 表示一个对角元素 由向量 v 中元素给定的对角方阵。计算乘法 diag(v)x，我们只需要将 x 中的每个元素 xi 放大 vi 倍。换 言之，diag(v)x = v⊙ x
对角方阵的逆矩阵存在， 当且仅当对角元素都是非零值 但通过将一些矩阵限制为对角矩阵，我们可以得到计算代价较低的（并且简明扼要的）算法 对称（symmetric）矩阵是转置和自己相等的矩阵
单位向量（unit vector）是具有单位范数（unit norm）的向量：$||x||_2 = 1$.
如果 x⊤y = 0，那么向量 x 和向量 y 互相正交（orthogonal）。如果这些向量不仅互相正交，并且范数都为 1，那么我们称它们 是标准正交（orthonormal）。
正交矩阵（orthogonal matrix）是指行向量和列向量是分别标准正交的方阵, 这意味着$A^{−1} = A^⊤$ 特征分解（eigendecomposition）是使用最广的矩阵分解之一，即我们将矩阵分 解成一组特征向量和特征值.方阵 A 的特征向量（eigenvector）是指与 A 相乘后相当于对该向量进行缩放 的非零向量 v:$Av=\lambda v$.标量 λ 被称为这个特征向量对应的特征值（eigenvalue）
每个实对称矩阵都可以分解成实特征向量和实特征值 按照惯例，我们通常按降序排列 Λ 的元素。在该 约定下，特征分解唯一当且仅当所有的特征值都是唯一的。 矩阵是奇异的当且仅当含 有零特征值
所有特征值都是正数的矩阵被称为正定（positive definite）；所有特征值都是非 负数的矩阵被称为半正定（positive semidefinite）。半正定矩阵受到关注是因为它们保证 ∀x, x⊤Ax ≥ 0。此外， 正定矩阵还保证 x⊤Ax = 0 ⇒ x = 0。
奇异值分解 被称为奇异值分解（singular value decomposition, SVD），将矩阵分 解为奇异向量（singular vector）和奇异值（singular value）。每 个实数矩阵都有一个奇异值分解，但不一定都有特征分解。
对角矩阵 D 对角线上的元素被称为矩阵 A 的奇异值（singular value）。矩阵 U的列向量被称为左奇异向量（left singular vector），矩阵 V的列向量被称右奇异向量（right singular vector）。 事实上，我们可以用与 A 相关的特征分解去解释 A 的奇异值分解。A 的左奇 异向量（left singular vector）是AA⊤ 的特征向量。A的右奇异向量（right singular vector）是 A⊤A 的特征向量。A 的非零奇异值是 A⊤A 特征值的平方根，同时也是 AA⊤ 特征值的平方根。
奇异值分解是类似的，只不过这回我们将矩阵 A 分解成三个矩阵的乘积$A = UDV^⊤$.矩阵 U和 V都定义为正交 矩阵，而矩阵 D 定义为对角矩阵。注意，矩阵 D 不一定是方阵。A 的奇异值（singular value）。矩阵 U的列向量被称为左奇异向量（left singular vector），矩阵 V的列向量被称右奇异 向量（right singular vector）。SVD最有用的一个性质可能是拓展矩阵求逆到非方矩阵上。
Moore-Penrose 伪逆. 当矩阵 A 的列数多于行数时，使用伪逆求解线性方程是众多可能解法中的一 种。特别地，x = A+y 是方程所有可行解中欧几里得范数 ∥x∥2 最小的一个。当矩阵 A 的行数多于列数时，可能没有解。在这种情况下，通过伪逆得到的 x 使得 Ax 和 y 的欧几里得距离 ∥Ax− y∥2 最小。
迹运算返回的是矩阵对角元素的和.多个矩阵相乘得到的方阵的迹，和将这些矩阵中的最后一个挪到最前面之后相 乘的迹是相同的。
行列式，记作 det(A)，如果行列式是 0，那么空间至少沿着某一维完全收缩了，使其失去了所有的 体积。如果行列式是 1，那么这个转换保持空间体积不变。
***主成分分析（principal components analysis, PCA）***是一个简单的机器学习算 法，可以通过基础的线性代数知识推导。对于每个点 x(i) ∈ Rn，会有一个对应的 编码向量 c(i) ∈ Rl。如果 l 比 n 小，那么我们便使用了更少的内存来存储原来的数据。f(x) = c；我们也希望找到一 个解码函数，给定编码重构输入，x ≈ g(f(x))。了简化解码器，我们使用矩阵乘 法将编码映射回 Rn，即 g(c) = Dc，其中 D ∈ Rn×l 是定义解码的矩阵。
目前为止所描述的问题，可能会有多个解. 为了使问 题有唯一解，我们限制 D 中所有列向量都有单位范数 首先我们需要明确如何根据每 一个输入 x 得到一个最优编码 c∗。一种方法是最小化原始输入向量 x 和重构向量 g(c∗) 之间的距离。我们使用范数来衡量它们之间的距离。在PCA算法中，我们使用 L2 范数： 我们可以用平方 L2 范数替代 L2 范数，因为两者在相同的值 c 上取得最小值。这是因为 L2 范数是非负的，并且平方运算在非负值上是单调递增的。 这使得算法很高效：最优编码 x 只需要一个矩阵-向量乘法操作。为了编码向量， 我们使用编码函数$f(x) = D^⊤x$.进一步使用矩阵乘法，我们也可以定义PCA重构操作：r(x) = g(f(x)) = DD⊤x. 具体来讲，最优的 d 是 X⊤X最大特 征值对应的特征向量。以上推导特定于 l = 1 的情况，仅得到了第一个主成分。更一般地，当我们希望 得到主成分的基时，矩阵 D 由前 l 个最大的特征值对应的特征向量组成。这个结论 可以通过归纳法证明，我们建议将此证明作为练习。 从线性代数的角度总结：PCA是最小化压缩后的矩阵和原始矩阵之间的距离。通常使用平方$L^2$范数来表示这个距离。经过在一维的运算，最优的d是$X^TX$最大特征值对应的特征向量。通过归纳法得知压缩矩阵D由前l个最大的特征直对应的特征向量组成。 从其他角度还有更广泛的解释： 方差：表示数据的分散程度。协方差表示两个变量的相关性。 思想方法： 将坐标轴移动到数据的中心，旋转坐标轴，使得数据在c1轴上的方差最大，即全部n个数据个体在该方向投影最分散。C1为第一主成分。 找一个C2，使得C2和C1的协方差（相关系数）为0，上市的数据在该方向的方差尽量最大。 依次类推。 数学方法： 将原始数据按列组成 n 行 m 列矩阵 X 将 X 的每一行进行零均值化，即减去这一行的均值； 求出协方差矩阵 求出协方差矩阵的特征值及对应的特征向量； 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 k 行组成矩阵 P； 即为降维到 k 维后的数据。 优缺点： 保留了主要信息。这个主要信息未必是重要信息。很可能是非主要信息取了决定性作用。 起到了降噪降维的作用。且PCA后的特征相互独立。 补充。方差，表示数据的分散程度。 协方差理解为一致性分散程度有多大。在分散程度的相关性有多大。协方差为0，分散程度不相关。</content></entry><entry><title>源代码恶意代码识别</title><url>http://next.lisenhui.cn/post/study/deeplearning/%E6%BA%90%E4%BB%A3%E7%A0%81%E6%81%B6%E6%84%8F%E4%BB%A3%E7%A0%81%E8%AF%86%E5%88%AB/</url><categories/><tags/><content type="html"> 现有方案 仅有的一个针对开源代码恶意代码的检测。提供了一个巨大的开源库，但是其是通过检测github库的字段、md文件等信息。
开源代码 恶意代码的特征： 大部分功能是正常的，小部分功能为异常行为。且针对之前安卓恶意代码的经验，几千个函数当中，仅仅有一两个函数存在恶意行为。 要尽量提取到语义信息以抗混淆。避免更改变量名就可以躲过检查 通过控制流、数据流等信息来提取语义信息。 有人向开源库中投毒，导致某一两个commit是包含病毒的。 启示：可以针对commit为单位来检测是否包含病毒。单个commit 或者 同作者的多个commit。 参考目前的方案，通过社区来挖掘恶意代码的情报。 针对Usenix的和面向开源社区漏洞情报挖掘平台来判断其是否包含恶意代码 动态检测存在问题 很多开源库是API调用，很难将所有API遍历来分析恶意代码。 恶意行为较为集中。恶意代码复用较多，且行为类似。都包括截取部分特征，向某个网站、ip发送网络请求等等。来自安卓恶意代码的经验。 目前的方案： 和江帅讨论了下：
针对恶意的行为建立匹配特征。 然后通过恶意行为特征匹配查找是否包含恶意行为。恶意行为特征的提取尽量考虑到抗混淆部分。 或者通过神经网络来判断行为是否是恶意的。 针对社区的情报判断其是否包含恶意代码 恶意代码作者。 恶意代码讨论帖子。 难点： 没有数据集。仅有恶意代码库，具体哪里是恶意的，并没有具体的标签。 需要对恶意行为进行统计，并设计特征提取方案。 数据集得发邮件要啊。 Notes：
考虑被调用函数的语义信息。 联合多个函数。内联函数判别多个函数是否是恶意的。</content></entry><entry><title>SourceFinder Finding Malware Source Code From Publiclyu Available Repositories in Github</title><url>http://next.lisenhui.cn/post/paperreading/sourcefinder-finding-malware-source-code-from-publiclyu-available-repositories-in-github/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag></tags><content type="html"> 目录： 1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 1. 综述翻译 我们在哪里可以找到恶意软件源代码？这个问题的动机是一个真正的需求：缺乏恶意软件源代码，这阻碍了各种类型的安全研究。我们的工作受到以下见解的推动：公共档案，如 GitHub，拥有数量惊人的恶意软件存储库。利用这个机会，我们提出了 SourceFinder，这是一种有效识别恶意软件源代码存储库的监督学习方法。我们使用来自 GitHub 的 97K 存储库评估和应用我们的方法。首先，我们展示了我们的方法使用标记数据集以 89% 的准确率和 86% 的召回率识别恶意软件存储库。其次，我们使用 SourceFinder 来识别 7504 个恶意软件源代码存储库，这可以说是最大的恶意软件源代码数据库。最后，我们研究了恶意软件存储库及其作者的基本属性和趋势。此类存储库的数量似乎每 4 年增加一个数量级，并且 18 位恶意软件作者似乎是具有良好在线声誉的“专业人士”。我们认为我们的方法和我们的大型恶意软件源代码存储库可以研究的催化剂，目前是不可能的。
1.1 发表于 2020 USENIX
2. Tag 3. 任务描述 4. 方法 我们打算将我们的数据集和工具用于研究目的。我们的愿景是创建社区驱动的参考中心，它将提供：(a) 恶意软件源代码存储库，(b) 社区审查的标签和反馈，以及 (c) 用于收集和分析恶意软件存储库的开源工具。我们的目标是用更多的软件档案扩展我们的数据库。尽管作者可以开始隐藏他们的存储库（见第 8 节），但我们认为我们已经检索到的数据库可能会对某些类型的研究产生重大影响。
GitHub 中的存储库具有以下数据字段：a) 标题，b) 描述，c) 主题，d) README 文件，e) 文件和文件夹，f) 创建日期和最后修改日期，g) 分支，h) 观察者， i) 明星，以及 j) 关注者和关注者，
建立基本事实：由于没有可用的基本事实，我们需要建立自己的。由于这是一项相当技术性的任务，我们选择了领域专家，而不是根据最近的研究 [22] 推荐的 Mechanical Turk 用户。我们使用三位计算机科学家从我们的每个数据集 RD137 和 RD50 以及来自 RD1 的 600 个存储库中手动标记 1000 个存储库，这些存储库是我们以均匀随机方式选择的。评委被指示对每个存储库进行彻底的独立调查。确保地面实况的质量。为了提高地面实况的可靠性，我们采取了以下措施。首先，我们要求法官仅在确定存储库是恶意的或良性且独特的情况下才对其进行标记，否则不对其进行标记。我们只保留法官一致同意的存储库。其次，通过手动检查删除重复的存储库，并从最终标记的数据集中排除以避免过度拟合。值得注意的是，我们在每个具有数百个存储库的数据集中仅发现了 3-5 个数量级的极少数重复项。通过这个过程，我们从每个查询的相应恶意软件存储库开始，建立了三个独立的标记数据集，分别命名为 LD137、LD50 和 LD1，如表 2 所示。虽然标记数据集不是 50-50，但它们代表了两个类相当不错，因此将所有内容标记为一个类的幼稚解决方案的性能会很差。相比之下，我们的方法表现得足够好，正如我们将在第 5 节中看到的。由于没有可用的数据集，我们认为我们通过手动努力制作了足够大小的数据集。
Amazon Mechanical Turk：亚马逊众包网站，雇佣远程众包工作者来执行计算机无法完成的离散任务。
步骤 1. 数据预处理：与任何自然语言处理 (NLP) 方法一样，我们从文本的一些初始处理开始，以提高解决方案的有效性。
a. 字符级预处理：我们通过删除特殊字符（例如标点符号和货币符号）来处理字符级“噪音”，并修复 Unicode 和其他编码问题。 b. 字级预处理：我们按照以下最佳实践消除或聚合字词自然语言处理[32]。首先，我们删除文章词和其他本身不具有重要意义的词。其次，我们使用词干技术处理屈折词。即我们希望降低数据的维数通过将具有相同“root”的单词分组。例如，我们将“organizing”、“organized”、“organize”和“organizes”这些词组合为一个词“organize”。第三，我们过滤掉常见的文件和文件夹名称我们不希望对我们的分类有帮助，例如“LEGAL”、“LICENSE”、“gitattributes”等。 c. 实体级过滤：我们过滤可能对描述存储库范围没有帮助的实体。具体来说，我们删除号码、URL 和电子邮件，这在文中经常出现。我们发现这种过滤提高了分类性能。未来，我们可以考虑挖掘 URL 和其他信息，例如人名、公司或 YouTube 频道的名称，以识别作者、验证意图并发现更多恶意软件活动。
步骤 2. 存储库字段：我们考虑存储库中可以是数字或文本的字段。基于文本的字段需要处理才能将它们转换为分类特征，我们将在下面解释这一点。我们使用并评估以下文本字段：标题、描述、主题、文件和文件夹名称以及自述文件字段。文本字段表示：我们考虑两种技术来通过分类中的特征来表示每个文本字段。
a. Bag ofWords (BoW)：词袋 (BoW) 模型是最广泛使用的文档表示之一。文档表示为其单词出现的次数，不考虑语法和词序 [69]。该模型通常用于文档分类，其中每个单词的频率用作训练分类器的特征值 [41]。我们使用带有计数向量化器和 TF-IDF 向量化器的模型来创建特征向量。 b. 词嵌入：词嵌入模型是文档中每个词的向量表示：每个词映射到实数的 M 维向量 [43]，或者等效地投影到 M 维空间中。 良好的嵌入可确保含义接近的单词在嵌入空间中具有附近的表示。 为了创建文档向量，词嵌入遵循两种方法（i）基于频率的向量化器（无监督）[55]和（ii）基于内容的向量化器（监督）[37]。
步骤 3. 选择字段：另一个关键问题是在我们的分类中使用存储库中的哪些字段。 我们对第 2 节中列出的所有领域进行试验，并在下一节中解释我们的发现。
步骤 4. 选择 ML 引擎：我们设计分类器将存储库分为两类：(i) 恶意软件存储库和 (ii) 良性存储库。 我们系统地评估了许多机器学习算法 [7,44]：朴素贝叶斯 (NB)、逻辑回归 (LR)、决策树 (CART)、随机森林 (RF)、K-最近邻 (KNN)、线性判别分析 (LDA) ) 和支持向量机 (SVM)。
步骤 5. 检测源代码存储库：我们还想确定存储库中是否存在源代码，作为向社区提供恶意软件源代码的最后一步。
我们提出了一种启发式方法，它在实践中似乎效果很好。 首先，我们要识别存储库中包含源代码的文件。 为此，我们首先检查它们的文件扩展名。 如果文件扩展名是已知的编程语言之一：Assembly、C/C++、Batch 他已知的编程语言：Assembly、C/C++、Batch File、Bash Shell Script、Power Shell 脚本、Java、Python、C#、Objective-C 、Pascal、Visual Basic、Matlab、PHP、Javascript 和 Go，我们将其标记为源文件。 其次，如果存储库中的源文件数量超过源百分比阈值（SourceThresh），我们认为存储库包含源代码。
5. 解决了什么问题（贡献） 难点： (a) 从可能庞大的档案中收集一组适当的存储库， (b) 识别包含恶意软件的存储库。
首先，几项研究分析了软件存储库以查找使用和限制，而没有任何关注恶意软件 [14]。 其次，一些努力维护恶意软件二进制文件的数据库，但没有源代码 [2, 3]。 第三，许多努力试图从二进制文件中提取更高级别的信息，例如提升到中间表示（IR）[19]，但是重新创建源代码真的很困难 [10]。 事实上，此类研究将受益于我们的恶意软件源代码档案，以评估和改进他们的方法。 从软件工程的角度来看，一项有趣的工作 [8] 将 150 个恶意软件源代码存储库的演变与良性软件的演变进行了比较。
贡献： (a) 我们提出了 SourceFinder，这是一种以高精度识别恶意软件源代码存储库的系统方法， (b) 我们创建了可以说是最大的非商业恶意软件源代码存档，拥有 7504 个存储库， (c) 我们研究存储库生态系统的模式和趋势，包括时间和以作者为中心的属性和行为。
我们确定多产和有影响力的作者。我们发现 3% 的作者拥有超过 300 名粉丝。我们还发现 0.2% 的作者拥有超过 7 个恶意软件存储库，其中最多产的网络威胁作者创建了 336 个存储库。 G。我们识别并分析了 18 名专业黑客。我们发现了 18 位恶意软件存储库的作者，他们似乎围绕他们的活动创建了一个品牌，因为他们在安全论坛中使用相同的用户名。例如，用户 3vilp4wn（发音为 evil-pawn）是 GitHub 中键盘记录恶意软件的作者，作者使用相同的用户名在 Hack This Site 论坛中推广该恶意软件
6. 实验结果 正如我们在第 5 节中讨论的那样。我们表明我们分类恶意软件存储库使用存储库中的五个字段，具有 89% 的准确率、86% 的召回率和 87% 的 F1 分数
总结：通过文档判断是否是恶意代码，虽然包含了恶意代码的库，但是并没有具体哪个地方是恶意代码的标签。
7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释</content></entry><entry><title>Python命名规范</title><url>http://next.lisenhui.cn/post/study/python%E5%BA%93/python%E5%91%BD%E5%90%8D%E8%A7%84%E8%8C%83/</url><categories><category>学习</category></categories><tags><tag>python相关库学习</tag></tags><content type="html"> 从大到小
从大到小 命名规范 样例 包名 全小写 sklearn 模块名 全小写，下划线分割 sklearn.model_selection 类名 首字母大写、驼峰(类名不加下划线) ThisIsMyClass, TruncatedSVD 函数名 全小写、用下划线增加可读性 read_pickle_file 变量名 全小写、用下划线增加可读性（函数名和变量名不可区分） my_variable 常量名 全大写、用下划线增加可读性 COUNTER_NUMBER</content></entry><entry><title>神经网络杂碎知识点记录</title><url>http://next.lisenhui.cn/post/study/deeplearning/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%82%E7%A2%8E%E7%9F%A5%E8%AF%86%E7%82%B9%E8%AE%B0%E5%BD%95lingsui/</url><categories/><tags/><content type="html"> 标准化和归一化 缩放到均值为0，方差为1（Standardization——StandardScaler()） 缩放到0和1之间（Standardization——MinMaxScaler()） 缩放到-1和1之间（Standardization——MaxAbsScaler()） 缩放到0和1之间，保留原始数据的分布（Normalization——Normalizer()） 1就是常说的z-score归一化，2是min-max归一化。 归一化Normalization，只是缩放到0，1 标准化Standardization，缩放到0-1，还改变了方差。所以改变了分布。不一定是正态分布。 Notes:
归一化和标准化的选择差异 当数据范围有严格要求的时候，使用归一化。不涉及距离度量、协方差计算则归一化。 标准化更通用，无从下手，则用标准化。数据极端变化用标准化。分类算法、聚类算法是标准化效果更好。 二分类一般都是输出一个值。 输出两个值的话，权重量增加了一倍。得不偿失。不用。在结尾层使用sigmoid激活函数。
现在二分类一般都是两个值。
但是如果你是使用交叉熵损失函数的话，那几个分类，就是输出几个值。
one-hot 的作用和意义 One-hot主要用来编码类别特征，即采用哑变量（dummy variables）对类别进行编码。 它的作用是避免因将类别用数字作为表示而给函数带来抖动。 直接使用数字会将人工误差而导致的假设引入到类别特征中，比如类别之间的大小关系，以及差异关系等等
TP FP FN TN P recall Accuracy tp = 预测为正，实际为正 fp = 预测为正，实际为负 fn = 预测为负，实际为正 tn = 预测为负，实际为负
p = tp/(tp+fp) R = tp/(tp+fn) acc = (tp+tn)/(tp+tn+fp+fn)</content></entry><entry><title>PyOD学习笔记</title><url>http://next.lisenhui.cn/post/study/python%E5%BA%93/pyod%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url><categories><category>学习</category></categories><tags><tag>python相关库学习</tag><tag>机器学习相关库</tag></tags><content type="html"> 注意 里面有很多需要其他库的神经网络。比如keras。比如tensorflow。为了避免最后太过复杂，这些自己写最好。
样例 生成污染数据 contamination = 0.1 # percentage of outliers n_train = 200 # number of training points n_test = 100 # number of testing points X_train, y_train, X_test, y_test = generate_data( n_train=n_train, n_test=n_test, contamination=contamination) X_train (numpy array of shape (n_train, n_features)) – Training data.
X_test (numpy array of shape (n_test, n_features)) – Test data.
y_train (numpy array of shape (n_train,)) – Training ground truth.
y_test (numpy array of shape (n_test,)) – Test ground truth.
一般模式 clf = models()
clf.fit(X_train)
y_train_scores = clf.decision_scores_ y_test_scores = clf.decision_function(X_test)
API fit(X) 填充检测器。y 在无监督方法中被忽略。用以训练网络 decision_function() 输出Y的异常分数 predict() 用来拟合 Y predict_proba() 用来输出Y的异常概率。 attribute decision_scores_ 训练数据的异常值。值越高越不正常。 labels_ 训练数据的二进制标签。0表示正常。1代表异常。 其他API metrics from pyod.utils.data import evaluate_print evaluate_print(clf_name, y_train, y_train_scores) 输出ROC和precision值。 可视化 visualize(clf_name, X_train, y_train, X_test, y_test, y_train_pred,y_test_pred, show_figure=True, save_figure=False) combination 四种模型组合 最大值平均，平均值最大。最大值。平均值。 先获取每一个样本所有点的异常分数 然后进行归一化 utils.utility.standardizer 然后进行计算分数 average maximization aom moa等 保存 from joblib import dump, load # save the model dump(clf, 'clf.joblib') # load the model clf = load('clf.joblib') Fast Train with SUOD from pyod.models.suod import SUOD # initialized a group of outlier detectors for acceleration detector_list = [LOF(n_neighbors=15), LOF(n_neighbors=20), LOF(n_neighbors=25), LOF(n_neighbors=35), COPOD(), IForest(n_estimators=100), IForest(n_estimators=200)] # decide the number of parallel process, and the combination method # then clf can be used as any outlier detection model clf = SUOD(base_estimators=detector_list, n_jobs=2, combination='average', verbose=False)</content></entry><entry><title>Leetcode2_链表</title><url>http://next.lisenhui.cn/post/study/leetcode/leetcode2_%E9%93%BE%E8%A1%A8/</url><categories><category>学习</category></categories><tags><tag>leetcode学习</tag></tags><content type="html"> 203 移除链表元素 头节点是什么呢。头节点就是一个空的节点但指向第一个节点。保证了在处理其他节点的时候保持操作一致。返回的时候返回头节点的下一个节点。
头指针是指向第一个节点
707 设计链表 基本功题目
用好头指针 不要重复造轮子 可以在leetcode中自己定义类。 206 反转链表 写的越来越得心应手了。目前看来没什么问题。在纸上思考好后再写就行。
可以使用临时变量存储部分值。
24 交换两个节点 想清楚一点。尽量想快一点。
多用头节点。 即使没传进来头节点。可以凭空创造一个头节点。
19 删除链表的倒数第N个节点 使用虚拟头节点。想清楚就行。
160 intersection of two Linked Lists 想清楚已经有的条件是什么。就从已经有的条件中进行分析。
这道题将尾部进行对齐。然后判断是否相同。
当然你投机取巧使用赋值也是不会做时的办法了。
这道题的解法真的非常有意思
网上存在的解法差异就是抵消差异，然后开始遍历。 有一种解法非常有意思： https://leetcode.com/problems/intersection-of-two-linked-lists/discuss/49785/Java-solution-without-knowing-the-difference-in-len! 我们可以使用两次迭代来做到这一点。在第一次迭代中，我们将一个链表的指针在到达尾节点后重置到另一个链表的头部。在第二次迭代中，我们将移动两个指针，直到它们指向同一个节点。我们在第一次迭代中的操作将帮助我们抵消差异。所以如果两个链表相交，那么第二次迭代的交点一定是交点。如果两个链表根本没有交集，那么第二次迭代的相遇指针一定是两个链表的尾节点，为空。 //if a &amp; b have different len, then we will stop the loop after second iteration while( a != b){ //for the end of first iteration, we just reset the pointer to the head of another linkedlist a = a == null? headB : a.next; b = b == null? headA : b.next; } 真的非常棒。
142 linked list cycle 这题非常有意思。判断链表是否有环。就要考虑有环的情况下会发生什么。
如果有环，我们会在里面绕圈圈。所以我们可以设置两个指针。一个快指针。一个慢指针。
快指针走两步。慢指针走一步。这样如果有环，两个指针一定会相遇。链表中能用到的就是指针。应该善用指针。
现在就需要找到环入口。
这里就要用到数学知识。这里的数学知识虽然比较简单但是很巧妙。最终得到的结论也是： 从入口到循环节点的距离等于相遇节点的距离到循环节点的距离。所以可以设置两个指针，同时出发。相遇时就找到了循环节点了。
可以说非常巧妙。
能学到的东西就是，善用指针。善用已知条件构造合理的等式。
如果有环，则龟兔赛跑一定会相遇。类似于分针和秒针。 根据条件设立实验条件。</content></entry><entry><title>Progress in Outlier Detection Techniques a Survey</title><url>http://next.lisenhui.cn/post/paperreading/progress-in-outlier-detection-techniques-a-survey/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag></tags><content type="html"> 目录： 1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 基于密度的方法 基于统计的方法 基于参数的 GAUSSIAN MIXTURE MODEL METHODS REGRESSION METHODS 非参数方法 其他统计方法 优缺点 基于距离的方法 基于聚类的方法 基于聚类的 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 1. 综述翻译 检测异常值是一个重要问题，已在各个研究和应用领域进行了研究。研究人员继续设计稳健的方案，以提供有效检测异常值的解决方案。在本次调查中，我们对 2000 年至 2019 年异常值检测方法的进展进行了全面而有组织的回顾。首先，我们提供了异常值检测的基本概念，然后将它们从不同的异常值检测技术中分类为不同的技术，例如距离- 、聚类、密度、集成和基于学习的方法。在每个类别中，我们介绍了一些最先进的异常值检测方法，并在性能方面进一步详细讨论它们。其次，我们描述了它们的优缺点和挑战，为研究人员提供每种技术的简要概述，并推荐解决方案和可能的研究方向。本文介绍了异常值检测技术的当前进展，并提供了对不同异常值检测方法的更好理解。最后的开放研究问题和挑战将为研究人员提供未来异常检测方法的清晰路径
1.1 发表于 IEEE Access
2. Tag 3. 任务描述 4. 方法 尽管在定义异常值时存在模糊性和复杂性，但通常可以将其描述为与其他数据点显着不同的数据点或不模仿其他点的预期典型行为的点 [5]。与异常值相反的数据点称为内点。
基于统计的方法 基于统计的技术在标记或识别异常值方面的基本思想取决于与分布模型的关系。这些方法通常分为两大类——参数方法和非参数方法。 • 基于距离的方法。基于距离的检测算法的基本原理侧重于观测之间的距离计算。一个点被视为离其附近邻居很远的离群点。 • 基于密度的方法。这些方法的核心原理是在低密度区域可以找到异常值，而在密集邻域中可以找到内部值。 • 基于聚类的方法。基于聚类的技术的关键思想是应用标准聚类技术从给定数据中检测异常值。异常值被认为是不在任何大型或密集集群内或附近的观测值。 • 基于图的方法 基于图的方法基于使用图技术来有效地捕获互连实体的相互依赖性以识别异常值。 • 基于集成的方法 集成方法侧重于组合不同模型的结果以生成更稳健的模型以有效检测异常值的想法。它们有助于回答异常值是否应该基于线性模型、基于距离或其他类型的模型的问题。 • 基于学习的方法 基于学习的方法，例如主动学习和深度学习，其基本思想是通过应用这些学习方法来学习不同的模型来检测异常值。 基于密度的方法 基于密度的离群点检测方法的核心原理是在低密度区域可以找到离群点，而假设非离群点（inliers）出现在密集的邻域中。与其最近的邻居有很大不同的对象，即那些远离最近邻居的对象，被标记并始终被视为异常值。他们将本地点的密度与其本地邻居密度进行比较。与基于距离的方法相比，在基于密度的异常值检测方法中，应用了更复杂的机制来对异常值进行建模。
提出了局部异常因子（LOF）方法，这是第一个基本的松散相关的基于密度的聚类异常值检测方法之一。该技术利用了 k 最近邻。在每个点的 KNN 集中，LOF 利用局部可达性密度 (lrd) 并将其与该 KNN 集的每个参与者的邻居的可达性密度进行比较。
Tang 等人介绍了对 LOF [8] 和简化的 LOF [79] 的改进。 [80]，他们称之为基于连接的离群因子（COF）。 该方法与 LOF 非常相似，唯一的区别是计算记录密度估计的方式。 COF 使用链接距离作为最短路径来估计邻居的局部密度，而 LOF 使用欧几里得距离来选择 K-最近邻居。 这种方法的缺点是对数据分布的间接假设，这会导致不正确的密度估计。 作者提出的关键思想是基于将“低密度”与“孤立性”区分开来。 孤立性被定义为一个物体与其他物体的连通程度。
在基于密度的方法中，使用的密度估计是非参数的； 它们不依赖于假设的分布来拟合数据。
在最广为人知的基于密度的方法之一 LOF [8] 中，必须注意的是，在局部异常值不显着的异常值检测过程中，该算法可能会产生大量误报。通常，由于基于密度的方法是非参数的，对于高维数据空间，样本量被认为太小 [27]
基于统计的方法 基于统计的方法通常分为两大类——参数方法和非参数方法。 两种方法的主要区别在于前者对给定数据中的底层分布模型有一个假设，并从已知数据中估计分布模型的参数。 后一种方法没有任何关于分布模型的先验知识的假设 [98]
我们将目前使用统计方法检测异常值的一些研究分为三类 - 参数方法、非参数方法和其他类型的统计技术。
基于参数的 GAUSSIAN MIXTURE MODEL METHODS 对于这种具有底层分布模型假设的方法，异常值检测采用的两种众所周知的方法是高斯混合模型和回归模型
高斯模型是用于检测异常值的最流行的统计方法之一。 在这个模型中，训练阶段使用最大似然估计 (MLE) 方法 [100] 来执行高斯分布的均值和方差估计。 在测试阶段，应用了一些统计不一致测试（箱线图、均值方差测试）。 杨等人。 [101]，介绍了一种具有全局最优 Exemplar-BasedGMM（高斯混合模型）的无监督异常值检测方法
可以降低这种计算复杂性的算法可以为未来的研究提供更大的可扩展性。 2015 年，为了更稳健的异常值检测方法，Tang 等人提出了使用具有局部保留投影的 GMM。 [102]。他们结合使用 GMM 和子空间学习在能量分解中进行稳健的异常值检测。在他们的方法中，子空间学习的局部保留投影（LPP）用于有效地保留邻域结构，然后揭示数据的内在流形结构。
[103] 主成分分析（PCA）方法。本研究解决了先前方法 LOF [8] 和 Tang 等人的研究空白
REGRESSION METHODS 多年来，使用回归技术进行异常值检测的一些标准方法包括使用马氏距离进行阈值处理、具有双平方权重的稳健最小二乘法、混合模型，然后是替代的振动贝叶斯回归方法 [26]
非参数方法 核密度估计方法：核密度估计 (KDE) 是一种用于检测异常值的常见非参数方法 [107]。 Latecki 等人在 [108] 中提出了一种使用核函数进行异常值检测的无监督方法。异常值检测过程是通过将每个点的局部密度与邻居的局部密度进行比较来执行的。
其他统计方法 如箱线图、修剪平均值、极端学生化偏差和狄克逊型检验 [40] .其中，Trimmed mean 更能抵抗异常值，而为了识别单个异常值，Extreme Studentized Deviate test 是正确的选择。 Dixon 型检验具有在样本量较小时表现良好的优点，因为无需假设数据的正态性。巴内特等人。
优缺点 它们在数学上是可以接受的，并且一旦模型建立起来就有一个快速的评估过程。这是因为大多数模型都是以紧凑的形式制作的，并且在给定概率模型的情况下它们表现出改进的性能。 这些模型通常适合定量实值数据集或一些定量有序数据分布。可以将序数数据更改为合适的值进行处理，从而缩短复杂数据的处理时间。 即使仅限于特定问题，它们也更容易实现。 缺点，挑战和差距：
由于它们的依赖性和参数模型中分布模型的假设，由于缺乏有关潜在分布的先验知识，所产生的结果的质量对于实际情况和应用而言大多是不可靠的。 由于大多数模型适用于单变量特征空间，因此它们通常不适用于多维场景。在处理多变量数据时，它们会产生很高的计算成本，这反过来又使大多数统计非参数模型成为实时应用程序的糟糕选择。 在直方图技术中，多元数据的一个基本缺点是无法捕捉不同属性之间的相互作用。这是因为他们不能同时分析多个特征。一般来说，一些流行的统计方法不适用于处理非常高维的数据。需要设计统计技术来支持能够同时分析多个特征的高维数据。 当面临维度增加的问题时，统计技术采用不同的方法。这会导致处理时间增加和误报发送数据的分布。 当捕获正确的分布模型时，基于统计的方法可以在异常值检测过程中有效。在一些现实生活中，例如，在传感器流分布中，没有可学习的先验知识。在这种情况下，当数据不遵循预定分布时，它可能变得不切实际。因此，非参数方法最有吸引力，因为它们不依赖于分布特征的假设。对于无法假设数据分布的大数据流也是如此。对于数据集中均匀分散的异常值，使用统计技术变得复杂。因此，参数方法不适用于大数据流，但对于非参数方法，它们适用。此外，定义标准分布的阈值以区分异常值具有更高的标记不准确概率。对于参数情况，使用高斯混合模型，
基于距离的方法 最常用的基于距离的异常值检测定义以局部邻域、k-最近邻 (KNN) [121] 和传统距离阈值的概念为中心。它与 k-最近邻分类不同。这些方法主要用于检测全局异常值。最初，搜索每条记录的 k 最近邻居，然后这些邻居用于计算异常值分数。他们主要检查给定对象邻域信息的性质，以确定它们是否靠近邻居或是否具有低密度
[76] 提出了一种基于局部距离的异常值检测方法，称为基于局部距离的异常值因子（LDOF）。他们的研究表明，与 LOF [8] 相比，在邻居大小范围内的性能有所提高。对成对距离计算的需求是 (O(k2))
它们简单易懂，因为它们大多不依赖于假设分布来拟合数据。 ii.在可扩展性方面，它们在多维空间中的扩展性更好，因为它们具有强大的理论基础，并且与统计方法相比，它们的计算效率更高。
缺点、挑战和差距： i．在高维空间方面，它们与基于统计和基于密度的方法有一些相似的缺点，因为它们的性能由于维数灾难而下降。数据中的对象通常具有离散的属性，这使得定义这些对象之间的距离具有挑战性。 ii.当使用基于距离的方法时，诸如邻域和 KNN 搜索之类的搜索技术在高维空间中是一项昂贵的任务。在大型数据集中，可扩展性也不具有成本效益。 三、现有的大多数基于距离的方法无法处理数据流的原因是它们难以维护局部邻域中的数据分布以及在数据流中寻找KNN。这是专门设计用于处理数据流的方法的一个例外。
基于聚类的方法 基于聚类的技术通常依赖于使用聚类方法来描述数据的行为。为此，包含比其他集群少得多的数据点的较小规模的集群被标记为异常值。需要注意的是，聚类方法与异常值检测过程不同。聚类方法的主要目的是识别聚类，而异常值检测是检测异常值。基于聚类的技术的性能高度依赖于聚类算法在捕获正常实例的聚类结构方面的有效性
分区聚类方法：也称为基于距离的聚类算法。 ii. 分层聚类方法：它们将对象集划分为不同级别的组并形成树状结构。 为了分组到不同的级别，它们通常需要最大数量的集群。 三、 基于密度的聚类方法：它们不需要像分区方法那样初始给出聚类的数量； 比如K-Means。 给定集群的半径，他们可以将集群建模为密集区域。 密度聚类方法的一些例子 包括 DBSCAN [153] 和 DENCLUE [154]。 iv． 基于网格的聚类方法 v. 高维数据的聚类方法： CLIQUE [157], HPStream [158] 基于聚类的 与其他相关方法相比，基于集成的方法通常用于机器学习，因为它们的性能相对更好 近年来，已经引入了几种技术，包括：（I）Bagging [37] 和 boosting [184] 用于分类问题（ii）隔离森林 [192] 用于并行技术。 (iii) 对于顺序方法 [185] 和极限梯度提升异常值检测 (XGBOD) [183]​​ 和用于混合方法的袋装异常值表示集成 (BORE) [186]。
典型：
isolation Forest 5. 解决了什么问题（贡献） 我们介绍了不同的最新异常值定义、不同的种类、原因、当代检测和处理过程，以及最新的挑战和应用领域。与其他调查不同，我们添加了需要更多关注的新应用领域。 我们扩展了异常值检测算法的类别，并在之前的调查中增加了不同的方法。我们介绍最先进的算法，讨论它们并突出它们的优点和缺点。我们主要引用和讨论在大多数重要调查 [26]、[33] 之后所做的最新研究。 与之前的调查相比，我们通过介绍最近方法的优缺点、公开挑战和不足，显着扩展了对每个不同类别的讨论。我们还总结了一些最先进算法的性能、解决的问题、缺点和可能的解决方案。 我们提出了一些评估异常值检测算法的当代开放挑战。然后我们介绍了标准工具和一些通常用于异常值检测研究的基准数据集。我们通过讨论 OD 工具选择和选择合适数据集的挑战来扩展我们的讨论。 我们确定了一些挑战，并最终为未来的研究推荐了一些可能的研究方向。 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释</content></entry><entry><title>Seaborn学习笔记</title><url>http://next.lisenhui.cn/post/study/python%E5%BA%93/seaborn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url><categories><category>学习</category></categories><tags><tag>python相关库学习</tag><tag>机器学习相关库</tag></tags><content type="html"> seaborn结合pandas来画好看的图。
最后网页显示的统计图，可以用seaborn画。
基本使用 seaborn中两种函数，第一种返回当前设置，第二种设置 管理图表样式 axes_style, set_style 返回样式、设置样式
主题 darkgrid 白线灰底。不影响数据表示
whitegrid 白底，简洁大方，数据元素较大使用。
dark 没有格子的灰底
white 没有格子的白底
ticks 给轴线分割线段
布局 使用plotting_context()返回布局。set_context来设置布局。
布局按相对大小排序分别是：paper, notebook, talk,和poster
影响标题、线型等。默认时notebook。
临时设置布局 with sns.plotting_context()
配色方案 使用color_palette()和set_palette()建立配色方案 color_palette()可以接受的颜色参数形式
HTML十六进制字符串（hex color codes）
color = &lsquo;#eeefff&rsquo; 合法的HTML颜色名字（HTML color names）
color = &lsquo;red&rsquo;/&lsquo;chartreuse&rsquo; 归一化到[0, 1]的RGB元组（RGB tuples）
color = (0.3, 0.3, 0.4) 色环 共用API： sns.palplot(色环) 显示色环 sns.color_palette(色环) 获取色环 sns.set_palette(色环) 设置色环， 和color_palette同参数 每个色环有自己的专门设置的函数，比如hsl: sns.hsl_palette() Qualitative color palettes 当需要区分离散的数据集，且数据集之间没有内在的顺序 使用色环代码： sns.color_palette(&ldquo;hls&rdquo;, 10) hls最为常用, husl 在亮度和饱和度上更平均 第一个参数是色环，第二个参数是颜色个数。 l 亮度 s 饱和度。 或者使用sns.huso_palette()函数来调用，两个函数类似。 参数 start 开始 rot number of rotations as_cmap=True 返回colormap对象。 dark, light 控制亮度。 sns.hls_palette(8, l=.3, s=.8) # l是亮度（lightness），s是饱和度（saturation） 使用hls循环配色 最为常用。 Color Brewer颜色循环。 在某些情况下颜色会循环重复 某些颜色循环系统对色盲比较友好（尤其是红绿色盲） Paired Set2 sequential color palettes 当数据集的范围从相对低值（不感兴趣）到相对高值（很感兴趣） &ldquo;Blues&rdquo; &ldquo;BuGn_r&rdquo; 添加r倒置，逆序。绿色。 &ldquo;GnBu_d&rdquo; 添加后缀d，则颜色加深 &ldquo;cubehelix&rdquo; 颜色线性变化，打印后也能区分不同颜色，对色盲友好。 cubehelix专属函数: sns.cubehelix_palette可以进一步设置这个色环的更多参数 可以通过dark, light控制亮度、暗度。 通过reverse参数控制是否reverse sns.light_palette(&ldquo;green&rdquo;) 从明亮向暗产生渐变。 返回可以通过set_palette()设置的颜色 sns.dark_palette(&ldquo;purple&rdquo;)从暗向明产生渐变 Diverging color palettes。当数据集的低值和高值都很重要，且数据集中有明确定义的中点时 BrBG 两端的颜色应该具有相似的亮度和饱和度，中间点的颜色不应该喧宾夺主 coolwarm 常用的diverging 调色 center=&ldquo;dark&rdquo; 将中间设置为黑色。 sep参数controls the width of the separation between the two ramps in the middle region of the palette diverging_palette 函数使用&rsquo;husl&rsquo;颜色系统，需要在函数中设置两个hue参数，也可以选择设置两端颜色的亮度和饱和度 color_palette()与set_palette()的关系，类似于axes_style()和set_style()的关系 set_palette()的参数与color_palette()相同。 区别在于，set_palette()会改变配色方案的默认设置，从而应用于之后所有的图表。总之就这两个函数来选择配色方案。 临时设置配色方案: with sns.color_palette(&ldquo;PuBuGn_d&rdquo;): sns.choose_colorbrewer_palette()能够以互动的方式测试、修改不同的参数 只能用于Jupyter Notebook 绘图 绘制单变量分布图 sns.displot()绘制分布图 旧版：kind=&ldquo;hist"直方图 kind=&ldquo;kde"核密度图， kind=&ldquo;ecdf&rdquo; 新版： hist=True设置为直方图。 rug=True为每个观察值添加一个tick kde=True 打印核密度 sns.histplot()函数，绘制直方图， sns.kdeplot() 相对于sns.distplot()能够设置更多选项 设置shade参数，填充核密度线下方区域 bw_method 确定要使用的平滑带宽的方法 bw_adjust 参数和bin很像。乘法缩放使用 选择的值的因子 cut 正如您在上面看到的，高斯 KDE 过程的性质意味着估计超出了数据集中的最大值和最小值。可以使用 cut 参数控制绘制曲线超过极值的距离。但是，这仅影响曲线的绘制方式，而不影响曲线的拟合方式。 ax参数选择图表绘制在哪个坐标系内 n_levels=60 通过n_levels参数，增加轮廓线的数量，达到连续化核密度图的效果 rugplot() 绘制出现点 notes: label 绘制双变量分布图 sns.jointplot 散点图 默认绘制散点图 (x=&ldquo;x&rdquo;, y=&ldquo;y&rdquo;, data=df) 注意Seaborn与DataFrame联合使用，data参数指定DataFrame，x、y参数指定列名 六边形图（hexbin plot） 六边形颜色的深浅，代表落入该六边形区域内观测点的数量，常应用于大数据集，与white主题结合使用效果最好 sns.jointplot(x=x, y=y, kind=&ldquo;hex&rdquo;, color=&ldquo;k&rdquo;) # kind参数设置六边形图 核密度图（kernel density estimation） kind=&ldquo;kde&rdquo; 而sns.jointplot()只能单独绘制，无法添加在其他图表之上 sns.jointplot()绘制后返回JointGrid对象对象，可以通过JointGrid对象来修改图表，例如添加图层或修改其他效果 g = sns.jointplot g.plot_joint(plt.scatter, c=&ldquo;w&rdquo;, s=30, linewidth=1, marker="+") 其他画图api FacetGrid 一次花多个图。 可视化数据集的成对关系 sns.pairplot() PairGrid对象 g = sns.PairGrid(iris) API despine()移除右侧上方的横线 默认上右。传入left等参数控制哪边被移除 临时设置主题：with sns.axes_style() seaborn 图 散点图 scatterplot hue设置不同点的标签。</content></entry><entry><title>常用其他python库</title><url>http://next.lisenhui.cn/post/study/python%E5%BA%93/%E5%B8%B8%E7%94%A8%E5%85%B6%E4%BB%96python%E5%BA%93/</url><categories><category>学习</category></categories><tags><tag>python相关库学习</tag></tags><content type="html"> 常用库 yellowbrick 用来可视化画图的。 seaborn高端matplotlib画图的。 prettyplotlib 配合seaborn来画漂亮的图。 plotly Express 可视化。 tqdm进度条。 linux后台运行 https://www.cnblogs.com/kaituorensheng/p/3980334.html 机器学习库 pyOD 用来做异常检测的。 plotly 可视化eda神器。 学习地址https://github.com/datawhalechina/wow-plotly pip 更新注意 pip install selectivesearch -i http://pypi.douban.com/simple &ndash;trusted-host pypi.douban.com 啊我吐了，一直retrying还有可能事因为pip版本太高。 python -m pip install pip==20.2 -i http://pypi.douban.com/simple &ndash;trusted-host pypi.douban.com 通过该命令降级。 pip 安装报错试试这个。
python 语法记录 params = dict(aa=bb, cc=dd) 来记录参数。</content></entry><entry><title>Outlier异常值检测技术记录</title><url>http://next.lisenhui.cn/post/study/deeplearning/outlier%E5%BC%82%E5%B8%B8%E5%80%BC%E6%A3%80%E6%B5%8B%E6%8A%80%E6%9C%AF%E8%AE%B0%E5%BD%95/</url><categories><category>科研学习笔记</category></categories><tags><tag>科研学习笔记</tag></tags><content type="html"> Tag:
异常值检测; 离群值检测; Outlier;
低维特征 Numeric Outlier Z-score 基于高斯分布（正态分布）外的值标记为异常
https://mp.weixin.qq.com/s?__biz=MzIzODExMDE5MA==&amp;mid=2694182460&amp;idx=1&amp;sn=a4842775394946bb643006e2e7c67be9#rd 多维高斯特征离群点检测
使用 Mahalanobis 距离检测多元离群点 使用 $x^2$ 统计量检测多元离群点 高维特征 1. DBSCAN
在DBSCAN聚类技术中，所有数据点都被定义为核心点（Core Points）、边界点（Border Points）或噪声点（Noise Points）。
核心点是在距离ℇ内至少具有最小包含点数（minPTs）的数据点； 边界点是核心点的距离ℇ内邻近点，但包含的点数小于最小包含点数（minPTs）； 所有的其他数据点都是噪声点，也被标识为异常值；
2. Isolation Forest
该方法是一维或多维特征空间中大数据集的非参数方法，其中的一个重要概念是孤立数
孤立数是孤立数据点所需的拆分数。通过以下步骤确定此分割数
随机选择要分离的点“a”； 选择在最小值和最大值之间的随机数据点“b”，并且与“a”不同； 如果“b”的值低于“a”的值，则“b”的值变为新的下限； 如果“b”的值大于“a”的值，则“b”的值变为新的上限； 只要在上限和下限之间存在除“a”之外的数据点，就重复该过程； 与孤立非异常值相比，它需要更少的分裂来孤立异常值，即异常值与非异常点相比具有更低的孤立数。因此，如果数据点的孤立数低于阈值，则将数据点定义为异常值。
即大于孤立数的特征被标记为异常。
其他方法 有哪些比较好的做异常值检测的方法？ - 张戎的回答 - 知乎 https://www.zhihu.com/question/38066650/answer/107801822
基于矩阵分解的异常值检测 通过PAC进行降维。然后再恢复到原始空间。
基于聚类的方法 基于聚类的方法 基于聚类的方法是一类无监督的检测方法，通过考察数据点与之间的关系检测异常值。考虑数据 样本中的数据点 大判所该数据点是否属于某个簇，如果不属于任何族，则认为是异常值 计算该数据点与最近的族之间的距离，如果距离很远，则认为是异常值 大判所该数据点是否是小或者稀硫簇的部分，如東是，则该簇中的所有点都是异常值。 上述三条中的第一条可以采用基于密度的聚类方法(如 DBSCAN)进行计算。第二条可以采用k means聚类方法。第三条寻找小族和稀疏簇一般采用 FINDCBLOFI算法，其方法为 通过设置一个参数 Alpha( Ovelalpha\le1)$S来区別大和小族，至少包含数据集中数据点占比为 S\alpha$的族是大族，其余的为小族 对每个数据点计算基于簇的局部류常因子（ CBLOF月：对于大族的点， CBLOFT为族的大小和该点与 疾的相似性的乘积；对于小族的点， CBLOF为小族的大小和该点于最近的大簇的相似性的乘积 点与族的相似性代表了点属于族的概率，因此 BLOFE的值可以检辺远离任何簇的异常值，具有最低 CBLOF值的点被认为是异常值 从论文中总结的 Anomaly Detection unsupervised Ensemebles
AE autoEncoder
input-> input/4 -> input/8 -> input/4 -> input -> output IF Isolation Forest
tree: 200. max sample size: 256. LODA
400 histograms $1/ \sqrt{d}$ sparsity LOF Local Outlier Factor
number of neighbors is 20. One-class SVM
Subspace Anomaly Detection
KDE 核密度检测。
离群算法原理总结 Isolation Forest 原理: 隔离：具体来说，该算法利用一种名为孤立树iTree的二叉搜索树结构来孤立样本。由于异常值的数量较少且与大部分样本的疏离性，因此，异常值会被更早的孤立出来，也即异常值会距离iTree的根节点更近，而正常值则会距离根节点有更远的距离。
检测离群点： 对于如何查找哪些点是否容易被孤立，iForest使用了一套非常高效的策略。假设我们用一个随机超平面来切割数据空间, 切一次可以生成两个子空间（想象拿刀切蛋糕一分为二）。之后我们再继续用一个随机超平面来切割每个子空间，循环下去，直到每子空间里面只有一个数据点为止。直观上来讲，我们可以发现那些密度很高的簇是可以被切很多次才会停止切割，但是那些密度很低的点很容易很早的就停到一个子空间了。
具体： iForest 由 T 个 iTree 组成，每个 iTree 是一个二叉树结构。该算法大致可以分为两个阶段，第一个阶段我们需要训练出 T 颗孤立树，组成孤立森林。随后我们将每个样本点带入森林中的每棵孤立树，计算平均高度，之后再计算每个样本点的异常值分数
第一阶段，步骤如下 从训练数据中随机选择Ψ个点样本点作为样本子集，放入树的根节点 随机指定一个维度（特征），在当前节点数据中随机产生一个切割点 p（切割点产生于当前节点数据中指定维度的最大值和最小值之间） 以此切割点生成了一个超平面，然后将当前节点数据空间划分为2个子空间：把指定维度里小于 p 的数据放在当前节点的左子节点，把大于等于 p 的数据放在当前节点的右子节点 在子节点中递归步骤(2)和(3)，不断构造新的子节点，直到子节点中只有一个数据（无法再继续切割）或子节点已到达限定高度。 循环(1)至(4)，直至生成 T 个孤立树iTree 第二阶段 获得T个iTree之后，iForest训练就结束，然后我们可以用生成的iForest来评估测试数据了。对于每一个数据点 xi，令其遍历每一颗孤立树iTree，计算点 xi 在森林中的平均高度 h(xi) 对所有点的平均高度做归一化处理。异常值分数的计算公式如下所示 https://www.cnblogs.com/guoyaohua/p/isolation_forest.html 总结： 评价： 相较于LOF，K-means等传统算法，孤立森林算法对高纬数据有较好的鲁棒性 Forest具有线性时间复杂度。因为是Ensemble的方法，所以可以用在含有海量数据的数据集上面。通常树的数量越多，算法越稳定。 iForest不适用于特别高维的数据。由于每次切数据空间都是随机选取一个维度，建完树后仍然有大量的维度信息没有被使用，导致算法可靠性降低。高维空间还可能存在大量噪音维度或无关维度(irrelevant attributes)，影响树的构建 iForest仅对Global Anomaly敏感，即全局稀疏点敏感，不擅长处理局部的相对稀疏点(Local Anomaly)。 LODA 没有找到相关介绍
LOF 原理 LOF通过计算一个数值score来反映一个样本的异常程度。这个数值的大致意思是：一个样本点周围的样本点所处位置的平均密度比上该样本点所在位置的密度。比值越大于1，则该点所在位置的密度越小于其周围样本所在位置的密度，这个点就越有可能是异常点。
1，对于每个数据点，计算它与其他所有点的距离，并按从近到远排序 2，对于每个数据点，找到它的K-Nearest-Neighbor，计算LOF得分
总结 是一种无监督的离群检测方法，是基于密度的离群点检测方法中一个比较有代表性的算法。
评价 DBSCAN DBSCAN的核心思想是从某个核心点出发，不断向密度可达的区域扩张，从而得到一个包含核心点和边界点的最大化区域，区域中任意两点密度相连。
Kmeans K-means 是我们最常用的基于欧式距离的聚类算法
选择初始化的 k 个样本作为初始聚类中心 [公式] ； 针对数据集中每个样本 [公式] 计算它到 k 个聚类中心的距离并将其分到距离最小的聚类中心所对应的类中； 针对每个类别 [公式] ，重新计算它的聚类中心 [公式] （即属于该类的所有样本的质心）； 重复上面 2 3 两步操作，直到达到某个中止条件（迭代次数、最小误差变化等）。 初始化质心。重新计算质心。最后得到质心。</content></entry><entry><title>Matplotlib-2学习</title><url>http://next.lisenhui.cn/post/study/python%E5%BA%93/matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url><categories><category>学习</category></categories><tags><tag>python相关库学习</tag><tag>机器学习相关库</tag><tag>matplotlib</tag></tags><content type="html"> 概览 setting 不同风格 plt.style.available 打印样式列表 plt.style.use(&lsquo;seaborn&rsquo;) 使用seaborn风格的图。
参数color可选项： b:blue c:cyan g:green k:black m:magenta r:red w:white y:yellow
结束时 plt.legend() 在图上放置图例。
设置坐标轴 plt.xlim(-1, 3.5) #设置x坐标在-1到3.5
plt.xlabel(&lsquo;degree&rsquo;); plt.ylabel(&lsquo;rms error&rsquo;) 设置横纵坐标的名称。
图 scatter plt.scatter(x, y, s=None float array, c array-like color map, marker, cmap)
c是一组数，标注了每个点根据标签使用不同的色彩。
s是 marker size in points**2 就是粗细 cmap 就是色彩图。 optional: rainbow; Blues; spring
线图 plt.plot
plot可以接收多个x,y参数，只要依次给过去就行。在show前绘制的所有plot都能画在一张图上。 柱状图 plt.bar
热力图 plt.heat
其他图 plt.box箱图 plt.hist 直方图 plt.pie 饼图 plt.area 面积图
imshow subplots fig, ax = plt.subplts(row, columns, figsize=(subplots figsize))
row是只子图占用父图多宽。 ax[i]就是子图对象，在上面画图 ax[i].scatter(x[:, 0], x[:, 1])等
plt.figure(figsize=(row, columns)) ax = fig.add_subplot(row, column, idx ) idx 从1开始增加。 ax 画图 如 ax.plot
参数 nrows, subplot行 ncols 列 sharex sharey subplot中x、y共享。同时影响所有plot 图的其他信息 plt.legend(loc) 画图例。在图上哪个地方画图例。 画线时标记label就会显示legand。或者legend有方法后添加图例 label = nolegend 就不会显示图例 loc=0就是自动选择best位置来显示。 ncol=2 控制legand有几列。 plt.grid(True) 显示图片背景中的格子 plt.grid(b=True, which=&lsquo;major&rsquo;, axis=&lsquo;both&rsquo;) which指定绘制的网格刻度类型（major、minor或者both） axis指定绘制哪组网格线（both、x或者y） 设置横纵轴 plt.axis() # shows the current axis limits values；如果axis方法没有任何参数，则返回当前坐标轴的上下限 (1.0, 4.0, 0.0, 12.0) plt.axis([0, 5, -1, 13]) # set new axes limits；axis方法中有参数，设置坐标轴的上下限；参数顺序为[xmin, xmax, ymin, ymax] 同样的方法可以用xlim, ylim来设置。 设置标题 plt.title(&lsquo;Simple plot&rsquo;) 保存图片 plt.savefig(&lsquo;plot123_2.png&rsquo;, figsize=[8.0, 6.0], dpi=200) 设置样式 设置颜色 plt.plot(x, y, &ldquo;color value&rdquo;) 可以为16进制字符串，可以为灰度值，可以为rgb三元组，可以为别名 颜色 别名 HTML颜色名 颜色 别名 HTML颜色名 蓝色 b blue 绿色 g green 红色 r red 黄色 y yellow 青色 c cyan 黑色 k black 洋红色 m magenta 白色 w white alpha 设置透明度 参数:ls 设置线形。 &ldquo;-&ldquo;实线， &lsquo;:&lsquo;虚线, &lsquo;&ndash;&lsquo;破折线 &lsquo;steps&rsquo;阶梯线, &lsquo;-.&lsquo;点划线 &lsquo;None&rsquo;什么都不画。表示实线 lw 表示线宽。 marker 设置标志。用来标志数据的位置。 标记 描述 标记 描述 &lsquo;1&rsquo; 一角朝下的三脚架 &lsquo;3&rsquo; 一角朝左的三脚架 &lsquo;2&rsquo; 一角朝上的三脚架 &lsquo;4&rsquo; 一角朝右的三脚架 &lsquo;v&rsquo; 一角朝下的三角形 &lsquo;&lt;&rsquo; 一角朝左的三角形 &lsquo;^&rsquo; 一角朝上的三角形 &lsquo;>&rsquo; 一角朝右的三角形 &rsquo;s&rsquo; 正方形 &lsquo;p&rsquo; 五边形 &lsquo;h&rsquo; 六边形1 &lsquo;H&rsquo; 六边形2 &lsquo;8&rsquo; 八边形 &lsquo;.&rsquo; 点 &lsquo;x&rsquo; X &lsquo;*&rsquo; 星号 &lsquo;+&rsquo; 加号 &lsquo;,&rsquo; 像素 &lsquo;o&rsquo; 圆圈 &lsquo;D&rsquo; 菱形 &rsquo;d&rsquo; 小菱形 &lsquo;&rsquo;,&lsquo;None&rsquo;,&rsquo; &lsquo;,None 无 &lsquo;_&rsquo; 水平线 ' ' 更多的plot设置:
参数 描述 参数 描述 color或c 线的颜色 linestyle或ls 线型 linewidth或lw 线宽 marker 点型 markeredgecolor 点边缘的颜色 markeredgewidth 点边缘的宽度 markerfacecolor 点内部的颜色 markersize 点的大小 设置背景色 设置背景色，通过向plt.axes()或者plt.subplot()方法传入axisbg参数，来设置坐标轴的背景色 坐标轴刻度： xticks()和yticks()方法。设置坐标轴的刻度。 其他图 柱状图 bar 水平柱状图 barh 箱线图 boxplot 散点图 scatter s设置散点大小 c设置散点颜色。 marker设置散点形状 阶梯图 step 条形图 bar 条带图 fill_between 直方图 hist bins 分类数量 normed 归一化处理 orientation 水平或垂直图 align 居中或者左右。 cumulative 累计直方图 误差条 errorbar 饼图 pie 极坐标图 polar 网格线 rgrids() thetagrids() 图形中的文字放置： plt.text(0.1, -0.04, &lsquo;sin(0)=0&rsquo;); # 位置参数是坐标 plt.annotate()在途中加注释 annotate()绘制箭头 matplotlib对象分为三层： FigureCanvas。直接使用pyplot as plt绘制的。 fig = plt.figure() 使用图像绘制的 Axes axes = fig.add_axe() 从属关系，plt最大，axes最小。都从上一级调用 使用axes和fig.add_subplot()的区别。subplot不能控制具体的位置。无法设置嵌套的结构。 twinx() twiny() 从其他数据那里复制x坐标和y坐标。 sharex sharey 共享x、y坐标轴 设置对数坐标轴 set_yscale, semilogx loglog etc. 设置中央坐标轴 ax.spines ax.set_position 设置等高线: pcolor contour() contourf() 地形图 3D 图 曲面图 映射等高线 API plt.fill_between x, x+i, x-i, facecolor=&ldquo;基础色&rdquo;, alpha：透明度</content></entry><entry><title>Numpy学习笔记</title><url>http://next.lisenhui.cn/post/study/python%E5%BA%93/numpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url><categories><category>学习</category></categories><tags><tag>python相关库学习</tag><tag>机器学习相关库</tag></tags><content type="html"> API 创建数组 numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0) np.linspace(2.0, 3.0, num=5) output => array([2. , 2.25, 2.5 , 2.75, 3. ]) np.random random.rand(d0, d1, &hellip;, dn) d为size np.random.randn(d0, d1, &hellip;, dn) Return a sample (or samples) from the “standard normal” distribution. random.randint(low, high=None, size=None, dtype=int) np.random.randint([1, 5, 7], 10) 多个下限，一个上限。 数组操作 np.newaxis 功能上等同于 np.expand_dims a.shape 2, 3 print(np.expand_dims(a, 2).shape) => (2, 3, 1) print(a[:, :, np.newaxis].shape) => (2, 3, 1) a = ndarry, b=ndarry a[b] = 在a中用坐标b去取值。即a[[1,2,3,4]]这样子 np.tile。对数组进行横向、纵向的复制。 判断操作 numpy.all(a, axis=None, out=None, keepdims=, *, where=) Test whether all array elements along a given axis evaluate to True. example: np.all(y == y_pred) numpy.any(a, axis=None, out=None, keepdims=, *, where=) Test whether any array element along a given axis evaluates to True. 计算操作 np.log()取自然对数。 array.std(1)计算第一维的标准偏差standard deviation array.mean(1)求第一维的均值。 value_counts()检查数据。</content></entry><entry><title>Scikit Learn学习笔记</title><url>http://next.lisenhui.cn/post/study/python%E5%BA%93/scikit-learn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url><categories><category>学习</category></categories><tags><tag>python相关库学习</tag><tag>机器学习相关库</tag></tags><content type="html"> 概览 整体框架 unsupervised unsupervised methods cluster API validation API metrics models embedding Cluster Overview Unsupervised dimensionality reduction PCA Random projections johnson_lindenstrauss_min_dim GaussianRandomProjection SparseRandomProjection Feature agglomeration 概览 整体框架 # set numpy seed. when it comes to random functions, please leave a function to set seed. # load data x, y = sklearn.data # train test data split. # create model entity model = sklearn.model(params) # model train model.fit(x, y) # which contains the train process. # model test results = model.predict(test) # predict the test result # metrics results_proba = model.predict_proba(test) # output the probablity of the each label. models.score() # scores are between 0 and 1, with a larger score indicating a better fit. # for unsupervised method model.transform() # transform new data into new basis model.fit_transform() # performs a fit and a transform on the same input data. model.predict() unsupervised est = KMeans(4) est.fit(X) out = est.predict(X)
or est.fit_predict(x)
unsupervised methods Isolation Forest params n_jobs = -1 means all cpu. contamination means the amount of contamination of the dataset. it means the threshold of whole data. n_estimators: the number of base estimators. max_samples: the number of samples to draw from X to train each estimator function: decision_function(X). 返回样本的异常评分。 score_samples(X) return abnormal score of samples.The lower, the more abnormal. 问题：
无监督的 fit_predict 和 fit 后 predict有什么区别？ 无监督学习基本使用fit_predict。 也不是，fit_predict如其名字，就是先预测。然后进行predict。我们按照神经网络中的做法。8预测，2个predict。 max_samples的作用 cluster clust.cluster_centers_ 是簇类的中心
API validation API sklearn.metrics.confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None) Compute confusion matrix to evaluate the accuracy of a classification. y_true = [2, 0, 2, 2, 0, 1] y_pred = [0, 0, 2, 2, 0, 2] confusion_matrix(y_true, y_pred) array([[2, 0, 0], [0, 0, 1], [1, 0, 2]])
sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)[source] Split arrays or matrices into random train and test subsets random_state保证每次划分结果都一样。 model_selection.cross_val_score交叉验证评估分数 通过不停将测试组和训练组分组来评估模型分数。 参数cv表示数据折叠数量。 返回分数。 model_selection.validation_curve cv是交叉验证的值。 确定不同参数值的训练和测试分数。 learning_curve 确定不同训练集大小的交叉验证训练和测试分数。 GridSearchCV使用网格搜索模型指定参数。 metrics accuracy_score Accuracy classification score. In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true. models model select svm sklearn.svm import SVC clf = SVC(kernel=&ldquo;linear&rdquo;) clf = SVC(kernel=&ldquo;rbf&rdquo;) random forest sklearn.tree import DecisionTreeClassifer() sklearn.ensemble.RandomForestClassifier sklearn.ensemble.RandomForestRegressor sklearn.ensemble.RandomTreesEmbedding 无监督随机森林 clf = rfclf(n_estimators=100, random_state=0) cluster KMeans fit(x)计算k均值聚类 fit_predict(x)计算样本的聚类中心，并预测聚类索引。 异常检测： sklearn.ensemble.IsolationForest sklearn.covariance.EllipticEnvelope sklearn.svm.OneClassSVM sklearn.neighbors.LocalOutlierFactor embedding manifold: sklearn.manifold.Isomap() Isomap Embedding. 用来做embedding的。将特征映射到2维后，进行画图。 Non-linear dimensionality reduction through Isometric Mapping mainfold就是来做映射embedding的
make_blobs 函数是为聚类产生数据集 产生一个数据集和相应的标签 Cluster Overview https://scikit-learn.org/stable/modules/clustering.html 中文: https://sklearn.apachecn.org/docs/master/22.html#k-means
聚类是把相似的对象通过静态分类的方法分成不同的组别或者更多的子集（subset），这样让在同一个子集中的成员对象都有相似的一些属性。
一些常见聚类方法简介：
k-means 以空间中的k个点为中心进行聚类。对最靠近它们的对象归类。
KNN 一个对象的分类由其邻居的多数表决确定。k个最近邻居中最常见的分类决定赋予了该对象的类别。
Unsupervised dimensionality reduction PCA 使用奇异值分解将data进行线性分解，来将其映射到一个低维的空间。来去除特征之间的相关性。 不支持稀疏输入。输入的数据居中化，但是并没有缩放。
一般来说先标准化后再进行pca分析。
Params
n_components: Number of components to keep. if is not set, n_components=min(n_samples, n_features) copy bool, True means copy data and transform whiten bool, when true whitening will remove some information from the transformed signal.会从转换后的信号中去除一些信息。 svd_solver auto full arpack randomized auto: selected based on X.shape and n_components full: full SVD arpack: SVD truncated randomized: run randomized SVD random_state Attribute:
components_ principal axes in feature space. The components are soted by explained_variance_ 具有最大方差的成分 explained_variance_ 保留的n个成分和各自的方差百分比 n_components_ 保留的成分个数n API:
fit transform(X) 将数据X转换为降维的数据。模型fit后，对新输入的数据，可以使用transform来降维。 Random projections johnson_lindenstrauss_min_dim JL随即投影 API： https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.johnson_lindenstrauss_min_dim.html#sklearn.random_projection.johnson_lindenstrauss_min_dim
原理解释： 通俗版JL引理： 塞下N个向量，只需要$O(logN)$维空间。 高维空间中任意两个向量几乎都是垂直的 从$N(0, 1/n)$采样出来的$n*n$矩阵几乎是一个正交矩阵 Params:
n_samples: Number of samples eps: Maximu distortion rate Return:
n_components: 最小的组件来保证很好的最小大小来保守估计随即子空间的最小大小。保证随机投影的有界失真。 GaussianRandomProjection 高斯随机投影
params:
n_components 目标投影空间的维度。 可以根据样本数量和johnson-Lindenstrauss引理给出的界限自动调整。嵌入的质量由参数eps控制 eps 当n_components设置为“自动”时，根据 Johnson-Lindenstrauss 引理控制嵌入质量的参数。 eps 默认0.1 越小则损失的越少。最终需要的特征数量越多。 random_state Attrs:
n_components_: 具体组件数。 其他具体看官方 API：
fit transform get_params 获取参数保存 set_params 设置参数。 SparseRandomProjection 稀疏随机投影随机矩阵
params:
n_components 同上 density: auto the value is set as recommended eps Parameter to control the quality of the embedding according to the Johnson-Lindenstrauss lemma when n_components is set to ‘auto’. dense_output if True will output dense output even input is sparse. random_state attr:
n_components_ components_ Random matrix used for the projection. Sparse matrix will be of CSR format. Api:
同上。 Feature agglomeration</content></entry><entry><title>Analyzing Data Granularity Levels for Insider Threat Detection Using Machine Learning</title><url>http://next.lisenhui.cn/post/paperreading/analyzing-data-granularity-levels-for-insider-threat-detection-using-machine-learning/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag></tags><content type="html"> 目录： 1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 Overview Data Collection and Pre-processing ML for Data Analytics 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 Overview 恶意行为和内部威胁检测系统的建议方法如图1所示。 系统流程如下：
数据采集：多源数据采集，统一格式存储。两个主要来源是： • 用户活动，例如网络流量、电子邮件、文件日志。 • 组织结构和用户资料信息。 数据预处理：对聚合后的数据进行处理，构建表示不同粒度级别的用户活动和个人资料信息的特征向量。 基于构建的特征向量，采用ML算法进行数据分析。 结果以不同的格式呈现，并向系统分析员提供详细的分析。 该系统旨在在安全分析师的参与/监督下进行许多步骤，特别是在初始检测中，其中调查恶意行为和异常活动的早期迹象。人类分析师不仅在分析系统警告和警报方面发挥重要作用，而且在执行必要的操作以在攻击后将系统恢复到“正常”运行方面发挥着重要作用。在这里报告的工作中，我们假设 CERT 提供的基准数据集（第 IV-A 部分）用于评估图 1 的数据分析组件的特定目的。具体来说，我们有兴趣评估在有限基础上训练的 ML 算法检测未知恶意内部人员的真相。为此，采用监督学习算法从获得的关于恶意/正常用户行为的知识（基本事实）中学习。然后，我们探索学习到的解决方案在检测未知恶意内部案例时能够泛化的能力如何。使用监督学习的好处是我们不需要假设数据集群总是与不同的行为同义。这可能会导致比无监督学习/异常检测算法 [13]（第 IV-C2 节）更高的精度。
采用监督学习??
基于用户的检测？“简而言之，我们认为突出恶意用户而不是事件的结果代表了更重要的系统性能衡量标准。”
此外，我们的分析将区分检测到的恶意操作和检测到的恶意用户，两者不一定相同。也就是说，组织内用户角色的多样性会影响所执行操作的数量/类型，包括正常的和恶意的。在许多情况下，用户操作可能会随着时间的推移而变化，并且需要考虑多个上下文，以便处理有关可疑行为的警报 [34]。因此，在这种情况下，高恶意实例检测率可能不一定会转化为检测到所有恶意内部人员。此外，如果将许多不同的正常用户标记为异常，看似很小的误报率可能仍然需要安全分析师的大量关注。简而言之，我们认为突出恶意用户而不是事件的结果代表了更重要的系统性能衡量标准。
最后，提出了几种措施，例如每个恶意内部人员的检测延迟，或对每个恶意内部人员警报的支持。通过提供这些措施，我们旨在为安全分析师提供更好的支持，并为所提议的系统在实际场景中的成功应用做出积极贡献。
Data Collection and Pre-processing 数据收集和预处理对于内部威胁检测尤其重要，而且对于一般的网络安全任务也至关重要。良好的监控程序与足够的数据收集相结合，可以成功应用 ML 技术并支持安全分析师做出正确的决策。从组织环境中收集的数据可能来自各种来源，并且有许多不同的形式 [4]、[35]。本研究假设组织数据收集在两个主要类别中：(i) 活动日志数据，以及 (ii) 组织结构和用户信息。第一类数据来自不同的日志系统，例如网络流量捕获、防火墙日志、电子邮件、Web 和文件访问。这些代表通常需要及时收集和处理的实时数据源，以便快速检测和响应恶意和/或异常行为。第二类数据代表背景或上下文数据，可以是员工信息、组织中的角色、与其他用户的关系。在许多情况下，该类别还包含更复杂的数据，例如用户的心理测量和行为模型。为了协助数据处理和特征构建，为组织中的每个用户创建了用户上下文模型。模型由与每个用户相关的辅助信息组成，例如分配的机器、与其他用户的关系、角色、工作时间、允许访问等。基于用户上下文模型，可以从传入的数据中快速有序地创建总结用户行为的特征向量。
特征提取：从收集到的数据和用户上下文模型中，可以进行特征提取以创建适合训练 ML 算法的数据向量。 首先，给定聚合条件 c，例如持续时间或执行的操作数，基于用户 id 聚合来自不同来源的数据。 随后，对聚合数据进行特征提取，生成固定长度 N 的数值向量 x_c，也称为数据实例，汇总用户动作。 每个向量都包含用户信息——主要是以数字格式编码的分类数据，用于为 ML 算法提供上下文——以及两种类型的特征： 给定聚合条件c，基于用户id聚合来自不同来源的数据。生成固定长度N的数值向量XC。包含频率特征和统计特征。
• 频率特征，即用户在聚合期间执行的不同类型操作的计数，例如发送的电子邮件数量、下班后访问文件的数量或在共享 PC 上访问的网站数量。 • 统计特征，即数据的描述性统计，例如均值、中位数、标准差。 统计功能中汇总的数据示例包括电子邮件附件大小、文件大小和访问过的网站中的字数。
图 2 展示了在这项工作中使用 CERT 数据集的情况下的特征创建过程。 该过程允许创建由许多细节组成的信息丰富的特征，例如 PC、时间和特定于动作的特征。 图 2 中显示的最多三个连接的信息组合在一起以生成一个特征，例如共享 PC 上的操作数、下班后的 HTTP 下载数、已发送电子邮件的平均附件大小。 因此，构建的功能集本质上是对信息片段的枚举。1 HTTP 和文件功能要求我们在企业环境中定义可能有助于内部威胁检测的网站和文件类别集。 此外，精心设计的用于收集活动信息的分类方案直接有助于保护隐私的用户监控，因为在数据预处理中不会检查用户访问的特定网站和文件及其内容[36]。
2）数据粒度：基于上述数据聚合条件c，提取的特征可以具有不同级别的粒度。我们探讨了 c 的两个主要标准：持续时间和执行的操作数量。表一根据不同的粒度级别总结了本研究中提取的数据类型。在持续时间的情况下，假设用户活动的三个数据聚合：周、日或会话 [27]，[29]。用户周和用户日数据实例汇总了用户在相应时间段内的活动。这些粗粒度类型的数据提供了一天或一周内行为的高级概述，其特征计数高于会话和子会话数据。因此，它们可以通过减少提取的数据实例的数量来潜在地加速学习过程。另一方面，用户会话数据点通过捕获用户在 PC 上的操作，从登录到相应的注销，提供更高的数据保真度；或从一次登录到下一次登录。基于会话的数据可用于隔离恶意操作，因为恶意用户倾向于在特定会话中执行恶意操作，而同一天或同一周的其他会话可能仍然正常 [8]。此外，由于会话的持续时间通常比一天短得多，因此当检测到恶意实例时，这种数据类型还可以允许更快的系统响应。
三个数据聚合。周、日或会话。这些粗粒度类型的数据提供了一天或一周内行为的高级概述，其特征计数高于会话和子会话数据。另一方面，用户会话数据点通过捕获用户在 PC 上的操作，从登录到相应的注销，提供更高的数据保真度；或从一次登录到下一次登录。基于会话的数据可用于隔离恶意操作，因为恶意用户倾向于在特定会话中执行恶意操作，而同一天或同一周的其他会话可能仍然正常 这句话的意思就是会话比周、日更合适。
由于会话可能持续数小时并包含数百个操作，因此我们进一步探索了每个数据实例中汇总的数据量与对恶意行为的潜在系统响应时间之间的平衡。这是通过使用持续时间和执行的动作数量作为将用​​户会话数据实例分成子会话数据实例的标准来完成的。通过这种方式，我们可以控制嵌入到每个数据实例中的信息量。因此，如果基于 ML 的系统能够成功地从短暂的子会话数据中学习以检测恶意行为，则系统的响应时间可能会得到改善。如表 I 所示，根据持续时间，从用户在 PC 上的会话开始时间起每 i 小时创建一个用户子会话 Ti 数据实例。类似地，从登录操作开始，用户在 PC 上的每 j 个操作后都会创建一个用户子会话 Nj 数据实例。 i 和 j 越小，数据的保真度越高，但实例中汇总的用户活动信息量也越少。在第 IV-A 节中进行了实证分析，以确定 CERT 数据集的 i 和 j 的最合适值。
为控制会话包括在里面的时间，来控制信息量。控制i，j（每i个小时，每j个动作的session量）。
总结下，特征提取，提取了两方面的特征，统计特征（发的邮件里的文本数）和频率特征（访问次数）。 提取了两方面的特征，用户信息，和组织结构信息。属于哪一组。
有这个用户的信息组。就是这个人的信息属于同一个组。需要查看一个组的用户的操作是否相似。
ML for Data Analytics 在本研究中，采用了以下四种众所周知且广泛使用的 ML 算法：逻辑回归、随机森林、神经网络和 XGBoost [7]、[13]、[37]、[38]。 下面给出了算法的简要描述，而更详细的描述可以在 [39] 中找到。
5. 解决了什么问题（贡献） 6. 实验结果 从受限的数据中进行训练（400个正常、恶意用户前37周，50%的时间）。
表 IV 和图 6 说明了 IF 实现的结果。结果清楚地表明，当标签信息（尽管有限）可用于训练 ML 算法时，监督学习中的引导搜索将实现卓越的性能，尤其是在 FPR 非常低的情况下。无监督学习算法不够好。
有监督可以更好的进行学习，哪怕是使用对比学习的自学习。
基于用户的报告效果更好。而且更贴近实际需求。当用户有周被报告为异常的话，就判定其为异常。
根据图10，EANOC 这些特征也对分类有帮助。根据图10定义部分特征。
每个版本都表征一个拥有 1000 到 4000 名员工的组织。这项工作中使用的数据集 (CERT r5.2) 的 5.2 版模拟了一个在 18 个月内拥有 2000 名员工的组织。 CERT r5.2 由用户活动日志组成，分类如下：登录/注销、电子邮件、Web、文件和 U 盘连接，以及组织结构和用户信息。
CERT r5.2 中的每个恶意内部人员都属于四种流行的内部人员威胁场景之一：数据泄露（场景 1）、知识产权盗窃（场景 2、4）到 IT 破坏（场景 3）。
有四个场景实例。
图 3 显示了用户会话数据按动作数量、每个会话的持续时间以及两个特征之间的关系的分布。现在很明显，大多数用户会话数据的动作少于 300 个，超过一半的会话少于 100 个动作。因此，我们得出结论 j = {25, 50} 用于提取 usersubsession Nj 数据。另一方面，会话时长更接近于均匀分布，很大一部分持续时间超过 8 小时。此外，如图 3 所示，许多少于 50 个动作的会话可能会持续超过 10 个小时。因此，我们探索 i = {2, 4} 的值以按时间提取子会话数据。表 II 概述了数据类型以及正常和恶意用户的数量。可以看出，数据分布极度偏斜，恶意内部人员相关数据分别仅占用户周、用户日和用户会话数据的 0.39%、0.19% 和 0.18%。在子会话数据上，这个数字更小，从 0.09% 到 0.15% 不等。此外，在检查不同的内部威胁情景时，似乎存在不同的模式。场景 3（与 IT 破坏相关的恶意行为）拥有最少的用户和数据实例。另一方面，场景 2 和场景 4 中的恶意行为（针对不同类型的知识产权盗窃行为）跨越了很长的时间——8 周（超过 240 个恶意用户周数据实例/30 个用户）。这可能表明恶意内部人员试图通过长时间执行恶意操作来避免检测。然而，当考虑单个恶意会话时，场景 2 和 4 之间会出现不同的特征。虽然场景 4 的几乎所有单个恶意会话都很短（少于 2 小时或 25 个操作），但很大一部分场景 2 会话超过 50 个操作并且跨度超过 4 小时。
这里明显突出了会话时间、周、日都会展现出不同的检测效果，会从不同的方面对恶意行为进行检测。
In this work, our aim is to obtain a realistic estimation of the proposed system’s performance on real-world networked systems, based on scenarios characterized by limitations to the amount of ground truth data available for training the ML algorithms.
以真实世界的ground-truth.
具体来说，在现实环境中，用于训练检测系统的标记（地面实况）数据很少。因此，真实情况只能从有限的一组经过验证的用户那里获得，而其他人的行为通常是未知的 [14]、[48]。为了模拟这种情况，我们假设了一个主要配置——即之后的现实条件——在给定的时间段内仅从有限的一组用户那里获得真实情况。因此，用于训练 ML 算法的真实数据仅限于 400 个识别出的“正常”和“恶意”用户（组织中的 2000 个用户）的数据，基于前 37 周 – 50% 的时间段数据集覆盖。根据用户数量，这允许 ML 算法从代表 18% 的“正常”用户和 34% 的恶意内部人员的数据中学习。值得注意的是，从检测器的角度来看，训练数据中的“正常”用户只能保证在前 37 周是良性的，而在测试周后期，他们可能会也可能不会变成“恶意”。此外，我们通过呈现仅从未知用户（即在前 37 周内未执行任何恶意操作的用户）获得的结果，进一步确保实验的真实性。通过从系统性能指标中排除已知的恶意用户，即训练数据中包含恶意行为的用户，我们认为所进行的评估反映了现实生活中的情况以及网络安全分析师的兴趣 [34]。评估结果是从一系列实验中获得的，其中每个设置（一种数据类型的 ML 算法）随机重复 20 次。
在第一个实验中，为了显示传统 ML 应用程序与现实世界网络安全情况之间的对比，我们将上述现实设置与理想（传统）设置进行比较，其中使用整个数据集中随机 50% 的数据进行训练ML 算法。这是在三个数据粒度级别完成的：用户周、用户日和用户会话。
第二个实验在所有上述数据粒度级别的现实设置中评估 ML 算法，以获得基于实例和基于用户的详细结果。对数据集中提供的每个内部威胁场景的结果进行详细分析。此外，在 CERT r5.2 上训练的模型还用于针对其他版本的 CERT 内部数据进行测试，以探索训练模型在新环境/未知环境下的表现的泛化（不同版本的 CERT 数据模拟不同的组织）。
学习算法——有监督与无监督：虽然这项工作的重点是使用 ML 算法从有限数量的标记数据中学习以检测看不见的恶意内部人员，但在本节中，我们将在实际训练条件下比较所使用的 ML 算法和隔离森林的性能(IF) [53]，一种突出的无监督学习方法，最近已在许多网络异常检测工作中使用 [54]。 IF 假设异常数据实例比正常实例更容易与数据的其余部分隔离，因此到异常实例的相应叶子的路径长度更短。对于训练 IF 模型，针对每种数据类型调整树的数量。根据用于调查标记异常事件的不同可用预算，我们将警告数据实例的三个不同阈值（1%、5% 和 10%）假设为“异常”。表 IV 和图 6 说明了 IF 实现的结果。结果清楚地表明，当标签信息（尽管有限）可用于训练 ML 算法时，监督学习中的引导搜索将实现卓越的性能，特别是在非常低的 FPR 时。
有监督学习算法比无监督要好很多。
) 基于实例的结果：基于实例的结果显示在表 V 和图 5 中。总体上一个明显的趋势是 ML 算法的基于实例的性能正在下降（w.r.t. IDR 和 IF1）由于更高的数据粒度级别。具体而言，在以下情况下可以观察到显着差异 在几乎所有情况下比较不同数据粒度级别的基于实例的结果 (IF1)。例如，比较用户周和用户会话之间或用户会话和用户子会话 T2 之间的 RF IF1 都产生 p = 9e−5。图 6 进一步证明了观察结果，其中用户周数据的 AUC 高于用户会话数据。这可以通过嵌入在不同数据类型的每个数据实例中的信息量来解释（参见第 III-B 节），其中粗粒度的数据类型，例如用户周和用户日，涵盖更长的时间段并汇总更多的行为信息，即用户操作，而不是细粒度的数据类型，例如 user-session 和 user-subsession。此外，细粒度数据类型（第 IV-A 部分）中更大的实例计数和更高的不平衡数据分布也可能导致观察到的基于实例的结果的退化。 2) 基于用户的结果：表 V 和图 5 和 7 显示 ML 算法在不同数据粒度级别上基于用户的结果。与在基于实例的结果中观察到的趋势相反，基于用户的结果（UDR、UF1）通常对不同的数据粒度级别更加稳健。除了 XG 算法之外，在大多数情况下，数据类型的度量之间没有大的变化 (>5%)。此外，尽管检测到的恶意内部数据实例的比例相对较低（表 V），但分类器可以学习为大多数恶意内部人员（80% 到 90%）检测至少一个恶意实例。基于用户的结果报告也大大调整了误报率。例如，NN 仅实现了 0.14% 的 IFPR，但在用户会话数据上实现了 3.44% 的 UFPR。这些观察显示了简单地报告每个数据实例而不是每个用户的结果的缺点，前者可能不一定证明检测器检测恶意用户的能力的真实估计。在实践中，似乎基于用户的指标 证明使用细粒度数据类型的合理性，例如用户会话
这个想法是很有用的，基于用户的实例的检测要比基于操作的检测更容易检测出异常。
问题：怎么基于用户进行检测的？
用户子会话数据没有优于用户会话数据。
问题：用户会话数据是怎么构建的？
通常在所有数据类型上，只有场景 2 中的内部人员被遗漏。值得注意的是，虽然 UDR 和 UFPR 与在 CERT r5.2 上观察到的相似，但鉴于 CERT r5.1 中恶意用户的数量较少，UPr 和 UF1 较低。另一方面，CERT r6.2 似乎提出了新的挑战，可能是由于不同的组织结构。值得注意的是，未检测到 CERT r6.2（场景 5）中的新内部威胁场景。该场景描述了“因裁员而大量减少的用户将文档上传到 Dropbox，并计划将其用于个人利益”的行为 [33]。这个场景不仅代表了训练模型的一种新的恶意行为，而且与其余四个场景相比，它显示出更少的突兀行为。在数据粒度上，用户会话表现出较低的性能 比来自 CERT r6.2 的其他数据类型。这表明在具有不同用户行为模型的不同组织中，用户数据的会话可能代表不同的操作过程。在这种情况下，更聚合的数据类型（例如用户日和/或用户周）可能会显示出更好的结果。总体而言，本节中的结果表明训练的模型对于组织中的内部威胁检测，只能用作不同环境中的初始检测步骤。特定模型需要从头开始重新训练或从现有模型发展而来以获得更高的准确性。此外，需要异常检测来识别新的恶意行为。
用户会话对于新的恶意行为效果不好。针对不同的恶意行为需要分别设立模型进行检测。
在这项研究中，提出了一种基于机器学习的系统，用于组织网络系统中的内部威胁检测。该工作对四种不同的 ML 算法（LR、NN、RF 和 XG）在多个数据粒度级别、有限的真实情况和不同的训练场景进行了基准测试，以支持网络安全分析师检测未知数据中的恶意内部行为。评估结果表明，所提出的系统能够成功地从有限的训练数据中学习并推广以检测具有恶意行为的新用户。该系统实现了很高的检测率和精度，特别是 当考虑基于用户的结果时。在四种 ML 算法中，RF 明显优于图 10. 用户会话数据中的特征重要性。表九 其他作品的结果 其他算法，在大多数情况下，它实现了高检测性能和 F1-score 以及低误报率。另一方面，NN 允许稍微更好的内部威胁检测性能，但代价是更高的误报率。在数据粒度上，用户会话数据提供高恶意内部检测率和最小延迟。另一方面，用户日数据显示在检测特定内部威胁场景（即场景 2 - 知识产权盗窃）方面的性能略好。此外，当应用于不同组织的数据时，它似乎更具普遍性。未来的工作将调查时间信息的使用 在用户操作中。具体来说，这项工作中的所有模型都提供了基于仅限于单个示例的状态描述的标签。使模型能够看到多个示例或保留状态（循环连接）有可能使模型成为可能 做出非马尔可夫决策。
7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释</content></entry><entry><title>无监督聚类算法学习总结</title><url>http://next.lisenhui.cn/post/study/deeplearning/%E6%97%A0%E7%9B%91%E7%9D%A3%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/</url><categories><category>科研学习笔记</category></categories><tags><tag>科研学习笔记</tag></tags><content type="html"> 目录： 概览 1. 基于规则 1.1. 随机森林 1.2. SVM 1.3. 投影寻踪 2. 基于神经网络 2.1. AutoEncoder 3. 其他 3.1. 关联分析 4. 其他知识 回归问题 概览 https://scikit-learn.org/stable/modules/clustering.html 中文： https://sklearn.apachecn.org/docs/master/22.html#k-means
1. 基于规则 1.1. 随机森林 众多树。每次选择部分信息进行训练。然后选择众数作为最终输出。
https://mp.weixin.qq.com/s?__biz=MzU4ODcyMTI1Nw==&amp;mid=2247483832&amp;idx=1&amp;sn=e23330ead0d312a94c926d54e92cbc67&amp;chksm=fdd93cbecaaeb5a803191c98f0aadac658fbb3f7a14f40c3b5fc6e35f67d0bde9272740b3adc&amp;mpshare=1&amp;scene=23&amp;srcid=&amp;sharer_sharetime=1570582903129&amp;sharer_shareid=8906c7c6e8077a7cd67e079a0339edc8#rd
RF的回归问题，就是将所有决策树的输出取平均值。最多的还是用来处理分类问题。
1.2. SVM 1.3. 投影寻踪 https://esl.hohoweiya.xyz/11-Neural-Networks/11.2-Projection-Pursuit-Regression/index.html
code：https://github.com/pavelkomarov/projection-pursuit
2. 基于神经网络 2.1. AutoEncoder 3. 其他 3.1. 关联分析 4. 其他知识 回归问题 回归问题是机器学习三大基本模型中很重要的一环，其功能是建模和分析变量之间的关系。
分类问题将回归问题离输出散化。
回归问题将分类问题输出连续化
一些常见聚类方法简介：
k-means 以空间中的k个点为中心进行聚类。对最靠近它们的对象归类。
KNN 一个对象的分类由其邻居的多数表决确定。k个最近邻居中最常见的分类决定赋予了该对象的类别。</content></entry><entry><title>Insider Threat Detection via Hierarchical Neural Temporal Point Process</title><url>http://next.lisenhui.cn/post/paperreading/insider-threat-detection-via-hierarchical-neural-temporal-point-process/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag><tag>异常行为分析</tag><tag>Insider Threat</tag><tag>RNN</tag></tags><content type="html"> 目录： 1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 1. 综述翻译 内部人员通常会给组织造成重大损失，而且很难被发现。 目前，已经提出了各种方法来基于分析记录员工活动类型和时间信息的审计数据来实现内部威胁检测。 然而，现有的方法通常侧重于对用户的活动类型进行建模，而没有考虑活动时间信息。 在本文中，我们通过将时间点过程和循环神经网络相结合，提出了一种分层神经时间点过程模型，用于内部威胁检测。 我们的模型能够通过有效模拟活动时间、活动类型、会话持续时间和会话间隔信息的两级结构来捕获对所有活动历史的一般非线性依赖性。 两个数据集的实验结果表明，我们的模型优于仅考虑活动类型或时间信息的模型。
1.1 发表于 2019 于 IEEE Internatioanl Conference on Big Data. C会。
2. Tag Insider Threat; RNN;
3. 任务描述 4. 方法 现有的大多数方法只关注操作类型（网络访问、发送电子邮件等）信息，而没有考虑关键的活动时间信息。在本文中，我们研究如何开发一个能够同时捕获活动时间和类型信息的检测模型。
在文献中，标记时间点过程（MTPP）是一个通用的数学框架，用于对序列的事件时间和类型信息进行建模。它已被广泛用于预测地震和余震[9]。传统的 MTPP 模型对事件如何发生做出假设，但在现实中可能会违反这些假设。最近，研究人员 [10]、[11] 提出将时间点过程与循环神经网络 (RNN) 相结合。由于神经网络模型不需要对数据做出假设，因此基于 RNN 的 MTPP 模型通常比传统的 MTPP 模型获得更好的性能。
我们提出了一种基于分层 RNN 的时间点过程模型，该模型能够捕获会话内和会话间时间信息。我们的模型包含两层长短期记忆网络 (LSTM) [12]，它们是传统 RNN 的变体。下层LSTM在会话内层捕获活动时间和类型，而上层LSTM在会话间层捕获时间长度信息。特别是，我们在较低级别的 LSTM 中采用了一个序列到序列模型，该模型被训练以在给定前一个会话的情况下预测下一个会话。上层 LSTM 将来自下层 LSTM 的编码器的第一个和最后一个隐藏状态作为输入来预测两个会话的间隔和下一个会话的持续时间。通过使用普通用户生成的活动序列训练所提出的分层模型，该模型可以利用较低级别的序列到序列模型、两个连续会话之间的时间间隔和会话持续时间来预测下一个会话的活动时间和类型来自上层 LSTM 的时间。
5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释</content></entry><entry><title>Log2Vec a Heterogeneous Graph Embedding Based Approach for Detecting Cyber Threats Within Enterprise</title><url>http://next.lisenhui.cn/post/paperreading/log2vec-a-heterogeneous-graph-embedding-based-approach-for-detecting-cyber-threats-within-enterprise/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag><tag>异常行为分析</tag><tag>Insider Threat</tag><tag>GNN</tag><tag>Doc2Vec</tag></tags><content type="html"> 目录： 1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 概览 Graph Construction 关系详细定义 Graph Embedding Detection k-means簇聚类算法简单讲解： log2Vec 的聚类算法 random walk 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 1. 综述翻译 内部员工的常规攻击和新兴的 APT 都是组织信息系统的主要威胁。现有的检测主要集中在用户的行为上，通常分析记录他们在信息系统中操作的日志。一般来说，这些方法中的大多数都考虑了日志条目之间的顺序关系并模拟用户的顺序行为。然而，他们忽略了其他关系，不可避免地导致在各种攻击场景下的表现不尽如人意。我们提出 log2vec，一种基于异构图嵌入的模块化方法。首先，它涉及一种启发式方法，该方法根据日志条目之间的不同关系将日志条目转换为异构图。接下来，它利用适用于上述异构图的改进图嵌入，可以自动将每个日志条目表示为低维向量。 log2vec 的第三个组件是一种实用的检测算法，能够将恶意和良性日志条目分成不同的集群并识别恶意条目。我们实现了 log2vec 的原型。我们的评估表明 log2vec 明显优于最先进的方法，例如深度学习和隐马尔可夫模型 (HMM)。此外，log2vec 显示了其在各种攻击场景中检测恶意事件的能力。
1.1 发表于 2019 于 Computer and Communications Security (CCS) A会
2. Tag Cyber Threat; Enterprise; Anomaly detection; GNN; Doc2Vec
3. 任务描述 现代信息系统已成为当今企业和组织的重要且不可替代的组件。然而，这些系统经常面临来自内部员工的攻击风险，他们授权访问它们并故意使用这种访​​问来影响它们的机密性、完整性或可用性。同时，另一种新兴攻击，高级持续威胁 (advanced persistent threat APT) 也威胁着这些系统。具体来说，APTactors 最初会破坏目标系统中的帐户和主机，然后从这些主机中，他们会通过内网秘密地、持续地破坏多台主机并窃取机密信息
常见攻击模式: 对抗模型包括以下三种企业和政府常见的攻击场景。 第一种情况是内部员工滥用职权进行恶意操作，例如访问数据库或应用服务器，然后破坏系统或窃取知识产权以谋取个人利益。 二、恶意内部人员获取其他合法用户的凭据 用户通过窥视或键盘记录器，并利用这个新身份来寻求机密信息或在公司中制造混乱,利用这个新身份窃取机密信息（即伪装攻击） 。这两种场景属于典型的内部员工攻击。 第三种攻击是 APTactor 破坏系统中的一台主机，并从该主机上持续破坏多台主机以提升其权限并窃取机密文件。
现有的方法通常将用户的各种操作（也包括日志条目）转换为可以保存信息的序列，例如日志条目之间的顺序关系，然后使用顺序处理技术，例如。深度学习，从过去的事件中学习并预测下一个事件 [12, 47]。本质上，这些日志条目级方法对用户的正常行为进行建模，并将与其的偏差标记为异常。
总之，我们面临三个问题：1）如何同时检测上述两种攻击场景，特别是考虑到检测系统的所有三种关系（sequential relationship among log entries, logical relationships among days, interactive relationship among hosts,）； 2）如何在APTscenario中进行细粒度的检测，特别是深度挖掘和分析主机内日志条目之间的关系； 3）如何在没有攻击样本的情况下对训练模型进行检测。
4. 方法 概览 Log2vec 包含三个组件，如图 1 所示： (1) Graph构建。 Log2vec 构建异构图来整合日志条目之间的多种关系； (2) 图嵌入（也是图表示学习）。这是一种强大的图处理方法，可以根据每个操作在此类图中的关系来学习它们的表示（向量）。对用户的操作进行矢量化，可以直接比较他们的相似性以找出异常； (3) 检测算法，有效地将恶意操作分成单独的集群和弄清楚他们。
首先，log2vec 的第一个组件构建了一个异构图。这个数据结构是基于前面的三个关系构建的，这是现有方法在解决两种攻击场景中使用的主要关系[4,12,38,41,47,50,57]（对于问题1） . 其次，我们将日志条目划分为五个属性。根据这些属性，我们深入考虑主机内日志之间的关系，并设计出精细的规则来关联它们。这种设计使正常和异常的日志条目在这样的图中拥有不同的拓扑结构，可以被 log2vec 的后面组件捕获和检测（对于问题 2）。 第三，log2vec 的图嵌入和检测算法在没有攻击样本的情况下将日志条目表示并分组到不同的集群中，适用于数据不平衡的场景（对于问题 3）。此外，图嵌入本身可以为每个操作自动学习表示（向量），而不是手动提取特定领域的特征，从而独立于专家的知识。我们的改进版本可以进一步从上述异构图中差分提取和表示操作之间的多种关系。 Graph Construction Log2vec 的第一个组件是一种基于规则的启发式方法，用于将反映用户典型行为和暴露恶意操作的日志条目之间的关系映射到图形中。log2vec主要考虑了三种关系：（1）causal and sequential relationships within a day 一天内的因果关系和顺序关系； (2) logical relationships among days 多天内的逻辑关系 (3) logical relationships among objects.对象之间的逻辑关系。
我们将日志条目分为五个主要属性（主题、对象、操作类型、时间和主机），称为元属性（参见第 3.1 节）这就将构建了一个异构图的基本元素。 在设计关于这三种关系的规则时，我们会考虑这些元属性的不同组合，以关联更少的日志条目并将更精细的日志关系映射到图中。我们使用一个规则，将同一用户的日志条目按时间顺序连接起来（规则 A），将这种关系映射到图形中。 我们考虑两个元属性，主题和时间 在另一个例子中，我们考虑另一个元属性，操作类型，并使用一个规则，将同一用户和同一操作类型的日志条目按时间顺序连接起来（规则B），将一天内的设备连接操作串联起来。在生成对应于 3 天的三个设备连接序列后，我们使用其他规则根据它们的相似性将它们关联起来。 通过日志属性的不同组合，我们设计了涉及较少日志条目的各种行为序列，并将一天内和主机内的日志条目之间的多个更精细的关系映射到图中。 经过图嵌入和检测算法，log2vec 产生小簇来揭示异常操作。 在实践中，可疑集群中涉及的日志条目数量非常少，甚至等于1。因此，log2vec在上述两种攻击场景中对用户的行为进行了更精细的挖掘。 根据 log2vec 中的每条规则，我们将日志条目转换为序列或子图，所有这些都构成了一个异构图。 每个规则，对应一个边类型，是从一个特定的关系派生出来的，如上例。 由于不同的关系在各种场景中扮演不同的角色，我们使用多种边类型而不是权重来区分它们。 关系详细定义 A log entry： &lt; sub, obj,A,T,H > 其中sub是用户的集合。obj 是对象的集合，例如文件、移动存储设备和网站； A是操作类型的集合，如文件操作、浏览器使用等； T 是时间的集合，H 是主机的集合，例如计算机或服务器。
此外，sub、obj、A 和 H 都有自己的属性集。sub 的属性涉及角色（例如系统管理员）和他所属的组织单位（例如现场服务部门）。 obj 的属性可能包括文件类型和大小。 H 的属性是服务器的功能（例如文件服务器）。也就是说，它把一个登录操作当作如下方式，一个用户（子）登录到（A）一个目的主机（obj） 源一 (H)，就像用户在服务器中写入文件一样。
关系涉及三类：（1）一天内的因果关系和顺序关系； (2) 天之间的逻辑关系； (3)对象之间的逻辑关系。
Rule1 (edge1): log entries ofthe samedayare connectedin chronological order with the highest weight (value 1).
Rule1（edge1）：按时间顺序连接当天的日志条目，权重最高（值为1）。
Rule2 (edge2): log entries ofthe same host and the same day are connected in chronological order with the highest weight (value 1).
规则2（edge2）：同一主机同一天的日志条目按时间顺序连接，权重最高（值为1）。
Rule3 (edge3): log entries ofthe same operation type, the same host and the same day are connected in chronological order with the highest weight (value 1)
规则 3（边 3）：相同操作类型、相同主机和同一天的日志条目按时间顺序连接，权重最高（值 1）
不同于恶意的行为模式。为了比较从 rule1∼rule3 导出的用户日常行为序列，我们提出 rule4∼rule6，包括分别对应于 rule1∼rule3 的元属性的元属性。通过这些规则，log2vec 分别隔离了异常的行为序列， 来自图中第 3.2.1 节中提到的各种场景。
Rule4 (edge4): daily log sequences are connected and their weights are positively related to similarity ofthe numbers oflog entries that they contain.
Rule4（edge4）：每日日志序列是连通的，它们的权重与它们包含的日志条目数量的相似性呈正相关。
Rule5 (edge5): daily log sequences ofthe same host are connected and weights are positively related to similarity ofthe numbers oflog entries that they contain.
Rule5（edge5）：同一主机的每日日志序列是相连的，权重与它们包含的日志条目数量的相似性呈正相关。
Rule6 (edge6): daily log sequences of the same operation type and the same host are connected and weights are positively related to similarity ofthe numbers of log entries that they contain.
Rule6（edge6）：相同操作类型和相同主机的每日日志序列是相连的，权重与它们包含的日志条目数量的相似度呈正相关。
Rule7 (edge7): log entries of accessing the same destination host from the same source host with the same authentication protocol are connected in chronological order with the highest weight (value 1).
Rule7（edge7）：使用相同认证协议从同一源主机访问同一目标主机的日志条目按时间顺序连接，权重最高（值为1）。
Rule8a (edge8): log sequences of the same destination host and source one with different authentication protocols are connected and weights are positively related to the similarities of the numbers of log entries that they contain.
Rule8a (edge8)：同一目标主机的日志序列和具有不同身份验证协议的源 1 连接在一起，权重与它们包含的日志条目数量的相似性呈正相关。
Rule8b (edge8): log sequences of different destination hosts or source ones with the same authentication protocol are connected and weights are positively related to the similarities of the numbers of log entries that they contain.
Rule8b（edge8）：具有相同认证协议的不同目的主机或源主机的日志序列相连，权重与它们包含的日志条目数量的相似性呈正相关。
ule9 (edge9): log entries of the same host and accessing the same domain name are connected in chronological order with the highest weight (value 1).
Rule9（edge9）：同一主机访问同一域名的日志条目按时间顺序连接，权重最高（值为1）。
Rule10 (edge10): log sequences of the same host and different domain names are connected and weights are positively related to the similarities of accessing modes and numbers of log entries that they contain.
Rule10（edge10）：同一主机不同域名的日志序列是相连的，权重与访问方式的相似性和它们所包含的日志条目数呈正相关。
对于每个类别，我们根据元属性的不同组合提出了一些规则。 如图3所示，我们首先提出rule1∼rule3将一天内的日志条目连接成序列，对应关系（1）。 这三个规则从不同方面对用户的行为进行建模，例如 日期、主机和操作类型。 通过这种设计，将用户在陌生主机上进行的操作，或者属于很少进行操作的操作类型，在图中进行隔离。 然后，我们提出了规则 4∼规则 6，以根据关系 (2) 分别桥接这些日常序列。 异常行为序列将与良性行为序列分开。 这六个规则将图 2c 中的关系映射到图形中。 它们主要关联用户跨天在多个主机上的各种类型的操作。
最后，提出了与关系（3）相对应的四个规则（rule7∼rule10），以考虑用户如何登录/入侵主机并向外部发送机密数据。 具体来说，它们构建了用户登录和网页浏览操作的模式。 有关登录操作的规则，规则 7 和规则 8，考虑如何在内联网内交互访问这些主机，例如图 2d 中的实例。 Web 操作规则，规则 9 和规则 10，侧重于用户通过 Internet 使用浏览器。 Intranet 和 Internet 是主要的入侵源，例如 登录主机或驱动下载。
Graph Embedding 具体来说，图嵌入涉及图 1 中的随机游走和 word2vec。
假设一个walker 位于图中的一个节点上，他根据每条边的权重和类型决定下一个要访问的节点。 由他生成的路径，一个节点序列，被视为这些节点的上下文（见第 4.1 节）。 例如，当一个步行者驻留在属于第1天或第2天设备连接序列的节点时（图2a），通过图构建生成，他很少会选择第3天序列中的节点（设备连接），因为链接低 重量。 同样，当他在第 3 天的序列中驻留节点时，他很少会到达第 1 天或第 2 天的序列。
Detection Log2vec 采用聚类算法对上述向量进行分析，并将良性操作（日志条目）和恶意操作分为不同的集群（第 5.1 节）。
在聚类之后，我们设置了一个阈值来识别恶意集群。 也就是说，大小小于阈值的集群被视为恶意（第 5.2 节）
k-means簇聚类算法简单讲解： 定义总共有多少个簇 将每个簇心随机定在一个点上 将每个数据节点关联到最近簇重心所属的簇上 对于每一个簇找到其所有关联点的中心点（取每一个点坐标的平均值） 将上述点变为新的簇心 不停重复直至每个簇所拥有的点不变。 简单来说，聚类算法只是通过某种方式，将结果进行分类。
log2Vec 的聚类算法 传统的更新聚类中心的想法，如k-means，不适合内部威胁检测，因为它严重依赖于聚类中心和k的初始化，导致性能不理想。这里提出了一个新的聚类算法，见论文。
random walk Log2vec 的改进是控制邻居节点 (neigh) 的数量并以不同比例的边类型 (ps) 集提取上下文，旨在解决不平衡数据集和各种攻击场景的问题。
改进版的random walk
5. 解决了什么问题（贡献） 提出了一种将log日志转为图结构的方法 设计了很多规则，使用这些规则，提出了异构图来构造图。异构图可以允许更多类型的节点互相交互，比如日志文件，用户，操作等。从实验结果来看，异构图会比同构图要好一点。 问题：
整个方案非常复杂，复现可行度需要画一个问号。 6. 实验结果 详见Table 3 和 Table 4.
7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释</content></entry><entry><title>An Insider Threat Detection Approach Based on Mouse Dynamics and Deep Learning</title><url>http://next.lisenhui.cn/post/paperreading/an-insider-threat-detection-approach-based-on-mouse-dynamics-and-deep-learning/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag><tag>异常行为分析</tag><tag>CNN</tag><tag>Insider Threat</tag></tags><content type="html"> 目录： 1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 1. 综述翻译 在当前的 Intranet 环境中，信息正变得更容易在广泛的互连系统中访问和复制。任何使用内联网计算机的人都可以访问他无权访问的内容。对于内部攻击者来说，窃取同事的密码或使用无人值守的计算机发起攻击相对容易。在这种情况下，常见的一次性用户身份验证方法可能不起作用。在本文中，我们提出了一种基于鼠标生物行为特征和深度学习的用户身份验证方法，可以准确高效地对当前计算机用户进行连续身份验证，从而应对内部威胁。我们使用一个有 10 个用户的开源数据集进行实验，实验结果证明了该方法的有效性。该方法大约每 7 秒完成一次用户认证任务，错误接受率为 2.94%，错误拒绝率为 2.28%。
1.1 发表于 Security and communication Networks C刊 由于发的不是很好，所以大致看看。
2. Tag Insider Threat; CNN;
3. 任务描述 4. 方法 本文提出了一种基于鼠标动态行为和深度学习的持续身份认证方法来解决内部威胁攻击检测问题
(i) 我们提出了一种使用鼠标动态行为和深度学习的新型连续身份认证方法。与现有方法相比，它实现了更高的准确性和更短的验证时间。 (ii) 我们不是从原始操作中手动提取特征来表征用户独特的鼠标行为特征，例如移动速度曲线，而是将鼠标动态行为映射到图片中。因此，可以保留鼠标行为的全部细节。 (iii) 我们构建了一个 7 层 CNN 网络来训练鼠标行为图片数据集。网络以少量数据（约18000张图片）收敛。此外，该网络可用于训练其他鼠标行为数据集并轻松实现身份验证
我们提出了一种可以完全保留所有基本鼠标操作并使用深度学习进行用户身份验证。首先，我们通过特定的方法将鼠标生成的所有动作映射到图像上。然后我们通过 CNN 网络训练图像数据集以创建分类模型。在认证过程中，将用户的鼠标操作按照相同的方法进行映射，然后通过训练好的模型进行分类，从而达到用户识别的目的。我们的方法充分利用了深度学习的优势。首先，在将鼠标行为映射到图像的过程中，我们保留了所有基本的鼠标操作。使用CNN网络时既不需要人工提取特征来训练这些图像数据集，也不需要传统机器学习中的特征提取算法。卷积神经网络可以在训练中自动完成特征提取和抽象。其次，对于如何使用CNN对图像进行分类的问题，有很多成功有效的解决方案。为了利用这一优势，我们将鼠标动作映射到图片上基于鼠标行为的行为轨迹。这将基于鼠标行为的用户认证问题变成了经典的图像分类问题
5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释</content></entry><entry><title>Deep Learning for Unsupervised Insider Threat Detection in Structured Cybersecurity DataStreams</title><url>http://next.lisenhui.cn/post/paperreading/deep-learning-for-unsupervised-insider-threat-detection-in-structured-cybersecurity-datastreams/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag><tag>异常行为分析</tag><tag>RNN</tag></tags><content type="html"> 目录： 1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 概述 细节 Feature Extraction Structured Stream Neural Network DNN RNN 输出 内部危害检测： 6. 实验结果 5. 解决了什么问题（贡献） 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 1. 综述翻译 分析组织的计算机网络活动是早期检测和缓解内部威胁的关键组成部分，这是许多组织日益关注的问题。原始系统日志是流数据的典型示例，可以快速扩展超出人类分析师的认知能力。作为人类分析师的预期过滤器，我们提出了一种在线无监督深度学习方法，以实时检测系统日志中的异常网络活动。我们的模型将异常分数分解为个人用户行为特征的贡献，以提高可解释性，以帮助分析师审查潜在的内部威胁案例。使用 CERT 内部威胁数据集 v6.2 和威胁检测召回作为我们的性能指标，我们新颖的深度和循环神经网络模型优于主成分分析、支持向量机和基于隔离森林的异常检测基线。对于我们的最佳模型，我们数据集中标记为内部威胁活动的事件的平均异常分数为 95.53 个百分点，这表明我们的方法有可能大大减少分析师的工作量。
1.1 发表于 aaai 2017
2. Tag Insider Threat; LSTM; unsupervised;
3. 任务描述 4. 方法 概述 我们提出了一个在线无监督深度学习系统来过滤系统日志数据以供分析师审查。 由于内部威胁行为千差万别，我们不会尝试对威胁行为进行明确建模。 相反，深度神经网络 (DNN) 和循环神经网络 (RNN) 的新变体经过训练，可以识别网络上每个用户的特征活动，并同时实时评估用户行为是正常还是异常。 考虑到流媒体场景，我们方法的时间和空间复杂度作为流持续时间的函数是恒定的； 也就是说，不会无限期地缓存任何数据，并且在将新数据输入到我们的 DNN 和 RNN 模型时进行检测。 为了帮助分析师解释系统决策，我们的模型将异常分数分解为人类可读的导致检测到的异常的主要因素的摘要（例如，用户在凌晨 12 点到早上 6 点之间将异常大量的文件复制到可移动媒体）。
图 1 概述了我们的异常检测系统。
首先，来自系统用户日志的原始事件被输入到我们的特征提取系统中，该系统聚合它们的计数并为每个用户每天输出一个向量。 然后将用户的特征向量输入神经网络，创建一组网络，每个用户一个。 在我们系统的一种变体中，这些是 DNN； 另一方面，它们是 RNN。 在任何一种情况下，不同的用户模型共享参数，但对于 RNN，它们保持独立的隐藏状态。 这些神经网络的任务是预测序列中的下一个向量； 实际上，他们学会了为用户的“正常”行为建模。 异常与预测误差成正比，有足够的异常行为被标记以供分析师调查。 细节 Feature Extraction 我们的系统从这些来源中提取了两种信息：分类用户属性特征和连续“计数”特征。
categorical user attribute features分类用户特征是指用户在组织中的角色、部门和主管等属性。有关我们实验中使用的分类特征列表（以及每个类别中不同值的数量），请参见表 1
除了这些分类特征之外，我们还累积了用户在某个固定时间窗口（例如 24 小时）内执行的 408 项“活动”的计数。计算活动的一个示例是下午 12:00到下午 6:00 之间来自可移动媒体的不常见非诱饵文件副本的数量。图 2 直观地列举了一组计数特征：只需沿着一条从右到左的路径，沿途在每个集合中选择一个项目。所有这些遍历的集合是计数特征的集合。对于每个用户 u，对于每个时间段 t，将分类值和活动计数串联起来变成一个414维的数字特征向量xu
414=6+408
Structured Stream Neural Network 我们系统的核心是两个神经网络模型之一，该模型将给定用户的一系列特征向量（每天一个）映射到用户序列中下一个向量的概率分布。 该模型以在线方式同时对所有用户进行联合训练。 首先，我们描述了我们的 DNN 模型，它没有明确地对任何时间行为进行建模，然后是 RNN，它可以。 然后我们讨论用于预测结构化特征向量和识别特征向量流中的异常的其余部分。
DNN 根据前文所说，一共分成了T个时间段。对于每个时间段组成了vectors $x_t^u$, DNN将这些$x_t^u$转为$h_t^u$
RNN 前面的T个特征值$h_t^u$按顺序送入LSTM。这样当前输入不仅与当前时间段有关，而且和前一个时间段有关。
输出 RNN得到的特征$h_{t-1}$用来进行预测。
对于408个统计量counter和6个角色特征C={R,P,F,D,T,S}分别进行预测
Therefore, the P is actually the joint probability over the counter vector and each of the categorical variables: role (R), project (P), functional unit (F), department (D), team (T) and supervisor (S).
这里使用7个单层隐藏层来得到输出概率： 对于6个角色属性使用softmax函数进行归一化得到离散概率。 对于counter使用正态分布获取连续概率分布。
我们将六个分类变量的条件概率建模为离散的，而我们将计数的条件概率建模为连续的。对于离散模型，我们使用标准方法：类别 k 的概率只是向量 θ(V) 的第 k 个元素，其维度等于类别的数量。例如，有 47 个角色，所以 θ(R) ∈ R47。因为我们使用 softmax 输出激活来产生 θ(V)，所以元素是非负的并且总和为一
对于计数向量，我们使用多元正态密度。我们考虑两个变体。第一个，我们的模型输出平均向量 μ (θ(ˆx) = μ)，我们假设协方差 Σ 是恒等式。使用恒等协方差，最大化真实数据的对数似然相当于​​最小化平方误差。在第二个中，我们假设对角协方差，我们的模型输出均值向量和 Σ 对角线的对数。模型的这部分可以看作一个简化的混合密度网络（Bishop 1994）。
文中使用两种预测方式运算。
下一个时间步预测：用本时间步特征预测下一个时间步的值 同一个时间步预测：用本时间步特征预测本时间步的值。 使用本时间步值的原因：
An auto-encoder is a parametric function trained to reproduce the input features as output. Its complexity is typically constrained to prevent it from learning the trivial identity function; instead, the network must exploit statistical regularities in the data to achieve low reconstruction error for commonly found patterns, at the expense of high reconstruction error for uncommon patterns (anomalous activity). Networks trained in this unsupervised fashion have been demonstrated to be very effective in several anomaly detection application domains (Markou and Singh 2003).
自动编码器是经过训练的参数函数，用于将输入特征再现为输出。 它的复杂性通常受到限制，以防止它学习琐碎的身份函数； 相反，网络必须利用数据中的统计规律来实现常见模式的低重构误差，但代价是不常见模式（异常活动）的高重构误差。 以这种无监督方式训练的网络已被证明在几个异常检测应用领域非常有效（Markou 和 Singh 2003）。
内部危害检测： 我们模型的目标是检测内部威胁。我们假设以下条件：我们的模型产生异常分数，用于从最异常到最少对用户天数进行排名，然后我们将排名最高的用户天对提供给判断异常行为是否表明内部威胁的分析师。我们假设有一个每日预算，它规定了每天可以判断的最大用户日对数，并且如果向分析师呈现内部威胁的实际案例，他或她将正确检测到它。
One key feature of our model is that the anomaly score decomposes as the sum over the negative log probabilities of our variables; the continuous count random variable further decomposes over the sum of individual feature terms: (xi − µi)/σi. This allows us to identify which features are largest contributors to any anomaly score;
我们模型的一个关键特征是异常分数分解为我们变量的负对数概率的总和；连续计数随机变量进一步分解单个特征项的总和：(xi − µi)/σi。这使我们能够确定哪些特征对任何异常分数的贡献最大；
为了适应在线场景，我们对标准训练方案进行了重要调整。对于 DNN，主要区别在于每个样本只能观察一次的限制。对于 RNN 来说，情况更为复杂。我们同时训练多个用户序列，每次看到用户的新特征向量时都会反向传播和调整权重。从逻辑上讲，这对应于每个用户训练一个 RNN，其中权重在所有用户之间共享，但隐藏状态序列是每个用户。在实践中，我们通过使用补充数据结构训练单个 RNN 来实现这一点，该结构存储每个用户的过去输入以及隐藏和单元状态的有限窗口。每次将用户的新特征向量输入模型时，在计算前向传播和反向传播误差时，该用户的隐藏状态和单元状态将用于上下文。
简单点讲，每个用户拥有自己的时间序列。
6. 实验结果 训练和测试时，仅训练和测试了工作日，将周末分开了。因为工作日和周末的正常情况在性质上是不同的。如果需要，可以训练第二个系统模拟正常的周末行为。
测试显示，当前时间步预测比下一个时间步预测效果要稍微更好点。 Cumulative Recall 累计召回率。 precision 衡量查准率。召回率衡量查全率。
我们进行了两次分析以更好地理解我们系统的行为，使用我们最好的 DNN 模型来说明。首先，我们看看时间对模型异常概念的影响。由于模型开始时完全未经训练，所有用户的异常分数在最初几天都非常高。当模型看到用户行为的例子时，它很快就会知道什么是“正常”。图 4 显示了作为天函数的异常（在前几天的“老化”期之后开始，以保​​持 y 轴刻度易于管理）。百分比范围显示（根据当天的用户计算），恶意（内部威胁）用户天覆盖为红点。请注意，所有恶意事件都高于异常的第 50 个百分位，大多数都高于第 95 个百分位。
在我们的第二个分析中，我们研究了每日预算的影响 关于最佳 DNN、最佳 LSTM 和三个基线模型的回忆。图 5 绘制了这些召回曲线。令人印象深刻的是，在每日预算为 425 的情况下，DNN-Diag、LSTM-Diag 和隔离森林模型都获得了 100% 的召回率。它还表明，使用我们的 LSTM-Diag 系统，只需 250 的预算即可获得 90% 的召回率（分析师需要考虑的数据量减少了 93.5%）。
5. 解决了什么问题（贡献） 我们的模型试图解决将机器学习应用于网络安全领域（Sommer and Paxson 2010）的几个关键困难。
网络上的用户活动在几秒到几小时内通常是不可预测的，这导致难以找到“正常”行为的稳定模型。我们的模型以在线方式持续训练，以适应数据中不断变化的模式。 此外，恶意事件的异常检测尤其具有挑战性，因为攻击者经常试图密切模仿典型行为。我们将系统日志流建模为具有用户元数据的交错用户序列，以便为网络上的活动提供精确的上下文；例如，这允许我们的模型识别用户、同一角色的员工、同一项目团队的员工等的真正典型行为。 我们评估我们的模型在合成 CERT Insider Threat v6.2 上的有效性数据集（Lindauer et al. 2014；Glasser and Lindauer 2013），其中包括带有内部威胁活动行级注释的系统日志。真实威胁标签仅用于评估。 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释</content></entry><entry><title>Anomaly Based Insider Threat Detection Using Deep Autoencoders</title><url>http://next.lisenhui.cn/post/paperreading/anomaly-based-insider-threat-detection-using-deep-autoencoders/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag><tag>异常行为分析</tag><tag>Unsupervised</tag></tags><content type="html"> 目录： 1. 综述翻译 1.1 发表 2. Tag 3. 任务描述 4. 方法 概览 特征提取： 自编码器 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 1. 综述翻译 近年来，恶意内部人员威胁已成为组织可能面临的最重要的网络安全威胁之一。由于内部人员具有逃避部署的信息安全机制（例如防火墙和端点保护）的天然能力，因此检测内部人员威胁可能具有挑战性。此外，与组织为入侵/异常检测目的收集的审计数据量相比，恶意内部人员的行为留下的数字足迹可能微不足道。为了从大量复杂的审计数据中检测内部威胁，在本文中，我们提出了一种检测系统，该系统使用深度自动编码器的集合来实现异常检测。集成中的每个自动编码器都使用特定类别的审计数据进行训练，这些数据准确地代表了用户的正常行为。原始数据和解码数据之间获得的重建误差用于衡量任何行为是否异常。在数据经过单独训练的自动编码器处理并获得各自的重建误差后，使用联合决策机制报告用户的整体恶意评分。使用用于内部威胁检测的基准数据集进行数值实验。结果表明，所提出的检测系统能够以合理的误报率检测所有恶意的内部人员行为。
1.1 发表 2018-ICDMW，数据库B会的workshop
2. Tag Insider threat; Autoencoders; Unsupervised
3. 任务描述 4. 方法 概览 在我们提议的内部人员检测系统中，我们总共创建了四个检测器，分别用于四类审计数据，即登录/注销活动记录、文件操作、USB 设备操作和 http 活动记录。假设用户的行为在整个审计数据集中是一致的，则为审计数据的每个类别提取许多基于频率的特征。这些特征用作训练深度自动编码器的输入。
当经过训练的自动编码器充当特定正常用户行为的基线模型时，任何受恶意行为影响的特征向量都应显着偏离基线模型，并应报告为异常。
由于自动编码器由编码器和解码器组成，因此偏差是通过使用解码器重建编码特征向量时产生的误差来衡量的。在四个自编码器检测器运行完数据后，对每个检测器应用 top-N 推荐算法，产生四维归一化的顶部异常向量。
然后，我们将异常向量归因于用户，并对每个用户的异常向量进行加权以获得恶意评分。随后再次应用top-N推荐算法以报告具有最高恶意评分的用户以供进一步调查。
我们使用 CMU 的内部威胁测试数据集进行数值实验，以评估每个自动编码器的最佳构造，这些参数包括层数、激活函数类型和损失函数类型等各种参数。最后，我们证明了我们可以构建一个检测系统，该系统结合了不同的自动编码器架构，以可管理的误报率检测恶意内部攻击。特别是，本文做出了以下贡献：
具体：
我们使用了基于主机的数据（即登录/注销活动、文件操作和 USB 设备操作）和基于网络的数据（即 http 活动）。 为每个类别的数据集提取了许多特征。这些特征被转发到自动编码器，从而形成一个对用户行为进行建模的神经网络。换句话说，每个自编码器代表给定审计数据集的特定基线模型，任何偏离模型的行为（即可检测的自编码器重建错误）都应标记为“异常”。 当所有自动编码器模型组合在一起时，可以从各种不同的审计数据视图中更好地识别用户的行为。一般而言，每个审计数据集可能仅提供微弱或不提供异常行为的指示。然而，当模型被用作异常行为的弱指标集合时，用户的恶意行为水平可能会被检测到。 概览总结：提取四种数据的特征=》自编码器重构，产生误差=》训练=》topN计算前N个恶意行为=》对每个用户使用topN计算恶意行为。
特征提取： 时间粒度通常是一个需要考虑的关键因素，它可以确保提取的特征能够反映用户的正常行为，同时能够检测任何异常/异常变化。一方面，小粒度通常会引入大方差，从而产生过拟合的基线模型。另一方面，大的时间粒度将导致无法突出任何异常行为的欠拟合基线模型。在我们提出的系统中，我们将时间粒度设置为每小时，因为就文献中观察到的内容以及我们在实验中获得的内容而言，这是最合适的程度。所有特征均从用户每天（24 小时）的活动中提取 计算每个用户的每小时登录频率以描述用户的每日登录行为/模式。 直观地说，这种登录模式对于用户来说很常见，因为他们通常在固定时间开始一天，比如 8:00 AM 或 9:00 AM。 同样，每个登录事件都会有一个相应的注销事件。 生成的每小时登录/注销模式形成一个长度等于 48 的正整数向量 文件审计数据提供有关用户对文件的操作的信息； 例如，对此类文件进行的操作（例如，打开、写入、复制和删除）以及文件传输到的位置。 这里应用与登录/注销相同的概念，以提取文件操作特征，即获取每个用户的每种类型文件操作的每小时频率。 对于来自同一业务单元的一组用户，期望用户以类似的方式访问和操作文件，这体现在文件副本数量等特征上。 否则，如果用户执行文件复制的频率明显高于基线建议的频率（根据他/她自己的个人资料和他/她在组织中的同事的个人资料），则应引发关注事件。 随后将所有四种文件操作模式串联起来，生成由 96 个正整数组成的文件审计特征向量。 USB 设备审计数据记录用户是否已将拇指驱动器连接到/从其计算机主机断开连接，并记录访问了哪些目录。 将文件从用户计算机传输到拇指驱动器通常与恶意内部人员行为密切相关。 事实上，它已被确定为从计算机网络中窃取数据的三种最常见方法之一 [3]。 在所提出的系统中，我们将拇指驱动器连接和断开操作的每小时频率统计数据连接在一起，产生 48 个正整数的特征向量。 传统上，http 审计数据能够提供非常丰富的信息，包括用户如何访问 Internet、数据如何传入和传出网络，甚至计算机与网络的交互行为模式。 然而，我们的工作旨在以有效和高效的方式检测恶意内部人员，因此，我们只关注用户在操作层面的活动。 特别是，用户基于 http 的活动被限制为三种类型，即：访问、上传和下载。 访问活动表示用户正在浏览网页； 一旦用户通过 HTTP 协议传出数据，这个活动就会触发一个上传活动； 相反，通过 HTTP 协议接收数据会触发下载活动。 http 审计数据特征向量连接每个活动的每小时频率。 这会产生 72 个正整数的特征向量。 表一总结了审计数据的提取特征。 从每一类审计数据中提取的特征然后被转发到一个单独的自动编码器。 自编码器从每个特征向量中提取最常见的信息模式。 然后，通过研究自动编码器重建特征向量的程度，我们可以确定用户的日常行为是否与以前的行为不同，以及它与他/她的同龄人的行为有何不同。 总结：特征提取部分每一天都提取了24个小时的特征，每个日志文件每小时提取2-4个特征，总共就提取了48-96个特征。训练时使用前k个做训练，使用第k+1天做测试。
自编码器 从异常检测的角度，我们希望将用户的正常行为表征为等同于该用户或用户组的基线模型。任何恶意行为都可能被识别为与基线模型的偏差。
在对用户的正常行为建模时，我们面临两个挑战：1）审计数据中存在复杂的非线性关系（例如，用户登录计算机的时间不一定与用户实际在计算机上花费的时间相关），以及2）很少有标签（如果有的话）可以提前表明“好”和“坏”审计数据实例，这意味着我们被迫进行无监督或半监督
我们使用深度自动编码器，因为它能够表示非线性关系，更重要的是，作为神经网络家族的一员，它用于无监督学习。
自编码器的有效性在很大程度上取决于自编码器模型及其相关参数的正确构建。例如，需要考虑各种参数，例如要采用的损失函数、要选择的层数以及对于每一层要采用的最佳激活函数。这些参数可能会对性能产生显着影响。在这项研究中，我们优化了这些自动编码器参数，
自编码器： 编码：$z=f(Wx+b)$
解码: $x'=f'(W&rsquo;z+b')$
Loss: $L(x, x') = ||x-x'||^2=||x-f'(W'(f(Wx+b))+b')||^2$
图 3 说明了从一天的登录/注销活动中提取的几个采样的正常/异常特征向量，其中每个像素代表登录/注销审计数据。一个 6×8 像素的图像表示发送特征向量的归一化值（从 0 到 1）。
对于正常特征向量的情况，前两行分别可视化原始特征向量和重建特征向量。底部两行显示相同的登录/注销特征向量，但用于异常用户行为情况。从图的顶部两行，我们可以观察到法线特征向量看起来非常相似，这与我们假设存在可以表示常见用户行为的基线模型一致。底部两行的模式表明被恶意登录/注销操作污染的特征向量与正常特征向量不同，并且它们的重构形式放大了这种差异。直观地，我们能够通过测量其原始形式和重建形式之间的差异来检测异常特征向量。
自编码器的架构设计可以对自动编码器的性能有显着影响 [13]。应考虑以下因素：1) 应使用的总层数，包括输入和输出层 2) 对于每一层，隐藏单元的数量和激活函数的选择，以及 3) 损失函数的选择被最小化。由于我们总共应用了四个自动编码器，四个审计数据类别中的每一个都有一个，因此可以通过为每个自动编码器使用不同数量的层来获得每个数据类别的最佳特征表示。
例如，如图 3 所示，一般每个用户每天只有 2-3 次登录和注销计算机，导致特征向量非常稀疏，而登录和注销的频率是按小时计算的.在这种特殊情况下，正如我们的数值实验将显示的那样，2-3 层足以用于登录/注销自动编码器。
图 3 表明这些特征不像从图像或文本中提取的特征那么复杂，并且其中一些特征具有很强的相关性。因此我们倾向于选择简单的激活函数，例如线性单元。为了最大化正常和异常用户行为之间的可分离性，我们试图使用一种损失函数来惩罚结构差异而不是数值差异
然而，在实践中，一般很难选择一个合适的阈值，尤其是当数据集很大时[19][20]。在我们的例子中，我们使用 top-N 推荐算法 [21] 只生成 top-N 异常行为特征向量。换句话说，每个审计数据检测器使用自己训练的自动编码器对所有特征向量进行重建，按降序对重建错误进行排序，并仅报告前 N 个。最后，将来自四个检测器中每一个的前 N ​​个重构误差组合在一起以做出最终决定。由于每个检测器都关注特定攻击向量的结果，因此组合不同的审计数据检测器以识别内部攻击更有意义。 1 其中 μ 和 σ 分别表示重构误差的均值和标准差，k 对应于检测器索引（k = 1, 2, 3, 4）。 其次，然后将前 N 个归一化重建错误组合在一起，按用户和日期对齐。 当用户没有一个检测器的输入特征向量时，错误率归零值。 然后将平均重建错误计算为恶意分数，对应于每对用户和日期。 最后，再次应用top-N算法以报告具有最大恶意评分的前N对用户和天。 然后，计算机应急响应小组 (CERT) 分析员可以使用此用户日对订购来进一步调查异常行为。
我们将误报率固定在 1% 到 10% 的范围内。图 5 说明了得到的 RoC 曲线，从中我们可以得出结论，我们可以通过仅调查总（组合）活动的 6% 来检测所有攻击（即，当 TPR=100% 时）。随后，我们还可以确定理论是什么 - 对于相同的检测精度，它可以达到的最小误报率。结果表明，通过调查 0.1% 的原始数据集，我们已经能够达到 66.7% 的真阳性率，通过调查 5.6% 的原始数据集，我们可以检测到所有的攻击。这样的结果证明了所提出的检测系统的实际可行性。在实际场景中，这意味着给定的组织每天监视 5000 个用户的行为，并且每个用户每天可能只生成一个事件（特征向量），那么最多只需要 300 个事件由 CERT 人类专家手动调查，这显着减少了分析师的整体认知工作量。
检测总结：针对四个检测器的前N个输出进行平均加权组合。
5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释</content></entry><entry><title>Deep Learning for Insider Threat Detection Review Challenges and Opportunities</title><url>http://next.lisenhui.cn/post/paperreading/deep-learning-for-insider-threat-detection-review-challenges-and-opportunities/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag><tag>异常行为分析</tag><tag>survey</tag></tags><content type="html"> 目录： 1. 综述翻译 2. Tag 3. 任务描述 4. 方法 Deep Feedforward Neural Network RecurrentNeuralNetwork. Convolutional Neural Network GraphNeuralNetwork. Challenges 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 1. 综述翻译 内部威胁作为网络空间中最具挑战性的威胁之一，通常会给组织造成重大损失。虽然安全和数据挖掘社区对内部威胁检测问题已经研究了很长时间，但传统的基于机器学习的检测方法严重依赖特征工程，难以准确捕捉内部人员和普通用户之间的行为差​​异，原因是与底层数据特征相关的各种挑战，例如高维、复杂、异构、稀疏、缺乏标记的内部威胁，以及内部威胁的微妙和适应性。先进的深度学习技术提供了一种从复杂数据中学习端到端模型的新范式。在这个简短的调查中，我们首先介绍一个常用的内部威胁检测数据集，并回顾有关此类研究的深度学习的最新文献。现有研究表明，与传统的机器学习算法相比，深度学习模型可以提高内部威胁检测的性能。然而，应用深度学习来进一步推进内部威胁检测任务仍然面临一些限制，例如缺乏标记数据、自适应攻击。然后，我们讨论这些挑战，并提出有可能应对挑战并进一步提高深度学习性能的未来研究方向。 内部威胁检测。
2021- Computer &amp; Security 网络与信息安全的B会。
2. Tag database insider threats; insider attack; deep learning
3. 任务描述 在协调中心 (CERT/CC) 中，恶意内部人员被定义为“已经或已经授权访问组织的网络、系统或数据的现任或前任员工、承包商或业务合作伙伴，并故意超出或故意使用该组织的网络、系统或数据。以对组织信息或信息系统的机密性、完整性或可用性产生负面影响的方式访问。”
最近的一项调查根据检测中使用的策略和特征将内部威胁检测技术进一步分为 9 类：（1）基于异常的方法，（2）基于角色的访问控制，（3）基于场景的技术，（4） 诱饵文件和蜜罐技术，(5) 使用心理因素进行风险分析，(6) 使用工作流进行风险分析，(7) 改进网络防御，(8) 通过访问控制改进防御，以及 (9) 过程控制以劝阻内部人员 .
异常检测是识别与所有其他实例不同的实例，这是欺诈检测、入侵检测和视频监控等多种应用的重要问题[13]。异常在数据挖掘和统计文献中被称为异常、异常或异常值，粗略地说，内部威胁可以被视为一种异常。
4. 方法 最近的一项调查 [12] 将深度分类基于标签的可用性将基于学习的异常检测分为三组，即监督、半监督和无监督的深度异常检测。在某些情况下，当正常和异常数据都可用时，有监督的深度异常检测方法被提议用于二元或多类分类 [10, 11]。一个更常见的场景是很容易收集到很多正常样本而只有少量异常样本可用，因此可以通过利用正常样本分离异常值来采用半监督深度异常检测[1, 67, 85 ]。当没有标记数据可用时，基于数据样本的内在属性，应用无监督的深度异常检测来检测异常
有一个数据集：卡内基梅隆大学软件工程研究所的 CERT 部门维护着一个包含 1000 多个内部威胁真实案例研究的数据库，并使用包含叛徒实例和伪装活动的场景生成了一系列合成内部威胁数据集。 CERT 数据集包含记录计算机的官方日志文件 -。他们进一步将常用的11个数据分为五类：基于伪装者、基于叛徒、杂项恶意、替代伪装者和基于识别/认证。
深度学习在内部威胁检测方面的潜在优势，可以总结如下。
表征学习。深度学习模型最显着的优势在于能够自动发现检测所需的特征。网络空间中的用户行为是复杂且非线性的。手动设计的功能很难捕获用户行为信息，而且效率低下。同时，具有浅层结构的学习模型，如 HMM 和 SVM，是相对简单的结构，只有一层用于将原始特征转换为可用于检测的高级抽象。这些浅层模型对于解决许多约束良好的问题是有效的，但是能力有限的浅层模型很难对复杂的用户行为数据进行建模。相比之下，深度学习模型能够利用深度非线性模块通过通用学习程序来学习表示。因此，使用深度学习模型来捕捉复杂的用户行为并精确检测用户的意图，尤其是那些恶意的意图是很自然的。 序列建模。 深度学习模型，例如循环神经网络 (RNN) 和新提出的 Transformer，在对序列数据（例如视频、文本和语音）进行建模方面表现出良好的性能 [27, 70]。 由于将审计数据中记录的用户活动表示为顺序数据是很自然的，因此利用 RNN 或 Transformer 捕获复杂用户行为的显着信息具有提高内部威胁检测性能的巨大潜力。 异构数据组合。 深度学习模型在融合的任务上也取得了出色的表现 异构数据，例如图像字幕 [16, 36]。 对于内部威胁检测，除了将用户活动数据建模为序列之外，其他信息，例如组织中的用户配置文件信息和用户结构信息，也很关键。 与仅使用单一类型的数据相比，结合所有用于内部威胁检测的有用数据有望获得更好的性能。 与传统的机器学习方法相比，深度学习模型更强大地结合异构数据进行检测。 Deep Feedforward Neural Network [48] Anomaly-based Insider Threat Detection using Deep Autoencoders 使用深度自动编码器来检测内部威胁。深度自动编码器由编码器和解码器组成，其中编码器将输入数据编码为隐藏表示，而解码器旨在基于隐藏表示重构输入数据。深度自动编码器的目标是使重构输入接近原始输入。由于组织中的大多数活动都是良​​性的，带有内部威胁的输入应该具有相对较高的重构误差。因此，深度自编码器的重构误差可以作为异常分数来识别内部威胁。利用自动编码器结构的另一个想法是，在根据重构误差学习隐藏表示之后，将一类分类器（例如一类 SVM）应用于学习到的隐藏表示以识别内部威胁 [45]
RecurrentNeuralNetwork. 因此，已经提出了许多基于 RNN 的方法来模拟用户活动 [51, 69, 79, 80] 以进行内部威胁检测。基本思想是训练一个 RNN 模型来预测用户的下一个活动或活动周期。只要预测结果和用户的真实活动没有显着差异，我们就认为用户遵循了正常的行为。否则，用户活动是可疑的
[69] Deep Learning for Unsupervised Insider Threat Detection in Struc- tured Cybersecurity Data Streams 提出了一个堆叠的 LSTM 结构来捕获一天中的用户活动，并采用用户活动的负对数似然作为异常分数来识别恶意会话。 [80]Insider_Threat_Detection_via_Hierarchical_Neural_Temporal_Point_Processes 不是仅使用活动类型（例如，网络访问或文件上传）进行内部威胁检测，而是提出了一种分层神经时间点过程模型来捕获用户会话中的活动类型和时间信息，然后得出异常分数基于预测结果与实际活动在类型和时间方面的差异。
Convolutional Neural Network 最近一项关于内部威胁检测的研究通过分析鼠标生物行为特征提出了一种基于 CNN 的用户身份验证方法 [35]An Insider Threat Detection Approach Based on Mouse Dynamics and Deep Learning。所提出的方法将计算机上的用户鼠标行为表示为图像。如果发生身份盗用攻击，用户的鼠标行为将与合法用户不一致。因此，将 CNN 模型应用于基于鼠标行为生成的图像，以识别潜在的内部威胁。
GraphNeuralNetwork. 最近的工作 [37]Anomaly Detection with Graph Convolutional Networks for Insider Threat and Fraud Detection. 采用 GCN 模型来检测内部人员。由于组织中的用户经常通过电子邮件或在同一设备上的操作相互联系，因此使用图结构来捕获用户之间的相互依赖关系是很自然的。除了以结构信息的邻接矩阵作为输入外，GCN还结合了丰富的用户画像信息作为节点的特征向量。在基于图结构应用卷积层进行信息传播后，GCN 采用交叉熵作为目标函数来预测图中的恶意节点（用户）。受图嵌入方法的启发，[47]Log2vec A Heterogeneous Graph Embedding Based Approach 中的研究提出 log2vec 来检测恶意活动。 Log2vec 首先通过将审计数据中的各种活动表示为节点，将活动之间的丰富关系表示为边来构建异构图，然后训练可以对活动关系进行编码的节点嵌入。最后，通过在节点嵌入上应用聚类算法，log2vec 能够将恶意和良性活动分成不同的集群并识别恶意活动。
Challenges Extremely Unbalanced Data. 与良性活动相比，来自内部人员的恶意活动在现实场景中极为罕见。 因此，内部威胁数据集是一个不平衡的数据集，这是训练深度学习模型的一大挑战。 一般来说，由大量参数组成的深度学习模型需要大量标记数据才能正确训练。 但是，在现实中收集大量的恶意内部人员是不可行的。 如何利用现有的小样本正确训练深度学习模型对于内部威胁检测任务至关重要。 Temporal Information in Attacks. 大多数现有的内部威胁检测方法只关注活动类型信息，例如将文件复制到可移动磁盘或浏览网页。 然而，仅仅根据用户进行的活动类型来检测攻击是不够的，因为相同的活动可能是良性的，也可能是恶意的。 一个简单的例子是，工作时间复制文件看起来很正常，但半夜复制文件却很可疑。 时间信息在分析用户行为以识别那些恶意威胁方面起着重要作用，如何合并这些时间信息具有挑战性 Heterogeneous Data Fusion. 除了时间信息，利用各种数据源并融合此类异构数据对于提高内部威胁检测也至关重要。 例如，在日常工作中处理文件的用户预见到他的潜在裁员并且有目的地将凭证文件复制到可移动磁盘的活动。 在这种情况下，考虑用户资料（即心理测量分数）或用户交互数据可能有助于识别潜在的内部威胁。 Subtle Attacks. 目前，大部分现有工作都将内部威胁检测任务视为异常检测任务，通常将异常样本建模为分布外样本。 现有模型通常在来自良性用户的样本上进行训练，然后应用于识别与观察到的良性样本不同的内部人员。 推导出阈值或异常分数来量化内部人员和良性用户之间的差异。 但是，在现实中，我们不能期望内部人员进行恶意活动的模式发生重大变化。 为了逃避检测，内部威胁是微妙且难以察觉的，这意味着内部人员和良性用户在特征空间中很接近。 传统的异常检测方法无法检测接近良性用户的内部人员。 Adaptive Threats. 内部人员总是改进攻击策略以逃避检测。 然而，基于学习的模型在训练后无法检测新的攻击类型。 当观察到新类型的攻击时，再次从头开始训练模型是低效的。 首先，通常需要一些时间来收集足够的样本来训练模型。 更重要的是，再训练策略不能确保及时发现和预防。 设计一个可以自适应地提高内部威胁检测性能的模型是一项重要且具有挑战性的任务。 Fine-grainedDetection. 现有的基于深度学习的方法通常会检测包含恶意活动的恶意会话。 然而，用户通常在一个会话中进行大量的活动。 这种粗粒度的检测面临着难以及时检测的问题。 因此，如何识别细粒度的恶意子序列或确切的恶意活动对于内部威胁检测很重要。 这也是一项非常具有挑战性的任务。 这是因为我们可以从每个活动中利用的信息非常有限，即我们只观察用户何时以及进行了哪些活动。 没有足够的信息，很难实现细粒度的内部威胁检测 EarlyDetection. 当前的方法侧重于内部威胁检测，这意味着恶意活动已经发生并且已经给组织造成了重大损失。 因此，一个新兴的话题是如何实现内部威胁的早期检测，即在潜在的恶意活动实际发生之前检测它们。 提出了几种通过使用通用 IT 安全机制来防御内部威胁的方法 [3, 66]，但没有基于学习的方法来实现早期检测。 主动识别在不久的将来很有可能进行恶意活动的用户至关重要，以便组织可以提前进行干预以防止或减少损失。 Explainability. 深度学习模型通常被视为黑匣子。 尽管深度学习可以在许多领域取得可观的表现，但模型工作的原因仍然没有得到充分利用。当员工被检测为内部人员时，了解模型做出此类预测的原因至关重要，因为员工通常是最重要的 组织中的宝贵资产。 特别是，深度学习模型无法在内部威胁检测上达到 100% 的准确度。 误报案例（将良性用户错误分类为内部人员）会严重影响员工对组织的忠诚度。 因此，模型的可解释性是向领域专家提供模型洞察力的关键，以便可以高可信度地执行进一步的操作。 Lack of Testbed. Lack of Practical Evaluation Metrics. 采用常用的分类指标，如真阳性率（TPR）、假阳性率（FPR）、准确率和召回率来评估内部威胁检测的性能。基于 TPR 和 FPR，可以通过将 FPR 和 TPR 分别设置为 x 和 y 轴来绘制接收器操作特征 (ROC) 曲线，该曲线表示真阳性和假阳性之间的权衡。理想情况下，我们期望内部威胁检测算法可以实现 TPR 为 1，FPR 为 0。目前，在文献中，ROC 曲线下面积（AUC）分数被广泛用于比较不同检测算法的性能 [45， 47、48、51、80]。另一个指标是精度-召回率 (PR) 曲线，它是召回率和精度作为 x 和 y 轴的图，用于评估不平衡的数据分类。与 ROC-AUC 相比，PR-AUC 在评估内部威胁检测算法方面可能更有用，因为 PR 曲线更关注分类器在少数类上的性能。然而，由于内部人员的数量极少以及相应的恶意活动，目前尚不清楚 ROC-AUC 或 PR-AUC 是否适用于评估内部威胁检测。例如，来自不同检测算法的 ROC-AUC 值通常很接近 [48,51,80]，这意味着很难根据 ROC-AUC 值确定更好的模型。 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释</content></entry><entry><title>DEMIDS a Misuse Detection System for Database Systems</title><url>http://next.lisenhui.cn/post/paperreading/demids-a-misuse-detection-system-for-database-systems/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag><tag>异常行为分析</tag></tags><content type="html"> 目录： 1. 综述翻译 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 1. 综述翻译 尽管有必要保护存储在数据库系统 (DBS) 中的信息，但现有的安全模型不足以防止滥用，尤其是合法用户的内部滥用。此外，现有的误用检测研究尚未充分解决 DBS 中误用检测的概念。 重刑。即使有可用的方法来保护存储在数据库系统中的信息不被滥用。它们很少被保安人员使用 因为组织的安全策略要么不精确，要么根本不知道。 本文介绍了一种称为 DEMIDS 的误用检测系统，该系统专为关系数据库系统量身定制。 DEMIDS 使用审计日志来派生描述 DBS 用户典型行为的配置文件。计算出的配置文件可用于检测滥用行为，尤其是内部人员滥用。此外，配置文件可以通过帮助安全人员定义/优化组织，作为组织安全再造的宝贵工具 安全策略并验证现有的安全策略，如果提出的方法有任何必要，那么用户的访问模式 通常形成一些工作范围，其中包括通常与查询中的某些值一起引用的属性集。 DEMIDS 认为 通过距离度量的概念，在给定的数据库模式中编码的数据结构和语义的领域知识。距离度量用于指导搜索描述用户工作范围的频繁项集。在 DEMIDS 中，使用数据库管理系统的数据管理和查询处理功能从审计日志中有效地计算出此类频繁项集
2. Tag 3. 任务描述 (Carter and Katz 1996) revealed that in computer systems the primary security threat comes from insider abuse rather than from intrusion 然而，现实表明，这种强制执行组织安全策略的机制通常没有得到充分利用。 这有多种原因。 首先，安全策略通常不为人所知或没有很好地指定，因此很难甚至不可能将它们转换为适当的安全机制。 此观察结果适用于一般安全策略以及针对单个数据库用户和应用程序定制的策略。 其次，更重要的是，安全策略没有充分保护存储在数据库系统中的数据免受“特权用户”的侵害。 （Carter 和 Katz 1996）透露，在计算机系统中，主要的安全威胁来自内部人员滥用而不是入侵。 这一观察结果导致必须更加重视系统的内部控制机制，例如审计日志分析。
4. 方法 所提出的方法的本质是，给定数据库模式和关联的应用程序，用户的访问模式将形成一些工作范围，包括某些属性集，这些属性集通常与查询中的某些值一起引用。工作范围的概念在概念上被频繁项集的概念所捕获，频繁项集是具有某些值的特征集。基于数据字典中编码的数据结构和语义（完整性约束）以及审计日志中反映的用户行为，DEMIDS 定义了距离度量的概念，用于度量一组属性相对于工作范围的接近程度。通过利用数据库管理系统的高效数据处理功能的新型数据挖掘方法，距离度量用于指导在审计日志中搜索频繁项集。滥用，例如篡改数据的完整性，然后可以通过将派生的配置文件与指定的安全策略或收集的有关用户的新信息（审计数据）进行比较来检测。
不行，不是想要的。
5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释</content></entry><entry><title>基于CNN LSTM的用户购买行为预测模型</title><url>http://next.lisenhui.cn/post/paperreading/%E5%9F%BA%E4%BA%8Ecnn-lstm%E7%9A%84%E7%94%A8%E6%88%B7%E8%B4%AD%E4%B9%B0%E8%A1%8C%E4%B8%BA%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag><tag>异常行为分析</tag><tag>用户行为预测</tag></tags><content type="html"> 目录： 1. 综述翻译 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 1. 综述翻译 利用电商平台上的购物历史数据对用户购买行为进行预测有助于提升用户体验和营销效果。提出一 种基于ＣＮＮＬＳＴＭ的用户购买行为预测模型。使用“分段下采样”对样本数据进行均衡化处理以获得购买用户 和未购买用户均衡样本；使用ＣＮＮＬＳＴＭ组合网络实现用户属性、商品属性及用户行为特征的自动抽取与选择， 并以此对用户购买行为进行预测。在阿里巴巴移动电商平台数据集的实验结果表明，基于ＣＮＮＬＳＴＭ的预测模 型Ｆ１值比基准模型平均提升了７％ ～１１％，使用“分段下采样”样本均衡算法Ｆ１值提升了２％左右。
2. Tag 用户行为预测; 电商; 卷积神经网络; LSTM; CNN;
3. 任务描述 4. 方法 首先提取特征:
将用户的历史数据制作成特征。这里不清不楚搞不清楚哪些东西作为了特征
5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释</content></entry><entry><title>Detection and Prevention of Malicious Activites on RdBMS Relational Database Management Systems</title><url>http://next.lisenhui.cn/post/paperreading/detection-and-prevention-of-malicious-activites-on-rdbms-relational-database-management-systems/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag><tag>异常行为分析</tag></tags><content type="html"> 目录： 1. 综述翻译 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 1. 综述翻译 内部攻击形成了对数据库管理系统的最大威胁。已经开发了许多机制来检测和防止称为检测数据库系统中的恶意活动 DEMIDS 的内部攻击。 DEMIDS 被认为是数据库安全系统的最后防御机制之一。已经开发了许多机制来检测和防止误用活动，例如删除和更新数据库系统上的数据。这些机制利用审计和分析方法来检测和防止恶意活动。然而，这些机制在检测滥用活动方面仍然存在问题，例如限制检测授权命令上的恶意数据。本研究将通过提出一种机制来解决这些问题，该机制利用项目之间的依赖关系，通过计算数据项之间的关系来检测和防止恶意数据。如果项目之间的关系数不允许任何修改或删除，则该机制将检测活动为恶意活动。检测、假阳性和假阴性率等评估参数用于评估所提出机制的准确性。
2. Tag 数据库; 内部攻击; 攻击防范; insider attacks
3. 任务描述 内部攻击分为合法访问和非法访问。 合法访问可以滥用其特权进行恶意操作。 非法访问试图利用系统的漏洞进行恶意操作。 恶意事务是破坏数据库完整性和可用性的内部攻击之一
内部威胁被认为是威胁机密性、完整性和可用性的最危险的威胁。 滥用活动是内部威胁之一，被认为是最危险的威胁，重点是破坏关键和敏感数据。 已经开发了许多方法来检测和防止恶意和异常活动。 该项目专门针对恶意活动，例如删除或更新已批准的记录。
数据库安全问题之一是在恶意活动中。 其中包括：使用恶意数据更新已批准的记录，以及删除已批准的记录。 本研究假设项目之间的依赖关系可用于检测和预防上述恶意活动。
i- 如何表示依赖关系来检测和防止恶意活动？ ii- 如何使用依赖关系来检测和预防恶意活动？
4. 方法 图 10.2：机制流程流程 根据提出的项间依赖算法，计算项之间的关系以及与这些关系相关的数据项之间的计算关系。例如，如果项目之间的关系总数大于或等于三个关系，则该属性使用较多且重要性较高。之后，检查项目中的数据。如果数据已经写入多个项目，则该项目被其他用户在其他地方使用，禁止更新或删除并归类为恶意。另一方面，如果项目之间的总关系等于 2（低重要性），并且这两个数据项已经被使用。因此，如果只有一项数据项上有更新或删除的命令，而没有其他项，则将确定为恶意命令。但是，如果对这两个数据项同时进行更新或删除，则判定为恶意，但会在数据库中通过并提交。建议的依赖算法如下： 当授权用户向数据库发送命令时，算法检查命令类型，如果插入则直接移动到数据库。但是，如果然后命令更新或删除，算法将首先检查项之间的依赖关系（TR）的总数，然后检查由关系依赖相关的数据项（TD）的总数。因此，如果TR大于或等于三个关系，则检查相关数据项，如果数据已经写入多个项，则该机制将检测该活动为恶意活动并阻止它并通知DBA以及将事件写入事件表。另一方面，如果 TR 等于两个关系，则检查 TD 如果写入多个项目，则检查两个数据项上的活动，如果并行活动则检测为恶意但可以传递到数据库，由于数据可能正确与否，但如果活动仅针对一个数据项，则检测为恶意活动并加以阻止，同时通知 DBA 并将该事件写入事件表中。算法 10.3 将解释所提议的项目之间的依赖算法。
5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释</content></entry><entry><title>DBrain数据库异常分析功能分析</title><url>http://next.lisenhui.cn/post/paperreading/dbrain%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BC%82%E5%B8%B8%E5%88%86%E6%9E%90%E5%8A%9F%E8%83%BD%E5%88%86%E6%9E%90/</url><categories><category>论文阅读笔记</category></categories><tags><tag>异常行为分析</tag></tags><content type="html"> https://cloud.tencent.com/document/product/1130/49731
功能记录 支持7，30，3个月的数据。
异常行为统计
异常时间登录 异常操作 异常ip登录 异常资源访问 告警列表
目前总结的几种检测异常的方法（全局，并不仅限于数据库）
偏离训练集统计分布的任何东西都被认为是异常。 最简单的统计学方法就是控制图。计算出训练集每个特性的平均和标准偏差，然后围绕平均值定义出阈值：k*标准偏差（k为通常在1.5到3.0之间的任意系数，取决于既定的算法保守程度）。在部署中正向或负向超出阈值的点就是异常事件的可疑备选。 聚类 其他方法往往属于聚类方法。因为训练集中缺失异常类，聚类算法听起来很适合异常检测任务。 算法在训练集上创建一些群集。部署中，当前数据点和群集间的距离被计算出来。如果距离高于给定阈值，该数据点即为异常事件的可疑备选。根据距离衡量方法和聚合规则，人们设计出了不同的聚合算法，创建了各种群集。 但是，该方法不适合时间序列数据，因为固定的群集无法捕获时间进程。 受监督的机器学习 受监督的机器学习算法竟然也能应用到异常检测上。而且，因为受监督的机器学习技术既能应用于静态分类，也能应用到时间序列预测问题，该方法能覆盖所有数据情况。不过，由于受监督的机器学习技术需要所有牵涉类型的样本集，我们还需做些调整。 在异常检测问题上，受监督的机器学习模型只能在“正常”数据上训练，比如，在描述系统“正常”运行情况的数据上训练。只有在分类/预测完成后，才能评估出输入数据是不是异常。依赖受监督机器学习技术的异常检测方法主要有两种。 其一是神经自联器(或自编码器)。该自联器经过训练，重生成输入模式到输出层。只要输入模式类似训练集中的样本——也就是 “正常”，该模式重生成就会运行良好。而当新的不一样的东西出现在输入层，系统就会卡壳。这种情况下，该网络将无法重生成足够的输入向量到输出层。如果计算网络的输入和输出差距，异常事件的差值必然高于 “正常” 事件的差值。此处，定义该距离度量的阈值就应当可以找出异常点备选。该方法对静态数据点应用良好，但不适用于时间序列数据。 其二是时间序列预测算法。算法模型经过训练，基于“正常”值训练集上的前n个样本历史，预测下一个样本的值。在部署中，如果过往历史来自于在“正常”情况下工作的系统，下一个样本值的预测将会相对准确，近似于真实样本值。如果过往历史样本来自于不再在“正常”情况下运行的系统，该预测值就会偏离实际值。这种情况下，计量出预测样本值与真实样本值之间的差距，就能圈定异常事件备选。</content></entry><entry><title>一种无监督的数据库用户行为异常检测方法</title><url>http://next.lisenhui.cn/post/paperreading/%E4%B8%80%E7%A7%8D%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E6%96%B9%E6%B3%95/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag><tag>异常行为分析</tag></tags><content type="html"> 目录： 1. 综述翻译 2. Tag 3. 任务描述 4. 方法 概述 定义 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 1. 综述翻译 User Behavior Anomaly Detection for Database Based on Unsupervised Learning
B刊
检测数据库内部合法用户的异常行为，对防范内部攻击和数据泄露具有重要意义，然而面临如下挑战：攻击模式不确定，真实异常样例少，数据集缺少准确标注。人工设定值和规则难以有效应对复杂多样的异常。本文提出了一种基于无监督学习的用户行为异常检测方法，通过划定时间窗口统计提取特征，运用核密度估计算法分别从单维度、多维度建模，实现在海量的无标注历史日志中发现简单异常和复杂异常、在新的缆上数据中检测异常。真实数据实验表明，该方法能够有效检测出简单异常，实验中检测三种简单异常的平均严格查准率和宽松查准率分别达90%和100%;能够从多维度找出存在攻击嫌疑的复杂异常，实验中成功检测出了一种单维度无法检测出的新的复杂异常
2. Tag 数据库; 异常行为检测;
malicious activites
数据库异常检测
数据库用户异常行为检测
数据库 用户行为检测
待看：https://wap.cnki.net/touch/web/Dissertation/Article/10286-1018037812.nh.html
无监督 行为预测
接下来的调研目标，分一部分到无监督算法。
3. 任务描述 对数据库用户行为建模、检测异常
4. 方法 概述 使用特征工程从原始日志中提取特征。 采用核密度估计算法从单维度、多维度训练得到某个用户正常行为的概率密度模型。计算合理的概率密度阈值 根据样本呢的概率密度是否低于阈值判断该样本是否偏离用户绝大多数正常行为的离群点，从而发出警告。 定义 本文将数据库用户行为“异常”定义如下：当用户的操作 行为严重偏离其绝大多数正常历史行为的轨迹时，或者当用 户的操作行为具有拖取数据滥用数据等攻击嫌疑，对数据库 的安全构成威胁时，称为“异常”
简单(单维度)异常:观察单个指标，确定其为异常 复杂异常：需要多个指标组合起来分析，发现其异常之处。
可疑异常率：历史数据中，异常样本占比例。
方法 特征提取 用户角色 用户工作状态 用户发器访问位置 天 访问数据量 天、两小时 访问不同表总个数 天、两小时 发器鉴权请求失败比例 天、两小时 访问绝密级列比例 天、两小时 操作行为的统计特征 将用户执行的query语句按照时间窗口聚合。 方法： 对对每一个维度分别归一化。 核密度估计 分布概率密度函数f的估值 $f_b(x)=\frac{1}{n}\sum_{i=1}^nK_b(x-x_i)=\frac{1}{nb}\sum_{i=1}^{n}K(\frac{x-x_i}{b})$ 其中K为核函数，是积分值为1的非负函数；b为带宽，是一个平滑参数。核密度估计算法可以简单地理解为将每个样本作为中心点对应的核函数的加权求和 超参自动搜索模块：GridSearchCV 概率密度判定阈值$S_0$，可以根据可疑异常率a求出。若假定数据库历史数据中只有1%出现某种简单异常，计算所有历史样本在改维度概率密度值并作升序排列，在1%后的样本即为简单异常。触发相应单维度异常告警。 多维度概率密度模型 当选取的m个特征之间相互独立时，样本的整体概率密度等于各个维度的概率密度值相乘。 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 相关工作：
基于参考阈值的方法 基于关联规则挖掘的方法 基于免疫原理的方法 基于隐Markov的方法 基于聚类的方法 基于模式挖掘的方法 基于query语句向量化特征的方法 基于数据库应用层检测的方法</content></entry><entry><title>Using RNN for Dexompiliation</title><url>http://next.lisenhui.cn/post/paperreading/using-rnn-for-dexompiliation/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag></tags><content type="html"> 目录： 1. 综述翻译 2. Tag 3. 任务描述 4. 方法 预处理 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 1. 综述翻译 摘要——反编译，从二进制恢复源代码， 在许多需要分析或理解源代码不可用的软件的情况下很有用。源代码比二进制代码更易于人类阅读，并且有许多工具可用于分析源代码。现有的反编译技术通常会生成人类难以理解的源代码，因为生成的代码通常不使用程序员使用的编码习惯用法。与人工编写代码的差异也降低了分析工具对反编译源代码的有效性。解决反编译之间的差异问题 代码和人工编写的代码，我们提出了一种使用基于循环神经网络的模型反编译二进制代码片段的新技术。该模型学习源代码中出现的属性和模式，并使用它们生成反编译输出。我们在从 C 源代码编译的二进制机器代码片段上训练和评估我们的技术。我们在本文中概述的一般方法不是特定于语言的，并且几乎不需要或根本不需要语言及其属性或编译器如何运行的领域知识，从而使该方法可以轻松扩展到新的语言和结构。此外，该技术可以扩展并应用于传统反编译器不针对的情况，例如用于对孤立的二进制片段进行反编译；快速、按需反编译；特定领域的学习反编译；优化反编译的可读性；并恢复控制流结构、注释和变量或函数名称。我们表明这种技术产生的翻译通常是准确的或接近的，并且可以提供有用的图片
Disassembler(反汇编)(Machine Code to Assembly): A disassembler is a software tool which transforms machine code into a human readable mnemonic representation called assembly language.
Decompiler(反编译)(Binary Code to Source code): Software used to revert the process of compilation. Decompiler takes a binary program file as input and output the same program expressed in a structured higher-level language.
2. Tag RNN; Decompiler; Binary Code; Translator; Abstract Syntax Tree(AST); AST
3. 任务描述 将二进制转为source code。
这个source code并不是truth source code，而是经过标记化的。就是将大多数字符串、变量、函数名进行替换，将其他的比如for this 等标识符标识为记号，然后进行还原。
4. 方法 backbone神经网络是Sequence-to-Sequence 模型 RNN。
数据集由C（source code）和与其一一匹配的二进制构成。
预处理 接下来的步骤对binary code 和 source code 进行相似的处理：
将二进制和源代码进行序列化。 二进制采用byte-by-byte的方式组成token序列。（第二种方式：bite-by-bite，相比效果没前一种好） 源代码采用词法分析后转为token序列。 源代码替换 将字符串替换为STRING 将top-20 函数名保留，其他函数名替换为function 将top-100 变量名保留，其他变量替换为var_XXX. XXX 是变量名的序号。 标志化 将这些token转为one-hot向量。 分桶 将二进制按照长度分成四个桶。这一步是因为RNN需要定长的输入和输出。所以需要针对长度不统一的序列进行padding或者舍弃。 将匹配好的匹配对，送入RNN进行训练。
5. 解决了什么问题（贡献） 6. 实验结果 实验结果的metrics：
我们比较预测的标记化输出 C 源代码由 RNN 针对标记化的已知基本事实值，采用两者之间的 Levenschtein 距离。
莱文斯坦距离，又称Levenshtein距离，是编辑距离的一种。指两个字串之间，由一个转成另一个所需的最少编辑操作次数。
edit distance大约在0.7左右。
7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释</content></entry><entry><title>Recognizing Functions in Binaries With Neural Networks</title><url>http://next.lisenhui.cn/post/paperreading/recognizing-functions-in-binaries-with-neural-networks/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag></tags><content type="html"> 目录： 1. 综述翻译 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 1. 综述翻译 二进制分析有助于许多重要的应用程序，如恶意软件检测和自动修复易受攻击的软件。在本文中，我们建议应用人工神经网络来解决二元分析中重要而困难的问题。具体来说，我们解决了函数识别问题，这是许多二进制分析技术中至关重要的第一步。尽管神经网络在过去几年经历了复兴，在视觉对象识别、语言建模和语音识别等多个应用领域取得了突破性的成果，但还没有研究人员尝试将这些技术应用于二进制问题。分析。使用来自先前工作的数据集，我们表明循环神经网络可以比最先进的基于机器学习的方法更准确和更高效地识别二进制文件中的函数。我们可以更快地训练模型一个数量级，并以数百倍的速度对二进制文件进行评估。此外，它在八个基准测试中的六个基准上将错误率减半，并且在其余两个基准上的表现相当。
2. Tag Binary Code Analysis, RNN, NLP, Function Recognizing.
3. 任务描述 Function start identification: Given C, find { f1,1, ·· · , fn,1}. In other words, recover the location of the first byte of each function.
Function end identification: Given C, find { f1,l1, ·· · , fn,ln}. In other words, find the bytes where each of the n functions in the binary ends. The length of each function is not given. Function boundary identification: Given C, find {( f1,1, f1,l1) ·· · , ( fn,1, fn,ln )}. In other words, dis-cover the location of the first and last byte within each function This task is more than a simple com- bination of function start and end identification. If the starts and ends of functions have been identi- fied separately, they need to be paired correctly so that each pair contains the start and end of the same function. General function identification: Given C, find {( f1,1, f1,2, ·· · , f1,l1) ·· · , ( fn,1, fn,2, ·· · fn,ln )}; i.e.,determine the number of functions in the file, and all of the bytes which make up each function. 4. 方法 创建标签：
我们将代码 C 本身视为一个字节序列 C[0],C[1],···,C[l]，其中C[i]∈$Z_{256}$是序列中的第i个字节。 我们将二进制中的 n 个函数表示为 f1,····,fn。 我们将属于每个函数 fi 的代码字节的索引（即对应于在运行该函数时可能执行的指令的字节）标记为 fi,1,···,fi,li，其中 li 是fi 中的总字节数。 不失一般性，我们假设 fi,1 &lt; fi,2 &lt; ·· · &lt; fi,k。 每个字节可能属于任意数量的函数，函数可以包含任何字节集，无论是否连续。
为什么使用字节而不是指令：
请注意，我们将代码和函数定义为字节集合而不是指令集合。 在 x86 和 x86-64 ISA 中，根据解码开始的偏移量，字节序列可以有许多合理的指令解码； 因此每个字节可能属于少数可能的指令。 以字节为单位工作使我们能够避免这种歧义。
将二进制按byte字节一个个送入双向LSTM判断，其是否是函数的开头和结尾。
注意：论文分别训练了两个网络判断是否是函数开头或者结尾。
5. 解决了什么问题（贡献） 论文中的一个示例很有意思：
原文：
在图 1 中，我们展示了一个简短的 C 函数示例以及在两个不同优化级别编译后的相应二进制代码。
图 1b 中的代码包含非常清晰的函数开始和结束标记：push %rbp 和 mov %rsp,%rbp 的函数序言保存了调用者的堆栈帧，函数以 retq 结束，它在函数中的其他任何地方都没有出现。相比之下，图 1c 根本不使用堆栈，因此函数开始时对 edi 和 esi 中传递的函数参数进行了一些访问；寻找 push %rbp 会失败。此外，对参数的类似访问再次出现在函数体内，因此很难仅仅依靠它作为函数开始的标记。同样，retq 在代码中出现两次，因此当我们看到这条指令时预测函数结束将失败。
这个例子说明了为什么函数识别会带来很多困难，简单的启发式方法不太可能足够，这与直觉可能建议的相反。
6. 实验结果 按照 Bao 等人的程序进行操作。 我们为四种（架构、操作系统）配置中的每一种都训练了一个单独的模型。 为了报告可比较的结果，我们还使用了 Bao 等人的 10 倍交叉验证； 我们为四种配置中的每一种训练十个模型，其中十个模型中的每一个都使用不同的 10% 的二进制文件作为测试集。
实验效果看论文呢，基本非常好。准确率普遍95%以上
论文提出了两个limitations：
在测试集没有出现的binaries上效果可能会较差 抗混淆性可能不够。如在一段话中插入连续无意义的NOP，不会影响函数功能，但是会影响神经网络的判断。 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 论文中说的future work：
尽管我们已经看到了一些关于 RNN 在各种条件下的性能的实验证据，但我们缺乏对模型内部机制的清晰解释。一种可能的解释方法是通过特征向量结构的分析来进行，方法是在网络随时间演变时线性化网络状态，并分析线性化系统的哪些特征向量携带与任务相关的信息 [12]。这种分析可以了解网络在选择、集成和通信相关信息时如何忽略不相关信息，并允许识别线性化系统的哪些特征向量负责网络执行的这些任务。然而，如果对破坏模型准确性感兴趣的对手可以使用神经网络的参数，他们可能能够使用这种分析来更有效地添加额外的指令，这些指令与携带任务相关信息的特征向量不正交，从而阻止其传输并显着影响 RNN 的性能。
12. 注释</content></entry><entry><title>Leetcode1_数组</title><url>http://next.lisenhui.cn/post/study/leetcode/leetcode1_%E6%95%B0%E7%BB%84/</url><categories><category>学习</category></categories><tags><tag>leetcode学习</tag></tags><content type="html"> 登录 https://zhuanlan.zhihu.com/p/119999079
704 二分法搜查数组 这个可以用递归处理的问题，就可以用循环处理。
多思考极端元素时的处理办法。一个元素，两个元素等。
注意处理边界，每次mid都应该根据值得区间减一或者加一。
注意保持区间不变性。就是区间要么一直是左闭右开。要么左闭右闭。 推荐左闭右开。这样是符合大多数编程的习惯的。
27 移除元素 这题开始没有意识到是什么类型
往后面写，意识到了是双向指针排序的问题。类似于快排这样的处理。
这里其实一开始的想法挺简单的，而且容易实现，没实现成功的原因就是对极限情况没考虑清楚。其实在极限情况下改一下就行了的。
977 有序数组的平方 好好读题，题目给的数组本身就有序，所以有更简单的做法
这里重写了下快速排序。
209 长度最小的子数组 读题。读题一定要仔细。
注意循环退出条件。思考极端情况。 思路清楚再动笔。 伪代码尽量贴近原生语言
首先我们创建一个数组，它的第 i 个索引是给定 nums 数组中所有前一个元素及其自身的总和。 然后使用二分搜索搜索满足条件的下标索引。使用空间换时间。这个想法要有。
59 螺旋矩阵II Initialize the matrix with zeros, then walk the spiral path and write the numbers 1 to n*n. Make a right turn when the cell ahead is already non-zero.
def generateMatrix(self, n): A = [[0] * n for _ in range(n)] i, j, di, dj = 0, 0, 0, 1 for k in xrange(n*n): A[i][j] = k + 1 if A[(i+di)%n][(j+dj)%n]: di, dj = dj, -di i += di j += dj return A 同样的想清楚再动手。别人为什么这么简介。 在思想上多下功夫。动笔时少下功夫。</content></entry><entry><title>Vue学习</title><url>http://next.lisenhui.cn/post/study/wechatdevelopment/vue%E5%AD%A6%E4%B9%A0/</url><categories><category>学习</category></categories><tags><tag>wechat开发</tag></tags><content type="html"> 其他 使用Vscode写html，一个！号后可以选择模板。 引入Vue框架 &lt;script src="https://unpkg.com/vue@next">&lt;/script> 练手： https://gitee.com/panjiachen/vue-admin-template 基础 创建实例, 支持链式。返回同一个实例。 Vue.createAPP({}) .component("") .directive 挂载到组件 const app = Vue.createAPP() const vm = app.mount("#app") // app 是dom 元素id号. 与id绑定。class是样式。 // 与大多数应用方法不同的是，mount 不返回应用本身。相反，它返回的是根组件实例k。现在理解为返回实例。 //应该等app所有都建立完成后，再进行绑定到html的组件上。 钩子 // 存在一些生命周期的钩子是可以绑定在Vue上的 Vue.createApp({ data(){ return {count: 1} }, created(){ console.log('count is: ' + this.count) // "count is 1" 创建时调用 } // 其他的还有 mounted updated unmounted等等 }) data是一个函数是返回这个对象的所有预先声明的变量
data 返回一个字典，字典所以用 :
插值，文本 &lt;span> Message:{{msg}}&lt;/span> // span 中放内容会实时更新 &lt;span v-once> 这个里面放的内容不会更新&lt;/span> &lt;p>Using v-html directive: &lt;span v-html="rawHtml">&lt;/span>&lt;/p> //这样才能放置html内容 //其中javascript脚本中，data[rawHtml]可以赋值为一段html。v-html后面跟的是data id。 Attribute //可以绑定属性 // 绑定属性前，前面一个没有引号，后面的有引号。 &lt;div v-bind:id="dynamicId">&lt;/div> //赋值dynamicId在data内可以更改id这个值，如果绑定的是null 或者undefined 则不会包含该attribute。 &lt;button v-bind:disabled="isButtonDisabled">按钮&lt;/button> //如果 isButtonDisabled 的值是 truthy[1]，那么 disabled attribute 将被包含在内。如果该值是一个空字符串，它也会被包括在内，与 &lt;button disabled=""> 保持一致。对于其他 falsy[2] 的值，该 attribute 将被省略 &lt;!-- html中的{{}} 可以填入data property 或者methods名。如{{name()}}。 同样的可以由v-bind:title="name()"这样子--> 使用javascript表达式 {{ number + 1 }} {{ ok ? 'YES' : 'NO' }} {{ message.split('').reverse().join('') }} &lt;div v-bind:id="'list-' + id">&lt;/div> //以上都是ok的。表明可以进行简单的运算 &lt;!-- 这是语句，不是表达式：--> {{ var a = 1 }} &lt;!-- 流控制也不会生效，请使用三元表达式 --> {{ if (ok) { return message } }} 指令。指令 (Directives) 是带有 v- 前缀的特殊 attribute。指令 attribute 的值预期是单个 JavaScript 表达式 (v-for 和 v-on 是例外情况，稍后我们再讨论)。指令的职责是，当表达式的值改变时，将其产生的连带影响，响应式地作用于 DOM。 &lt;p v-if="seen">现在你看到我了&lt;/p> &lt;!-- 这里，v-if 指令将根据表达式 seen 的值的真假来插入/移除 &lt;p> 元素。 --> &lt;!-- 一些指令能够接收一个“参数”，在指令名称之后以冒号表示。例如，v-bind 指令可以用于响应式地更新 HTML attribute：--> &lt;a v-bind:href="url"> ... &lt;/a> &lt;!-- 在这里 href 是参数，告知 v-bind 指令将该元素的 href attribute 与表达式 url 的值绑定。--> &lt;!-- 动态参数 也可以在指令参数中使用 JavaScript 表达式，方法是用方括号括起来： --> &lt;a v-bind:[attributeName]="url"> ... &lt;/a> &lt;!-- 如果data property中存在attributeName，其值为 "href"，那么这个绑定将等价于 v-bind:href --> &lt;!-- 修饰符 (modifier) 是以半角句号 . 指明的特殊后缀，用于指出一个指令应该以特殊方式绑定。例如，.prevent 修饰符告诉 v-on 指令对于触发的事件调用 event. preventDefault() --> &lt;form v-on:submit.prevent="onSubmit">...&lt;/form> 缩写 &lt;!-- 完整语法 --> &lt;a v-bind:href="url"> ... &lt;/a> &lt;!-- v-bind 缩写 --> &lt;a :href="url"> &lt;!-- 动态参数的缩写 --> &lt;a :[key]="url"> ... &lt;/a> &lt;!-- 完整语法 --> &lt;a v-on:click="doSomething"> ... &lt;/a> &lt;!-- 缩写 --> &lt;a @click="doSomething"> ... &lt;/a> &lt;!-- 动态参数的缩写 (2.6.0+) --> &lt;a @[event]="doSomething"> ... &lt;/a> 注意 &lt;!-- 动态参数表达式有一些语法约束，因为某些字符，如空格和引号，放在 HTML attribute 名里是无效的。例如： --> &lt;!-- 这会触发一个编译警告 --> &lt;a v-bind:['foo' + bar]="value"> ... &lt;/a> &lt;!-- 在 DOM 中使用模板时 (直接在一个 HTML 文件里撰写模板)，还需要避免使用大写字符来命名键名，因为浏览器会把 attribute 名全部强制转为小写： --> &lt;!-- 在 DOM 中使用模板时这段代码会被转换为 `v-bind:[someattr]`。 除非在实例中有一个名为“someattr”的 property，否则代码不会工作。 --> &lt;a v-bind:[someAttr]="value"> ... &lt;/a> &lt;!-- 所有attribute都是小写 --> Data property 组件的 data 选项是一个函数。Vue 在创建新组件实例的过程中调用此函数。它应该返回一个对象，然后 Vue 会通过响应性系统将其包裹起来，并以 $data 的形式存储在组
const app = Vue.createApp({ data() { return { count: 4 } } }) const vm = app.mount('#app') // vm变量表示的就是应用实例， 先这么理解把 console.log(vm.$data.count) // => 4 console.log(vm.count) // => 4 // 这些实例 property 仅在实例首次创建时被添加，所以你需要确保它们都在 data 函数返回的对象中。必要时，要对尚未提供所需值的 property 使用 null、undefined 或其他占位的值。 // 直接将不包含在 data 中的新 property 添加到组件实例是可行的。但由于该 property 不在背后的响应式 $data 对象内，所以 Vue // vm.data是响应式的。 methods 这里是methods。打错一个字都不行!!!
我们用 methods 选项向组件实例添加方法，它应该是一个包含所需方法的对象：
const app = Vue.createApp({ data() { return { count: 4 } }, methods: { increment() { // `this` 指向该组件实例 this.count++ } } }) const vm = app.mount('#app') console.log(vm.count) // => 4 vm.increment() console.log(vm.count) // => 5 &lt;!-- 这些 methods 和组件实例的其它所有 property 一样可以在组件的模板中被访问。在模板中，它们通常被当做事件监听使用： --> &lt;button @click="increment">Up vote&lt;/button> 函数防抖：将几次操作合并为一此操作进行。 &hellip; 函数节流：使得一定时间内只触发一次函数。 这里稍微没看懂。后面再看。
侦听器 watch。可以侦测属性变换。 watch 会监听属性是否发生变化。发生变化后执行某种操作。最好还是使用计算属性。能用计算属性就别用侦听器。 watch顾名思义就是监听变量值的变化。
&lt;div id="watch-example"> &lt;p> Ask a yes/no question: &lt;input v-model="question" /> &lt;/p> &lt;p>{{ answer }}&lt;/p> &lt;/div> &lt;script src="https://cdn.jsdelivr.net/npm/axios@0.12.0/dist/axios.min.js">&lt;/script> &lt;script> const watchExampleVM = Vue.createApp({ data() { return { question: '', answer: 'Questions usually contain a question mark. ;-)' } }, watch: { // whenever question changes, this function will run question(newQuestion, oldQuestion) { if (newQuestion.indexOf('?') > -1) { this.getAnswer() } } }, methods: { getAnswer() { this.answer = 'Thinking...' axios .get('https://yesno.wtf/api') .then(response => { this.answer = response.data.answer }) .catch(error => { this.answer = 'Error! Could not reach the API. ' + error }) } } }).mount('#watch-example') &lt;/script> v-model // 你可以用 v-model 指令在表单 &lt;input>、&lt;textarea> 及 &lt;select> 元素上创建双向数据绑定。如： &lt;input v-model="question" /> // v-model 会忽略所有表单元素的 value、checked、selected attribute 的初始值而总是将当前活动实例的数据作为数据来源。你应该通过 JavaScript 在组件的 data 选项中声明初始值。 // v-model 绑定的输入表单会始终将数据与Vue实例中的数据。 &lt;input v-model="message" placeholder="edit me" /> &lt;p>Message is: {{ message }}&lt;/p> class 和style绑定。样式绑定 对象语法： {string: js_var_name} 意思是js_var_name是truty的时候，string才存在。
数据类型：变量要么是对象，要么是字典形式的对象，即active 或者 {active:isActive}
&lt;div class="static" :class="{ active: isActive, 'text-danger': hasError }" >&lt;/div> data() { return { isActive: true, hasError: false } } &lt;!-- 渲染结果： --> &lt;div class="static active">&lt;/div> 绑定的数据对象不必内联定义在模板里：
&lt;div :class="classObject">&lt;/div> data() { return { classObject: { active: true, 'text-danger': false } } } // 或者用计算属性 data() { return { isActive: true, error: null } }, computed: { classObject() { return { active: this.isActive &amp;&amp; !this.error, 'text-danger': this.error &amp;&amp; this.error.type === 'fatal' } } } 使用数组 &lt;div :class="[activeClass, errorClass]">&lt;/div> data() { return { activeClass: 'active', errorClass: 'text-danger' } } 记住html 的 id位置输送的进去的都是变量名。html中的vue变量用{{}}取值。用"&ldquo;将变量包裹起来。
数组语法和对象语法可以嵌套：
&lt;div :class="[{active:isActive}, errorClass]">&lt;/div> 条件渲染 v-if 指令用于条件性地渲染一块内容。这块内容只会在指令的表达式返回 truthy 值的时候被渲染。
&lt;h1 v-if="awesome">Vue is awesome!&lt;/h1> &lt;h1 v-else>Oh no 😢&lt;/h1> 在 元素上使用 v-if 条件渲染分组 可以作为常见分组包裹在最外层
因为 v-if 是一个指令，所以必须将它添加到一个元素上。但是如果想切换多个元素呢？此时可以把一个 元素当做不可见的包裹元素，并在上面使用 v-if。最终的渲染结果将不包含 元素。
&lt;template v-if="ok"> &lt;h1>Title&lt;/h1> &lt;p>Paragraph 1&lt;/p> &lt;p>Paragraph 2&lt;/p> &lt;/template> 你可以使用 v-else 指令来表示 v-if 的“else 块”：
&lt;div v-if="Math.random() > 0.5"> Now you see me &lt;/div> &lt;div v-else> Now you don't &lt;/div> v-else-if，顾名思义，充当 v-if 的“else-if 块”，并且可以连续使用： &lt;div v-if="type === 'A'"> A &lt;/div> &lt;div v-else-if="type === 'B'"> B &lt;/div> &lt;div v-else-if="type === 'C'"> C &lt;/div> &lt;div v-else> Not A/B/C &lt;/div> &lt;!-- v-else 元素必须紧跟在带 v-if 或者 v-else-if 的元素的后面，否则它将不会被识别。 --> v-show 另一个用于条件性展示元素的选项是 v-show 指令。用法大致一样：
&lt;h1 v-show="ok">Hello!&lt;/h1> &lt;!-- 不同的是带有 v-show 的元素始终会被渲染并保留在 DOM 中。v-show 只是简单地切换元素的 CSS property display。 --> 注意，v-show 不支持 元素，也不支持 v-else。
v-if 是“真正”的条件渲染，因为它会确保在切换过程中，条件块内的事件监听器和子组件适当地被销毁和重建。
v-if 也是惰性的：如果在初始渲染时条件为假，则什么也不做——直到条件第一次变为真时，才会开始渲染条件块。
相比之下，v-show 就简单得多——不管初始条件是什么，元素总是会被渲染，并且只是简单地基于 CSS 进行切换。
一般来说，v-if 有更高的切换开销，而 v-show 有更高的初始渲染开销。因此，如果需要非常频繁地切换，则使用 v-show 较好；如果在运行时条件很少改变，则使用 v-if 较好。
切换少量：if。切换多次 show.
不推荐同时使用 v-if 和 v-for
当 v-if 与 v-for 一起使用时，v-if 具有比 v-for 更高的优先级
v-for v-for 指令需要使用 item in items 形式的特殊语法，其中 items 是源数据数组，而 item 则是被迭代的数组元素的
&lt;ul id="array-rendering"> &lt;li v-for="item in items"> {{ item.message }} &lt;!-- 使用.访问属性 --> &lt;/li> &lt;/ul> Vue.createApp({ data() { return { items: [{ message: 'Foo' }, { message: 'Bar' }] } } }).mount('#array-rendering') 在 v-for 块中，我们可以访问所有父作用域的 property。即和当前list property同一作用域的property
v-for 还支持一个可选的第二个参数，即当前项的索引。
list使用[{},{}]这样的语法。
{{}}只支持访问一个变量。
当访问对象的时候，可以for (value, name, index) in object
当访问list的时候 可以 for (value, index) in list
列表可以添加value, index。访问字典对象时，可以value, name, index
&lt;ul id="array-with-index"> &lt;li v-for="(item, index) in items"> {{ parentMessage }} - {{ index }} - {{ item.message }} &lt;/li> &lt;/ul> Vue.createApp({ data() { return { parentMessage: 'Parent', items: [{ message: 'Foo' }, { message: 'Bar' }] } } }).mount('#array-with-index') 在使用for时使用key。它在更新时，不移动dom元素，而是更新dom的内容。
嵌套循环可以实时获取父作用域的变量。但要求有上下级关系。如嵌套的.或者等。
v-for 也可以接受整数。在这种情况下，它会把模板重复对应次数
&lt;div id="range" class="demo"> &lt;span v-for="n in 10" :key="n">{{ n }} &lt;/span> &lt;/div> 类似于 v-if，你也可以利用带有 v-for 的 来循环渲染一段包含多个元素的内容。比如：
&lt;ul> &lt;template v-for="item in items" :key="item.msg"> &lt;li>{{ item.msg }}&lt;/li> &lt;li class="divider" role="presentation">&lt;/li> &lt;/template> &lt;/ul> 当 Vue 正在更新使用 v-for 渲染的元素列表时，它默认使用“就地更新”的策略。如果数据项的顺序被改变，Vue 将不会移动 DOM 元素来匹配数据项的顺序，而是就地更新每个元素，并且确保它们在每个索引位置正确渲染。
这个默认的模式是高效的，但是只适用于不依赖子组件状态或临时 DOM 状态 (例如：表单输入值) 的列表渲染输出。
为了给 Vue 一个提示，以便它能跟踪每个节点的身份，从而重用和重新排序现有元素，你需要为每项提供一个唯一 key attribute：
&lt;div v-for="item in items" :key="item.id"> &lt;!-- content --> &lt;/div> 数组操作和渲染相关 部分方法替换原list push() pop() shift() unshift() splice() sort() reverse()
部分方法返回新数组 如 filter()、concat() 和 slice()
example1.items = example1.items.filter(item => item.message.match(/Foo/))
可以用计算属性 或者methods来帮助进行排序什么的。
事件处理 然而许多事件处理逻辑会更为复杂，所以直接把 JavaScript 代码写在 v-on 指令中是不可行的。因此 v-on 还可以接收一个需要调用的方法名称。 除了直接绑定到一个方法，也可以在内联 JavaScript 语句中调用方法：
&lt;div id="inline-handler"> &lt;button @click="say('hi')">Say hi&lt;/button> &lt;button @click="say('what')">Say what&lt;/button> &lt;/div> Vue.createApp({ methods: { say(message) { alert(message) } } }).mount('#inline-handler') 有时也需要在内联语句处理器中访问原始的 DOM 事件。可以用特殊变量 $event 把它传入方法： &lt;button @click="warn('Form cannot be submitted yet.', $event)"> Submit &lt;/button> 事件处理程序中可以有多个方法，这些方法由逗号运算符分隔： &lt;!-- 这两个 one() 和 two() 将执行按钮点击事件 --> &lt;button @click="one($event), two($event)"> Submit &lt;/button> 事件修饰符 在事件处理程序中调用 event.preventDefault() 或 event.stopPropagation() 是非常常见的需求。尽管我们可以在方法中轻松实现这点，但更好的方式是：方法只有纯粹的数据逻辑，而不是去处理 DOM 事件细节。
为了解决这个问题，Vue.js 为 v-on 提供了事件修饰符。之前提过，修饰符是由点开头的指令后缀来表示的。
.stop .prevent .capture .self .once .passive
&lt;!-- 阻止单击事件继续传播 --> &lt;a @click.stop="doThis">&lt;/a> &lt;!-- 提交事件不再重载页面 --> &lt;form @submit.prevent="onSubmit">&lt;/form> &lt;!-- 修饰符可以串联 --> &lt;a @click.stop.prevent="doThat">&lt;/a> &lt;!-- 只有修饰符 --> &lt;form @submit.prevent>&lt;/form> &lt;!-- 添加事件监听器时使用事件捕获模式 --> &lt;!-- 即内部元素触发的事件先在此处理，然后才交由内部元素进行处理 --> &lt;div @click.capture="doThis">...&lt;/div> &lt;!-- 只当在 event.target 是当前元素自身时触发处理函数 --> &lt;!-- 即事件不是从内部元素触发的 --> &lt;div @click.self="doThat">...&lt;/div> 使用修饰符时，顺序很重要；相应的代码会以同样的顺序产生。因此，用 v-on:click.prevent.self 会阻止所有的点击，而 v-on:click.self.prevent 只会阻止对元素自身的点击 按键修饰符 在监听键盘事件时，我们经常需要检查详细的按键。Vue 允许为 v-on 或者 @ 在监听键盘事件时添加按键修饰符：
&lt;input @keyup.enter=&ldquo;submit&rdquo; />
可以用如下修饰符来实现仅在按下相应按键时才触发鼠标或键盘事件的监听器。 .ctrl .alt .shift .meta
&lt;input @keyup.alt.enter=&ldquo;clear&rdquo; />
Do something .exact 修饰符允许你控制由精确的系统修饰符组合触发的事件。
.left .right .middle 这些修饰符会限制处理函数仅响应特定的鼠标按钮
表单输入绑定 v-model 会忽略所有表单元素的 value、checked、selected attribute 的初始值而总是将当前活动实例的数据作为数据来源。你应该通过 JavaScript 在组件的 data 选项中声明初始值。
v-model 在内部为不同的输入元素使用不同的 property 并抛出不同的事件：
text 和 textarea 元素使用 value property 和 input 事件； checkbox 和 radio 使用 checked property 和 change 事件； select 字段将 value 作为 prop 并将 change 作为事件。
使用v-model来将输出绑定数据。v-model后跟着就是data名称。
多个复选框，绑定到同一个数组：
&lt;div id="v-model-multiple-checkboxes"> &lt;input type="checkbox" id="jack" value="Jack" v-model="checkedNames" /> &lt;label for="jack">Jack&lt;/label> &lt;input type="checkbox" id="john" value="John" v-model="checkedNames" /> &lt;label for="john">John&lt;/label> &lt;input type="checkbox" id="mike" value="Mike" v-model="checkedNames" /> &lt;label for="mike">Mike&lt;/label> &lt;br /> &lt;span>Checked names: {{ checkedNames }}&lt;/span> &lt;/div> for 属性规定 label 与哪个表单元素绑定。 这个不是vue规定的，是html规定的。注意不是v-for
但是有时我们可能想把值绑定到当前活动实例的一个动态 property 上，这时可以用 v-bind 实现，此外，使用 v-bind 可以将输入值绑定到非字符串
&lt;select v-model="selected"> &lt;!-- 内联对象字面量 --> &lt;option :value="{ number: 123 }">123&lt;/option> &lt;/select> // 当被选中时 typeof vm.selected // => 'object' vm.selected.number // => 123 组件基础 const app = Vue.createApp({}) // 定义一个名为 button-counter 的新全局组件 app.component('button-counter', { data() { return { count: 0 } }, template: ` &lt;button @click="count++"> You clicked me {{ count }} times. &lt;/button>` }) 组件是带有名称的可复用实例，在这个例子中是 &lt;button-counter>。我们可以把这个组件作为一个根实例中的自定义元素来使用： &lt;div id="components-demo"> &lt;button-counter>&lt;/button-counter> &lt;/div> app.mount('#components-demo') component拥有自己的data域。注意这个data域是子data域。同为template的子域是无法访问的。 通过prop 向子组件传递数据 Prop 是你可以在组件上注册的一些自定义 attribute。为了给博文组件传递一个标题，我们可以用 props 选项将其包含在该组件可接受的 prop 列表中：
这里props就是一个预先申明你会传入哪些参数，所以template可以使用哪些参数。
props 传入的值并不是从父类继承的。是在创建组件的时候集成的。即这个时候传入value 用props接收。
整个步骤是什么？ 父级：Vue 初始app 子级： html 组件 和 定义的component。component就是定义一段可复用的html。它应该是独立于实例的。所以它不访问实例内容。但是被实例mount的html是可以访问实例的。所以通过被mount的html访问实例。然后传值给component。
component的html 可以通过来绑定属性到slot上。然后在html上就可以通过v-slot:slotName=&ldquo;variableName&rdquo; 来访问。
总的来说component是一个独立出来的component html， 和创建的实例隔离开来，互不相访问。但是 实例可以和挂载的html互相访问。html可以使用该实例的component来访问其变量。并通过slot互相交互。component的目的就是复用。
template和组件配合使用分发插槽等。
$emit 组件的作用就是来可复用。所以html理论上不能访问组件的变量。应该由组件内部自己显示变量。
数据流向，是从父html单向流向子组件的。子组件不应该修改父html的值。
javascript传值都是传引用。所以值会引发连串修改。
子组件使用props来接收父组件的传值。 子组件使用$emit向父组件来发送值，父组件通过子组件的$emit来监听传值。
子组件 @click=$emit(&lsquo;functionName&rsquo;, args)
父组件 @functionName=&ldquo;balabala&rdquo;
这个意思是子组件点击的时候发送functionName。 父组件接收functionName后执行balabala 父组件可以通过$event来访问这个参数。或者将这个参数原封不动传给balabala。
注意emit出去的需要用 字符串符号括起来。它发射的是字符串。
然后发射的只能是小写字母。html不区分大小写。Vue区分。
子组件传出单个参数时： // 子组件 this.$emit(&lsquo;test&rsquo;,this.param) // 父组件 @test=&lsquo;test($event,userDefined)&rsquo;
子组件传出多个参数时： // 子组件 this.$emit(&lsquo;test&rsquo;,this.param1，this.param2, this.param3) // 父组件 arguments 是以数组的形式传入 @test=&lsquo;test(arguments,userDefined)&rsquo;
// 父组件 @test=&lsquo;test($event,userDefined)&rsquo; //userDefined 为父组件的附加对象/参数 常写成 @test=&lsquo;test&rsquo; 这个时候$event默认传参了。
当执行函数时，没有参数时，$event参数默认在第一个。 当有多个参数时，$event需要显示传参。位置随意。
$emit 传出参数的也可以进行验证。
插槽 在template可以使用插槽来代替html 插槽中的内容。
但是注意，子作用域可以访问父作用域。 父作用域先渲染。 子作用域后渲染。 父级模板里的所有内容都是在父级作用域中编译的；子模板里的所有内容都是在子作用域中编译的。 子作用域的内容不可以互相访问。
中的内容，如果html的template中有内容会替换，没内容会保留这里的内容。
作为vue的基本模板。大多数的内容都可以放在template上。但少数如v-slot不能放在template上。
&lt;template v-slot:&ldquo;定义的name&rdquo;=&ldquo;一个对象用来接收item的值&rdquo;> 这样html可以操控布局应该怎样呈现数据。也可以访问子组件定义的值。
在组件上使用v-model v-model是vue的语法
正常 详细 &lt;input :value=&ldquo;searchText&rdquo; @input=&ldquo;searchText=$event.target.value&rdquo;> 在组件上 &lt;component :model-value=&ldquo;searchText&rdquo; @update:model-value=&ldquo;searchText=$event&rdquo;>
v-###:asdf 使用v开头的都是vue的语法。后面跟着的asdf都不用引号的。
动态组件 组件命了名后，如果有组件需要切换的话 来切换
组件可以包裹信息，不让他消失。
与Dom兼容关系 由于table内允许的只能是tr标签。所以这里通过is来让component显示出来，此时加上vue:前缀。 当你用驼峰大小写的时候，在html 属性上，因为不区分大小写，所以驼峰大小写会被解释为带一个横线
建议直接使用kebab-case 来定义组件名称。就是全部小写带横线。
局部注册 使用对象实例来注册局部组件。
props prop可以预先声明接受的值的类型。还可以进行预先的验证。 类型检查： String Number Boolean Array Object Date Function Symbol
这个类型检查可python不同。python类型检查错了也行，但这里必须检查正确才行。
provide 这里provide :{} 和provide(){return{}}有什么区别？ 感觉是一样的。直觉告诉我，一个是属性，一个是函数。 感觉真的要学javascript啊。=》javascript找不到答案。
这个provide 和 inject 并非是响应式的。
使用vue.computed(()=>)来响应式更改值。
没读懂 异步组件 非Prop的Attribute 模板引用 API 计算属性 &lt;div id="computed-basics"> &lt;p>Has published books:&lt;/p> &lt;span>{{ publishedBooksMessage }}&lt;/span> &lt;/div> &lt;script> Vue.createApp({ data() { return { author: { name: 'John Doe', books: [ 'Vue 2 - Advanced Guide', 'Vue 3 - Basic Guide', 'Vue 4 - The Mystery' ] } } }, computed: { // 计算属性的 getter publishedBooksMessage() { // 这里的函数名就是data 的property了。 // `this` 指向 vm 实例 return this.author.books.length > 0 ? 'Yes' : 'No' } } }).mount('#computed-basics') //通过computed来计算得到data 的 property，函数名就是property，记住它是个函数，处于data层 // 你可能已经注意到我们可以通过在表达式中调用方法来达到同样的效果： methods: { calculateBooksMessage() { return this.author.books.length > 0 ? 'Yes' : 'No' } } // 我们可以将同一函数定义为一个方法而不是一个计算属性。两种方式的最终结果确实是完全相同的。然而，不同的是计算属性是基于它们的响应依赖关系缓存的。计算属性只在相关响应式依赖发生改变时它们才会重新求值。这就意味着只要 author.books 还没有发生改变，多次访问 publishedBookMessage 计算属性会立即返回之前的计算结果，而不必再次执行函数。 //计算属性默认只有 getter，不过在需要时你也可以提供一个 setter： computed: { fullName: { // getter get() { return this.firstName + ' ' + this.lastName }, // setter set(newValue) { const names = newValue.split(' ') this.firstName = names[0] this.lastName = names[names.length - 1] } } } // 现在再运行 vm.fullName = 'John Doe' 时，setter 会被调用，vm.firstName 和 vm.lastName 也会相应地被更新 // get是从需要的值到最终值。 // set是从最终值到初始值的改变。 // fullname是data property &lt;/script> 补充知识 axios &lt;script src="https://cdn.jsdelivr.net/npm/axios@0.12.0/dist/axios.min.js">&lt;/script> // 引入axios // 发送 POST 请求 axios({ method: 'post', url: '/user/12345', data: { firstName: 'Fred', lastName: 'Flintstone' } }); // 请求响应的处理在 then 和 catch 回调中，请求正常会进入 then ，请求异常则会进 catch axios({ method: 'post', url: '/user/12345', data: { firstName: 'Fred', lastName: 'Flintstone' } }).then(res => { consloe.log(res) }).catch(err => { console.log(err) }) // 通过 axios 发出的请求的响应结果中， axios 会加入一些字段，如下 { // `data` 由服务器提供的响应 data: {}, // `status` 来自服务器响应的 HTTP 状态码 status: 200, // `statusText` 来自服务器响应的 HTTP 状态信息 statusText: 'OK', // `headers` 服务器响应的头 headers: {}, // `config` 是为请求提供的配置信息 config: {}, // 'request' // `request` is the request that generated this response // It is the last ClientRequest instance in node.js (in redirects) // and an XMLHttpRequest instance the browser request: {} } 箭头函数 javascript中箭头函数就是匿名函数 x => x * x 等价于 function (x) { return x * x; }
判定符号 a==b 先类型转换后，再左右判断 a===b 直接左右判断是否相等。
注意 html 中里是没有逗号的，用空格分隔。</content></entry><entry><title>前人所思所想</title><url>http://next.lisenhui.cn/post/essay/thought/%E5%89%8D%E4%BA%BA%E6%89%80%E6%80%9D%E6%89%80%E6%83%B3/</url><categories><category>随笔</category></categories><tags/><content type="html"> 自我感动 我们总是容易用一种自虐的方式制造出一种痴情的假象来使得自己站在感情的道德制高点上，获得一种畸形的满足感和安全感。其实无论是雪夜去对方家楼下站会儿或者是冒着大雨给她送一杯奶茶什么的，自己回想起来往往觉得如乔峰大战聚贤庄、关羽千里走单骑一样壮怀激烈，而对于对方来说，一杯奶茶就是一杯奶茶，无法承载起你想要在上面寄托的山崩地裂的情怀。少年的时候，总是迫不及待地将自己的满腔爱意表达出来，而结果往往是陷入表演之中而不自知。所以两个人的记忆才会出现偏差，那些你觉得刻骨铭心的过去，对方往往没有同样的感觉，甚至茫然不知。成长的标志就是懂得克制自己。克制自己的情绪，克制自己的表演欲，甚至克制自己的喜欢。少年时候，喜欢一个人恨不能把她变成自己身体的一部分，她刚说冷，我这边心里已经结冰了，她说难过，我立马如丧考妣，比她还难过，唯恐无法将自己的爱意表达出来。而事实上，谁也无法承担起另一个人的价值寄托，只有做一个独立、有价值的人，才能真正学会去爱另一个人。也千万不要尝试改变另一个人，这注定是徒劳的。做自己就好，爱情的真谛在于相互的吸引、志趣相投的同行，而不是追逐和依附以及自我感动。转自B站评论区
七次鄙视自己的灵魂 我曾七次鄙视自己的灵魂： 第一次，当它本可进取时，却故作谦卑； 第二次，当它空虚时，用爱欲来填充； + 第三次，在困难和容易之间，它选择了容易； 第四次，它犯了错，却借由别人也会犯错来宽慰自己 第五次，它自由软弱，却把它认为是生命的坚韧； 第六次，当它部夷一张丑恶的嘴脸时，却不知那正是自己面具中的一副； 第七次，它侧身于生活的污泥中虽不甘心，却又畏首畏尾
爱自己：对未来的自己充满信心。 无欲则刚，欲则强
我相信每一个自卑的，不爱自己的人，都看了太多所谓的“改变方法论”。无非是“接受自己”“接受过去”，或者“给自己安全感”“一些积极的暗示”等等。我自己也写过这类文章，他们确实会对得到一定益处。但更多情况下，他们也只是在读的时候感觉很好，看完之后并不会有什么大的改变。他们都知道怎样做对自己有益，怎样做能够让自己变得更好，但是等到在现实中真正面临问题时，他们又会选择那个习惯选择的，在感觉上安全的选项。理论指导永远无法超出一个人的生活经历，你用你远远高于对方的“正确”让对方模仿，这并不是属于对方意义上的正确。一个内心过不了“坎”的人，一个心底无比厌恶自己的人，即使暗示在多次“接受自己”，对着镜子说再多“骗自己”的话，都没办法真正改变。
“理论学习”和“自我分析”都对一个人帮助非常大，但是你永远不要想着只靠一个人整天看书和思考就能解决掉实际问题。想要在现实中得到改变，就必须要参与到现实中。所以，如果你是一个自卑的，不爱自己的人，请认真听我说。没有任何一个人，任何一种方法论可以一下子让你变成一个充满自信的，爱自己的人。如果你想要变成一个这样的人，那你就要把“想要”这部分无限扩大。真正能帮助你改变的，其实是“欲望”，你要唤起自己的欲望。大部分自卑和自我厌恶的人，他们直接面临的最大问题就是习惯性的被动和回避。他们不相信自己可以得到自己想要的东西，可以做成想做成的事情，配得上自己喜欢的人。自卑自厌与现实中的逃避退缩形成了一种互相推动的模式，你明明想要，却因为畏惧无法想象的结果而懦弱的退缩，这样的自己当然会被自己讨厌。而被讨厌的自己身上又有太多不够好的点，让自己觉得配不上很多好的事情和人，于是退缩和逃避发生的更加轻易。所以即便看了再多文章告诉他们“正确选项”，他们仍然不会走出那一步。因为自己是一个自卑的，自我厌恶的人。这是他们给自己的标签，是多年来不断重复的“安全选项”。
我见过真正改变的人有三种。
第一种是得到一个强大的外力不断推动，这种外力就像一副拐杖一样，支撑着他们敢于走出去，给他们支撑和鼓励，让他们逐渐学会自己走路。
第二种是面对巨大的打击或者生活上的绝境，这种冲击力和要么“战斗”要么“死亡”的绝境让他们不得不做出选择，依靠一种“病态式”的强大力量一路前进。
第三种则是被长久的时间冲刷，最终对于过去的自己“释然”了，说是改变，更像是一种“算了吧”的心境。
所以，想要依靠自己“意识上”的觉醒，突然间主动的选择改变，其实是非常难的。因为自卑和自厌其实都是你自己选择的，很多都是你的经历带给你的。试想一下，如果没有自卑和自厌这两个词，你要怎样形容自己？它们完全可以是另外两个词。你知道它们是错的，不好的。但其实是你自己需要它们，你需要在你一次次退缩和放弃的时候，给自己合理化。用它们来解释你自己而已。如果能回到过去，我最想告诉自己的不是“你要接受自己。”而是，“如果你想要，你就要去追，要去做，要去刚到底。”如果当初的我能够稍微主动一点，能够开口说几次“我想要”，那么很多事情都会变得不同。
你想要在没有任何外力的情况下改变自卑自厌，那么就在下一次选择到来时，不顾一切的强迫自己选择未知的和更危险的那个选项。只要你逼自己这样选，这样做。你就能改变。如果你想要变得更好，不再那么自卑自厌，那么你为什么不去改变呢？去变得更漂亮更好看，更受人欢迎。去变得自尊自爱性格大方，去跟那些你一直远远望着的人打成一片。
如果你有想要的东西，那你为什么不去争取呢？你要去一次次努力，一次次尝试，直到它到达你的手中。这没什么可耻的，明明想要却压抑自己的欲望最后什么也不做才可耻。
如果你有喜欢的人，那你为什么不去追求呢？如果总要有一个人站在那个人身边，那么这个人为什么不能是你呢？如果你觉的你不配，那你为什么不能变得更优秀呢？没有去经历过体验过，永远无法想象到那些事情真正发生时的感觉。
我曾得到过很多次机会和选择，却从来没有为自己争取过一次。当我在过往人生里看到大片大片的空白，甚至连个落脚点都找不到时，我不得不承认遗憾。你在自己内心世界里的风和日丽或者怒浪滔天，全都无人知晓。我当然知道对于一个自卑自厌的人来说，去面对失败和被拒绝的可能，是一件多么恐怖的事情。我知道对于自卑自厌的人来说，尴尬和出丑是一件多么恐怖的事情。我知道对于自卑自厌的人来说，被喜欢的人拒绝是一件多么恐怖的事情。我知道对于自卑自厌的人来说，一次重大的失败和打击是一件多么恐怖的事情。他们很可能会崩溃或者长期一蹶不振。但请你知道，每一个自信的人，他们的自信都源自这种“对于未知的适应”。重要的不在于他们拥有多少，见过多少。而是在面临新的未知时，他们懂得如何处理和适应，这种能够训练出的能力会让一个人更热衷于挑战和追逐自己的目标。
只要你敢于一步步走出去，你就会发现最伟大的治愈师其实是“客观事实”。一切并没有你想象中的那么可怕、你想象中不可承受的结果并没有那么可怕、你认为自己绝对做不好的事情原来也能做好、原来你配得上那些很好很好的东西……没有什么比现实的结果更能直接影响一个人了。你想要，你选择，你得到。这个过程你每重复一次，就向自信和自爱更靠近一次。你想要改变，就要去做，这就是最真实有效的方法。就像我们可以接受自己的不够好一样，我们也要学会接受自己最真实的欲望，去为自己的想要而争取，这没什么可耻的。不断把自己跟喜欢的东西放在一起，你也会被自己喜欢的。
爱自己。尊重自己。有自己的欲望。</content></entry><entry><title>信息安全技术大赛两个项目汇报</title><url>http://next.lisenhui.cn/post/paperreading/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%8A%80%E6%9C%AF%E5%A4%A7%E8%B5%9B%E4%B8%A4%E4%B8%AA%E9%A1%B9%E7%9B%AE%E6%B1%87%E6%8A%A5/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag></tags><content type="html"> DeepVuler 面向开源社区的漏洞挖掘平台 目标 通过社区对代码漏洞的讨论进行分析，完成以下几个任务：
发现社区讨论的新漏洞情报，使用神经网络对漏洞讨论内容进行分析，并对漏洞进行评级和分类。 发现社区讨论的漏洞，已经被厂家安全更新，但是没有收录为CVE条目的。提醒厂家更新版本。 关注漏洞情报挖掘者和易损仓库，及时发现新的漏洞情报。 方法 使用神经网络对漏洞讨论内容进行检测，用于区分讨论内容是普通内容还是漏洞讨论内容。 数据采集和标注 采集Github讨论内容数据。通过Github的Commits和Security采集漏洞讨论内容。 特征分析和元特征抽取 提取60个元特征分为五类。文本、行为、用户、仓库、会话。 行为：帖子链接数量，帖子正向表情、负面表情 绘画：发帖时间，字数等。 PTFT深层特征提取模块 在每个空间特征内，五大特则会那个组选择处的特征进一步构建具有时空概念的三维特征。（本子原话，但是根本没说怎么组合的） 每一个帖子的特征向量按时间顺序送入注意力LSTM。 轻量级幻影残差网络。 进一步将特征送入卷积网络进行分类。目标是判断这是漏洞事件还是普通事件。 幻影网络来自华为2020CVPR:GhostNet: More Features from Cheap Operations。 主要贡献是通过线性操作来减少运算代价，加速网络。就是一种加速CNN。 使用FGA对差异性代码判断代码更新是普通更新还是安全性更新。 数据获取，从commit获取代码更新签后的代码和针对代码更新的描述 设计统计特征组，分别对代码描述和代码提取相关特征。共计40多个特征。 针对代码描述，先去除停用词，词形还原等预处理方式，送入word2vec获取100维特征。 针对代码，将代码的函数名、变量名进行替换，然后word2vec获取200维特征。 将这些特征先后拼接在一起。 使用神经网络进行学习 送入FCN(MLP)+BiGRU(BiLSTM) 进行二分类，判断是普通更新还是安全性更新。 使用知识图谱对漏洞挖掘者和易损仓库进行识别 老东西了，也不是擅长领域。粗略看了后也没有什么革新。略。 数据收集 根据CVE,NVD 收集数据。 github中监听Security模块。 亮点 设计方面：
立意足够有趣。通过commit判断代码是否为漏洞，并进行漏洞挖掘。是实用且有趣的思路。 跨模型种类的模型组合：RNN结合CNN。这是一种比较新颖的模型缝合方式。 模型方面：
设计了非常多的统计特征。 将统计特征和神经网络训练得到的特征连接起来共同训练。增加可解释性，也增加效果。 将文本特征和代码特征共同作为判断依据。 评价 立意新颖。 本子看似内容多，实则结构非常混乱，读的很糟心。比如序号1），a），I）混用，无法分清层级关系 很多重点内容关键内容模糊带过，无法理解。 “白泽”反诈骗网站智能侦察取证研判系统 目标 设计一个系统，完成以下目标：
根据已有的数据，在互联网中发掘诈骗网站 针对一个网页，判断其是否为诈骗网站 方法 根据警方提供的3000个诈骗网站，通过下面原理找出其他诈骗网站： 网站内容分析。各个诈骗网站会互相创建友情链接，或者调用相同的静态文件（同一个模板制作的诈骗网站）。 网站证书加密。很多诈骗网站使用证书加密。且大多使用多域名证书加密，即一个证书加密多个域名（250个），根据证书注册信息可以挖掘出其他网站 网站域名分析。网站域名简单且相似，大多为字母和数字的排列组合，如 https://www.40939b.com/ 等。根据所有域名生成词典，然后排列组合可以发掘其他域名。 网站注册。 网站集中注册。如果服务商某短时间内收到大量注册信息，其中一个是诈骗网站，其他可能都是 统一身份注册。同一注册人注册的其他网站大多为诈骗网站。 DNS解析。根据动态DNS技术，一个域名可以对应多个IP，通过PDNS（DNS反向解析）技术找出一个诈骗网站域名下的其他IP IP解析。为了节约成本，常常多个域名指向一个IP，通过被动DNS，可以找到指向同一个IP的多个不同域名 IP子网技术。诈骗分子往往大批量部署服务器。一个IP网段的子网段窝藏诈骗网站的概率往往很大。 使用域名生成器挖掘 根据遗传变异算法生成新的潜在网站域名集合。 使用TextCNN分析网站内容。 脚本访问IP获取网页HTML内容。 过滤出关键信息、链接等。 通过语义信息和结构信息分别提取出特征向量后组合，送入分类器判断，输出为诈骗网站的分数。 数据收集 由警方提供 根据已有数据集挖掘。 亮点 立意新颖。核心的两个功能，一个通过遗传算法生成域名合集，二通过TextCNN判断网站内容。两个功能都非常实用。 通过已有IP域名挖掘其他域名，然后使用神经网络进行检查。是传统安全和人工智能的比较好的结合。 评价 涉及的神经网络技术并不复杂，主要是有目标需求，然后针对需求进行分析并完成项目。 通过已有IP域名发掘其他诈骗网站IP，需要对诈骗网站进行详尽的分析、调查。本项目工作调查准备的时间可能会比项目实现的时间更多。 本子过于简洁，且章节之间没有逻辑。详细技术没有阐明。 遗传算法 设定：
染色体：问题的一组解，由若干基因组成（基因即为基础元素，变量） 适应度函数：遗传算法迭代N次，每次迭代生成若干染色体。判断染色体的适应度。淘汰适应度低的染色体，保留适应度高的染色体。 交叉：每次迭代生成的染色体的生成方式： 从上一代中选择两条染色体，选择爸爸的某一个位置切断，选择妈妈的某一个位置切断，拼接成新的染色体。 选择哪些染色体由轮盘赌注决定： 染色体i被选择的概率 = 染色体i的适应度/ 所有染色体的适应度之和。 变异：每次生成了一条新的染色体后，在新染色体上随机选择若干基因然后修改基因的值。 复制：每次保留上一代中适应度最高的几条染色体，原封不动传给下一代。 整体流程：
在算法初始阶段，它会随机生成一组可行解，也就是第一代染色体。 然后采用适应度函数分别计算每一条染色体的适应程度，并根据适应程度计算每一条染色体在下一次进化中被选中的概率 下面正式进入“进化”过程。 通过“交叉”，生成N-M条染色体； 再对交叉后生成的N-M条染色体进行“变异”操作； 然后使用“复制”的方式生成M条染色体； 到此为止，N条染色体生成完毕！紧接着分别计算N条染色体的适应度和下次被选中的概率。这就是一次进化的过程，紧接着进行新一轮的进化。</content></entry><entry><title>EnglishLearning</title><url>http://next.lisenhui.cn/post/study/%E7%94%9F%E6%B4%BB%E6%8A%80%E8%83%BD/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0/</url><categories><category>学习</category></categories><tags><tag>英语学习</tag></tags><content type="html"> 英语学习方法：
跟读 你有什么相见恨晚的英语学习方法？ - 翘囤奶爸的回答 - 知乎 https://www.zhihu.com/question/26677313/answer/777772073
输入+输出
背课文+写作。</content></entry><entry><title>科研工具mark</title><url>http://next.lisenhui.cn/post/study/deeplearning/%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7mark/</url><categories><category>科研记录</category></categories><tags><tag>科研工具</tag></tags><content type="html"> 画图： 如何在论文中画出漂亮的插图？ - 隐生宙的回答 - 知乎 https://www.zhihu.com/question/21664179/answer/52278440 论文写作注意: 论文常用词汇i.e.，e.g.，etc.，viz.，et al.的前世今生 - 薛动谔的喵的文章 - 知乎 https://zhuanlan.zhihu.com/p/63640148 论文检索 登录web of science 右上角选择产品->classic 选择dataset 为 web of science 就是sci检索合集。 journal citation report 可以搜索期刊排名。 Q1取前百分之二十 Q2取前百分之五十 取论文发表那一年的为哪一区。 EI Compendex收录，即为EI检索。大多会议是EI检索。 可以在engineering village中进行搜索。 科研工具 deepl 翻译软件 grammarly quillbot
论文下载 知网前缀kns改成oversea可以下载pdf。</content></entry><entry><title>前人科研经验</title><url>http://next.lisenhui.cn/post/study/deeplearning/%E5%89%8D%E4%BA%BA%E7%A7%91%E7%A0%94%E7%BB%8F%E9%AA%8C/</url><categories><category>科研记录</category></categories><tags><tag>科研记录</tag><tag>经验</tag></tags><content type="html"> 科研十条，学会讲故事，说他人想听的话 作者：刀可它盆盆 链接：https://www.zhihu.com/question/337000233/answer/1362826082 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
我老板有一个保留多年的习惯，就是每天看到好的文章和经验贴都会email发给他的一众学生阅读。他曾经发过一个“投稿十条”，赠人玫瑰手留余香，希望对你们写好SCI有帮助。第一条 令人信服：讲好你的故事一篇好的稿件就是围绕一个简单的要点讲一个故事。你写的一切都必须支持这个要点。我解释一下：就是你文章的亮点，你解决了什么问题，突破了什么最高值，发展了什么新材料，为什么你做的这个东西这么重要，你的数据是怎么证明你的结果的等等，围绕你的亮点有理有据的展开，别试图说你的工作有六七八个亮点，小心贪多你一个都讲不好。
第二条 令人信服：学着写好绝大多数科学家都不是作家。因此，你不仅得花时间自学，还得学习他人。学着写好的一个有效的方法就是找出那些你喜欢阅读的文献，弄明白它写得究竟好在哪里，然后试着用自己的表达方式模仿出来就够了。我解释一下：别跟写考研英语作文似的，东一句西一句最后也能拿个基本分，SCI投稿写不好可能就失去了发到一个期刊的机会。打开相关领域的文章好好读一下，甚至写的过程中都要时不时看看人家的句子表述。不过大佬的话就可能下笔如有神，洋洋洒洒一晚上就写完一篇文章，不需要任何参考。
第三条 令人信服：爱图稿件中的插图是传递你想表达的意思的关键，因此插图必须让那些对你论文数据不怎么熟悉的读者容易理解、读得懂。尽可能利用色彩搭配、图形组合和附图文字让插图越清晰越明了。我解释一下：不得不说现在发文章标准越来越高了，看人家的文章图示一个比一个抢眼。自己会几个绘图软件更好，不会的话就花钱找公司吧。
第四条 不要藏掖: 投稿前要请人把关尽管将初稿给别人看并不太好，但是作为读者的别人往往会看出稿件中那些你往往忽视的地方。把初稿早点给周围的同事看，他们会挑出那些后来审稿人会指出的毛病，你就可以及早改正，这往往会让后来的审稿更容易通过。我解释一下：这点挺重要的，自己一遍又一遍的读，已经脑袋僵硬，视觉疲劳，很难看出来什么显而易见的问题。这时候找同行看看，受益良多，可以提升很多自己忽视的点。
第五条 有备无患：做好原始数据及备份的打算越来越多的期刊要求作者提供稿件中的原始数据和结果的源代码。因此，你得养成数据备份、存储和保管好原始记录的习惯，万一需要你提供或分享数据时，你可以在别人面前嘚瑟。我解释一下：很多文章数据被质疑，期刊要求提供原始数据，结果作者说丢了的例子你们听过不少吧？
第六条 有的放矢: 投给该投的期刊，多从读者角度思考你的论文是写给谁看的？换位思考总能提前帮助你找到最适合的某些期刊，并提前想到可能哪些专家会审你的稿。写论文时时刻刻把读者放心头，就会帮助你遣词、造句、表意、表达等更恰当更合适。我解释一下：很多文章投稿的时候都会让写五个左右关键词，或者投稿过程中让在线选自己工作所属领域。这些都要慎重，会影响编辑选哪个方向的专家审你的文章。
第七条 简洁明了：写摘要要格外用心认真写摘要可能是写稿时的最后一个步骤，但是这往往最重要也值得你最用心对待。类似的摘要材料还有好多，譬如论文重要性陈述、几点干货提纲、图文摘要、可供社交媒体转载的总结等等。我解释一下：正文尤其是article形式的文章，里面的某一句话倒没那么重要，但是摘要一定要逐字逐句的改，短短几句话呈现整个文章的精华，审稿人都是通过摘要和示意图了解你的工作到底重不重要。
第八条 知己知彼：你得知晓编辑过程中的各种可能期刊的编辑过程往往有些神秘兮兮，但是通过与共同作者的讨论、读一些有关的博客、与一些担任学术期刊编辑的学者交流，你总能对典型的编辑流程了解个七七八八。你知晓得越多，编辑过程对你而言就越不神秘，在接下来的投稿及应对编辑的过程中你就越有信心。我解释一下：我理解的这一条的意思是通过期刊显示的你文章的状态比如submitted/with editor/peer review/under consideration这种了解文章审理到了哪一步，不同数据库不一样，自己明白才能应对自如。
第九条 开诚布公：不放过每一点改正、提升自己论文的机会如何面对各种审稿意见往往是个麻烦事，你原以为差不多完事的稿件现在要你重新来过，哪怕只是要求修改其中的一小部分，心里总是不舒服。但是，所有的编辑都认为，审稿及返修的过程对稿件都是有百利而无一害的。你收到的审稿意见值得你重视。即便有些审稿建议你坚持己见不作修改，起码可以通过修改表述更明晰的表达出你想传递给读者的信息。我解释一下：尊重审稿人非常重要，拿出虚心受教的态度来会让编辑更有好感，从而赢得机会。
第十条 与人为善：与人打交道需要好态度尽管事关你自己的论文投稿，最后发表与否很大程度上取决于别人的态度和别人的服务，以及别人是否愿意真心为你投稿质量的提升义务奉献。换个角度就很容易理解了，比方说你可能就是一本期刊的编辑或者审稿人，或者你的同事拿他的论文初稿请你提意见的时候。我解释一下：别人请你提意见，你诚诚恳恳说几条，结果对方不但不改还态度不屑，你说气人不气人，肯定想立马毙掉他。你文章的审稿人也是这样想的。这一条似乎与上一条意思差不多吧。
自我科研心得 1. 数据对神经网络更重要。模型的重要程度一般。 科研良好习惯： 作者：叶小飞 链接：https://www.zhihu.com/question/470047139/answer/2080775493 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
我有一套叶氏四大切换定律支撑着我的科研新鲜感！其实科研就像涮火锅，就算它再好吃，你再爱吃，如果逼着你你天天吃，顿顿吃，到后面一定会“见锅色变”。所以，保持科研新鲜感的诀窍就是学会切换二字，这个诀窍让我无论是在奔驰研究院工作还是在UCLA读博时都始终干劲十足。这切换二字可以扩展为四大方面, 即：多个科研课题之间的切换，输入与输出之间的切换，体力劳动与脑力劳动之间的切换，工作日与周末的切换。多个科研课题之间的切换每个科研人员在做研究的道路上一定会遇到这样的情景：某个课题已经有了很明确的思路，但是总感觉前路漫漫不知何时做完，熬的越久越觉得筋疲力尽；抑或卡在了某个节点上很久，左想右想找不到出路，感觉心力交瘁。今年我在开发一款协同驾驶仿真开源框架（OpenCDA) 时就经常遇到这两种情况。因为我几乎是一个人用python重写了一套完整的自动驾驶系统，写到后面多车协同规划时简直每天都要被bug淹没，可以说看到那块代码就想呕吐。但是我并没有硬着头皮蛮写，而是又在业余时间开了一个关于GAN（对抗神经网络）的小课题。我每周都会拿出一天来研究这个小课题，虽然看似我攻克主项目的时间变少了，但在那个阶段却让我效率得到极大提高。由于这两个项目领域不同，我在研究GAN的最新成果时感觉十分新鲜，就好像看惯了江南山水后忽然看到了大漠一般整个人都被refresh了。而当我再次回到之前的主课题时，疲劳感几乎一扫而光，又重新充满了无尽的斗志。最后这个OpenCDA历时半年后终于成功开源并在协同驾驶领域产生了一定的影响力，而我所研究的小课题也在不久前投了今年的AAAI。所以我个人认为，科研人员有自己深挖主方向固然重要，但你同时也可以花相对较少的时间开拓一个子方向（最好与主方向有一定的overlap), 这样你可以避免长期窝在同一个课题时产生不可逆的疲劳，通过来回切换保持新鲜感。同时，有时候研究一下不太相同的领域也能给你带来新的灵感，最经典的例子之一就是把NLP的transformer用到了vision任务上，开拓了一个新的大热方向。输入与输出的切换其实科研的本质就是一个不断输入（例如阅读论文，上网课、听讲座、看代码）与输出（例如写代码、头脑风暴、参与学术研讨）的过程。一旦这个输入与输出关系失去平衡，人就会陷入科研疲劳。举个实际的例子，当一个研究人员因为新接触一个领域而阅读大量文献时，往往会特别兴奋，但如果让他持续读这些文献一个月不做别的，他再看到这个论文一定会受不了，因为输入本身产生的正反馈较小，而科研人员需要正反馈来保持动力。同样，当一个人一直在对外输出，却不去接受新知识、新观点时，他所产生的idea一定都是翻来覆去那一套，时间一久，自然会失去新鲜感。所以我的做法是，每天输出（例如敲代码、码论文）到有一些疲倦时，会找个舒服的地方葛优躺一下，拿着提前打印好的最新的顶会论文或者录好的讲座（譬如我最近就在空余时间看特斯拉AI DAY）观赏起来，这个时候感觉特别享受。脑力劳动与体力劳动的切换一个人无论再怎么热爱科研，不停地进行脑力劳动而不让自己放松迟早会放空他所有的精力与热情。有的人喜欢通过补觉来休息大脑，但是科学研究表明，最好休息大脑的方式是进行体育锻炼。我一般每天科研进行到下午四点时就会觉得头昏脑胀，注意力涣散，进入了科研“贤者模式”。这个时候我就会离开座椅，去户外或者健身房活动一个小时，等我锻炼完再次回到书桌前，感觉自己又焕然一新。所以强烈建议各位小伙伴每周至少进行三到四次体育锻炼，时间段放在你科研疲劳的临界点即可。工作日与周末的切换这一条非常重要，因为它会提醒你，科研只是生活的一部分，而不是生活的全部。当一个人为了科研失去了全部的生活，他要么会变成一个偏执的工作狂，要么就会渐渐对所有事情失去兴趣，更不要说是新鲜感了。这一条我以前也不太懂，我一直以为巨佬们都是一周七天不停地工作，直到我看到了我的偶像周博磊老师的一篇回答， 发现周老师在非ddl时期工作与周末分的十分清楚。无论平时多忙，他到了周末就是去攀岩和搞乐队，只要没有ddl压力，他会把所有工作留在周一再做。国外名牌大学 PhD 的学习工作强度有多大？3505 赞同 · 132 评论回答后来我就尝试在不赶ddl的情况下，每周至少拿出一整天来完全放松，出去转一转，练练拳击、和哥们打打球，结果发现效果奇佳，每次到了周一都有一种一切都是崭新的感觉。写在最后保持科研新鲜感固然重要，更重要的是保持生活的新鲜感，愿诸君在努力科研的同时莫要忽视了生活。</content></entry><entry><title>厨神培养计划</title><url>http://next.lisenhui.cn/post/study/%E7%94%9F%E6%B4%BB%E6%8A%80%E8%83%BD/%E5%8E%A8%E7%A5%9E%E5%9F%B9%E5%85%BB%E8%AE%A1%E5%88%92/</url><categories><category>学习</category></categories><tags><tag>厨艺学习</tag></tags><content type="html"> 美食试错表 烤披萨：
有肉很好吃。不要将鸡蛋打在中间，有水很难熟。饼底需要提前烤5分钟。玉米很难熟。放肉好了，蔬菜有点难熟。
土豆饼
土豆切丝，调味（记得给盐）。放入锅中煎。底部煎的焦一点更好吃。然后翻面，煎另一面，可以在煎好的面上撒上各种其他东西，洋葱，番茄酱，黑胡椒，芝士碎。
炸鸡翅：
提前改刀、腌制。直接空气炸锅炸。给上调料。
炸五花肉：
切片，瘦多，肥肉少的好吃，然后抹上调料。一勺生抽，两勺料酒，一勺耗油。用生菜卷着吃。就是一次能炸的不多。可以多炸一会儿，脆脆的好吃。
酱炒一切 看视频做菜的时候，要能明白做这一步是要干什么。能想象出来最终味道是怎样的。</content></entry><entry><title>钢琴学习记录</title><url>http://next.lisenhui.cn/post/study/%E7%94%9F%E6%B4%BB%E6%8A%80%E8%83%BD/%E9%92%A2%E7%90%B4%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</url><categories><category>学习</category></categories><tags><tag>音乐学习</tag></tags><content type="html"> 记录自己的钢琴音乐学习之旅。
自我总结经验： 1. 练琴时，脑子要处理很多信息。绝对不是闭着眼睛弹 2. 还有人问练琴的意义。要明白练琴本身带来的快乐已经足够我去完成这件事。 它人经验： 1. https://www.douban.com/group/topic/15405739/ 别人的教学。 2. https://www.youtube.com/channel/UCCnATk0U9H4sz-Bx1pfm7fA/videos 零基础免费学钢琴 3. https://www.zhihu.com/question/52268773/answer/146625916 带着脑子弹琴。 它人经验总结： 自己学习安排。 现在从基础开始练习。练习拜厄。</content></entry><entry><title>“Code Completion by Modeling Flattened Abstract Syntax Trees as Graphs”阅读笔记</title><url>http://next.lisenhui.cn/post/paperreading/code-completion-by-modeling-flattened-abstract-syntax-trees-as-graphs%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url><categories><category>论文阅读笔记</category></categories><tags><tag>论文阅读笔记</tag></tags><content type="html"> 目录： 1. 综述翻译 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 1. 综述翻译 代码完成已成为集成开发环境的重要组成部分。当代代码完成方法依赖于抽象语法树（AST）来生成语法正确的代码。 但是，它们无法完全捕获编写代码的顺序和重复模式以及AST的结构信息。 为了缓解这些问题，我们提出了一种名为CCAG的新代码完成方法，该方法将部分AST的平坦序列建模为AST图。 CCAG使用我们提出的AST Graph Attention Block捕获AST图中的不同依存关系，以学习代码完成中的表示形式。 通过CCAG中的多任务学习优化了代码完成的子任务，并且无需调整任务权重即可使用不确定性自动实现任务平衡。 实验结果表明，CCAG比最先进的方法具有更好的性能，并且能够提供智能的代码完成功能。
2. Tag 论文阅读笔记; 代码完成; 代码补全; 图神经网络; AST树; 抽象语法树;
3. 任务描述 定义 AST：AST是一颗树，其中所有的非叶子节点都能对应于CFG中一个特定的非终结符，其中包含了特定的结构信息，如同IfStatement等。每一个叶子节点对应于CFG中的终结符。AST树可以轻易的转换成源代码。本文赋予了每个节点value和type。非叶子节点type为控制类型，如IfStatement，value为Empty。叶子节点value为变量值，type为变量类型。
定义 部分AST树：对于一个完整的AST树，部分树是完整树的一个子集。对于部分树中的每个节点n，其左序列(left sequence)节点都在该部分树中。节点n的左序列节点是指在树的先序深度优先遍历中比n早出现的所有节点。
本文的代码完成任务定义如下：
 对于一个AST图的每一个部分图T&rsquo;都存在一个最右节点$n_R$，部分图中的所有其他节点都是$n_R$的左序列节点，而在AST的先序深度优先遍历中紧跟于$n_R$后的那个节点则称为下一个节点(the next node)，本文的目标即在给出部分图的情况下，预测the next node的value和type。
如下图所示，对于绿色的部分图，最右节点为NameLoad:b，代码补全的任务就是预测Return:EMPTY节点。 4. 方法 处理数据： 将部分AST树展平（flatten AST），即按照先序深度优先遍历，展开成一个序列。然后再将flatten AST 转换为AST Graph。遵循以下规则：
在flatten AST序列中相邻的节点，在AST Graph中会用node-node边来连接。 flatten AST中重复的节点会在AST Graph 中合并成一个节点。如果有多条边重合，则叠加为一条边，边的权重相应增加。 AST Graph中的边是无向边。保证权重双向传播。 在AST Graph中使用parent-child边来表示AST树中的父子关系，这个边是有向的。 Flatten AST转为AST Graph后丢失了在Flatten中的顺序信息，所以这里采用位置embedding来记录位置信息。对于序列$\{n_1, n_2, n_3, n_2\}$,假设n_2是最右节点，则$n_1, n_2, n_3$的位置embedding为全维度的3,0,1。位置embedding固定且不会被更新。 则节点$n_i$的初始embedding$h_i$为： $h_i = ReLU(W^{(p)}([t_i||v_i]+p_i)+b^{(p)})$ $t_i、v_i$是type embedding和value embedding，$p_i$是位置embedding。
网络结构： 本文设立了一个ASTGab（graph attention block）来捕获图中的注意力关系：
Neighbor Graph Attention Layer (NGAT)： 直接在AST graph上跑注意力网络。$e_{i,j}^{(n)} = a(W^{(n)}h_i,W^{(n)}h_j, w_{i,j})$,其中a为注意力机器。这里使用了多头注意力机制。 更新公式：$h_i^{(n)} = ReLU(\frac{1}{M} \sum_{m=1}^M \sum_{j \in N_i}\alpha_{i,j}^{(n)}W^{(a)}h_j)$， 其中M为多头。$\alpha=softmax(e)$。 Global Self-attention Layer(GSAT): 一个全局的自注意力机制。 Parent-child Attention Layer(PCAT): 在子节点和所有父节点之间添加注意力机制。具体公式可以看论文。这里不多介绍 将上述三个注意力机制串联起来作为一个block。多个block串联起来，并用残差捷径连接。来降低训练难度。效果如图所示： 输出： 由于最右节点（right-most）节点蕴含最多的信息，所以这里对每个节点都对最右节点做了个soft-attention： $$\beta_{i,n_j} = z^{(t1)} \delta ( W_1^{(t1)} h_i^{(r)} + W_2^{(t1)}h_{n_j}^{(r)} + b^{(t1)})$$ $$s_j = \sum_{i \in G_j} \beta_{i, n_j} h_i^{(r)}$$
其中$h_{n_j}$是最右节点的embedding，$h_i$是左序列节点中的节点。通过计算左序列中每个节点和最右节点的自注意力系数，再和该节点embedding相乘相加起来。得到一个图的embedding，用这个图的embedding去做分类。
loss计算： 本文中每个AST Graph的节点既有value也有type，所以两种方法来预测这两个值：
分别使用两个模型预测这两个值。但论文觉得这两个值是有关联的，应该一起训练。 通过某种方式将两个loss结合起来：$L = w_v L_v + w_t L_t$ 但是$w_v, w_t$这两个权重的设置影响会很大，如果设定为超参，则超参调试会很消耗时间。如果设定为可学习的，那学习的代价又太大了。所以这里采用了一个约束条件：$L \approx \frac{1}{\theta^2}L_v + \frac{1}{\tau^2}L_t + \log \theta + \log \tau$。这个公式保证了两个任务权重是可以学习的，同时$\log$的存在保证参数不能为负，当参数太大的时候，$\frac{1}{\theta^2}$又会过小。从而限制loss在一定的范围。
5. 解决了什么问题（贡献） 论文提出了一种使用图网络学习embedding，并完成了code completion任务。 论文中提到的各种模型，各种网络结构有多少用，很难说。但是理论上有用，再加上结果正确就能提高说服力。
这篇论文的贡献我认为还是有的。起码在这之前我一直怀疑到底图网络能不能做代码还原。该文如果实验没有差错的话，是证明了有用的。它这里用的图网络广泛使用了注意力机制。可以去思考下其他几种图网络能不能做代码还原呢？
这里还有一个比较大的贡献点，就是它用图分类去做代码还原，用节点分类我始终认为学习道的知识不够，用图分类，就有更多的信息。而且图分类时将更多的关注集中在最右节点，因为最右节点包含了最多的信息，对于预测下一个节点来说。这里充分借鉴了RNN的思想。
我前段时间仔细思考了下rnn，认为rnn学习到的就是一种概率。对于英文单词I，那下一个出现的此中：am，are，is，毫无疑问am的几率最高。同样高的还有have等单词。再根据上文等相关信息，判断到底是am还是have。
这里我也认为是这样，最能决定下一个单词是什么的就是上一个单词，然后再根据相关上下文信息来帮助神经网络做出抉择。
我认为不能把神经网络想的太过复杂。要帮助神经网络去减负。其他结构有没有用，有多少用得做消融实验，不能简单的下出结论。
当然论文自己是做了类似结构的对比实验的。可以看到有一定优化。这个的意义是一定要保证整体模型能够学习到知识的情况下，再去做删删减减做优化。
6. 实验结果 这里贴一张图。表示了论文的所有实验：
可以看到CCAG效果提升还是很不错的。自己对比的实验中，多头确实效果会更好一点。这也是点小启示，要尽可能从多个维度学习信息。
7. 如何想到该方法 对RNN比较熟悉。要理解RNN是怎么运作的。改进合理。网络的改进不宜大脚步。一步步改，确保能学到知识后，再慢慢改进改造结构。这篇论文很多参考文献都是Li，Liu，不出意外是一个在前人基础上慢慢改进的文章。
8. 我能否想到该方法 可以，要多思考，多看相关论文。
9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 这里只说一个猜测，图结构中大多数控制节点的value都是empty，这会不会对accuracy的准确性有影响？毕竟预测empty可比较容易。
12. 注释 启迪：
合适的理由（动机） + 更优的结果 = 有效的改进。改造一定要有动机。 不要凭空创造，从已知推未知更加有效。 实验时，多个类似结构相互对比，能够有效说明结构的合理性。 不要把神经网络想的太复杂。也别太小看神经网络。 保证整体模型能学习到东西的情况下，再去删删减减做优化，效率更高。一个小结构的优化最多可能百分之2百分之3.所以整体结构正确是很重要的一件事。</content></entry><entry><title>关于我</title><url>http://next.lisenhui.cn/about.html</url><categories/><tags/><content type="html"> Hugo是用Go编写的一个开放源代码静态站点生成器，可在Apache许可证2.0下使用。 Hugo支持TOML, YAML和JSON数据文件类型，Markdown和HTML内容文件，并使用短代码添加丰富的内容。其他值得注意的功能包括分类法、多语言模式、图像处理、自定义输出格式、HTML/CSS/JS缩小和对Sass SCSS工作流的支持。
Hugo使用了多种开源项目，包括:
https://github.com/yuin/goldmark https://github.com/alecthomas/chroma https://github.com/muesli/smartcrop https://github.com/spf13/cobra https://github.com/spf13/viper Hugo是博客、企业网站、创意作品集、在线杂志、单页应用程序甚至是数千页的网站的理想选择。
Hugo适合那些想要手工编写自己的网站代码，而不用担心设置复杂的运行时、依赖关系和数据库的人。
使用Hugo建立的网站非常快速、安全，可以部署在任何地方，包括AWS、GitHub Pages、Heroku、Netlify和任何其他托管提供商。
更多信息请访问GitHub.</content></entry><entry><title>好词好句随记</title><url>http://next.lisenhui.cn/post/essay/thought/%E5%A5%BD%E8%AF%8D%E5%A5%BD%E5%8F%A5%E9%9A%8F%E8%AE%B0/</url><categories><category>随笔</category></categories><tags/><content type="html"> 如何用鲁迅的口吻官宣恋爱 1.先前总觉得思念二字极为俗气，自遇你以后我自觉只是个俗人。 2.我平素里是不善言辞的，故而写下这三两句闲词，也依稀记不清了，大抵是俗气的很，我望着窗外的两棵银杏树，一棵树在想你，另一颗也在想你。 3.深蓝的天空中挂着一轮金黄的圆月，下面是海边的沙地，都种着一望无际的碧绿的西瓜，我的意思是，我想和你一起偷西瓜。 4.我大抵不再是一个人，横竖都是在一起，我的生活里一个是你，另一个还是你 5.今日提笔，想写封信给你，想了想，正准备落笔，发觉这日头已落了山。 6.往昔买的伞觉着小了些，今日肩膀竟被淋湿了些许，原来是撑不下两人了。 7.世上本没有我们，遇见了你，便成了我们 8.我本来没有喜欢的人，见你的次数多了，也便有了。 9.走在街上，看着一轮明月，便想起了你，于是我快步向家走去。 10.大概是晚秋了，这橘子也是有些苦涩，但记起昨夜梦里的你，总归还是甜了一些 11.其实这世上本来没有“你” 想你的次数多了，便清晰了你的轮廓。 12.院内的枣树不知怎的，今年竟也结了几颗果实，不好，略显苦涩，但转念一想昨晚梦中的那位女子，倒也甜了几分。 13.我真傻，真的。我单知道我喜欢他，不知道他竟然也喜欢我 14.“今晚月色真美，适合刺猹” 15.我寄你的信，总要送往邮局，不喜欢放在街边的绿色邮筒中，我总疑心那里会慢一点 16.我本不想和风讨论你，可风说可以替我去见你。 17.这是浪漫的夜晚，上接明月，天上满星遍布，闪闪如你的眼眸。夜晚有心上人，眼眸如星辰，一边是爱恋，一边是心动，我终坠入这眼眸中。 18.大概是晚秋了，这橘子也有了些苦涩，但记起昨夜梦里的你，总归还是甜了一些。 19.月光下有两个影子，一个是我的，另一个也是我的。 20.门前的桃树开花了，不知怎的，想起你也喜欢桃花。 21.我大抵是恋爱了，今日是你，明日还是你。
网络好词好句赏记
接吻 你说舌吻啊，其实接上的刹那并没有什么感觉。只觉得软，温暖。你将嘴唇贴在她的唇上，没刹那的功夫，伸出的舌头探进她的嘴巴。你和她的舌头发生碰撞，舌尖上的细胞发生了纠缠。敌进我退，敌退我往。有时，她的舌头被挤到稍微里面的位置了，仍凭你的舌头来来去去。身体里的某种反应也莫名的出来了，感觉下腹在生产一种奇妙的物质， 口腔的里的唾液分泌越发旺盛，略微有一点点甜，很难分清楚是你的还是他的，大脑空白地只能跟着你的舌头咬动而摇动。渐渐的，你们想要分开，却分不开。刚想要分开，却又弯着头冲刺般聚合在一起，接吻。身体慢慢乏力，她感觉自己的身体好像都快被掏空了，想要救命就只能继续这么做。 中途可能停下来休息片刻，寒冷的冬天没能一下让你们唾液中的激情蒸发。周围是什么样，有没有人肆意来往，你们都看不见，也全然听不见。你们休息了片刻，在吃了两颗桃子味的果糖后又开始了下一轮唾液交换。 武德 武德其实是很多文明共有的东西，欧佬有骑士精神，脚盆有武士道。它们都很理想主义地提出了种种信条。如果大致总结一下的话都可以概括成“武德”——你拥有了强大的武力，但你要约束自身，让它成为帮助别人的工具，而不是成为恃强凌弱的资本。听起来很好，但理想很丰满，现实很骨感，大部分人也都知道。 舞刀弄枪可以学，拳法可以学，什么都可以学，但问题是这种关于武德的体悟是学不来的。它是一个很抽象的东西，也与现实价值观相背：你既然有了如此之致命的武术，为什么不拿它谋利？不谋利才是傻子。很多人都是这样想的，这种社达想法很普遍，这才造成了武德的可贵。 师父其实某种程度上让玩家真的体会到了失落的“武德”——你可以下杀手，把对面噶了，噶个千遍万遍都是可以的，还可以换着法子噶。你有杀生的权力，这是很多游戏给你的。但师父也给了你一个宽恕的权力。为什么美末2风评如此，就是因为它剥夺了前者，强行让玩家去做后者的事。它剥夺了玩家的“选择权”，又怎能让玩家去真心感悟？师父的每个boss（除了扬）你都可以选择是否杀死，选择权在玩家手里。很多人都在纠结其他一些方面，我觉得这个其实才是比较关键的。武德不是无底线的宽恕，而是一种高尚的“选择”。玩家要有权力做出这个选择，然后才能慢慢体悟。 本来想把车卖了，给你换一个舰长的，但收废品的不敢要共享单车，我才知道骑了这么久的车不是我的，喜欢这么久的你也不是我的。、
昨天考试，我把缸子的名字写满了试卷，没想到今天卷子发下来才发现没有批改，老师说爱一个人没有答案，也不分对错
我对合金铜的爱就像是尿裤子，所有人都看的出来，但只有自己才能真正感觉到那股暖意
唱功还可以，但没有感情上的共鸣。这些主要还是阅历浅的缘故，所以在感情表达上稍有欠缺。如果咩咩同学真的想有所发展的话，可以跟我试一试爱情的苦，歌声会具有更丰满的情感
忽有故人心头过，回首山河已至冬。 两处相思同淋雪，此生也算共白头。
:姨！！！！今天跟朋友去吃饭，点了一条鱼，朋友问我为啥只吃鱼头，我说因为鱼身要留着和老姨一起过
α 阿尔法， β 贝塔， γ 伽玛，δ 德尔塔， ε 伊普西隆， ζ 泽塔， η 伊塔， θ 西塔， ι 约塔， κ 卡帕， λ 兰姆达，μ 米欧 ，ν 纽， ξ 克西， ο 欧米克隆， π 派， ρ 柔 ，σ 西格玛， τ 陶 ，υ 玉普西隆， φ 弗爱， χ 凯， ψ 普赛，♡孙蕊
我可以充满怀念地看向过去的自己，并头也不回地走向未来</content></entry></search>