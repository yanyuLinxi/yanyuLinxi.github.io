<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>spark on 阳阳的人间旅游日记</title><link>https://yanyulinxi.github.io/tags/spark/</link><description>Recent content in spark on 阳阳的人间旅游日记</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Tue, 31 May 2022 17:03:14 +0800</lastBuildDate><atom:link href="https://yanyulinxi.github.io/tags/spark/index.xml" rel="self" type="application/rss+xml"/><item><title>Spark代码示例</title><link>https://yanyulinxi.github.io/post/study/spark/spark%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B/</link><pubDate>Tue, 31 May 2022 17:03:14 +0800</pubDate><guid>https://yanyulinxi.github.io/post/study/spark/spark%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B/</guid><description>用来展示一些基础的代码示例，方便查询、回顾。 入口函数 基础 from pyspark import SparkConf, SparkContext conf = SparkConf().setMaster(&amp;#34;local[*]&amp;#34;).setAppName(&amp;#34;pairRDD&amp;#34;) sc = SparkContext(conf=conf) SparkSQL相关 from pyspark.sql import SparkSession spark = SparkSession.builder.appName(&amp;#34;sparkSQL&amp;#34;).master(&amp;#34;local[*]&amp;#34;).config().getOrCreate() sc = spark.sparkContext() 数据输入输出 RDD数据</description></item><item><title>Spark总结</title><link>https://yanyulinxi.github.io/post/study/spark/spark%E6%80%BB%E7%BB%93/</link><pubDate>Tue, 31 May 2022 15:45:19 +0800</pubDate><guid>https://yanyulinxi.github.io/post/study/spark/spark%E6%80%BB%E7%BB%93/</guid><description>1. Spark运行机制 Spark是一个运行大数据的框架。基本的思想就是，数据和算力都分散在各个Executor端。每个Executor端使用算</description></item><item><title>Spark学习计划</title><link>https://yanyulinxi.github.io/post/study/spark/spark%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/</link><pubDate>Tue, 24 May 2022 09:43:28 +0800</pubDate><guid>https://yanyulinxi.github.io/post/study/spark/spark%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/</guid><description>spark学习计划：https://zhuanlan.zhihu.com/p/384903354 Scala学习《Scala实用指南》 然后书籍</description></item></channel></rss>