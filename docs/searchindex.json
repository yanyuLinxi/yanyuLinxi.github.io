{"categories":[{"title":"english","uri":"https://yanyuLinxi.github.io/categories/english/"},{"title":"其他","uri":"https://yanyuLinxi.github.io/categories/%E5%85%B6%E4%BB%96/"},{"title":"厨艺学习","uri":"https://yanyuLinxi.github.io/categories/%E5%8E%A8%E8%89%BA%E5%AD%A6%E4%B9%A0/"},{"title":"科研学习笔记","uri":"https://yanyuLinxi.github.io/categories/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"title":"科研记录","uri":"https://yanyuLinxi.github.io/categories/%E7%A7%91%E7%A0%94%E8%AE%B0%E5%BD%95/"},{"title":"自学","uri":"https://yanyuLinxi.github.io/categories/%E8%87%AA%E5%AD%A6/"},{"title":"论文阅读笔记","uri":"https://yanyuLinxi.github.io/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"title":"音乐","uri":"https://yanyuLinxi.github.io/categories/%E9%9F%B3%E4%B9%90/"}],"posts":[{"content":"图特征提取出来之后怎么使用 从神经网络的角度上，图神经网络怎么使用的：\n图：邻接矩阵、结点（结点属性）。\n  使用图神经网络\n 图结构特征和图神经网络是天然契合的。    使用其他网络\n CNN网络需要二维特征，这个和图结构中的邻接矩阵也是契合的。 RNN网络需要的是序列化的特征，所以这里面一般会使用图嵌入技术，将图结构降维成一维特征。  深度优先遍历序列化。将图结构序列化成一维特征 随机游走。另一种方式的序列化，会根据邻居节点生成多条序列化队列 Graph2Vec 一种无监督的方法，通过graph的结构将Graph映射为一维结构 Node2Vec 随机游走的改进版。      提取了一维的特征后，就可以用一维特征去做其他训练。有监督、无监督都可以。\n  有监督学习算法\n 上面的方法基本都可以使用有监督学习算法    无监督学习算法\n 神经网络中的无监督学习算法并不多。比较典型的应用是重建损失。  AutoEncoder 比较典型  AE需要一维特征，先降维再恢复。是比较典型的无监督算法。对于图的使用和图嵌入无异     并不很看好无监督。无监督在学习数据分布，但并没有学习到针对目标的条件概率。    自监督学习\n 使用结构自身的信息作为标签，从而提取特征。 GNN是能做自监督学习的。比如说图对比学习。通过学习两个图的联系，来提取特征。 其他神经网络的自监督学习，也有很多，也是通过图结构之间的内在联系来学习这种分布特征。原理可以参考word2vec。上面的Graph2vec、Node2Vec也是从此而来 无监督学习在学习一种有指导性的概率分布，比纯粹的概率分布来说，可以提取到更有指向性的特征，效果上看比无监督要好很多。    建议：\n 搜索Graph embedding 图嵌入相关知识  知乎上很多   搜索code vulnerability 相关知识。  code vulnerability是在源码上运行的。只不过目标换成了漏洞检测，搜索这方面的论文可以对方法有个大概的了解。  推荐一篇。《Software Vulnerability Detection Using Deep Neural Networks: A Survey》      ","id":0,"section":"posts","summary":"图特征提取出来之后怎么使用 从神经网络的角度上，图神经网络怎么使用的： 图：邻接矩阵、结点（结点属性）。 使用图神经网络 图结构特征和图神经网络是天","tags":[],"title":"图结构特征的使用","uri":"https://yanyuLinxi.github.io/2021/11/%E5%9B%BE%E7%BB%93%E6%9E%84%E7%89%B9%E5%BE%81%E7%9A%84%E4%BD%BF%E7%94%A8/","year":"2021"},{"content":"解决思路流程：\n工作流程阶段 竞赛解决方案工作流程经历了数据科学解决方案书中描述的七个阶段。\n 问题或问题定义。 获取训练和测试数据。 整理、准备、清理数据。 分析、识别模式并探索数据。 建模、预测和解决问题。 可视化、报告和呈现问题解决步骤和最终解决方案。 提供或提交结果。 工作流指示每个阶段如何跟随另一个阶段的一般顺序。 但是，也有例外的用例。  我们可以结合多个工作流程阶段。 我们可以通过可视化数据进行分析。 执行比指示更早的阶段。 我们可能会在争论前后分析数据。 在我们的工作流程中多次执行一个阶段。 可视化阶段可以多次使用。 完全放弃一个阶段。 我们可能不需要供应阶段来生产或为我们的数据集提供服务以进行竞争。\n问题和问题定义 Kaggle 等竞赛网站定义要解决的问题或要提出的问题，同时提供用于训练数据科学模型的数据集，并针对测试数据集测试模型结果。在 Kaggle 中描述了泰坦尼克号生存竞赛的问题或问题定义。\n从列出在泰坦尼克号灾难中幸存或没能幸存的乘客的训练样本集中知道，我们的模型能否基于给定的不包含生存信息的测试数据集确定测试数据集中的这些乘客是否幸存下来。\n我们可能还想对我们的问题领域有一些早期的了解。这在此处的 Kaggle 比赛描述页面上有描述。以下是要注意的重点。\n 1912 年 4 月 15 日，在她的处女航期间，泰坦尼克号在与冰山相撞后沉没，2224 名乘客和船员中有 1502 人遇难。翻译成 32% 的存活率。 沉船造成如此大的生命损失的原因之一是没有足够的救生艇供乘客和船员使用。 尽管在沉没中幸存下来有一些运气因素，但某些人群比其他人群更有可能幸存下来，例如妇女、儿童和上层阶级  工作流目标 数据科学解决方案工作流解决了七个主要目标。\n  分类。我们可能想要对我们的样本进行分类或分类别。我们可能还想了解不同类别与我们的解决方案目标的含义或相关性。\n  相关性分析。人们可以根据训练数据集中的可用特征来解决问题。数据集中的哪些特征对我们的解决方案目标有重大贡献？从统计上讲，特征和解决方案目标之间是否存在相关性？随着特征值的变化，解决方案的状态也会发生变化，反之亦然？这可以针对给定数据集中的数值和分类特征进行测试。我们可能还想确定后续目标和工作流程阶段的生存以外的特征之间的相关性。关联某些特征可能有助于创建、完成或修正特征。\n  数据转换。对于建模阶段，需要准备数据。根据模型算法的选择，可能需要将所有特征转换为数值等效值。例如，将文本分类值转换为数值。\n  数据补全。数据准备还可能需要我们估计特征中的任何缺失值。当没有缺失值时，模型算法可能效果最好。\n  数据纠正。我们还可以分析给定训练数据集的错误或特征中可能不准确的值，并尝试校正这些值或排除包含错误的样本。一种方法是检测我们的样本或特征中的任何异常值。如果某个特征对分析没有贡献或可能显着扭曲结果，我们也可能会完全丢弃该特征。\n  数据创造。我们是否可以基于现有特征或一组特征创建新特征，以便新特征遵循相关性、转换、完整性目标。\n  数据制图。如何根据数据的性质和解决方案目标选择正确的可视化图和图表。\n   从7个步骤对数据进行分析。\n 通过描述数据进行分析   Which features are available in the dataset?\n  Which features are categorical?\n这些值将样本分类为相似样本的集合。 分类特征中的值是基于名义、有序、比率还是基于区间？ 除其他外，这有助于我们选择合适的图表进行可视化。\n分类： 幸存，性别，和登船。 序数：Pclass。\n  Which features are numerical?\n哪些特征是数字的？ 这些值因样本而异。 在数值特征中，数值是离散的、连续的还是基于时间序列的？ 除其他外，这有助于我们选择合适的图表进行可视化。 连续：年龄，票价。 离散：SibSp、Parch。\n  Which features are mixed data types? 同一特征中的数字、字母数字数据。 这些是待纠正的特征。\nTicket是数字和字母数字数据类型的混合。 Cabin 是字母数字的。\n  Which features may contain errors or typos?\n哪些功能可能包含错误或拼写错误？ 这对于大型数据集来说更难审查，但是从较小的数据集中审查一些样本可能会直接告诉我们哪些特征可能需要更正。\n名称特征可能包含错误或拼写错误，因为有多种方法可以用来描述名称，包括标题、圆括号和用于替代名称或简称名称的引号\n  Which features contain blank, null or empty values?\n  What are the data types for various features?\n在转换目标期间帮助我们。\n七个特征是整数或浮点数。 六个在测试数据集的情况下。 五个特征是字符串（对象）。\n   组合有关联的特征为新特征（孩子、伴侣组成家庭）。对部分特征提取关键信息（名字提取姓氏）。将强相关的分类特征单独拎出来。弱相关的特征进行组合。弱相关的特征组合成强特征。可以删除弱特征，来强化特征表达力。\n  由于决策树的性质。尽量减少特征的类别。可以增强树的判断力。\n  本身强相关的信息不用组合？embarked和\n  从现有特征中挖掘和goal相关联的信息。比如名称中挖掘title和goal相关。\n  什么时候需要one-hot，什么时候可以用类别数字代替？ 总体来说，没有严格的大小偏序关系的，使用类别。还跟具体的使用方法有关。像决策树中，0，1是两种类别。但深度神经网络中，0，1所带来的权重影响不如one-hot所带来的更多。\n  通过关联分析补全缺失值。相关联的值的中位数等等补全值。 补齐缺失值后，使用band区域是一种比较好的方式来降低噪音。同时band也确实比具体值更优秀在某些场景下。 对于连续值band特征会比连续特征效果更好。\n  使用年龄段为什么比年龄更有效？ 23岁和17岁之间相差可能不大，但一个属于成年，一个未成年，很可能结果上的区分更明确一点。年龄这种我们人认为的年龄段比具体的年龄更容易对结果造成影响。 票价范围也是一样的，具体的数字并没有票价分段来的更有效。\n  名称是否有年龄的相关性？\n  每执行一步，测试一次效果。\n ","id":1,"section":"posts","summary":"解决思路流程： 工作流程阶段 竞赛解决方案工作流程经历了数据科学解决方案书中描述的七个阶段。 问题或问题定义。 获取训练和测试数据。 整理、准备、清理","tags":[],"title":"TitanicSolutionGold1","uri":"https://yanyuLinxi.github.io/2021/11/titanicsolutiongold1/","year":"2021"},{"content":"资料 https://github.com/datawhalechina/joyful-pandas\npandas 临时杂碎知识点记录  .null()检测是否有空值 .sum()对pandas数值进行求和 .unique()检测pandas中的独一值。 train_df.info()查看dataframe的信息。 train_df.describe(include=[\u0026lsquo;O\u0026rsquo;])会计算离散变量的统计特征。 df.groupby(\u0026ldquo;字段\u0026rdquo;)  这个操作会返回一个df对象，后续的agg,apply等操作可以基于这个对象进行操作。   df.sort_values(by=\u0026ldquo;字段\u0026rdquo;, ascending=False)降序排列。 字符串处理 df.字符串字段.str.extract(\u0026lsquo;正则化语言\u0026rsquo;) df.replace 替换字段值。 df.map() 通过字典进行map df.fillna(0)将na 填为0. pd.cut 对数据进行分割。  ","id":2,"section":"posts","summary":"资料 https://github.com/datawhalechina/joyful-pandas pandas 临时杂碎知识点记录 .null()检测是否有空值 .sum()对pandas数值进行求和 .unique()检测pandas中的独一值。 t","tags":[],"title":"Pandas学习笔记","uri":"https://yanyuLinxi.github.io/2021/11/pandas%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","year":"2021"},{"content":"网站 最全资料 https://github.com/Mikoto10032/DeepLearning\n书籍 周志华 机器学习 深度学习 deep learning 李航 统计学方法\n视频 吴恩达机器学习\n吴恩达深度学习。\nNLP入门 https://github.com/datawhalechina/team-learning-nlp\n","id":3,"section":"posts","summary":"网站 最全资料 https://github.com/Mikoto10032/DeepLearning 书籍 周志华 机器学习 深度学习 deep learning 李航 统计学方法 视频 吴恩达机器学习 吴恩达深度学习。 NLP入门 https://github.com/datawhalechina/team-learning-nlp","tags":[],"title":"-其他推荐书籍资料","uri":"https://yanyuLinxi.github.io/2021/11/%E5%85%B6%E4%BB%96%E6%8E%A8%E8%8D%90%E4%B9%A6%E7%B1%8D%E8%B5%84%E6%96%99/","year":"2021"},{"content":"引用 https://mp.weixin.qq.com/s/QFpolIXvOQjUsPAngqvqmg\n论文： Correct Normalization Matters: Understanding the Effect of Normalization On Deep Neural Network Models For Click-Through Rate Prediction：https://arxiv.org/pdf/2006.12753.pdf\n介绍 谷歌在2015年就提出了Batch Normalization(BN)，该方法对每个mini-batch都进行normalize，会把mini-batch中的数据正规化到均值为0，标准差为1，同时还引入了两个可以学的参数，分别为scale和shift，让模型学习其适合的分布。\n那么为什么在做过正规化后，又要scale和shift呢？ 因为scale和shift是模型自动学习的，神经网络可以自己琢磨前面的正规化有没有起到优化作用，没有的话就\u0026quot;反\u0026quot;正规化，抵消之前的正规化操作带来的影响。\nBatchNormalization是对一批样本进行处理, 对一批样本的每个特征分别进行归一化\nLayerNormalization是对一个样本进行处理，对一个样本的所有特征进行归一化，乍一看很没有道理，因为如果对身高体重和年龄一起求一个均值方差，都不知道这些值有什么含义，但存在一些场景却非常有效果——NLP领域。 在NLP中，N个特征都可能表示不同的词，这个时候我们仍然采用BatchNormalization的话，对第一个词进行操作，很显然意义就不是非常大了，因为任何一个词都可以放在第一个位置，而且很多时候词序对于我们对于句子的影响没那么大，而此时我们对N个词进行Normalization等操作可以很好地反映句子的分布。(LN一般用在第三维度，[batchsize, seq_len,dims])，因为该维度特征的量纲是相同的，所以并没有太多区别。\n为什么要用Normalization？   解决梯度消失 拿sigmoid激活函数距离，从图中，我们很容易知道，数据值越靠近0梯度越大，越远离0梯度越接近0，我们通过BN改变数据分布到0附近，从而解决梯度消失问题。\n  解决了ICS internal Covariate Shift. 由于训练过程中参数的变化，导致各层数据分布变化较大，神经网络就要学习新的分布，随着层数的加深，学习过程就变的愈加困难，要解决这个问题需要使用较低的学习率，由此又产生收敛速度慢，因此引入BN可以很有效的解决这个问题。\n  数据分布变化很大\n加速了模型收敛速度。  和对原始特征做归一化类似，BN使得每一维数据对结果的影响是相同的，由此就能加速模型的收敛速度。\n和归一化的不同 BatchNormalization层和正规化/归一化不同，BatchNormalization层是在mini-batch中计算均值方差，因此会带来一些较小的噪声，在神经网络中添加随机噪声可以带来正则化的效果。\n解析 我们发现 Normalization有效的最大一个原因在于方差的影响而不是均值。\n特征Embedding上加入Normalization是否有效？ 从上面的实验中,我们发现,在特征Embedding层加入Normalization都是有效的,而且LayerNorm以及相关的变种是效果相对稳定以及最好的;\n","id":4,"section":"posts","summary":"引用 https://mp.weixin.qq.com/s/QFpolIXvOQjUsPAngqvqmg 论文： Correct Normalization Matters: Understanding the Effect of Normalization On Deep Neural Network Models For Click-Through Rate Prediction：https://arxiv.org/pdf/2006.12753.pdf 介","tags":[],"title":"Batchnorm讲解","uri":"https://yanyuLinxi.github.io/2021/11/batchnorm%E8%AE%B2%E8%A7%A3/","year":"2021"},{"content":"目录：   1. 综述翻译  1.1 发表于   2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释  1. 综述翻译 本文介绍了 SimCLR：一个用于视觉表示对比学习的简单框架。我们简化了最近提出的对比自监督学习算法，而不需要专门的架构或存储库。为了了解是什么使对比预测任务能够学习有用的表示，我们系统地研究了我们框架的主要组成部分。我们表明（1）数据增强的组合在定义有效的预测任务中起着关键作用，（2）在表示和对比损失之间引入可学习的非线性转换大大提高了学习表示的质量，以及（3）对比学习与监督学习相比，它受益于更大的批量和更多的训练步骤。通过结合这些发现，我们能够大大优于以前在 ImageNet 上进行自监督和半监督学习的方法。在 SimCLR 学习的自监督表示上训练的线性分类器实现了 76.5% 的 top-1 准确率，相对于之前的最新技术水平提高了 7%，与监督 ResNet-50 的性能相匹配。当仅对 1% 的标签进行微调时，我们实现了 85.8% 的 top-5 准确率，在标签数量减少 100 倍的情况下优于 AlexNet。\n1.1 发表于 ICML 2020.\n分析文章： https://mp.weixin.qq.com/s/tV5eSx73fzMovq0d7Jvu9Q https://zhuanlan.zhihu.com/p/258958247\n2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 ","id":5,"section":"posts","summary":"目录： 1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专","tags":["论文阅读笔记"],"title":"SimCLR a Simple Framework for Contrastive Learning of Visual Representations","uri":"https://yanyuLinxi.github.io/2021/11/simclr-a-simple-framework-for-contrastive-learning-of-visual-representations/","year":"2021"},{"content":"https://cloud.tencent.com/developer/news/393218\nBagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个性能更加强大的分类器，更准确的说这是一种分类算法的组装方法。即将弱分类器组装成强分类器的方法。\nhttps://zhuanlan.zhihu.com/p/37730184\n首先介绍Bootstraping，即自助法：它是一种有放回的抽样方法（可能抽到重复的样本）。\n Bagging (bootstrap aggregating)  Bagging即套袋法，其算法过程如下：\n从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）\n每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）\n对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）\n具体流程： 每次使用一份训练集训练一个模型，k 个训练集共得到 k 个基模型 利用这k个基模型对测试集进行预测，将k个预测结果进行聚合。\n特点   可并行的集成方法。每个基模型可以分别、独立、互不影响地生成。\n  主要降低 Variance，对 Bias 无明显作用。因此，适用于 High Variance \u0026amp; Low Bias 的模型。\n  Boosting 助推法 其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。\n  关于Boosting的两个核心问题：\n2.1 在每一轮如何改变训练数据的权值或概率分布？\n通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。\n2.2 通过什么方式来组合弱分类器？\n通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。\n而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。\nBagging，Boosting二者之间的区别  Bagging和Boosting的区别：\n1）样本选择上：\nBagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。\nBoosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。\n2）样例权重：\nBagging：使用均匀取样，每个样例的权重相等\nBoosting：根据错误率不断调整样例的权值，错误率越大则权重越大。\n3）预测函数：\nBagging：所有预测函数的权重相等。\nBoosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。\n4）并行计算：\nBagging：各个预测函数可以并行生成\nBoosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。\n5）其他： Bagging主要关注降低方差，而Boosting主要关注降低偏差 Boosting是一族算法，其主要目标为将弱学习器“提升”为强学习器，大部分Boosting算法都是根据前一个学习器的训练效果对样本分布进行调整，再根据新的样本分布训练下一个学习器，如此迭代M次，最后将一系列弱学习器组合成一个强学习器。 而这些Boosting算法的不同点则主要体现在每轮样本分布的调整方式上\n总结  这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。\n下面是将决策树与这些算法框架进行结合所得到的新的算法：\nBagging + 决策树 = 随机森林\nAdaBoost + 决策树 = 提升树\nGradient Boosting + 决策树 = GBDT\nAdaBoost https://zhuanlan.zhihu.com/p/37358517\nAdaBoost是一个具有里程碑意义的算法，因为其是第一个具有适应性的算法，即能适应弱学习器各自的训练误差率，这也是其名称的由来（Ada为Adaptive的简写）。\nAdaBoost的具体流程为先对每个样本赋予相同的初始权重，每一轮学习器训练过后都会根据其表现对每个样本的权重进行调整，增加分错样本的权重，这样先前做错的样本在后续就能得到更多关注，按这样的过程重复训练出M个学习器，最后进行加权组合，\n加法模型，第m次迭代中，前m-1个基学习器是固定的。 AdaBoost最后得到的强学习器是由一系列的弱学习器的线性组合，此即加法模型。这个是说训练第二个基学习器的时候，用的是第一个基学习器的结果。\n这里有两个关键问题：\n每轮训练过后如何调整样本权重 w ？ 如何确定最后各学习器的权重 $\\alpha$ ？\n=\u0026gt;\n前一轮正确分类的样本减少权重。 线性组合后，准确率越高的学习器赋予较大的系数。\nAdaBoost采用指数损失的原因 这说明指数损失函数是分类任务原本0-1损失函数的一致性替代函数。由于这个替代函数是单调连续可微函数，因此用它代替0-1损失函数作为优化目标。\n其他 Weight Trimming weight trimming不是正则化的方法，其主要目的是提高训练速度。在AdaBoost的每一轮基学习器训练过程中，只有小部分样本的权重较大，因而能产生较大的影响，而其他大部分权重小的样本则对训练影响甚微。\n只用高权重样本进行训练。具体是设定一个阈值 (比如90%或99%)，再将所有样本按权重排序，计算权重的累积和，累积和大于阈值的权重 (样本) 被舍弃，不会用于训练。\n注意每一轮训练完成后所有样本的权重依然会被重新计算，这意味着之前被舍弃的样本在之后的迭代中如果权重增加，可能会重新用于训练。\nGradient Boosting https://zhuanlan.zhihu.com/p/38329631\n上一篇介绍了AdaBoost算法，在AdaBoost中每一轮基学习器训练过后都会更新样本权重，再训练下一个学习器，最后将所有的基学习器加权组合。AdaBoost使用的是指数损失，这个损失函数的缺点是对于异常点非常敏感， 因而通常在噪音比较多的数据集上表现不佳。Gradient Boosting在这方面进行了改进，使得可以使用任何损失函数(只要损失函数是连续可导的)，这样一些比较robust的损失函数就能得以应用，使模型抗噪音能力更强。\nBoosting的基本思想是通过某种方式使得每一轮基学习器在训练过程中更加关注上一轮学习错误的样本，区别在于是采用何种方式.\nAdaBoost采用的是增加上一轮学习错误样本的权重的策略，而在Gradient Boosting中则将负梯度作为上一轮基学习器犯错的衡量指标，在下一轮学习中通过拟合负梯度来纠正上一轮犯的错误。\n为什么通过拟合负梯度就能纠正上一轮的错误了？Gradient Boosting的发明者给出的答案是：函数空间的梯度下降。\nGradient Boosting 采用和AdaBoost同样的加法模型，在第m次迭代中，前m-1个基学习器都是固定的。\n负梯度也被称为“响应 (response)”或“伪残差 (pseudo residual)”，从名字可以看出是一个与残差接近的概念。直觉上来看，残差 r=y-f(x) 越大，表明前一轮学习器 f(x)的结果与真实值 y 相差较大，那么下一轮学习器通过拟合残差或负梯度，就能纠正之前的学习器犯错较大的地方。\n就是说让h_m(x)去拟合f(x)和h_{m-1}(x)的差值。\n在Gradient Boosting框架中，最常用的基学习器是决策树 (一般是CART)，二者结合就成了著名的梯度提升树 (Gradient Boosting Decision Tree, GBDT)。注意GBDT不论是用于回归还是分类，其基学习器 (即单颗决策树) 都是回归树，即使是分类问题也是将最后的预测值映射为概率。\nXGBOOST https://zhuanlan.zhihu.com/p/46683728\neXtreme Gradient Boosting\n体来看，XGBoost 在原理方面的改进主要就是在损失函数上作文章。 一是在原损失函数的基础上添加了正则化项产生了新的目标函数，这类似于对每棵树进行了剪枝并限制了叶结点上的分数来防止过拟合。 二是对目标函数进行二阶泰勒展开，以类似牛顿法的方式来进行优化（事实上早在 Friedman, J., Hastie, T. and Tibshirani, R., 1999 中就已有类似方案，即利用二阶导信息来最小化目标函数，陈天奇在论文中也提到了这一点）。 XGBoost 之所以快的一大原因是在工程上实现了Column Block 方法，使得并行训练成为了可能。\n由于已经预先保存为block 结构，所以在对叶结点进行分裂时，每个特征的增益计算就可以开多线程进行，训练速度也由此提升了很多。而且这种 block 结构也支持列抽样，只要每次从所有 block 特征中选择一个子集作为候选分裂特征就可以了，据我的使用经验，列抽样大部分时候都比行抽样的效果好。\n最后总结一下 XGBoost 与传统 GBDT 的不同之处：\n传统 GBDT 在优化时只用到一阶导数信息，XGBoost 则对目标函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。另外 XGBoost 工具支持自定义损失函数，只要函数可一阶和二阶求导。 XGBoost 在损失函数中加入了正则化项，用于控制模型的复杂度，防止过拟合，从而提高模型的泛化能力。 传统 GBDT 采用的是均方误差作为内部分裂的增益计算指标（因为用的都是回归树），而 XGBoost 使用的是经过优化推导后的式子，即式 [公式] 。 XGBoost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算量，这也是 XGBoost 异于传统 GBDT 的一个特性。 XGBoost 添加了对稀疏数据的支持，**在计算分裂增益时不会考虑带有缺失值的样本，这样就减少了时间开销。**在分裂点确定了之后，将带有缺失值的样本分别放在左子树和右子树，比较两者分裂增益，选择增益较大的那一边作为默认分裂方向。 并行化处理：由于 Boosting 本身的特性，无法像随机森林那样树与树之间的并行化。**XGBoost 的并行主要体现在特征粒度上，在对结点进行分裂时，由于已预先对特征排序并保存为block 结构，每个特征的增益计算就可以开多线程进行，**极大提升了训练速度。 传统 GBDT 在损失不再减少时会停止分裂，这是一种预剪枝的贪心策略，容易欠拟合。XGBoost采用的是后剪枝的策略，先分裂到指定的最大深度 (max_depth) 再进行剪枝。而且和一般的后剪枝不同， XGBoost 的后剪枝是不需要验证集的。 不过我并不觉得这是“纯粹”的后剪枝，因为一般还是要预先限制最大深度的呵呵。 ​\n说了这么多 XGBoost 的优点，其当然也有不完美之处，因为要在训练之前先对每个特征进行预排序并将结果存储起来，对于空间消耗较大。另外虽然相比传统的 GBDT 速度是快了很多，但和后来的 LightGBM 比起来还是慢了不少，不知以后还会不会出现更加快的 Boosting 实现。\n","id":6,"section":"posts","summary":"https://cloud.tencent.com/developer/news/393218 Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个性能更加强大的分类器，更准确的说这是一种分类算法的","tags":[],"title":"Bagging和Boosting,adboost,xgboost","uri":"https://yanyuLinxi.github.io/2021/11/bagging%E5%92%8Cboostingadaboostxgboost/","year":"2021"},{"content":"https://easyai.tech/ai-definition/decision-tree/ https://zhuanlan.zhihu.com/p/85731206 决策树更详细点。 https://blog.csdn.net/zjsghww/article/details/51638126 c4.5\n什么是决策树？ 决策树算法采用树形结构，使用层层推理来实现最终的分类。决策树由下面几种元素构成：\n根节点：包含样本的全集 内部节点：对应特征属性测试 叶节点：代表决策的结果\n预测时，在树的内部节点处用某一属性值进行判断，根据判断结果决定进入哪个分支节点，直到到达叶节点处，得到分类结果。\n决策树学习的 3 个步骤 特征选择。 特征选择决定了使用哪些特征来做判断。在训练数据集中，每个样本的属性可能有很多个，不同属性的作用有大有小。因而特征选择的作用就是筛选出跟分类结果相关性较高的特征，也就是分类能力较强的特征。在特征选择中通常使用的准则是：信息增益。\n决策树生成 选择好特征后，就从根节点触发，对节点计算所有特征的信息增益，选择信息增益最大的特征作为节点特征，根据该特征的不同取值建立子节点；对每个子节点使用相同的方式生成新的子节点，直到信息增益很小或者没有特征可以选择为止。\n决策树剪枝 剪枝的主要目的是对抗「过拟合」，通过主动去掉部分分支来降低过拟合的风险。\n3 种典型的决策树算法 ID3 算法 ID3 是最早提出的决策树算法，他就是利用信息增益来选择特征的。 ID3 计算信息增益来对特征进行划分。信息增益基于信息熵。如果选取了这个特征，针对分类的信息熵减小了，则认为这个信息增益增大了。\nID3 步骤：\n 初始化特征集合、数据集合。 计算信息熵和所有特征的条件熵（在选取这个特征进行分类后，每一个子树的信息熵）。信息增益=信息熵-条件熵。选取特征。 根据信息增益对子节点进行划分 重复2-3直至子集包含纯的标签。则为分支叶子节点。  ID3弊端：\n 只能处理离散值。 更倾向于选择特征种类多的特征。特征种类多的特征，信息增益会更大。特征种类多，分类就多，每个分类数据就越少，分类标签的纯度就容易越高。信息熵显然会低，信息增益会更大。 无法处理缺失值 没有剪枝，容易过拟合。  c4.5 我自己总结一下。c4.5是为了解决ID3的一些弊端：\n 对于连续值，进行离散化后排序，取相邻值进行平均并进行分节点。将连续特征离散化，假设 n 个样本的连续特征 A 有 m 个取值，C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点； 选择使用信息增益率来代替信息增益。信息增益率=信息增益/信息内在信息。信息内在信息是指信息本身所拥有的信息。信息熵越大，信息越不确定。所以信息增益率越低。越不选它。  当叶子节点是纯的时候，就停止划分叶子节点。   对于缺失值。  在缺失特征下选择样本？使用没有缺失的样本子集 对于缺失特征的样本，按不同概率划分到不同的子节点。   剪枝采用悲观后剪枝。  后剪枝：  训练完成后，进行剪枝，如果这个减去这个子树，效果保持或者不下降，则可以替换这个子树。后剪枝欠拟合风险小很多，泛化性能会优于预剪枝。但是时间开销大。 悲观后剪枝更麻烦点，优势就是只使用训练集。不使用测试集。   预剪枝：三种  节点数据比例低于某一阈值 节点特征都已经分裂。 节点划分前准确率比划分后准确率高。   预剪枝可以减少过拟合，减少训练时间。但容易带来欠拟合。    C4.5 算法最大的特点是克服了 ID3 对特征数目的偏重这一缺点\nC4.5 缺点：\n c4.5包括ID3用的都是多叉树。 c4.5只能用于分类。 对于连续值，需要排序。对内存需求很大。数据不足以放进内存的时候，无法进行运行。 熵模型有大量的耗时的对数运算。  CART CART针对前面的缺点的提升：\n 采用二叉树运算快  正是因为CART树是二叉树，所以对于样本的有N\u0026gt;=3个取值的离散特征的处理时也只能有两个分支，这就要通过组合人为的创建二取值序列并取GiniGain最小者作为树分叉决策点 所以cart是不停的二分离散特征。若没有停止准则，则树会一直增长。 叶子节点的标签也可能有多个标签分类。   CART可以用于分类或者回归 CART使用Gini作为变量的不纯度量。减少对数运算。CART采用Gini系数来代替熵模型。基尼指数约等于熵模型的一阶泰勒展开。公式Gini = 1 - \\sum p^2。 p为选择这个节点后的，标签选择概率的分类。  Gini越小，不纯度越低。   CART 采用代理测试来估计缺失值，而 C4.5 以不同概率划分到不同节点中；  如何在特征值缺失的情况下进行划分特征的选择？  一开始只使用没有缺失的数据。后来如果缺失是20％，则惩罚20%的权重   选定该划分特征，模型对于缺失该特征值的样本该进行怎样处理？  CART 算法的机制是为树的每个节点都找到代理分裂器。无论在训练数据上得到的树是否有缺失值都会这样做。在代理分裂器中，特征的分值必须超过默认规则的性能才有资格作为代理。   代理分裂：  一个分裂使用另一个预测属性。满足与主分裂最相似，切有着正的关联一簇额度量。来对缺失值分到哪一类进行判别。     CART 采用“基于代价复杂度剪枝”方法进行剪枝，而 C4.5 采用悲观剪枝方法。  采用一种“基于代价复杂度的剪枝”方法进行后剪枝，这种方法会生成一系列树，每个树都是通过将前面的树的某个或某些子树替换成一个叶节点而得到的，这种方法需要使用一个单独的测试数据集来评估所有的树，根据它们在测试数据集熵的分类性能选出最佳的树。 在所有的子树中，找出那些使用了较多节点，却使错误率下降最低的子树，把这些子树剪裁掉。   对于连续值，与C4.5思想相同，都是将连续的特征离散化。区别在选择划分点时，C4.5是信息增益率，CART是基尼系数。。注意的是，与ID3、C4.5处理离散属性不同的是，如果当前节点为连续属性，则该属性在后面还可以参与子节点的产生选择过程。  运行过程：\n 分裂：分裂过程是一个二叉递归划分过程，其输入和预测特征既可以是连续型的也可以是离散型的，CART 没有停止准则，会一直生长下去； 剪枝：采用代价复杂度剪枝，从最大树开始，每次选择训练数据熵对整体性能贡献最小的那个分裂节点作为下一个剪枝对象，直到只剩下根节点。CART 会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树； 树选择：用单独的测试集评估每棵剪枝树的预测性能（也可以用交叉验证）。  优点: 1.\nCART（Classification and Regression Tree）\n这种算法即可以用于分类，也可以用于回归问题。CART 算法使用了基尼系数取代了信息熵模型。 CART是“Classification and Regression Tree”的缩写。\ncart  cart既是分类也是回归树 cart分类时，采用基尼值。是回归时，采用最小方差作为分类依据 依然是二叉树。  基尼值:GINI = 1-\\sum p^2. 节点不纯（0,1标签相同，则gini最大为1/2.如果只有一个类别就是0。选择jini值作为分裂依据。\n方差：方差越大，表示该节点的数据越分散，预测的效果就越差。如果一个节点的所有数据都相同，那么方差就为0,此时可以很肯定得认为该节点的输出值；如果节点的数据相差很大，那么输出的值有很大的可能与实际值相差较大。因此，无论是分类树还是回归树，CART都要选择使子节点的GIN值或者回归方差最小的属性作为分裂的方案。\nhttps://www.bilibili.com/video/BV1Pk4y1C7m9\n决策树的优缺点 优点 决策树易于理解和解释，可以可视化分析，容易提取出规则； 可以同时处理标称型和数值型数据； 比较适合处理有缺失属性的样本； 能够处理不相关的特征； 测试数据集时，运行速度比较快； 在相对短的时间内能够对大型数据源做出可行且效果良好的结果。\n缺点 容易发生过拟合（随机森林可以很大程度上减少过拟合）； 容易忽略数据集中属性的相互关联； 对于那些各类别样本数量不一致的数据，在决策树中，进行属性划分时，不同的判定准则会带来不同的属性选择倾向；信息增益准则对可取数目较多的属性有所偏好（典型代表ID3算法），而增益率准则（CART）则对可取数目较少的属性有所偏好，但CART进行属性划分时候不再简单地直接利用增益率尽心划分，而是采用一种启发式规则）（只要是使用了信息增益，都有这个缺点，如RF）。 ID3算法计算信息增益时结果偏向数值比较多的特征。\n随机森林 随机森林属于bagging\n构造随机森林的 4 个步骤 一个样本容量为N的样本，有放回的抽取N次，每次抽取1个，最终形成了N个样本。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。 当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m \u0026laquo; M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。 决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。 按照步骤1~3建立大量的决策树，这样就构成了随机森林了。\n","id":7,"section":"posts","summary":"https://easyai.tech/ai-definition/decision-tree/ https://zhuanlan.zhihu.com/p/85731206 决策树更详细点。 https://blog.csdn.net/zjsghww/article/details/51638126 c4.5 什么是决策树？ 决策树算法采用树形结构，使用层层推理来实现最终的分类。决策树由下面几种元素构成： 根节点：包含样本的全集","tags":[],"title":"随即森林和决策树","uri":"https://yanyuLinxi.github.io/2021/11/%E9%9A%8F%E5%8D%B3%E6%A3%AE%E6%9E%97%E5%92%8C%E5%86%B3%E7%AD%96%E6%A0%91/","year":"2021"},{"content":"怎么学习？ Kaggle怎么学？比赛驱动。技术驱动。 我们按照这样的路径： 三方面驱动。\n 比赛驱动。 然后记录比赛中的相关技术，去学习相关技术。 并学习相关的writeup  deadline 网址 https://iphysresearch.github.io/DataSciComp/hostby?sub=PF,AC\n常用工具。  自动化训练工具  AutoX.   自动化EDA工具  pandas_profiling   EDA可视化工具  plotly    ","id":8,"section":"posts","summary":"怎么学习？ Kaggle怎么学？比赛驱动。技术驱动。 我们按照这样的路径： 三方面驱动。 比赛驱动。 然后记录比赛中的相关技术，去学习相关技术。 并学习","tags":[],"title":"比赛学习网址或工具","uri":"https://yanyuLinxi.github.io/2021/11/%E6%AF%94%E8%B5%9B%E5%AD%A6%E4%B9%A0%E7%BD%91%E5%9D%80%E6%88%96%E5%B7%A5%E5%85%B7/","year":"2021"},{"content":"NLP比赛目录 https://github.com/zhpmatrix/nlp-competitions-list-review\n适合初学者学习的NLP开源项目有哪些？ - 武博文的回答 - 知乎 https://www.zhihu.com/question/264352009/answer/386628568\n","id":9,"section":"posts","summary":"NLP比赛目录 https://github.com/zhpmatrix/nlp-competitions-list-review 适合初学者学习的NLP开源项目有哪些？ - 武博文的回答 - 知乎 https://www.zhihu.com/question/264352009/answer/386628568","tags":[],"title":"_NLP案例目录","uri":"https://yanyuLinxi.github.io/2021/11/_nlp%E6%A1%88%E4%BE%8B%E7%9B%AE%E5%BD%95/","year":"2021"},{"content":"新手入门 Kaggle NLP类比赛总结 - jiazhuamh的文章 - 知乎 https://zhuanlan.zhihu.com/p/109992475\n","id":10,"section":"posts","summary":"新手入门 Kaggle NLP类比赛总结 - jiazhuamh的文章 - 知乎 https://zhuanlan.zhihu.com/p/109992475","tags":[],"title":"NLP总结1","uri":"https://yanyuLinxi.github.io/2021/11/nlp%E6%80%BB%E7%BB%931/","year":"2021"},{"content":"api 随笔记 shap.force_plot这个函数应该是打印对于这个样本，哪些权重做出了贡献。\n","id":11,"section":"posts","summary":"api 随笔记 shap.force_plot这个函数应该是打印对于这个样本，哪些权重做出了贡献。","tags":[],"title":"Shap学习笔记","uri":"https://yanyuLinxi.github.io/2021/11/shap%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","year":"2021"},{"content":"深度学习是机器学习的一个特定分支。我们要想充分理解深度学习，必须对机器 学习的基本原理有深刻的理解。\n机器学习本质上属于应用统计学，更多地关注于如何用 计算机统计地估计复杂函数，不太关注为这些函数提供置信区间。因此我们会探讨 两种统计学的主要方法：频率派估计和贝叶斯推断。\n学习算法  我们所谓的 ‘‘学习’’ 是什 么意思呢？Mitchell (1997) 提供了一个简洁的定义：‘‘对于某类任务 T 和性能度量 P，一个计算机程序被认为可以从经验 E中学习是指，通过经验 E 改进后，它在任务 T 上由性能度量 P 衡量的性能有所提升。 样本是 指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的特征（feature）的集合 一些非常常见的机器学习任务列举如下  分类  学习算法通常会返回一个函数 f : Rn → {1, . . . , k}。当 y = f(x) 时，模型将向量 x 所代表的输入分类到数字码 y 所代表的类别。   输入缺失分类  学习算法只需要定义一个从输入向量映射到输出类别的函数。当一些输入可能丢失时，学习算法必须学习一组函数，而不是 单个分类函数。 有 效地定义这样一个大集合函数的方法是学习所有相关变量的概率分布，然后通 过边缘化缺失变量来解决分类任务。使用 n 个输入变量，我们现在可以获得每 个可能的缺失输入集合所需的所有 2n 个不同的分类函数，但是计算机程序仅需要学习一个描述联合概率分布的函数。   回归  在这类任务中，计算机程序需要对给定输入预测数值。学习算法需要输出函数 f : Rn → R。除了返回结果的形式不一样外，这类 问题和分类问题是很像的   转录  机器学习系统观测一些相对非结构化表示的数据，并转 录信息为离散的文本形式。 深度学习是现代语音识别系统的重要组成部分   机器翻译  在机器翻译任务中，输入是一种语言的符号序列，计算机程序必须 将其转化成另一种语言的符号序列   结构化输出  结构化输出任务的输出是向量或者其他包含多个值的数据结构， 并且构成输出的这些不同元素间具有重要关系 这类任务被称为结构化输出任务是因为输出值之 间内部紧密相关。例如，为图片添加标题的程序输出的单词必须组合成一个通顺的句子。   异常检测  计算机程序在一组事件或对象中筛选，并标记不正 常或非典型的个体。 异常检测任务的一个示例是信用卡欺诈检测。   合成和采样  机器学习程序生成一些和训练数据相似的新样本。 通过机器学习，合成和采样可能在媒体应用中非常有用 例如，视频游戏可以自动生成大型物体或风景 的纹理，而不是让艺术家手动标记每个像素 (Luo et al., 2013)。 我们提供书写的句子，要求程序输出这个句子语音的音频 波形。这是一类结构化输出任务，但是多了每个输入并非只有一个正确输出的 条件，并且我们明确希望输出有很多变化，这可以使结果看上去更加自然和真实   缺失值填补：  在这类任务中，机器学习算法给定一个新样本 x ∈ Rn，x 中某些 元素 xi 缺失。算法必须填补这些缺失值。   去噪：  干净样本 x ∈ Rn 经过未知损 坏过程后得到的损坏样本 ˜x ∈ Rn。   密度估计或概率质量函数估计：  在密度估计问题中，机器学习算法学习函数 pmodel : Rn → R，其中 pmodel(x) 可以解释成样本采样空间的概率密度函数（如果 x 是连续的）或者概率质量函数（如果 x 是离散的）。 算法必须知道什么情况下样本聚集出现。例如，如果我们通过密度估计得到了概率分布 p(x)， 我们可以用该分布解决缺失值填补任务。如果 xi 的值是缺失的，但是其他的变量值 x−i 已知，那么我们可以得到条件概率分布 p(xi | x−i)。因为在很多情况下 p(x) 是难以计算的。      性能度量p 性能度量 P。我们通常度量模型的准确率（accuracy）。们也可以通过错误率（error rate）得到相同的信息 我们通常把错 误率称为 0 − 1损失的期望。在一个特定的样本上，如果结果是对的，那么 0 − 1损失是 0；否则是 1。  但是对于密度估计这类任务而言，度量准确率，错误率或者其他 类型的 0 − 1损失是没有意义的。最常用的方法是输出模型在一些样本上概率对 数的平均值   在某些情况下，这是因为很难确定应该度量什么。这些设计的选择取决 于应用.在这些情况下，我们必须设计 一个仍然对应于设计对象的替代标准，或者设计一个理想标准的良好近似。  经验E  机器学习算法可以大致分类为无监督（unsupervised）算法和监督（supervised）算法。 本书中的大部分学习算法可以被理解为在整个数据集（dataset）上获取经验。有时我们也将样本称为数 据点（data point） 无监督学习算法。我们通常要学习生成数据集的整个概率分布，显式地，比如密度估计，或是隐式地，比如合成或去噪。还有一些其他类型的无监督学习任务，例如聚类，将数据集分成相似样本的集合。  无监督学习算法的三个特性：  没有标签 没有目的 无法量化效果。     监督学习算法。训练含有很多特征的数据集，不过数据集中的样本都有一个标签（label）或目标（target）。 大致说来，无监督学习涉及到观察随机向量 x 的好几个样本，试图显式或隐式 地学习出概率分布 p(x)。而监督学习包含观察随 机向量 x 及其相关联的值或向量 y，然后从 x 预测 y，通常是估计 p(y | x) 无监督学习和监督学习不是严格定义的术语。例如，概率的链式法则表明对于向量 x ∈ Rn， 联合分布可以分解成p(x) = ∏n i=1p(xi | x1, . . . , xi−1).该分解意味着我们可以将其拆分成 n 个监督学习问题 尽管无监督学习和监督学习并非完全没有交集的正式概念，传统地，人们将回归、分类或者结构化输出问题称为监督学习。支持其他任务的密度估计通常被称为无监督学习。 有些机器学习算法并不是训练于一个固定的数据集上。例如，**强化学习（reinforcement learning）**算法会和环境进行交互，所以学习系统和它的训练过程会有反馈回路。 在所有的情况下，数据集都是样本的集合，而样本是特征的集合。 示例：线性回归  理解如何进行梯度下降的。对于一个目标值y，建立函数y=f(x)=ax+b。其中a、b为权重。每次得到一个loss值。比如loss = min (y-y_label)^2计算得到。则根据loss求每一个权重的偏导数，并进行梯度下降。为的是知道，在权重取什么值的时候，y能够取到最小值。 在更复杂的情况下，我们最简单的方法是使用牛顿法来计算得到最小点。如果f不是真正二次，而是局部近似为正定二次，则需要多次迭代应用式子。使用梯度下降是一阶优化算法。使用Hessian矩阵是二阶优化算法。如牛顿法。    容量、过拟合和欠拟合  机器学习的主要挑战就是能够在先前未观测的新输入上表现良好。在先前未观测的输入上表现良好称为泛化。 机器学习的目的希望泛化误差（也被称为测试误差）很低。 统计学理论可以让我们通过训练集影响测试集。  训练集和测试被数据生成过程的概率分布生成。 会进行独立同分布的假设。该假设是说，每个数据集中的样本都是彼此相互独立的（independent），并且训练集和测试集是同分布的（identically distributed）， 我们将这个共享的潜在 分布称为数据生成分布（data generating distribution），记作 pdata。 我们能观察到训练误差和测试误差之间的直接联系是，随机模型训练误差的期 望和该模型测试误差的期望是一样的。假设我们有概率分布 p(x, y)，   以下是决定机器 学习算法效果是否好的因素：  . 降低训练误差。 缩小训练误差和测试误差的差距。   这两个因素对应机器学习的两个主要挑战：欠拟合（underfitting）和过拟合 （overfitting）。欠拟合是指模型不能在训练集上获得足够低的误差。而过拟合是指训练误差和和测试误差之间的差距太大。 通过调整模型的容量（capacity），我们可以控制模型是否偏向于过拟合或者欠 拟合。  通俗地，模型的容量是指其拟合各种函数的能力。容量低的模型可能很难拟 合训练集。容量高的模型可能会过拟合，因为记住了不适用于测试集的训练集性质。   一种控制训练算法容量的方法是选择假设空间（hypothesis space），即学习算 法可以选择为解决方案的函数集 我们探讨了通过改变输入特征的数目和加入这些特征对应的参数，改 变模型的容量  模型增加特征是：增加特征的维数。这里从x变成了x+x^2学习参数是，y=wx+b不变，通过学习w,b来学习y。增加特征是增加x来拟合y，学习参数是学习权重来拟合y 网上查到的解释：  参数模型，对目标函数有一个假设，如y=ax+b。通过最小二乘法来拟合目标函数的参数 非参数模型：不对目标函数有一个确定的假设。通过训练来拟合学习某种函数。     容量不足的模型不能解决复杂任务。容量高的模型能够解决 复杂的任务，但是当其容量高于任务所需时，有可能会过拟合。 模型规定了调整参数降低训练目标时，学习算法可以从哪些函数族中选择函数。这被称为模型的表示容量（representational capacity）. 额外的限制因素，比如 优化算法的不完美，意味着学习算法的**有效容量（effective capacity）**可能小于模型族的表示容量 现在广泛被称为**奥卡姆剃刀（Occam’s razor）（c. 1287-1387）。**该原则指出，在同样能够解释已知观测现象的假设中，我们应该挑选 ‘‘最简单’’ 的那一个。 统计学习理论提供了量化模型容量的不同方法。在这些中，最有名的是VapnikChervonenkis 维度（Vapnik-Chervonenkis dimension, VC）。 VC维定义为该分类器能够分类的训练样本的最大数目。假设存在 m 个 不同 x 点的训练集，分类器可以任意地标记该 m 个不同的 x 点，VC维被定义为 m的最大可能值。 统计学习理论中最重要 的结论阐述了训练误差和泛化误差之间差异的上界随着模型容量增长而增长，但随着训练样本增多而下降 但是它们很少应用于实际中的深度学习算法。  一部分原因是边界太松， 另一部分原因是很难确定深度学习算法的容量。 由于有效容量受限于优化算法的能力， 确定深度学习模型容量的问题特别困难。而且对于深度学习中的一般非凸优化问题，我们只有很少的理论分析   为考虑容量任意高的极端情况，我们介绍非参数（non-parametric）模型的概 念。  参数模型学习的函数在观测到新 数据前，参数向量的分量个数是有限且固定的。非参数模型没有这些限制 非参数模型典例：决策树。最近邻回归。   理想模型假设我们能够预先知道生成数据的真实概率分布。然而这样的模型仍 然会在很多问题上发生一些错误，因为分布中仍然会有一些噪声。从预先知道的真实分布 p(x, y) 预测而出现的误差被称为贝叶斯误差（Bayes error）。知道测试集的分布，仍然产生的误差就是贝叶斯误差。 训练误差和泛化误差会随训练集的大小发生变化。  泛化误差的期望从不会因训 练样本数目的增加而增加。 对于非参数模型而言，更多的数据会得到更好的泛化能 力，直到达到最佳可能的泛化误差。 任何模型容量小于最优容量的固定参数模型会渐近到大于贝叶斯误差的误差值。   归纳推理，或是从一组有限的样本中推断一般的规则，在 逻辑上不是很有效。为了逻辑地推断一个规则去描述集合中的元素，我们必须具有集合中每个元素的信息。上升到哲学。 机器学习保证找到一个在所关注的大多数样本上可能正 确的规则 机器学习的没有免费午餐定理（no free lunch theorem）表明 (Wolpert, 1996)，每 一个分类算法在未事先观测的点上都有相同的错误率。换言之，在某种意义上，没有一个机器学习算法总是比其他的要好。我们能够设想的最先进的算法和简单地将所有点归为同一类的简单算法有着相同的平均性能（在所有可能的任务上）。 反之，我们的目标是理解什么样的分布与人工智能获取经验的 ‘‘真实世界’’ 相 关，什么样的学习算法在我们关注的数据生成分布上效果最好。 最小二乘法：h = X$\\theta$。损失函数定义为h = 1/2(X$\\theta$-Y)^T(X$\\theta$-Y) 和最小均方差公式很类似。。  正则化  没有免费午餐定理暗示我们必须在特定任务上设计性能良好的机器学习算法。没有一个设计可满足所有的任务。 算法的效果不仅很大程度上受影响于假设空间的函数数量，也取决于这些函数 的具体形式。在假设空间中，相比于某一个学习算法，我们可能更偏好另一个学习算法。只有非偏好函数比偏 好函数在训练数据集上效果明显好很多时，我们才会考虑非偏好函数。 例如，我们可以加入权重衰减（weight decay）来修改线性回归的训练标准。带权重衰减的线性回归最小化训练集上的均方误差和正则项的和 J(w)，其偏好于平方L2 范数较小的权重  J(w) = MSE_{train} + λw⊤w, 其中 λ 是提前挑选的值，控制我们偏好小范数权重的程度。越大的 λ偏好范数越小的权重.或是将权重放在较少 的特征上。 常用的惩罚项是所有权重的平方乘以一个衰减常量之和。其用来惩罚大的权值。 所以W^T W 就是权重的乘积。表现为如果权重大的话，这个值也大。损失函数的目的让这个数变小。所以加入这个可以惩罚权重大的值。   更一般地，正则化一个学习函数 f(x; θ) 的模型，我们可以给代价函数添加被称为正则化项（regularizer）的惩罚。在权重衰减的例子中，正则化项是 Ω(w) = w⊤w。 正则化是指我们 修改学习算法，使其降低泛化误差而非训练误差。正则化是机器学习领域的中心问题之一，只有优化能够与其重要性相媲。这些不同的方法都被称为正则化（regularization）。  超参数和验证集  有时一个选项被设为学习算法不用学习的超参数，是因为它太难优化了。更多 的情况是，该选项必须是超参数，因为它不适合在训练集上学习 如果在训练集上学习超参数，这些超参数总是趋向于最大可 能的模型容量，导致过拟合。例如，相比低次多项式和正的权重衰减 设定，更高次的多项式和权重衰减参数设定 λ = 0 总能在训练集上更好地拟合。 其重点在于测试样本不能以任何形式参与到 模型的选择中，包括设定超参数。测试集中的样本不能用于验证集。 因此，我们总是从训练数据中构建验证集。  特别地，我们将训练数据分成两个不相 交的子集。其中一个用于学习参数。另一个作为验证集，用于挑选超参数的数据子集被称为验 证集（validation set）。通常，80% 的训练数据用于训练，20% 用于验证。   交叉验证。这些过程是基于在原始数据上随机采样或分离出的不同数据集上重复训练和 测试的想法。最常见的是 k-折交叉验证过程，  估计、偏差和方差 Notes: 估计这一章理论性较强，可以说你只看懂了概念，其他都没看懂。\n参数估计、偏差和方差，用于正式的刻画泛化、欠拟合和过拟合。\n点估计、函数估计，就是利用已知的样本结果信息，反推导致这些样本结果出现的模型参数值！通过已知的数据分布推断样本出现结果的模型的参数。\n点估计。  点估计试图为一些感兴趣的量提供单个 ‘‘最优’’ 预测 为了区分参数估计和真实值，我们习惯将参数 θ 的点估计表示为 ˆθ。 令 {x(1), . . . , x(m)} 是 m 个独立同分布（i.i.d.）的数据点。点估计（point estimator）或统计量（statistics）是这些数据的任意函数： 点估计也可以指输入和目标变量之间关系的估计。我们将这种类型的点估计称 为函数估计。 函数估计 有时我们会关注函数估计（或函数近似）。我们假设有一个函数 f(x) 表示 y 和 x 之间的近似关系。例如，我们可能 假设 y = f(x) + ϵ，其中 ϵ 是 y 中未能从 x 预测的一部分。 理解统计量和真实值。统计量是统计出来的，和真实的值是存在差异的。真实值\\theta, 统计量就是\\theta的计算公式的出来的值，这两个值是有差距的。估计量是从统计量中计算得到的。 函数估计，就是通过输入向量算出来的值。基于观测数据推断一个已知量的的估计值。如y=f(x)+e, 用模型估计去近似f。 无偏估计就是，函数估计的均值减去这个值为0.那这个估计就是无偏的。那这个估计量是优良的。通过这个估计评价的估计函数是没有理论上的偏差的。  偏差 我们通过偏差来衡量一个估计是否合理。\n 估计的偏差被定义为：bias( ˆθm) = E( ˆθm) − θ,如果bias( ˆθm) = 0，那么估计量 ˆθm 被称为是无偏 （unbiased），这意味着 E( ˆθm) = θ。如果 limm→∞ bias( ˆθm) = 0，那么估计量 ˆθm 被 称为是渐近无偏（asymptotically unbiased），这意味着 limm→∞ E( ˆθm) = θ。 均值的高斯分布估计是无偏的。高斯分布方差估计是有偏估计。无偏样本方差（unbiased sample variance）估计是无偏的。 一个是有偏的，另一个是无偏的。尽管无偏估计显然是令 人满意的，但它并不总是 ‘‘最好’’ 的估计。 估计量的**方差（variance）就是一个方差Var(ˆθ)方差的平方根被称为标准差（**standard error），记作 SE(ˆθ)。 样本方差的平方根和 方差无偏估计的平方根都不是标准差的无偏估计。这两种计算方法都倾向于低估真实的标准差，但仍用于实际中。相较而言，方差无偏估计的平方根较少被低估。 均值的标准差在机器学习实验中非常有用。测试集中样本的数量决定了这个估计的精确度。中心极限定理告 诉我们均值会接近一个高斯分布，我们可以用标准差计算出真实期望落在选定区间 的概率。例如，以均值 ˆµm 为中心的 95% 置信区间是。在机器学习实验中，我们通 常说算法 A 比算法 B 好，是指算法 A 的误差的 95% 置信区间的上界小于算法 B的误差的 95% 置信区间的下界。  均方误差  偏差和方差度量着估计量的两个不同误差来源。偏差度量着偏离真实函数或参 数的误差期望。而方差度量着数据上任意特定采样可能导致的估计期望的偏差 当我们可以在一个偏差更大的估计和一个方差更大的估计中进行选择时，会发 生什么呢？我们该如何选择？判断这种权衡最常用的方法是交叉验证。 另外，我们也可以比较这些估计的均方误差。MSE度量着估计和真实参数 θ 之间平方误差的总体期望偏差。MSE=E(Var(x)) 方差的总体期望偏差。 使用MSE度量泛化误差时，增加容量会增加方差，降低偏差。  最大似然估计 概率函数 f(x|o)，已知o，来预测x。看x出现的概率是多少 似然函数 f(x|o), 已知x，来预测o。在知道数据分布的情况下，推断数据分布o。对不同的o，x出现的概率是多少。\n似然函数和条件概率函数从不同的视角来看的。 最大似然估计，就是在所有可能的o中取一个值，使得出现x这种分布的可能性最大。即最大似然估计。\n 考虑一组含有 m 个样本的数据集 X = {x(1), . . . , x(m)}，独立地由未知的真实数 据生成分布 pdata(x) 生成。令 pmodel(x; θ) 是一族由 θ 确定在相同空间上的概率分布。 对 θ 的最大似然估计被定义为。$\\theta = argmax_{\\theta} p_{model}(X;\\theta)=argmax_{\\theta}\\prod_{i=1}^m p_{model}(x^i;\\theta)$ 一种解释最大似然估计的观点是将它看作最小化训练集上的经验分布 ˆpdata 和模型分布之间的差异。就是通过模型参数训练出来的分布和真实分布之间的差异，通过最大似然估计来减少这个差异。 两者之间的差异程度可以通过 KL 散度度量。KL 散度被定义为：分布交叉熵的均值。  任何一个由负对数似然组成的损失都是定义在训练集上的经验分布和定义在模型上的概率 分布之间的交叉熵。   最大似然估计很容易扩展到估计条件概率 P(y | x; θ)，从而给定 x 预测 y。实 际上这是最常见的情况，因为这构成了大多数监督学习的基础。 最大似然估计最吸引人的地方在于，它被证明当样本数目 m → ∞ 时，就收敛 率而言是最好的渐近估计。 因为这些原因（一致性和统计效率），最大似然通常是机器学习中的首选估计。 当样本数目小到会发生过拟合时，正则化策略如权重衰减可用于获得训练数据有限时方差较小的最大似然有偏版本。  贝叶斯统计  至此我们已经讨论了频率派统计（frequentist statistics）方法和基于估计单一 值 θ 的方法，然后基于该估计作所有的预测。另一种方法是在做预测时会考虑所有可能的 θ。后者属于贝叶斯统计（Bayesian statistics）的范畴。 频率派的视角是真实参数 θ是未知的定值，而点估计 θˆ是考虑数据集上函数（可以看作是随机的）的随机变量。贝叶斯统计的视角完全不同。贝叶斯用概率反映知识状态的确定性程度 我们将 θ 的已知知识表示成先验概率分布。在贝叶斯估计常用的情景下，先验开始是相对均匀的分布或高熵的高斯分布，观测 数据通常会使后验的熵下降，并集中在参数的几个可能性很高的值。 相对于最大似然估计，贝叶斯估计有两个重要区别。  第一，不像最大似然方法预 测时使用 θ 的点估计，贝叶斯方法使用 θ 的全分布。 贝叶斯方法和最大似然方法的第二个最大区别是由贝叶斯先验分布造成的。先 验能够影响概率质量密度朝参数空间中偏好先验的区域偏移。   当训练数据很有限时，贝叶斯方法通常泛化得更好，但是当训练样本数目很大 时，通常会有很大的计算代价。 检查此后验分布可以让我们获得贝叶斯推断效果的一些直觉。。我们不能将贝叶斯学习过程初始化为一个无限宽的 w 先验。更重 要的区别是贝叶斯估计会给出一个协方差矩阵，表示 w 所有不同值的可能范围，而不仅是估计 µm。  最大后验估计MAP  希望使用点估计的一个常见原因是，对于大多数有意义的模型而 言，大多数涉及到贝叶斯后验的计算是非常棘手的，点估计提供了一个可行的近似解。我们仍然可以让先验影响点估计的选择来利用贝叶斯方法的优点，而不是简单 地回到最大似然估计。一种能够做到这一点的合理方式是选择最大后验点估计   监督学习算法  在许多情况下，输出 y 很难自动收集，必须由人来 提供 ‘‘监督’’，不过该术语仍然适用于训练集目标可以被自动收集的情况。 本书的大部分监督学习算法都是基于估计概率分布 p(y | x) 的。我们可以使用最 大似然估计找到对于有参分布族p(y | x; θ) 最好的参数向量 θ。 逻辑回归（logistic regression），一种方法是使用 logistic sigmoid 函数将线性函数的输出压缩进区间 (0, 1)。该值可以解释为概率。 支持向量机（support vector machine, SVM）是监督学习中最有影响力的方法 之一 支持向量机不输出概率，只输 出类别。**当 w⊤x+ b 为正时，支持向量机预测属于正类。**类似地，当 w⊤x+ b 为负时，支持向量机预测属于负类。  支持向量机二分类。   支持向量机的一个重要创新是核技巧（kernel trick）。 学习算法重写为这种形式允许我们将 x 替 换为特征函数 ϕ(x) 的输出，点积替换为被称为核函数（kernel function）的函数k(x, x(i)) = ϕ(x) · ϕ(x(i))。就是说通过将点积替换为核函数可以让这种算法能过够有非线性的表达能力。核函数完全等价于用 ϕ(x) 预处理所有的输入，然后在新的转换空间学习线性模 型。 核技巧十分强大有两个原因  首先，它使我们能够使用保证有效收敛的凸优化 技术来学习非线性模型（关于 x 的函数） 其二，核 函数 k 的实现方法通常有比直接构建 ϕ(x) 再算点积高效很多   在很多情况下，即使 ϕ(x) 是难算的k(x, x′) 却会是一个关于 x 非线性的、易算的函数.假设这个映射返回一个由开头 x 个 1，随后是无限个 0 的向量。我们可以写一个核函数 k(x, x(i)) = min(x, x(i))，完全等价于对应的无限维点积。  这种简单的比喻非常容易理解为什么自定义核函数可以比点积更有效。就是因为核函数可以自定义，根据特征的性值对函数进行自定义。   最常用的核函数是高斯核（Gaussian kernel）， 我们可以认为高斯核在执行一种模板匹配(template matching)。训练标签 y 相 关的训练样本 x 变成了类别 y 的模版。当测试点 x′ 到 x 的欧几里得距离很小，对应的高斯核响应很大时，表明 x′ 和模版 x 非常相似。 。使用核技巧的算法类别被称为核机器（kernel machine） 或核方法（kernel method） 核机器的一个主要缺点是计算决策函数的成本关于训练样本的数目是线性的。 因为第 i 个样本贡献 αik(x, x(i)) 到决策函数。 支持向量机能够通过学习主要包含零 的向量 α，以缓和这个缺点。那么判断新样本的类别仅需要计算非零 αi 对应的训练样本的核函数。这些训练样本被称为支持向量 带通用核的核机器致力于泛化得更好 当前深度学习的复兴始于 Hinton et al. (2006b) 表明神经网络能够在 MNIST 基准数据上胜过 RBF 核的支持向量机。 其他简单的监督学习算法  更一般地，k-最 近邻是一类可用于分类或回归的技术 训练过程：反之，在 测试阶段我们希望在新的测试输入 x 上产生 y，我们需要在训练数据 X上找到 x 的k-最近邻。然后我们返回训练集上对应的 y 值的平均值 k-最近邻的高容 量使其在训练样本数目大时能够获取较高的精度。然而，它的计算成本很高，另外 在训练集较小时泛化能力很差。k-最近邻的一个弱点是它不能学习出哪一个特征比其他更具识别力。 决策树（decision tree）及其变种是另一类将输入空间分成不同的区域，每个区 域有独立参数的算法 (Breiman et al., 1984)    无监督学习算法  无监督算法只处理 \u0026ldquo;特征\u0026rdquo;，不操作监督信号。无监督学习的大多数尝试是指从不需要人为注释的样 本的分布中抽取信息。该术语通常与密度估计相关。一个经典的无监督学习任务是找到数据的 **‘‘最佳’’**表示。但是一般来说，是指该表示在比本身表示的信息更简单或更易访问而受到一 些惩罚或限制的情况下，尽可能地保存关于 x 更多的信息。 最常见的三种包括低维表示、稀疏表示和独立 表示。低维表示尝试将 x 中的信息尽可能压缩在一个较小的表示中。稀疏表示通常用于需要增加表示维数的情况，使得 大部分为零的表示不会丢失很多信息。独立表示试图分开数据分布中变化的来源，使得表示的维 度是统计独立的。  主成分分析 PCA通过线性变换找到一个 Var[z] 是对角矩阵的表示 z =W⊤x。 主成分也可以通过奇异值分解 (SVD) 得 到。假设 W是奇异值分解 X= UΣW⊤ 的右奇异向量。以W作为特征向量基，我们可以得到原来的特征向量。以上分析指明当我们通过线性变换 W将数据 x 投影到 z 时，得到的数据表示 的协方差矩阵是对角的（即 Σ2），立刻可得 z 中的元素是彼此无关的。 见p157。 PCA这种将数据变换为元素之间彼此不相关表示的能力是PCA的一个重要性 质。它是消除数据中未知变化因素的简单表示示例\nK-均值据类  k-均值聚类提供的 one-hot 编码也是一种稀疏表示,因为每个输入的表示中大 部分元素为零。one-hot 编码是稀疏表示的一个极端示例，丢失 了很多分布式表示的优点。one-hot 编码仍然有一些统计优点（自然地传达了相同聚类中的样本彼此相似的观点）,也具有计算上的优势，因为整个表示可以用一个单独 的整数表示. k-均值聚类初始化 k 个不同的中心点 {µ(1), . . . , µ(k)}，然后迭代交换两个不同 的步骤直到收敛。步骤一，每个训练样本分配到最近的中心点 µ(i) 所代表的聚类 i。步骤二，每一个中心点 µ(i) 更新为聚类 i 中所有训练样本 x(j) 的均值。 关于聚类的一个问题是聚类问题本身是病态的。这是说没有单一的标准去度量 聚类的数据在真实世界中效果如何。 然而我们不知道聚类的性质是否很好地对应到真实世界的性质.我们可能希望找到和 一个特征相关的聚类，但是得到了一个和任务无关的，同样是合理的不同聚类。我们只知道它们是不同的。 这些问题说明了一些我们可能更偏好于分布式表示（相对于 one-hot 表示而言） 的原因。分布式表示可以对每个车辆赋予两个属性——一个表示它颜色，一个表示它是汽车还是卡车。但是多个属性减少了算法去猜我们关心哪一个属性的负担.  随机梯度下降  几乎所有的深度学习算法都用到了一个非常重要的算法：随机梯度下降 （stochastic gradient descent, SGD）。 机器学习中反复出现的一个问题是好的泛化需要大的训练集，但大的训练集的 计算代价也更大。机器学习算法中的代价函数通常可以分解成每个样本的代价函数的总和。. 这个运算的计算代价是 O(m)。随着训练集规模增长为数十亿的样本，计算一步梯度 也会消耗相当长的时间。 随机梯度下降的核心是，梯度是期望。期望可使用小规模的样本近似估计.在算法的每一步，我们从训练集中均匀抽出一小批量（minibatch）样本.使用来自小批量 B 的样本。然后，随机梯度下降算法使用如下的梯度下降估计：θ ← θ − ϵg,其中，ϵ 是学习率。 它是在大规模数据上训练大 型线性模型的主要方法。对于固定大小的模型，每一步随机梯度下降更新的计算量不取决于训练集的大小 m。而，当 m 趋向于无穷大时，该模型最终会在随机梯度下降抽样完训练集上的所有样本之前收敛到可能的最优测试误差。我们可以认为用SGD训练模型的渐近代价 是关于 m 的函数的 O(1) 级别。 在深度学习兴起之前，学习非线性模型的主要方法是结合核技巧的线性模型。 很多核学习算法需要构建一个 m×m 的矩阵.  深度学习从 2006 年开始受到关注的原因是，在数以万计样本的中等规模数据集上， 深度学习在新样本上比当时很多热门算法泛化得更好。不久后，深度学习在工业界受到了更多的关注，因为其提供了一种训练大数据集上的非线性模型的可扩展方式。    ","id":12,"section":"posts","summary":"深度学习是机器学习的一个特定分支。我们要想充分理解深度学习，必须对机器 学习的基本原理有深刻的理解。 机器学习本质上属于应用统计学，更多地关注于","tags":[],"title":"4 机器学习基础","uri":"https://yanyuLinxi.github.io/2021/11/4-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/","year":"2021"},{"content":"机器学习算法通常需要大量的数值计算。这通常是指通过迭代过程更新解的估 计值来解决数学问题的算法，而不是通过解析过程推导出公式来提供正确解的方法\n上溢下溢  连续数学在数字计算机上的根本困难是，我们需要通过有限数量的位模式来表 示无限多的实数。意味着我们在计算机中表示实数时，几乎总会引入一些近似误 差。 一种极具毁灭性的舍入误差是下溢（underflow）。当接近零的数被四舍五入为 零时发生下溢。 另一个极具破坏力的数值错误形式是上溢（overflow）。当大量级的数被近似为 ∞ 或 −∞ 时发生上溢。进一步的运算通常会导致这些无限值变为非数字。 上溢和下溢进行数值稳定的一个例子是softmax 函数 Theano (Bergstra et al., 2010a; Bastien et al., 2012a) 就是这样软件包的一个例子，它能自动检测并稳定深度学习中许多常见的数值不稳定的表达式。  病态条件  考虑函数 f(x) = A−1x。当 A ∈ Rn×n 具有特征值分解时，其条件数为$max_{i,j} |\\frac{\\lambda_i}{\\lambda_j}|$。这是最大和最小特征值的模之比1。当**该数很大**时，矩阵求逆对输入的**误差特别敏感**。  基于梯度的优化方法   我们把要最小化或最大化的函数称为目标函数（objective function）或准则 （criterion）。\n 当我们对其进行最小化时，我们也把它称为代价函数（cost function）、损失函数（loss function）或误差函数（error function）。 我们通常使用一个上标 ∗表示最小化或最大化函数的 x 值。如我们记 x∗ = arg min f(x)。    这个函数的导数（derivative） 记为 f′(x) 或 dy/dx。导数 f′(x) 代表 f(x) 在点 x 处的斜率。\n  因此导数对于最小化一个函数很有用，因为它告诉我们如何更改 x 来略微地改 善 y。这种技术被称为梯度下降 （gradient descent）(Cauchy, 1847)。\n  f′(x) = 0 的点称为临界 点（critical point）或驻点（stationary point）。\n f′(x) = 0 的点称为临界 点（critical point）或驻点（stationary point）。 f′(x) = 0 的点称为临界 点（critical point）或驻点（stationary point）。 有些临界点既不是最小点也不是最大点。这些点被称为鞍点（saddle point）。    因此，我们通常寻找使 f 非常小的 点，但这在任何形式意义下并不一定是最小。\n  我们需要用到偏导数（partial derivative）的概念\n  **梯度（gradient）**是相对一个向量求导的导数:f 的导数是包含所有偏导数的向量，记为 ∇xf(x)。梯度的第i 个元素是 f 关于 xi 的偏导数。\n  在 u（单位向量）方向的方向导数（directional derivative）是函数 f 在 u 方向 的斜率。\n  负梯度向量指向下坡。我们在负梯度方向上移动可以减小 f。这被称为最速下降法 (method of steepest descent) 或梯度下降（gradient descent）。\n  最速下降建议新的点为 x′ = x− ϵ∇xf(x) (4.5)其中 ϵ 为学习率（learning rate），是一个确定步长大小的正标量\n 最速下降在梯度的每一个元素为零时收敛（或在实践中，很接近零时） 但不断向更好的情况移动一小 步（即近似最佳的小移动）的一般概念可以推广到离散空间。递增带有离散参数的目标函数被称为爬山（hill climbing）算法 (Russel and Norvig, 2003)    有时我们需要计算输入和输出都为向量的函数的所有偏导数。包含所有这样的 偏导数的矩阵被称为 Jacobian 矩阵\n 二阶导数告诉我们，一阶导数将如何随着输入 的变化而改变。它表示只基于梯度信息的梯度下降步骤是否会产生如我们预期的那样大的改善，因此它是重要的。我们可以认为，二阶导数是对曲率的衡量 如果这样的函数具有零二阶导数，那就没有曲率。也就是一条完全 平坦的线，仅用梯度就可以预测它的值。我们使用沿负梯度方向大小为 ϵ 的下降步， 当该梯度是 1 时，代价函数将下降 ϵ。如果二阶导数是负的，函数曲线向下凹陷 (向 上凸出)，因此代价函数将下降的比 ϵ 多。如果二阶导数是正的，函数曲线是向上凹陷 (向下凸出)，因此代价函数将下降的比 ϵ 少    当我们的函数具有多维输入时，二阶导数也有很多。我们可以将这些导数合并 成一个矩阵，称为 Hessian 矩阵\n  Hessian 等价于梯度的 Jacobian 矩阵。\n  微分算子在任何二阶偏导连续的点处可交换，也就是它们的顺序可以互换：这意味着 Hi,j = Hj,i，因此 Hessian 矩阵在这些点上是对称的。因为 Hessian 矩阵是实对 称的，我们可以将其分解成一组实特征值和一组特征向量的正交基。在特定方向 d上的二阶导数可以写成 d⊤Hd。这个方向的二阶导 数就是对应的特征值。对于其他的方向 d，方向二阶导数是所有特征值的加权平均， 权重在 0 和 1 之间，且与 d 夹角越小的特征向量的权重越大。最大特征值确定最大二阶导数，最小特征值确定最小二阶导数。\n Hessian 的特征值决定了学 习率的量级    当 f′(x) = 0 且 f′′(x) \u0026gt; 0 时，x 是一个局 部极小点。同样，当 f′(x) = 0 且 f′′(x) \u0026lt; 0 时，x 是一个局部极大点。这就是所谓的二阶导数测试（second derivative test）。\n  在临界点处（∇xf(x) = 0），我们通 过检测 Hessian 的特征值来判断该临界点是一个局部极大点、局部极小点还是鞍点。 当 Hessian 是正定的（所有特征值都是正的），则该临界点是局部极小点。因为方 向二阶导数在任意方向都是正的，参考单变量的二阶导数测试就能得出此结论。同 样的，当 Hessian 是负定的（所有特征值都是负的），这个点就是局部极大点。在多 维情况下，实际上我们可以找到确定该点是否为鞍点的积极迹象（某些情况下）。如 果 Hessian 的特征值中至少一个是正的且至少一个是负的，那么 x 是 f 某个横截面 的局部极大点，却是另一个横截面的局部极小点。见图4.5中的例子。最后，多维二 阶导数测试可能像单变量版本那样是不确定的。当所有非零特征值是同号的且至少 有一个特征值是 0 时，这个检测就是不确定的。这是因为单变量的二阶导数测试在零特征值对应的横截面上是不确定的。\n  维度多于一个时，鞍点不一定要具有 0 特征值：仅需要同时具有正特征值和负特征值。我 们可以想象这样一个鞍点（具有正负特征值）在一个横截面内是局部极大点，而在另一个横截面内是局部极小点。\n  中期稍微总结一下：\n jacobian矩阵是梯度矩阵，表明了下一步权重下降的方向。Hessian衡量梯度下降的速率。如果hessian值正定，则权重是局部极小点。如果hessian负定，则权重是局部极大点。当涉及多维的时候条件更苛刻一点。有非零特征值时，这个检测是不确定的。根据梯度下降的方向进行下降，就是梯度下降。证明需要看书。大约就是二阶导，高中学的那一套。    在多维情况下，单个点在每个方向上的二阶导数时不同的，Hessian 的条件数衡量 这些二阶导数的变化范围。当 Hessian 的条件数很差时，梯度下降法也会表现得很差。这是因为一个方向上的导数增加得很快，而在另一个方向上增加得很慢。\n 梯度下降把时间浪费于在峡谷壁反复下 降，因为它们是最陡峭的特征。由于步长有点大，有超过函数底部的趋势，因此需要在下一次迭代时在对面的峡谷壁下降    我们可以使用 Hessian 矩阵的信息来指导搜索，以解决这个问题。其中最简单 的方法是牛顿法（Newton’s method）。\n 当 f 是一个正定二次函数时，牛顿法只要应用一次式(4.12)就能直接跳到函数的最 小点。如果 f 不是一个真正二次但能在局部近似为正定二次，牛顿法则需要多次迭代应用式。 如式(8.2.3)所讨论的，当附近的临界点是最小点（Hessian 的所有特征值 都是正的）时牛顿法才适用，而梯度下降不会被吸引到鞍点(除非梯度指向鞍点)。    仅使用梯度信息的优化算法被称为一阶优化算法 (first-order optimization algorithms)，如梯度下降。使用 Hessian 矩阵的优化算法被称为二阶最优化算法(second-order optimization algorithms)(Nocedal and Wright, 2006)，如牛顿法\n  在深度学习的背景下，限制函数满足Lipschitz 连续（Lipschitz continuous）或 其导数Lipschitz连续可以获得一些保证。最成功的特定优化领域或许是凸优化（Convex optimization）。凸优化通过更强 的限制提供更多的保证。凸优化算法只对凸函数适用，即 Hessian 处处半正定的函数。\n  约束优化  在 x 的所有可能值下最大化或最小化一个函数 f(x) 不是我们所希望 的。相反，我们可能希望在 x 的某些集合 S 中找 f(x) 的最大值或最小值。这被称为约束优化（constrained optimization）。集合 S 内的点 x 被称可行（feasible）点。 我们常常希望找到在某种意义上小的解。 约束优化的一个简单方法是将约束考虑在内后简单地对梯度下降进行修改。  如 果我们**使用一个小的恒定步长 ϵ，**我们可以先取梯度下降的单步结果，然后将结果投影回 S。 如果我们使用线搜索，我们只能在步长为 ϵ 范围内搜索可行的新 x 点 或者 我们可以将线上的每个点投影到约束区域。 在梯度下降或线搜索前 将梯度投影到可行域的切空间会更高效 (Rosen, 1960)。   一个更复杂的方法是设计一个不同的、无约束的优化问题，其解可以转化成原 始约束优化问题的解**Karush–Kuhn–Tucker（KKT）**方法2是针对约束优化非常通用的解决方案。 我们引入一个称为广义 Lagrangian（generalized Lagrangian） 或广义 Lagrange 函数（generalized Lagrange function）的新函数。 那么 S 可以表示为 S = {x | ∀i, g(i)(x) = 0 and ∀j, h(j)(x) ≤ 0}。其中涉及 g(i) 的等式称为等式约束（equality constraint），涉及 h(j) 的不等式称为不等式约束（inequality constraint）。  这是因为当约束满足时，广义lagrangian函数的值为f(x)的值。 当约束不满足时，广义lagrangian为无穷大。   我们可以使用一组简单的性质来描述约束优化问题的最优点。这些性质称 为Karush–Kuhn–Tucker（KKT）条件  广义 Lagrangian 的梯度为零 所有关于 x 和 KKT 乘子的约束都满足。 不等式约束显示的 ‘‘互补松弛性’’：α⊙ h(x) = 0。    实例分析：线性最小二乘法 没看懂\n","id":13,"section":"posts","summary":"机器学习算法通常需要大量的数值计算。这通常是指通过迭代过程更新解的估 计值来解决数学问题的算法，而不是通过解析过程推导出公式来提供正确解的方法","tags":[],"title":"3 数值统计","uri":"https://yanyuLinxi.github.io/2021/11/3-%E6%95%B0%E5%80%BC%E7%BB%9F%E8%AE%A1/","year":"2021"},{"content":"快速入门 MySQL 为关系型数据库(Relational Database Management System)，一个关系型数据库由一个或数个表格组成\n表头(header): 每一列的名称; 列(col): 具有相同数据类型的数据的集合; 行(row): 每一行用来描述某个人/物的具体信息; 值(value): 行的具体信息, 每个值必须与该列的数据类型相同; 键(key): 表中用来识别某个特定的人\\物的方法, 键的值在当前列中具有唯一性。\n登录MySQL mysql -h 127.0.0.1 -u 用户名 -p mysql -D 所选择的数据库名 -h 主机名 -u 用户名 -p mysql\u0026gt; exit # 退出 使用 “quit;” 或 “\\q;” 一样的效果 mysql\u0026gt; status; # 显示当前mysql的version的各种信息 mysql\u0026gt; select version(); # 显示当前mysql的version信息 mysql\u0026gt; show global variables like \u0026lsquo;port\u0026rsquo;; # 查看MySQL端口号\n创建数据库 \u0026ndash; 创建一个名为 samp_db 的数据库，数据库字符编码指定为 gbk create database samp_db character set gbk; drop database samp_db; \u0026ndash; 删除 库名为samp_db的库 show databases; \u0026ndash; 显示数据库列表。 use samp_db; \u0026ndash; 选择创建的数据库samp_db show tables; \u0026ndash; 显示samp_db下面所有的表名字 describe 表名; \u0026ndash; 显示数据表的结构 delete from 表名; \u0026ndash; 清空表中记录\n创建数据库表 使用 create table 语句可完成对表的创建, create table 的常见形式: 语法：create table 表名称(列声明);\n\u0026ndash; 如果数据库中存在user_accounts表，就把它从数据库中drop掉 DROP TABLE IF EXISTS user_accounts; CREATE TABLE user_accounts ( id int(100) unsigned NOT NULL AUTO_INCREMENT primary key, password varchar(32) NOT NULL DEFAULT '' COMMENT \u0026lsquo;用户密码\u0026rsquo;, reset_password tinyint(32) NOT NULL DEFAULT 0 COMMENT \u0026lsquo;用户类型：0－不需要重置密码；1-需要重置密码\u0026rsquo;, mobile varchar(20) NOT NULL DEFAULT '' COMMENT \u0026lsquo;手机\u0026rsquo;, create_at timestamp(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6), update_at timestamp(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6), \u0026ndash; 创建唯一索引，不允许重复 UNIQUE INDEX idx_user_mobile(mobile) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=\u0026lsquo;用户表信息\u0026rsquo;;\n数据类型的属性解释: NULL：数据列可包含NULL值； NOT NULL：数据列不允许包含NULL值； DEFAULT：默认值； PRIMARY KEY：主键； AUTO_INCREMENT：自动递增，适用于整数类型； UNSIGNED：是指数值类型只能为正数； CHARACTER SET name：指定一个字符集； COMMENT：对表或者字段说明；\n增删改查 SELECT SELECT 语句用于从表中选取数据。 语法：SELECT 列名称 FROM 表名称 语法：SELECT * FROM 表名称\n\u0026ndash; 表station取个别名叫s，表station中不包含 字段id=13或者14 的，并且id不等于4的 查询出来，只显示id SELECT s.id from station s WHERE id in (13,14) and id not in (4);\n\u0026ndash; 从表 Persons 选取 LastName 列的数据 SELECT LastName FROM Persons\n\u0026ndash; 从表 users 选取 id=3 的数据，并只拉一条数据(据说能优化性能) SELECT * FROM users where id=3 limit 1\n\u0026ndash; 结果集中会自动去重复数据 SELECT DISTINCT Company FROM Orders \u0026ndash; 表 Persons 字段 Id_P 等于 Orders 字段 Id_P 的值， \u0026ndash; 结果集显示 Persons表的 LastName、FirstName字段，Orders表的OrderNo字段 SELECT p.LastName, p.FirstName, o.OrderNo FROM Persons p, Orders o WHERE p.Id_P = o.Id_P\n\u0026ndash; gbk 和 utf8 中英文混合排序最简单的办法 \u0026ndash; ci是 case insensitive, 即 “大小写不敏感” SELECT tag, COUNT(tag) from news GROUP BY tag order by convert(tag using gbk) collate gbk_chinese_ci; SELECT tag, COUNT(tag) from news GROUP BY tag order by convert(tag using utf8) collate utf8_unicode_ci;\nUPDATE Update 语句用于修改表中的数据。 语法：UPDATE 表名称 SET 列名称 = 新值 WHERE 列名称 = 某值\n\u0026ndash; update语句设置字段值为另一个结果取出来的字段 update user set name = (select name from user1 where user1 .id = 1 ) where id = (select id from user2 where user2 .name=\u0026lsquo;小苏\u0026rsquo;); \u0026ndash; 更新表 orders 中 id=1 的那一行数据更新它的 title 字段 UPDATE orders set title=\u0026lsquo;这里是标题\u0026rsquo; WHERE id=1;\nInsert INSERT INTO 语句用于向表格中插入新的行。 语法：INSERT INTO 表名称 VALUES (值1, 值2,\u0026hellip;.) 语法：INSERT INTO 表名称 (列1, 列2,\u0026hellip;) VALUES (值1, 值2,\u0026hellip;.)\n\u0026ndash; 向表 Persons 插入一条字段 LastName = JSLite 字段 Address = shanghai INSERT INTO Persons (LastName, Address) VALUES (\u0026lsquo;JSLite\u0026rsquo;, \u0026lsquo;shanghai\u0026rsquo;); \u0026ndash; 向表 meeting 插入 字段 a=1 和字段 b=2 INSERT INTO meeting SET a=1,b=2; \u0026ndash; SQL实现将一个表的数据插入到另外一个表的代码 \u0026ndash; 如果只希望导入指定字段，可以用这种方法： \u0026ndash; INSERT INTO 目标表 (字段1, 字段2, \u0026hellip;) SELECT 字段1, 字段2, \u0026hellip; FROM 来源表; INSERT INTO orders (user_account_id, title) SELECT m.user_id, m.title FROM meeting m where m.id=1;\n\u0026ndash; 向表 charger 插入一条数据，已存在就对表 charger 更新 type,update_at 字段； INSERT INTO charger (id,type,create_at,update_at) VALUES (3,2,\u0026lsquo;2017-05-18 11:06:17\u0026rsquo;,\u0026lsquo;2017-05-18 11:06:17\u0026rsquo;) ON DUPLICATE KEY UPDATE id=VALUES(id), type=VALUES(type), update_at=VALUES(update_at);\nDELETE DELETE 语句用于删除表中的行。 语法：DELETE FROM 表名称 WHERE 列名称 = 值\n\u0026ndash; 在不删除table_name表的情况下删除所有的行，清空表。 DELETE FROM table_name \u0026ndash; 或者 DELETE * FROM table_name \u0026ndash; 删除 Person表字段 LastName = \u0026lsquo;JSLite\u0026rsquo; DELETE FROM Person WHERE LastName = \u0026lsquo;JSLite\u0026rsquo; \u0026ndash; 删除 表meeting id 为2和3的两条数据 DELETE from meeting where id in (2,3);\nAND 和 OR AND - 如果第一个条件和第二个条件都成立； OR - 如果第一个条件和第二个条件中只要有一个成立；\nAND \u0026ndash; 删除 meeting 表字段 \u0026ndash; id=2 并且 user_id=5 的数据 和 \u0026ndash; id=3 并且 user_id=6 的数据 DELETE from meeting where id in (2,3) and user_id in (5,6);\n\u0026ndash; 使用 AND 来显示所有姓为 \u0026ldquo;Carter\u0026rdquo; 并且名为 \u0026ldquo;Thomas\u0026rdquo; 的人： SELECT * FROM Persons WHERE FirstName=\u0026lsquo;Thomas\u0026rsquo; AND LastName=\u0026lsquo;Carter\u0026rsquo;; OR \u0026ndash; 使用 OR 来显示所有姓为 \u0026ldquo;Carter\u0026rdquo; 或者名为 \u0026ldquo;Thomas\u0026rdquo; 的人： SELECT * FROM Persons WHERE firstname=\u0026lsquo;Thomas\u0026rsquo; OR lastname=\u0026lsquo;Carter\u0026rsquo;\nORDER BY 语句默认按照升序对记录进行排序。 ORDER BY - 语句用于根据指定的列对结果集进行排序。 DESC - 按照降序对记录进行排序。 ASC - 按照顺序对记录进行排序。\n\u0026ndash; Company在表Orders中为字母，则会以字母顺序显示公司名称 SELECT Company, OrderNumber FROM Orders ORDER BY Company\n\u0026ndash; 后面跟上 DESC 则为降序显示 SELECT Company, OrderNumber FROM Orders ORDER BY Company DESC\n\u0026ndash; Company以降序显示公司名称，并OrderNumber以顺序显示 SELECT Company, OrderNumber FROM Orders ORDER BY Company DESC, OrderNumber ASC\nIN IN - 操作符允许我们在 WHERE 子句中规定多个值。 IN - 操作符用来指定范围，范围中的每一条，都进行匹配。IN取值规律，由逗号分割，全部放置括号中。 语法：SELECT \u0026ldquo;字段名\u0026quot;FROM \u0026ldquo;表格名\u0026quot;WHERE \u0026ldquo;字段名\u0026rdquo; IN (\u0026lsquo;值一\u0026rsquo;, \u0026lsquo;值二\u0026rsquo;, \u0026hellip;);\n\u0026ndash; 从表 Persons 选取 字段 LastName 等于 Adams、Carter SELECT * FROM Persons WHERE LastName IN (\u0026lsquo;Adams\u0026rsquo;,\u0026lsquo;Carter\u0026rsquo;)\nNOT NOT - 操作符总是与其他操作符一起使用，用在要过滤的前面。\nSELECT vend_id, prod_name FROM Products WHERE NOT vend_id = \u0026lsquo;DLL01\u0026rsquo; ORDER BY prod_name;\nUNION UNION - 操作符用于合并两个或多个 SELECT 语句的结果集。\n\u0026ndash; 列出所有在中国表（Employees_China）和美国（Employees_USA）的不同的雇员名 SELECT E_Name FROM Employees_China UNION SELECT E_Name FROM Employees_USA\n\u0026ndash; 列出 meeting 表中的 pic_url， \u0026ndash; station 表中的 number_station 别名设置成 pic_url 避免字段不一样报错 \u0026ndash; 按更新时间排序 SELECT id,pic_url FROM meeting UNION ALL SELECT id,number_station AS pic_url FROM station ORDER BY update_at; \u0026ndash; 通过 UNION 语法同时查询了 products 表 和 comments 表的总记录数，并且按照 count 排序 SELECT \u0026lsquo;product\u0026rsquo; AS type, count() as count FROM products union select \u0026lsquo;comment\u0026rsquo; as type, count() as count FROM comments order by count;\nAS as - 可理解为：用作、当成，作为；别名 一般是重命名列名或者表名。 语法：select column_1 as 列1,column_2 as 列2 from table as 表\nSELECT * FROM Employee AS emp \u0026ndash; 这句意思是查找所有Employee 表里面的数据，并把Employee表格命名为 emp。 \u0026ndash; 当你命名一个表之后，你可以在下面用 emp 代替 Employee. \u0026ndash; 例如 SELECT * FROM emp.\nSELECT MAX(OrderPrice) AS LargestOrderPrice FROM Orders \u0026ndash; 列出表 Orders 字段 OrderPrice 列最大值， \u0026ndash; 结果集列不显示 OrderPrice 显示 LargestOrderPrice\n\u0026ndash; 显示表 users_profile 中的 name 列 SELECT t.name from (SELECT * from users_profile a) AS t;\n\u0026ndash; 表 user_accounts 命名别名 ua，表 users_profile 命名别名 up \u0026ndash; 满足条件 表 user_accounts 字段 id 等于 表 users_profile 字段 user_id \u0026ndash; 结果集只显示mobile、name两列 SELECT ua.mobile,up.name FROM user_accounts as ua INNER JOIN users_profile as up ON ua.id = up.use\nJOIN 用于根据两个或多个表中的列之间的关系，从这些表中查询数据。\nJOIN: 如果表中有至少一个匹配，则返回行 INNER JOIN:在表中存在至少一个匹配时，INNER JOIN 关键字返回行。 LEFT JOIN: 即使右表中没有匹配，也从左表返回所有的行 RIGHT JOIN: 即使左表中没有匹配，也从右表返回所有的行 FULL JOIN: 只要其中一个表中存在匹配，就返回行(MySQL 是不支持的，通过 LEFT JOIN + UNION + RIGHT JOIN 的方式 来实现) SELECT Persons.LastName, Persons.FirstName, Orders.OrderNo FROM Persons INNER JOIN Orders ON Persons.Id_P = Orders.Id_P ORDER BY Persons.LastName;\n","id":14,"section":"posts","summary":"快速入门 MySQL 为关系型数据库(Relational Database Management System)，一个关系型数据库由一个或数个表格组成 表头(header): 每一列的名称; 列","tags":[],"title":"Mysql_21mins_tutorial","uri":"https://yanyuLinxi.github.io/2021/11/mysql_21mins_tutorial/","year":"2021"},{"content":"建议看原文，然后进行总结。\n作者：美途旅行链接：https://zhuanlan.zhihu.com/p/367113622 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n因为工作的原因，我前前后后的原因去了五次西安，哈哈，这是有多爱西安，但是也踩了不少的坑，今天我要总结出来，送给想来西安的小伙伴们一些建议，记得点赞收藏哦！去景点建议1、陕西历史博物馆！免费票需要提前3-5天在gzh“陕西历史博物馆”预订，晚了根本预约不到免费票，我们当时买的30元票，亏了亏了!注意周一闭馆，不要白跑一趟、陕博人很多，不允许带三脚架和自拍杆2、千万不要错过3月20日-11月3日的长恨歌表演，长恨歌的表演观赏性非常震撼，强烈推荐，一定注意开放时间哦，今年是3月20日-11月3日表,其他时间不演出，别错过3、大唐不夜城千万不要白天去，白天就是一个步行街啊，不夜城只有晚上才能感受到大唐盛世，有一种穿越的感觉，而且还可以偶遇不倒翁小姐姐4、如果要逛西安城墙，一定要骑车，当时我们因为骑车交租金那里人太多，就想着走到下一个门直接出去了。我们应该走了一个小时，累死人了5、回民街只适合逛，不适合吃!回民街，一个我超级想吐槽的地方，里面基本都是清真，吃不惯的别买，我们是晚上去的，人很多，本地人不会去吃的，专坑外地人6、西安碑林博物馆，里面有很多石碑、墓志等，我们对这个没兴趣，就是来打打卡，也没请讲解，就很无聊!所以千万要请讲解呀!7、建议去看18:00场的大慈恩寺北广场音乐喷泉，是亚洲最大的室外喷泉，要早点去，提前30分钟，抢占三排，拥有全世界规模最大的音响组合的纪录，晚上看效果最好，注意!周二检修不开放!8、西安大唐芙蓉园预约流程:搜“大唐芙蓉园”gzh进行预约，记得预约的时候选好出行日期哦(可预约三日内的门票，每人每天仅可预约一次）唐代贵妃的皇家园林，而晚上有大型水幕灯光表演秀，还可以划船，穿着古装瞬间穿越啊9、永兴坊的摔碗酒，真是排队三小时，摔碗5秒钟，人真的超级多，不建议去体验10、不绕路攻略，能在一起玩的景点!古城墙，钟楼鼓楼，回民街，小雁塔一线比较集中，建议花一天游玩;大雁塔，陕西历史博物馆，芙蓉园一线也比较集中;兵马俑，华清宫，骊山一条线可以安排在一起、离市区比较远11、这些景点不建议去乾陵不建议去，没有特殊之处！华清宫不推荐,贵妃洗澡的池子并没什么好看的西安秦岭地宫展览馆和西安半坡遗址博物馆，建议对历史特别感兴趣的可以去不感兴趣不建议去12、小雁塔皮影戏表演和博物馆门口的幻影成像可看性都不太高。敲钟要收费，旅游纪念品都明显偏贵一些，想买的书院门都有；大雁塔位于大慈恩寺内，门票50元，登塔30元，学生半价，大雁塔是唐玄奘法师的地盘，几个取经打扮的人邀请你一起拍照，千万要拒绝!ps-省钱小妙招 -如果你有西安旅游的计划，我可以推荐一个导游小姐姐不管是报团还是自由行，她都会给你一对一耐心指导和介绍的，我之前踩过一些坑，后面找到这位小姐姐后，把行程安排非常好，避了很多坑，下面就是她的联系方式，现在暑假他们家的结伴自由行有很大的优惠福利，可以加她微信领取点击添加佳佳微信免费咨询详细行程：13201682435 （复制打开微信添加）点击跳转至第三方点击链接，添加本地导游微信，即可了解更多详情​lvyou.sltjiancai.com/xian交通上的建议13、千万不要买到西安南站的票!因为西安南站距离市区很远且交通不是很方便，建议买西安站或者西安北站的票出站就是地铁和公交站，非常方便14、别坐黑车！一下高铁站火车机场外面，外面有很多揽客的人，那些都是黑车，专坑外地人，千万别去坐,直接去地铁站或公交站15、不建议乘坐西安公交！西安虽然不大,但是西安的交通真的很堵，坐公交车容易堵车，出行尽量选择地铁，坐公交车容易浪费时间、优选地铁，不堵车，在ZFB搜索“长安电子卡”扫码就可以乘坐啦16、去兵马俑可以选择包车，价格不等，但注意询问清楚司机有没有额外费用;在火车站东广场公交车乘坐5路，可以直接到达(首班七点，未班19点)在旅行中的建议17、一定要提前准备好健康码，特殊时期，不要不做准备就直接去西安提前准备好`“西安绿码”，在VX/zFB搜索“西安市民一码通，地址填写你订的酒店或者民宿的地址就可18、不要穿高跟鞋，双脚全废，古都议带双平底鞋，虽然西安的路都比较平，但是旅游每天走2-3万步，舒适的鞋子会让你西安之旅更愉快19、有学生证的宝宝一定要带上，可以给你剩一半的钱，西安学生证可以享受景区半价，一定要带哟20、一定要提前预定民宿，不是节假日100多就可以享受在家的温暖住宿，首选钟楼、大雁塔附近的酒店，交通方便，离各个景点都比较近去吃美食的建议21、回民街的羊肉泡，拿了两个馍给我们自己掰，过程挺有趣的。然后再给老板帮我们做好，味道怪怪的，我们几个人都吃不来，是可以体验一下，建议几个人吃一份试试，喜欢吃再多买22、回民街的凉皮，我不知道是因为麻酱还是什么原因，吃起来酸酸的，还有点辣舌头，我反正不喜欢，总体来说在回民街踩的坑比较多23、满街上都写着老米家泡馍老店,傻傻分不清楚，进去一定会踩雷、到现在我也没吃到过特别好吃的羊肉泡馍，尝试一下就行了，因为是回民吃的，一般人可能吃不惯24、西安的大多数餐馆都不提供餐纸，大家要自己带好纸巾，不然去餐馆买会很贵，西安的服务态度比南方要差纪多千万不要对服务有很高的期望25、西安的美食分量真的超级大，特别是面食，女孩子2人吃一份才是正确方式，不然都得吃半碗剩下半碗，太浪费了26、不要被西安的辣椒颜色而影响，看着辣椒很多,实际一点都不辣，他们的辣子只是香，如果你是川渝贵或者湖南，能吃辣的可以挑战他们的最辣27、别去装修很高大上的地方吃陕菜陕菜很多，千篇一律的唐风，我吃了几次还是觉得钟楼附近的三根电杆陕菜馆，地道的陕菜味，吃的是儿时的回忆，推荐葫芦鸡28、可以去回民街逛、但是不建议去吃小吃，价格贵，味道还一般，回民街都是给外地人吃的，怕踩雷就去洒金桥、大皮院和北广济街等街区行程推荐Day1 ：抵达古都西安，接站入住,自由活动。 Day2：寻古之旅：兵马俑—华清宫（华清池）—骊山Day3：探险之旅：五岳之一——西岳华山一整天Day4：文化之旅：西安古城墙—钟鼓楼广场—大雁塔广场—回民街—下午自由活动Day5：回归温暖的小家\n","id":15,"section":"posts","summary":"建议看原文，然后进行总结。 作者：美途旅行链接：https://zhuanlan.zhihu.com/p/367113622 来源：知乎 著作权归","tags":[],"title":"西安攻略1","uri":"https://yanyuLinxi.github.io/2021/11/%E8%A5%BF%E5%AE%89%E6%94%BB%E7%95%A51/","year":"2021"},{"content":" 科大讯飞 X光检测  https://mp.weixin.qq.com/s/H0NMXOAj6A7jpdpyFsKt_A\n 水下目标检测  https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==\u0026amp;mid=2247498164\u0026amp;idx=3\u0026amp;sn=918d151c748c123e27812ef8ece23bb6\u0026amp;chksm=f9a18b3bced6022d9a7a0c1b9164acfebea33c437656dee662bc6fbb447cd7f902e5976e5c72\u0026amp;mpshare=1\u0026amp;scene=1\u0026amp;srcid=\u0026amp;sharer_sharetime=1587119860977\u0026amp;sharer_shareid=42a896371dfe6ebe8cc4cd474d9b747c\u0026amp;exportkey=AfuYjLAdEIfOR%2FIuYRD8dpM%3D\u0026amp;pass_ticket=781k4y6q5ReAogsulOWaRaDoMxACZW%2F%2FDMiqguAJb7T0aup7%2BNgkdVxCQ9fTKGHF#rd\n 详细思路介绍kaggle  https://zhuanlan.zhihu.com/p/25742261\n  Transformer 典型应用 https://mp.weixin.qq.com/s/KFVN9PmYMMvV60aM1_QQzw\n  ATEC比赛\n  这两个比赛writeup\nhttps://www.zhihu.com/search?q=atec\u0026amp;type=content https://mp.weixin.qq.com/s/F9GLj7NtnXLBWTVdpXlTAQ https://github.com/minghaochen/ATEC2021-Track1/blob/master/train.py 代码\n","id":16,"section":"posts","summary":"科大讯飞 X光检测 https://mp.weixin.qq.com/s/H0NMXOAj6A7jpdpyFsKt_A 水下目标检测 https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==\u0026amp;mid=2247498164\u0026amp;idx=3\u0026amp;sn=918d151c748c123e27812ef8ece23bb6\u0026amp;chksm=f9a18b3bced6022d9a7a0c1b9164acfebea33c437656dee662bc6fbb447cd7f902e5976e5c72\u0026amp;mpshare=1\u0026amp;scene=1\u0026amp;srcid=\u0026amp;sharer_sharetime=1587119860977\u0026amp;sharer_shareid=42a896371dfe6ebe8cc4cd474d9b747c\u0026amp;exportkey=AfuYjLAdEIfOR%2FIuYRD8dpM%3D\u0026amp;pass_ticket=781k4y6q5ReAogsulOWaRaDoMxACZW%2F%2FDMiqguAJb7T0aup7%2BNgkdVxCQ9fTKGHF#rd 详细思路介绍kaggle https://zhuanlan.zhihu.com/p/25742261 Transformer 典型应用 https://mp.weixin.qq.com/s/KFVN9PmYMMvV60aM1_QQzw ATEC比赛 这两个比赛writeup https://www.zhihu.com/search?q=atec\u0026amp;type=content https://mp.weixin.qq.com/s/F9GLj7NtnXLBWTVdpXlTAQ https://github.com/minghaochen/ATEC2021-Track1/blob/master/train.py 代码","tags":[],"title":"案例目录","uri":"https://yanyuLinxi.github.io/2021/11/_%E6%A1%88%E4%BE%8B%E7%9B%AE%E5%BD%95/","year":"2021"},{"content":"本篇地址 https://mp.weixin.qq.com/s/H0NMXOAj6A7jpdpyFsKt_A\n一、赛题背景 X光安检机是目前我国使用最广泛的安检技术手段，广泛应用于城市轨交、铁路、机场、重点场馆、物流寄递等场景。使用人工智能技术，辅助一线安检员进行X光安检判图，可以有效降低因为人员疲劳或注意力不集中带来的漏报等问题。但在实际场景中，因物品的多样性、成像角度、遮挡等问题，为算法的开发带来了一定的挑战。\nhttp://challenge.xfyun.cn/topic/info?type=Xray-2021\n赛题内容 赛题数据组成\n  初赛：\n1）带标注的训练数据，即待识别物品在包裹中的X光图像及其标注文件；\n2）不带标注的测试数据；\n  复赛：\n1）无标注训练数据即包裹X光图像（其中有的包裹包含待识别物品）；\n2）部分待识别物品X光图像（无背景）；\n  目标类别：\n刀、剪刀、尖锐工具、甩棍、小玻璃瓶、电棍、塑料饮料瓶、带喷嘴塑料瓶、电子设备、电池、公章、伞， 共12类。\n  模型评价指标\nwAP50，即各个类别的AP50按照权重进行加权的结果。\n其中各类别权重为： 刀1、剪刀1、尖锐工具1、甩棍1、小玻璃瓶1、电棍1、塑料饮料瓶0.7、带喷嘴塑料瓶0.7、电子设备0.7、电池0.7、公章0.7、伞0.7。\n  模型大小600M以内\n  分析  赛题数据中，提供了大量的无标注数据，利用好这些无标注数据进行半监督学习是关键。 数据可视化发现数据背景较复杂且差异较大，设计合适的数据增强方法是关键。 模型评价指标为AP50，因此更关注于模型的分类效果。 在模型大小范围内，允许进行一定的模型融合。  数据增强   数据均衡\n发现数据严重不平衡。所以需要对数据进行平衡。\n思想：让每一类样本数尽量一样\n方法：数量多的不变，数量少的多采样几次\n操作：\n 统计每个类别bbox数量，做归一化得到n 类别采样次数= max（1，oversample_thr/n）  类别样本量足够多时，则oversample_thr/n是小于1的，则原封不动选取 不足时，多采样几次   图片实际的采样次数等于图片中最大的类别采样次数    数据增强\n对图像数据进行变换，增强数据量，增强泛化性\n方法： 随机反转RandomFlip、随机90°旋转RandomRotate。\n几何层面的数据增强一般都能提升模型性能，比较稳定。X光图像对于色彩比较敏感，因此常见的color层面的数据增强经测试基本没什么效果。\n  MixUpObject\n可供训练的数据只有带标签的训练数据，为了提升模型对前景的识别能力，在训练期间，从训练集中随机选取一张图片的目标bbox，通过mixup的方式粘贴到正在训练的样本上。\n复赛阶段：官方提供了一些目标的patch，因此训练时可以直接将目标patch给mixup到正在训练的样本上。\nmixup效果如图所示（看起来贴的一般，但训起来好啊图片）\n资料：\n mixup 利用多目标检测方案。将一些图片贴到上面。相当于增强学习能力。 mixup邻域分布可以被理解为一种数据增强方式，它令模型在处理样本和样本之间的区域时表现为线性。我们认为，这种线性建模减少了在预测训练样本以外的数据时的不适应性。 mixup常在alpha层进行粘贴，loss可以加权或者拼接。拼接更简单。    FixScaleResize\n结合X光安检图像的成像特点，由于设备限制，其光源到物品的目标的距离是在一定范围内的，因此同一类别的目标的尺寸差异不会特别大。这一点和自然场景下的目标有较大不同，自然场景下的目标是有近大远小的情况的，也就是同一目标在不同距离的成像上，尺度可能会有非常大的差异。\n因此，在进行多尺度训练时，首先需要统计数据集中同一类别目标的面积差异分布，然后据此设计出大致的缩放范围，再进行消融实验找到最佳的缩放尺度。\n基本步骤：\n首先，以图片原始大小为基准，设置缩放比例范围为(1.5, 3.5)进行图片和目标的缩放；\n然后，设置最大缩放面积，对于缩放后超出最大面积的图片，使用最大面积进行截断处理。\n  StackImage\n在比赛后期，由于提供了大量的无标签数据，因此自然想到为无标签数据生成伪标签来进行半监督学习，为此我们开发了StackImage数据增强方法，其目的在于：\na. 增强样本多样性\nb. 学习无标签数据上的前景和背景信息\nc. 通过拼接强监督信息和弱监督信息，达到弱化伪标签中噪音数据的目的。\n基本步骤：\na. 同时取一张带标签的图片和一张无标签的图片，无标注图片使用半监督标注信息\nb. 将两张图像以水平或垂直的方式进行stack，方向按照面积最小的原则\nc. 无法完全对齐的地方用255填充\n  模型选择和训练   模型选择\n检测框架：mmdetection\n检测模型：\n虽然目前swin-transformer很火，但是由于对其不是非常熟悉，另外transformer系列模型训练一般都需要较长时间和较大的GPU显存，因此选用二阶段经典网络faster-rcnn作为基线模型。\n这里没有使用去年冠军方案使用的cascade-rcnn作为检测模型基线，主要是考虑到以下几点：\na. cascade-rcnn主要是对与gt的iou大于0.5的bbox的进一步优化坐标，对AP50的提升贡献较小。\nb. cascade-rcnn模型较大，不利于后期模型融合策略的使用。\nc. cascade-rcnn模型占用显存较大，且需要更长的训练时间。\nbackbone选择：\nres2net101，不解释了，又强又快。\n模型选择小技巧：根据经验，coco检测模型预训练相比于imagenet分类预训练有更好的效果，因此优先选择mmdetection中有coco检测预训练权重的模型。\n总结：\n 尽量使用框架来训练。减少编码负担。 对赛题进行分析选择baseline，不是拿到手就用。    Tricks\n基本调参，根据对赛题的分析和对赛题的理解，对模型内相关参数尽量调整，比如学习率、rcnn正负样本采样数量、学习率衰减策略等。\n模型组件\u0026ndash;FPN，增强对小目标的识别能力，一般mmdetection实现的fasterrcnn有自带。\n模型组件\u0026ndash;DCN，DCN一般都会有比较好的效果，但是也会增加较长训练时间，可以考虑在比赛的后期再加上\n模型组件\u0026ndash;GC，有类似于空间注意力机制的作用。\n训练策略\u0026ndash;Class Loss Weight，由于AP50的指标更看重模型的分类性能，因此可以适当调大分类损失的权重，经过实验由1调整至1.25效果较好。\n预训练权重\u0026ndash;CocoPretrain，相比较于仅使用ImageNet的分类模型预训练，使用COCO的检测模型预训练能稳定涨点。\n模型压缩\u0026ndash;FP16，本次比赛有模型大小限制，因此在模型训练之后将其权重由FP32转变为FP16，其大小能降低一半。\n  半监督学习 本次比赛提供了大量的无标签数据，如表所示，可以看到有四批数据共15000张无标签数据，仅4000张有标签数据，因此如何使用半监督学习的方式利用好这15000张半监督数据样本，也是本次比赛的关键。\n基本流程有以下步骤： a. 首先使用有标签数据，训练出一个较好的模型，然后在无标签数据上推理，得到伪标签，并使用阈值进行过滤掉分数低的目标框。 b. 然后重新训练模型，将有标签的样本和伪标签的样本使用StackImage的方式进行拼接，然后送入到模型进行训练。 c. 训练后的模型，在测试集上效果变的更好了，再使用这个模型重新生成无标注数据的伪标签并进行阈值过滤，然后再重复上述训练过程。 d. 直至模型在测试集上的分数不再上升为止。  模型融合   模型内部融合\n模型内部融合我们采取的策略是结合图像尺度和数据增强，其实也就是TTA。(Test-Time Augmentation) 训练时数据增强。\n在尺度方面，在训练时设置的(1.5, 3.5)范围内选择多个尺度进行消融实验，最终确定使用2.0, 2.5, 3.0的缩放比例，然后分别进行模型推理。\n在数据增强方面，在不同的尺度下，进行verticalFlip、Rotate90、Horizontal Flip数据增强，然后进行模型推理。\n最终将得到的多尺度多数据增强的推理结果进行融合。 // 多尺度 多数据增强 的结果融合。\n  模型之间的融合\n比赛最后发现，仅使用res2net101-fasterrcnn单模型TTA就已经能够稳坐第一的位置，但是为了能有更好的成绩，我们选择在比赛限制内，融合更多的模型。\n由于比赛限制模型大小600M，因此在这个范围内，经过了FP16的压缩，可以进行以下三个模型的融合：\na. res2net101-fasterrcnn\nb. resnext101_32x4d-fasterrcnn\nc. resnext101_64x4d-fasterrcnn\n使用这个三个模型，分别完成上述的模型训练过程，然后各自进行模型内部融合，将各自的融合结果进行模型之间的融合。\n  模型融合方法\n模型融合采用WBF融合策略，如下图所示。 // WBF 加权融合。 将bbox的坐标根据置信度加权平均，将置信度平均。\n注意：针对不同的任务，WBF的过程需要一定程度的调参实验，可得到较好的效果。\n  总结 模型选择\n基线模型：Faster-RCNN\nBackBone：Res2Net101、Resnext101_32×4d、Resnext101_64×4d\n检测框架：MMDetection\n主要技术\n数据增强：数据平衡、StackImage、 MixupObject、FixScale\nTricks：半监督学习、DCN、 Global Context、COCO预训练、TTA、 WBF、ClsLossWeight\n思考  充分理解赛题和评价指标，充分理解经典模型的原理及其适用场景，以便更好地进行模型选型。 关注新技术，决赛时发现大部分选手都使用了swin-transformer，说明应该还是很强的。 充分理解数据，决赛时才知道domain5、domain6是来自两个不同X光机器的图片，如果更细致去分析其中的特点进行相关工作，可能会有更好的效果。 充分理解评价指标，可以看到计算AP50时，不同类别的权重是不一样的，这一点暂时没想好更好的办法利用这一点得到更好的结果。  备注知识  bbox。在目标检测里，我们通常使用边界框（bounding box）来描述目标位置。 WBF 加权融合。 将bbox的坐标根据置信度加权平均，将置信度平均。 TTA (Test-Time Augmentation)。 通过训练的时候，对数据进行增强，来强化效果。  总结  基线模型并不复杂。但是数据清理很重要。包括对数据进行增强也很重要。 使用Tricks等，帮助模型提高效果。 思路很直接，发现问题，穷追问题，解决问题。  数据类别不均衡。=》平衡类别 样本少+常用tricks =》 mixup贴图到前景图片上 目标有近大远小的特点，但X光大小较位相同。但不完全一样 =》 所以统计同一类目标的差异分布，再找到最佳的缩放尺度。 又有了大量的无标签数据。 =》 寻找方式进行stack。根据多目标检测的特征，将无标签数据和标签数据stack在一张图里 避免采用不熟悉的算法，对算法熟悉才能使用更好的算法 = 》 选用res2net 模型调参，知道哪些参数对当前任务效果贡献更大。 =》进一步调参 又来了无标签数据 =\u0026gt; 伪标签 模型融合 =》 内部融合、模型之间融合、融合方法的确定   你自己总结一边就知道，全部都是对症下药。发现问题解决问题。这才是为什么能拿高分的思路。 所以首先对这种问题足够熟悉，第二对相关技术、模型熟悉，第三才能发现问题，针对问题进行改进。  ","id":17,"section":"posts","summary":"本篇地址 https://mp.weixin.qq.com/s/H0NMXOAj6A7jpdpyFsKt_A 一、赛题背景 X光安检机是目前我国使用最广泛的安检技术手段，广泛应用于城市轨交、铁路、机场、重点场馆、物流寄递等场景。使用人工智能技","tags":[],"title":"科大讯飞 X光检测","uri":"https://yanyuLinxi.github.io/2021/11/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9E-x%E5%85%89%E6%A3%80%E6%B5%8B/","year":"2021"},{"content":" 概率论意义 随机变量 概率分布 边缘概率 条件概率 条件概率的链式法则 独立性和条件独立性 期望、方差、协方差 常用概率分布 常用函数的有用性质 贝叶斯规则 连续型变量的技术细节 信息论 结构化概率模型  概率论是用于表示不确定性声明的数学框架。它不仅提供了量化不确定性的方 法，也提供了用于导出新的不确定性声明（statement）的公理\n概率法则告诉我们 AI 系统如何推理。其次，我们可以用概率和统计从 理论上分析我们提出的 AI 系统的行为\n概率论意义  几乎所有的活动都需要一些在不确定性存在的情况下进行推理的能力 不确定性有三种可能的来源  被建模系统内在的随机性 不完全观测。 不完全建模。当我们使用一些必须舍弃某些观测信息的模型时，舍弃的信息会 导致模型的预测出现不确定性。   使用一些简单而不确定的规则要比复杂而确定的规则更为实用  ‘多数鸟儿都会飞’’ 这个简单的规则描述起来很简单很并且使用广泛 ‘除了那些还没学会飞翔的幼鸟，因为生病或是受伤而失去了飞翔能力的 鸟，包括食火鸟 (cassowary)、鸵鸟 (ostrich)、几维 (kiwi，一种新西兰产的无翼鸟)等不会飞的鸟类……以外，鸟儿会飞’’，很难应用、维护和沟通，即使经过这么多的 努力，这个规则还是很脆弱而且容易失效。   频率派概率和 贝叶斯概率  当我 们说一个结果发生的概率为 p，这意味着如果我们反复实验 (例如，抽取一手牌) 无限次，有 p 的比例可能会导致这样的结果 在医生诊断病人的例 子中，我们用概率来表示一种信任度（degree of belief）， 前面那种概率，直接与事件发生的频 率相联系，被称为频率派概率（frequentist probability）； 而后者，涉及到确定性水 平，被称为贝叶斯概率（Bayesian probability） 表征信任度的概率，我们称为贝叶斯概率。   概率可以被看作是用于处理不确定性的逻辑扩展。逻辑提供了一套形式化的规 则，可以在给定某些命题是真或假的假设下，判断另外一些命题是真的还是假的。概率论提供了一套形式化的规则，可以在给定一些命题的似然后，计算其他命题为真的似然。  似然：likelihood 即文言文版的可能性。    随机变量  随机变量（random variable）是可以随机地取不同值的变量  例如，x1 和 x2 都是随机变量 x 可能的取值 向量值变量，我们会将随机变量写成 x    概率分布  概率分布（probability distribution）用来描述随机变量或一簇随机变量在每一 个可能取到的状态的可能性大小。我们描述概率分布的方式取决于随机变量是离散 的还是连续的 离散型变量的概率分布可以用概率质量函数（probability mass function, PMF）我们通常用大写字母 P 来表示概率质量函数  有时为了使得PMF的使用不相互混淆，我们会明确写出随 机变量的名称：P(x = x)。 有时我们会先定义一个随机变量，然后用 ∼ 符号来说明它遵循的分布：x ∼ P(x)   这种多个变量的概率分布被称 为联合概率分布（joint probability distribution）。P(x = x, y = y) 表示 x = x 和 y = y 同时发生的概率 如果一个函数 P 是随机变量 x 的 PMF，必须满足下面这几个条件：  P 的定义域必须是 x 所有可能状态的集合。 ∀x ∈ x, 0 ≤ P(x) ≤ 1. 不可能发生的事件概率为 0，并且不存在比这概率更低 的状态。 ∑ x∈x P(x) = 1. 我们把这条性质称之为归一化的（normalized）。   考虑一个离散型随机变量 x 有 k 个不同的状态。我们可以假设 x 是**均匀 分布（uniform distribution）**的。通常用 x ∼ U(a, b) 表示 x 在 [a, b] 上是均匀分布的。 连续型变量和概率密度函数  当我们研究的对象是连续型随机变量时，我们用概率密度函数（probability density function, PDF） 如果一个函数 p 是概率密度函数，必须满足下面这几个条件  p 的定义域必须是 x 所有可能状态的集合 ∀x ∈ x, p(x) ≥ 0. 注意，我们并不要求 p(x) ≤ 1。 ∫p(x)dx = 1.   概率密度函数 p(x) 并没有直接对特定的状态给出概率，相对的，它给出了落在 面积为 δx 的无限小的区域内的概率为 p(x)δx。    边缘概率  但想要了解其中一个子集的概 率分布。这种定义在子集上的概率分布被称为边缘概率分布（marginal probability distribution）。  条件概率  在很多情况下，我们感兴趣的是某个事件，在给定其他事件发生时出现的 概率。这种概率叫做条件概率。 条件概率只在 P(x = x) \u0026gt; 0 时有定义 计算一个行动的后果被称为干预 查询（intervention query）。干预查询属于因果模型（causal modeling）的范畴，我 们不会在本书中讨论  条件概率的链式法则  任何多维随机变量的联合概率分布，都可以分解成只有一个变量的条件概率相 乘的形式： P(x(1), . . . , x(n)) = P(x(1))Πn i=2P(x(i) | x(1), . . . , x(i−1)). 这个规则被称为概率的链式法则（chain rule）或者乘法法则（product rule）。  独立性和条件独立性  两个随机变量 x 和 y，如果它们的概率分布可以表示成两个因子的乘积形式，并 且一个因子只包含 x 另一个因子只包含 y，我们就称这两个随机变量是相互独立的 那么这两个随机变量 x 和 y 在给定随机变量 z 时是条件独立的（conditionally 我们可以采用一种简化形式来表示独立性和条件独立性：x⊥y 表示 x 和 y 相互 独立，x⊥y | z 表示 x 和 y 在给定 z 时条件独立。  期望、方差、协方差  函数 f(x) 关于某分布 P(x) 的期望（expectation）或者期望值（expected value）是指，当 x 由 P 产生，f 作用于 x 时，f(x) 的平均值 我们假设 E[·] 表示对方括号内的所有随机变量的值求平均。 类似的，当没有歧义时，我们还可以省略方括号。 **方差（variance）**衡量的是当我们对 x 依据它的概率分布进行采样时，随机变 量 x 的函数值会呈现多大的差异：Var(f(x))  当方差很小时，f(x) 的值形成的簇比较接近它们的期望值。方差的平方根被称为标准差（standard deviation）。   **协方差（covariance）**在某种意义上给出了两个变量线性相关性的强度以及这些 变量的尺度  协方差的绝对值如果很大则意味着变量值变化很大并且它们同时距离各自的均值很 远。 如果协方差是正的，那么两个变量都倾向于同时取得相对较大的值 如果协方 差是负的，那么其中一个变量倾向于取得相对较大的值的同时，另一个变量倾向于取得相对较小的值， 它们是有联系的，因为 两个变量如果相互独立那么它们的协方差为零，如果两个变量的协方差不为零那么它们一定是相关的 两个变量相互依赖但具有零协方差是可能的。例如，假 设我们首先从区间 [−1, 1] 上的均匀分布中采样出一个实数 x。然后我们对一个随机 变量 s 进行采样。s 以 12 的概率值为 1，否则为-1。    常用概率分布  Bernoulli 分布（Bernoulli distribution）是单个二值随机变量的分布。它由单 个参数 ϕ ∈ [0, 1] 控制，ϕ 给出了随机变量等于 1 的概率。 Multinoulli 分布（multinoulli distribution）或者范畴分布（categorical distribution） 实数上最常用的分布就是正态分布（normal distribution），也称为高斯分布（Gaussian distribution）  正态分布由两个参数控制，µ ∈ R 和 σ ∈ (0,∞) 当我们由于缺乏关于某个实 数上分布的先验知识而不知道该选择怎样的形式时，**正态分布是默认的比较好的选择，**其中有两个原因。  第一，我们想要建模的很多分布的真实情况是比较接近正态分布的。中心极限 定理（central limit theorem）说明很多独立随机变量的和近似服从正态分布。 第二，在具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大 的不确定性。因此，我们可以认为正态分布是对模型加入的先验知识量最少的分布。     我们常常把协方差矩阵固定成一个对角阵。一个更简单的版本是各向同性（isotropic）高斯分布，它的协方差矩阵是一个标量乘以单位阵。 指数分布和 Laplace 分布  我们经常会需要一个在 x = 0 点处取得边界点 (sharp point) 的 分布。为了实现这一目的，我们可以使用指数分布 一个联系紧密的概率分布是Laplace 分布（Laplace distribution），它允许我们 在任意一点 µ 处设置概率质量的峰值   在一些情况下，我们希望概率分布中的所有质量都集中在一个点上。这可以通 过Dirac delta 函数（Dirac delta function）δ(x) 定义概率密度函数来实现。 通过组合一些简单的概率分布来定义新的概率分布也是很常见的。一种通用的组 合方法是构造混合分布（mixture distribution）。 一个非常强大且常见的混合模型是高斯混合模型（Gaussian Mixture Model）， 它的组件 p(x | c = i) 是高斯分布  高斯混合模型的参数指明了给每个组件 i 的先验概率 （prior probability）αi = P(c = i)。 ‘‘先验’’ 一词表明了在观测到 x 之前传递给模 型关于 c 的信念 P(c | x) 是后验概率（posterior probability），因为它 是在观测到 x 之后进行计算的   高斯混合模型是概率密度的万能近似器（universal approximator），在这种意义下，任何平滑的概率密度都可以用具有足够多组件的高斯混合模型以任意精度来逼近。  常用函数的有用性质  sigmoid 函数 在变量取绝对值非常大的正值或负值时会出现饱和（saturate）现象，意味着函数会变得很平，并且对输入的微小改变会变得不敏感 另外一个经常遇到的函数**softplus **函数（softplus function）  softplus 函数名来源于它是另外一个函数的平滑（或 ‘‘软化’’）形式，这个函数是  x+ = max(0, x).      贝叶斯规则  我们经常会需要在已知 P(y | x) 时计算 P(x | y)。幸运的是，如果还知道 P(x)， 我们可以用贝叶斯规则（Bayes’ rule）来实现这一目的：  它通常使用 P(y) = 所以我们并不需要事先知道 P(y) 的信息。    连续型变量的技术细节  对于我们的目的，测度论更多的是用来描述那些适用于 Rn 上的大多数点，却不 适用于一些边界情况的定理。测度论提供了一种严格的方式来描述那些非常微小的点集。这种集合被称为 “零测度（measure zero）’’ 的 另外一个有用的测度论中的术语是 “几乎处处（almost everywhere）’’。某个性 质如果是几乎处处都成立的，那么它在整个空间中除了一个测度为零的集合以外都是成立的。  信息论  信息论的基本想法是一个不太可能的事件居然发生了，要比一个非常可能的事 件发生，能提供更多的信息。  非常可能发生的事件信息量要比较少，并且极端情况下，确保能够发生的事件 应该没有信息量。 较不可能发生的事件具有更高的信息量。 独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量， 应该是投掷一次硬币正面朝上的信息量的两倍。   我们定义一个事件 x = x 的自信息（self-information）  I(x) = −logP(x).   自信息只处理单个的输出。我们可以用香农熵（Shannon entropy）来对整个概 率分布中的不确定性总量进行量化  当 x 是连续的，香农熵被称为微分熵（differential entropy）。 说明了更接近确定性的分布是如何具有较低的香农熵，而更 接近均匀分布的分布是如何具有较高的香农熵。   如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可 以使用KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异：  KL 散度衡量的是，当我们使用一种被设计成能够使 得概率分布 Q 产生的消息的长度最小的编码 KL 散度为 0 当且仅当 P 和 Q 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是 ‘‘几乎处处’’ 相同的   个和 KL 散度密切联系的量是交叉熵（cross-entropy）  结构化概率模型  由 一些可以通过边互相连接的顶点的集合构成。当我们用图来表示这种概率分布的分 解，我们把它称为结构化概率模型（structured probabilistic model）或者图模型（graphical model）。 有向（directed）模型使用带有有向边的图，它们用条件概率分布来表示分解， **无向（undirected）模型使用带有无向边的图，**它们将分解表示成一组函数 随机变量的联合概率与所有这些因子的乘积成比例（proportional）——意味着因子的值越大则可能性越大 这些图模型表示的分解仅仅是描述概率分布的一种语言。它们不是互 相排斥的概率分布族。有向或者无向不是概率分布的特性；它是概率分布的一种特 殊描述（description）所具有的特性  ","id":18,"section":"posts","summary":"概率论意义 随机变量 概率分布 边缘概率 条件概率 条件概率的链式法则 独立性和条件独立性 期望、方差、协方差 常用概率分布 常用函数的有用性质 贝叶斯规则 连续","tags":[],"title":"2 概率与信息论","uri":"https://yanyuLinxi.github.io/2021/11/2-%E6%A6%82%E7%8E%87%E4%B8%8E%E4%BF%A1%E6%81%AF%E8%AE%BA/","year":"2021"},{"content":"学习路线 kaggle要怎么学？\n kaggle相关技术。当空闲时确定一个kaggle技术进行学习 kaggle比赛总结。 kaggle比赛尝试。  kaggle资料  Kaggle 竞赛复盘汇总：  https://mp.weixin.qq.com/mp/appmsgalbum?__biz=MzIwNDA5NDYzNA==\u0026amp;action=getalbum\u0026amp;album_id=1380279189986787330\u0026amp;scene=173\u0026amp;from_msgid=2247495639\u0026amp;from_itemidx=1\u0026amp;count=3\u0026amp;nolastread=1#wechat_redirect   经验贴  https://zhuanlan.zhihu.com/p/25742261    一般流程总结 流程：\n 首先最开始的时候对数据进行处理：  将数据处理成pandas（详细学习使用pandas） 查看是否有NAN值 查看是否有异常值 查看数据的众数 画图看数据的分布。使用log查看分布差异较大的数。 查看数据和label的相关性 // 重要，因为数据和label紧密相关，就应该好好对待。   针对不同的数据使用不同的算法：  特征数据：  变换为图像，然后使用CV相关思路。1d CNN, Resnet 1d等   文本数据  使用bert对数据进行预训练。  预训练思路有很多，包括使用masked在文本上微调。 或者使用文本分类，直接运算。   使用tf-idf+svd分析文本中的词频。 使用ldm对文本主题进行分析。   归一化处理  很多树模型不需要归一化，但是神经网络模型大多必需归一化。归一化也有很多，高斯归一化等等，了解他们之间的差异。     然后快速确定一个baseline  根据评分公式计算评分。方便后面评价模型 对数据进行训练集、测试集、验证集的分类。  不要觉得这样分类，减少了数据集会影响分类，其实不是的。可以帮助前期自己对模型的验证。   常用的树模型：lgbm、catboost、xgboost。等快速的api可以帮助快速出一个api 常用的神经网络模型。MLP等。 使用常用的ML框架：Pycaret等。   在baseline上进行改进  前期脑洞大开，后期小心谨慎  包括ms等方法只能提升微小且不可忽略的进步，但不能依靠这个就大幅度增长分数。前期必须要脑洞大开。后期必须小心谨慎   双塔模型。  双塔最初用在推荐系统上，将人的行为特征和商品的行为特征进行匹配。在这里我们可以分别使用不同的模型对不同类型的特征进行建模。这是十分重要的。不同类型的数据显然不应该放在一起进行训练、测试。   对特征进行重新提取、筛选。 伪标签技术。  使用正负置信度为0.9-0.99的标签二次加入训练 使用所有标签加入训练，但是为伪标签使用不同的权重矩阵。权重取决于它们的loss函数。     后期优化：  使用meta_stacking对模型进行搭建。  这个时候可以用上之前的验证集了。使用验证集训练元学习器。   对选定的模型进行调参。使用GridSearch或者使用贝叶斯调参   整体注意事项  需要动手去写的东西并不多，更多的是思路。题目一样的，解法一样，如何取得更好的分数呢？ 数据特征往往很重要，是训练的基石。 不要全身心投入去做一件事。多件事情交叉，让自己的思路打开。 不要在程序开始就进行优化。边写脚本，边对模块进行集成。在写模块时，尽量和现有的框架的api进行匹配，减少记忆负担。对看到比较好的方法进行记录。    Notes:\n 学好pandas 学好seaborn 学好bert模型。对transform系统的学习，能够手撸代码。 以后学习不能不求甚解。对不了解的一定要了解清楚。刨根问底。如果一个知识点足够重要，就开一个md专门写。 多少数据维度是大维度？心里没概念。多看看别人的经验贴，总结能够明白的信息。总结不能明白的信息。要多多调参。  失败总结经验：\n bert没用熟，很多框架第一次上手，完全不知道怎么弄。 不清楚nn网络能有这么大的效果。 以为不是靠调参解决的，实际上就是靠调参解决的。 调参经验不足。就是经验太少。 多少数据维度是大维度？心里没概念。多看看别人的经验贴，总结能够明白的信息。总结不能明白的信息。  工具记录  时序工具。 https://mp.weixin.qq.com/s/sO-Od9x_QH27zJOg6e_FKg  ","id":19,"section":"posts","summary":"学习路线 kaggle要怎么学？ kaggle相关技术。当空闲时确定一个kaggle技术进行学习 kaggle比赛总结。 kaggle比赛尝试。 ka","tags":[],"title":"Kaggle记录总结","uri":"https://yanyuLinxi.github.io/2021/11/kaggle%E6%80%9D%E8%B7%AF%E6%80%BB%E7%BB%93%E5%92%8C%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99/","year":"2021"},{"content":"这里给出祖传代码list  使用高斯归一化 使用贝叶斯搜寻参数 常用的utils。包括归一化函数、pytorch训练等。 使用bert进行masked 预训练。  编写祖传代码  尽量高类聚，低耦合的代码。提高可复用性。意思就是，我是用这个函数，可以快速适配。  ","id":20,"section":"posts","summary":"这里给出祖传代码list 使用高斯归一化 使用贝叶斯搜寻参数 常用的utils。包括归一化函数、pytorch训练等。 使用bert进行masked","tags":[],"title":"Kaggle祖传代码","uri":"https://yanyuLinxi.github.io/2021/11/kaggle%E7%A5%96%E4%BC%A0%E4%BB%A3%E7%A0%81/","year":"2021"},{"content":"大佬经验总结 一位master： 网址:https://mp.weixin.qq.com/s?__biz=Mzk0NDE5Nzg1Ng==\u0026amp;mid=2247490499\u0026amp;idx=1\u0026amp;sn=b9b21c447a31c9532b1ce66961ae0b20\u0026amp;chksm=c329024cf45e8b5a2834a0be9c50420c6933ffe5bc3e348b29a0bd7a1417a0aa7056fc1a0a97\u0026amp;scene=21#wechat_redirect\n【起初，我的挑战是我不是很好。我没想到最后会进入前1%，但我喜欢进步。这有助于我每天继续工作。如果我明确地想进入前1%，我可能在到达前就放弃了。这是一个如此艰难的目标，我会放弃我永远无法实现的想法。】\n【我认为先学很多理论，然后再开始做项目是错误的。我看到一些人花了很多年时间成为数据科学家，但他们仍然不太了解在实践中这些是如何工作的。 相反，我更倾向于学习最起码的你需要尝试一个项目，如Kaggle竞赛。在你有了实践经验之后，再学习更多的理论来理解理论的适用性。 另外，您绝对需要学习如何使用Git以及如何与其他人协作。 最后，学会用好Pandas。 大多数数据科学家花在处理和清理数据上的时间是使用花在算法的10倍。深入学习也许很有趣，但Pandas更实用。】\n 学会使用pandas。 学会seaborn 目标是进步，而不是某个成绩。追求卓越，成功自然追着你跑。 不要陷入理论。应该实践和理论相结合。  DOTA 朱宇翔 “只定目标，不做计划” 一个连自己情绪都控制不了的人，又能在未来走多远呢？\n如果你没有项目的磨炼，那你应该有着扎实的基本功，并且在基础知识之上你应该有些自己的思考，大多数面试官是在发现你的优点，这也是由你简历入手去了解你的一个过程，初始分50分，每次回答与对问题的理解，为你加减分，所以扎扎实实，拿出你最好的一面，展示自己。最后，好好刷LeetCode！LeetCode！LeetCode！\n 工作与学习并行 工作习惯的养成 人生目标的设定 战斗意志-逢敌亮剑 拥抱变化 志同道合的小伙伴。  ","id":21,"section":"posts","summary":"大佬经验总结 一位master： 网址:https://mp.weixin.qq.com/s?__biz=Mzk0NDE5Nzg1Ng==\u0026am","tags":[],"title":"Kaggle前人经验","uri":"https://yanyuLinxi.github.io/2021/11/kaggle%E5%89%8D%E4%BA%BA%E7%BB%8F%E9%AA%8C/","year":"2021"},{"content":"使用腾讯中文语料库  腾讯词向量使用方法： https://www.jianshu.com/p/65a29663130a\n  语料库地址： https://ai.tencent.com/ailab/nlp/zh/embedding.html\n 语料库  chinese-word-vectors:  https://github.com/Embedding/Chinese-Word-Vectors 300维   腾讯词向量：  https://www.jianshu.com/p/65a29663130a 200维，800万词。   其他多语言语料库  https://sites.google.com/site/rmyeid/projects/polyglot    使用gensim 来读取预训练语料库 ","id":22,"section":"posts","summary":"使用腾讯中文语料库 腾讯词向量使用方法： https://www.jianshu.com/p/65a29663130a 语料库地址： https://ai.tencent.com/ailab/nlp/zh/embedding.html 语料库 chinese-word-vectors: https://github.com/Embedding/Chinese-Word-Vectors 300维 腾讯词向量： https://www.jianshu.com/p/65a29663130a 200维，800万词。 其他多语言语料库 https://sites.google.com/site/rmyeid/projects/polyglot 使用gensi","tags":[],"title":"Gensim学习笔记","uri":"https://yanyuLinxi.github.io/2021/10/gensim%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","year":"2021"},{"content":"比赛题目 第一道题：\n赛题从当前社会中高发的电信网络欺诈识别场景入手，提供模拟的“用户”投诉欺诈信息，要求选手识别投诉中的欺诈风险。\n本赛道将选取工业应用中常见的、由于“数据源差异”、“数据维度特征缺失”而导致的、模型应用困难的问题， 考察AI模型如何通过多源数据的有效应用以及半监督学习技术，实现有限数据下的模型决策，从而思考如何减少AI对数据依赖的问题。 赛题从当前社会中高发的电信网络欺诈识别场景入手，提供模拟的“用户”投诉欺诈信息，要求选手识别投诉中的欺诈风险。\n11.5 10am 截止。从现在开始满打满算15天。\n官方网址: https://www.atecup.cn/competitionIntroduction\n第一道题网址：https://www.atecup.cn/trackDetail/1\n评分 参赛选手提交模型并对榜单数据进行打分（对样本进行欺诈概率预测），后台通过对打 分结果测算出精确率(Precision)、召回率(Recall)曲线，从中推算出：到达精确率为90% 的模型分数阈值下对应的召回率、到达精确率为85%的模型分数阈值下对应的召回率、 到达精确率为80%的模型分数阈值下对应的召回率，三项召回率进行权重分别为0.4、 0.3和0.3加权融合，作为比赛榜单分数。\nScore: 0.4 x Recall precision = 90 + 0.3 x Recall precision = 85 + 0.3 x Recall precision = 80\n服务器使用规则 本地服务器配置及使用规则：CPU:8 core；Memory:16GB；GPU:无；Storage:256G ，不限时使用。\n公有池服务器配置及使用规则：模型训练可使用含GPU的公有池服务器资源，训练任务占用资源为CPU:6 core，memory:16g； GPU 1卡(v100), Storage:20G。每次任务最长使用2小时。 每队每周公有池服务器资源使用上限30小时，不作累积。\n比赛期间排行榜显示A榜成绩排名，比赛结束后 24小时内 展示B榜成绩排名。比赛结束倒计时24小时内允许提交B榜验证任务；提交后，首次运行成功所得分数即为B榜成绩，之后不能再提交打分任务。如需支持，可联络“咨询中心”。 最终比赛成绩以B榜单为准（如B榜成绩持平，则A榜成绩高者排名在前）。 另，为了鼓励更切合实际应用的模型及方案，A/B榜模型提交的打分任务均将限制在 10分钟内 完成。\n关于“周”的说明\n线上赛合计21天，每7*24小时为1周，共3周。\n第一周 指2021年 10月15日 10:00 AM – 10月22日 10:00 AM\n第二周 指2021年 10月22日 10:00 AM – 10月29日 10:00 AM\n第三周 指2021年 10月29日 10:00 AM – 11月5日 10:00 AM\nA榜验证打分每队每24小时最多 10 次（自2021年10月15日10:00AM起算），剩余次数不做累计。 选手可以通过Jupyternotebook进行编码开发。\n数据 数据为模拟生成的用户支付宝欺诈投诉举报数据，\n 标签1代表欺诈案件， 标签0代表非欺诈案件， 标签-1代表未知， 另，测试数据不含-1标签。  数据包含481个特征，其中480个为结构化特征，1个为非结构化的特征。结构化特征包含：欺诈投诉举报案件中主被动双方的相关风控特征，非结构化特征为举报描述信息。无具体特征含义说明。\n本赛道所有相关数据（包括但不限于训练数据集）不得以任何形式下载，仅限在主办方提供的本地服务器及含GPU的公有池服务器上、以比赛为目的使用， 违者将被视作“获取未授权数据”，将依照大赛规则，作禁赛处理。\n比赛全程只允许使用主办方提供的本地服务器以及含GPU的公有池服务器资源。\n如模型训练非必要用到GPU资源，建议优先使用每队不限时的本地服务器资源，以节约每队公有池服务器用时限额。\n数据分析  第一维为id，\u0026ldquo;x0\u0026rdquo;-\u0026ldquo;x479\u0026quot;为特征。特征不清楚定义。memo_polish为文本特征  id没有重复用来输出这个样本是否是欺诈案件。    方案  480维度特征，按列进行归一化。 最后一维度特征首先使用word2vec，编码为特征 对数据特征进行降维：PCA, t-SVD，dimension * 0.8-0.9 特征增强(nlp)概念 使用神经网络。autoencoder。CNN、transformer 使用combination，联合多个模型输出test评分。  神经网络三种方式：\n 深度神经网络。如Transformer CVPR最好的网络 标签为1的半监督  用模型推标签 无标签的自监督学习。    神经网络方法备注:\n lgbm方法调研  问题：\n 特征和nlp特征分别处理再合起来 中文文本处理是否需要用TF-IDF这种编码。处理词频的？ 中文文本处理的问题  预训练语料库太大。腾讯的这个有6.3g 先进行中文分词。然后进行语料库的pretrain导入。 朋飞学长资料：  使用jieba进行分词 使用语料库读取pre-train模型   对训练集数据的词进行统计，然后给出词列表。然后读取embedding然后存储。 资源上传大小限制2G 文本中有中文有数字。   2个小时，是指提交到服务器运行就算时间。不管用不用gpu  时间安排  出炉以天为单位的任务安排 每两天对接一次情况  任务安排：  首先摸清楚数据集的情况 摸清楚实验环境、熟悉实验环境。 完成初步的神经网络搭建  搭建一个使用gpu训练的方式。 在jupyter 中完成归一化、降维、使用autoencoder。 弄清楚整个gpu的训练流程 再弄清楚整个训练流程、方式。 调研：  transformer, light transformer 自监督学习 训练的trick 数据测试tta   统计gpu服务器上的数据   将整个步骤打通。 安装朋飞学长依赖库 放到csv中。  统计有没有缺失 如果是使用csv处理的话，肯定不好。因为gpu服务器的数据不行，你不能转成csv再处理。那样比较费时间吧。   搭建投票系统 按照lgbm调参指南来训练  还没发给我   根据评分来计算表格 添加伪标签 过滤掉冗余特征 对过度重合的特征进行筛选  登录服务器步骤 登录信息 ssh -p 60022 6brdcr99q8@onyiwjcofa-public.bastionhost.aliyuncs.com\npasswd zIxH5KfSXl3_XIfjqM\n堡垒机\n 账号 6brdcr99q8 密码 zIxH5KfSXl3_XIfjqM  本地服务器： 账号： 2bv211k1vq 密码 Dko2502eZNGmA0mk0_\nJupyter:\n 账号 2bv211k1vq 密码 Dko2502eZNGmA0mk0_  登录后常用操作  激活环境  source ~/atec_project/train/your_name_env/bin/activate    提交一次训练 // 将所有文件放在train目录下，通过run.sh来 python train.py // vtag每次不一样。 cd ~/atec_project docker build -f Dockerfile.train \u0026ndash;network=host -t atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com/zark/atec-228:vtag .\n// 获取临时用户名密码 adabench_cli auth get-docker-token\n// 登录 docker login \u0026ndash;username=cr_temp_user atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com\n// 提交 docker push atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com/zark/atec-228:vtag\n// 运行任务 -t 指定任务类型，trian为训练，rank_a/rank_b分别刷a/b榜。 adabench_cli task run -i atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com/zark/atec-228:vtag -t train // 返回job id\nv_ttw_1\n// 查看任务 adabench_cli task status -j\nadabench_cli task status -j job_id\n// 查看日志 adabench_cli task logs -j\nadabench_cli task logs -j job_id\n// 下载产出的模型(只有训练) 只保存24个小时 adabench_cli task download \u0026ndash;path /home/2bv211k1vq/atec_project/trained_models -j job_id // 解压产出的打包模型 tar xzf ***.tar.gz\n// 任务停止 adabench_cli task stop \u0026ndash;j [任务id]\n============================================================= // 打榜 // 生成镜像 cp -af ~/atec_project/train/your_name_env ~/atec_project/rank/\ndocker build -f Dockerfile.rank \u0026ndash;network=host -t atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com/zark/atec-228:vtag .\nadabench_cli auth get-docker-token\ndocker login \u0026ndash;username=cr_temp_user atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com\ndocker push atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com/zark/atec-228:vtag\n// 运行:rank_a/rank_b分别刷a/b榜。 比赛结束倒计时24小时内允许提交B榜验证任务；提交后，首次运行成功所得分数即为B榜成绩，之后不能再提交打分任务 adabench_cli task run -i atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com/zark/atec-228:vtag -t rank_a // 返回job_id\nadabench_cli task status -j adabench_cli task status -j job_id\n第一次提交 {'__metrics': {\u0026lsquo;code\u0026rsquo;: 0, \u0026lsquo;score\u0026rsquo;: 0.5302481389578164, \u0026lsquo;msg\u0026rsquo;: \u0026lsquo;success\u0026rsquo;}\n注意事项  赛道一服务器操作指南： https://www.atecup.cn/machineGuide\n  使用jupyter开发的话，记得保持jupyter的环境和your_env_name环境一致。  每次提交rank的时候复制实验环境到rank   本地ECS机器训练数据集存在于本机的/mnt/atec/train.jsonl 服务器端数据：  本机  /mnt/atec/train.jsonl   train数据  /home/admin/workspace/job/input/train.jsonl   模型输出数据存放  /home/admin/workspace/job/output/   关键信息存放  /home/admin/workspace/job/output/result.json   测试数据  /home/admin/workspace/job/input/test.jsonl   预测结果放到：  /home/admin/workspace/job/output/predictions.jsonl     创建docker相关命令  docker images 查看所有镜像 镜像制作：  cd ~/atec_project docker build -f Dockerfile.train \u0026ndash;network=host -t atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com/zark/atec-228:v1 . v1为tag名。每次tag不一样。   docker登录。每次传输前获取镜像密码  adabench_cli auth get-docker-token 账号：cr_temp_user docker login \u0026ndash;username=cr_temp_user atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com   登录后提交：  docker push atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com/zark/atec-228:v1     大赛触发任务执行：  adabench_cli task run -i atec-repo-registry-vpc.cn-beijing.cr.aliyuncs.com/zark/atec-228:v1 -t train  会返回一个job_id 就是后面的task id   adabench_cli task status -j job_id adabench_cli task logs -j job_id adabench_cli task download -j job_id   训练说明：  训练和刷榜任务的输出文件必须放置在/home/admin/workspace/job/output/目录下 如果为刷榜任务：打分的结果文件必须输出到/home/admin/workspace/job/output/predictions.jsonl，平台会使用这个结果文件进行打分   数据集格式：  train {\u0026ldquo;id\u0026rdquo;: \u0026ldquo;1\u0026rdquo;, \u0026ldquo;input_feat1\u0026rdquo;: \u0026ldquo;xx\u0026rdquo;, \u0026ldquo;label\u0026rdquo;: 0} {\u0026ldquo;id\u0026rdquo;: \u0026ldquo;2\u0026rdquo;, \u0026ldquo;input_feat1\u0026rdquo;: \u0026ldquo;xx\u0026rdquo;, \u0026ldquo;label\u0026rdquo;: 1} test {\u0026ldquo;id\u0026rdquo;: \u0026ldquo;1\u0026rdquo;, \u0026ldquo;input_feat1\u0026rdquo;: \u0026ldquo;xx\u0026rdquo;} {\u0026ldquo;id\u0026rdquo;: \u0026ldquo;2\u0026rdquo;, \u0026ldquo;input_feat1\u0026rdquo;: \u0026ldquo;xx\u0026rdquo;} predictions.jsonl {\u0026ldquo;id\u0026rdquo;: \u0026ldquo;1\u0026rdquo;, \u0026ldquo;label\u0026rdquo;: 0.4} {\u0026ldquo;id\u0026rdquo;: \u0026ldquo;2\u0026rdquo;, \u0026ldquo;label\u0026rdquo;: 0.8}    数据统计  首先统计各个数值平均数、众数、最大值、最小值 看看标签为 1 的label长什么样子。  相关资料  一些其他比赛的资料。 https://mp.weixin.qq.com/s/vSVhearDrZB3OXW4CXaDGQ  我们最后进行赛后自我总结 如果后面有人分享方案的话，时刻保持关注一下，分析一下前三的每个人的方案。\n然后我们开始分析下自己的方案进行总结：\n流程：\n 首先最开始的时候对数据进行处理：  将数据处理成pandas（详细学习使用pandas） 查看是否有NAN值 查看是否有异常值 查看数据的众数 画图看数据的分布。使用log查看分布差异较大的数。   针对不同的数据使用不同的算法：  特征数据：  变换为图像，然后使用CV相关思路。1d CNN, Resnet 1d等   文本数据  使用bert对数据进行预训练。  预训练思路有很多，包括使用masked在文本上微调。 或者使用文本分类，直接运算。   使用tf-idf+svd分析文本中的词频。 使用ldm对文本主题进行分析。   归一化处理  很多树模型不需要归一化，但是神经网络模型大多必需归一化。归一化也有很多，高斯归一化等等，了解他们之间的差异。     然后快速确定一个baseline  根据评分公式计算评分。方便后面评价模型 对数据进行训练集、测试集、验证集的分类。  不要觉得这样分类，减少了数据集会影响分类，其实不是的。可以帮助前期自己对模型的验证。   常用的树模型：lgbm、catboost、xgboost。等快速的api可以帮助快速出一个api 常用的神经网络模型。MLP等。 使用常用的ML框架：Pycaret等。   在baseline上进行改进  前期脑洞大开，后期小心谨慎  包括ms等方法只能提升微小且不可忽略的进步，但不能依靠这个就大幅度增长分数。前期必须要脑洞大开。后期必须小心谨慎   双塔模型。  双塔最初用在推荐系统上，将人的行为特征和商品的行为特征进行匹配。在这里我们可以分别使用不同的模型对不同类型的特征进行建模。这是十分重要的。不同类型的数据显然不应该放在一起进行训练、测试。   对特征进行重新提取、筛选。 伪标签技术。  使用正负置信度为0.9-0.99的标签二次加入训练 使用所有标签加入训练，但是为伪标签使用不同的权重矩阵。权重取决于它们的loss函数。     后期优化：  使用meta_stacking对模型进行搭建。  这个时候可以用上之前的验证集了。使用验证集训练元学习器。   对选定的模型进行调参。使用GridSearch或者使用贝叶斯调参   整体注意事项  需要动手去写的东西并不多，更多的是思路。题目一样的，解法一样，如何取得更好的分数呢？ 不要全身心投入去做一件事。多件事情交叉，让自己的思路打开。 不要在程序开始就进行优化。边写脚本，边对模块进行集成。在写模块时，尽量和现有的框架的api进行匹配，减少记忆负担。对看到比较好的方法进行记录。    Notes:\n 学好pandas 学好seaborn 学好bert模型。 以后学习不能不求甚解。对不了解的一定要了解清楚。刨根问底。如果一个知识点足够重要，就开一个md专门写。 对transform系统的学习，能够手撸代码。  ","id":23,"section":"posts","summary":"比赛题目 第一道题： 赛题从当前社会中高发的电信网络欺诈识别场景入手，提供模拟的“用户”投诉欺诈信息，要求选手识别投诉中的欺诈风险。 本赛道将选取","tags":[],"title":"ATEC科技精英赛","uri":"https://yanyuLinxi.github.io/2021/10/atec%E7%A7%91%E6%8A%80%E7%B2%BE%E8%8B%B1%E8%B5%9B/","year":"2021"},{"content":"基础知识 hash将通过hash函数将某个东西转为数值，然后存在哈希表上。这个哈希表可能是数组，也可能是红黑树等等。\n如果hashCode的数值大于哈希表了，通常会有操作如取模将值限制在哈希表内。\n哈希碰撞仍然不可避免，解决方案：\n 拉链法。在碰撞的地方建立链表。碰撞发生后依次查找。 线性探测法：碰撞后依次往后寻找空位。需要保证dataSize\u0026lt;hashTableSize.  hash表一般在语言中分为三种：array, set, map\n总体来说，有set, multiset, unordered_set这三种。\n具体区别参考网址：https://github.com/youngyangyang04/leetcode-master/blob/master/problems/%E5%93%88%E5%B8%8C%E8%A1%A8%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80.md\n总的来说，set、multiset用的是红黑树。set不允许重复。multiset允许key重复。unordered_set用的是哈希表。key是无序的，也不能重复。\n有先使用Unordered_set或者_map，然后有序要求用set、map。要重复则multi_set, multi_map\n1002 查找常用字符 这是个hash表的问题。它要找重复字符。有一种方法是所有的字符串共用一个hash表。这是统计所有字符串中字符出现的次数\n还有一种方法就是每个字符串一个hash表。这是统计该字符串中字符的出现次数\n结合具体情况具体分析。\n","id":24,"section":"posts","summary":"基础知识 hash将通过hash函数将某个东西转为数值，然后存在哈希表上。这个哈希表可能是数组，也可能是红黑树等等。 如果hashCode的数值","tags":[],"title":"Leetcode3_hash表","uri":"https://yanyuLinxi.github.io/2021/10/leetcode3_hash%E8%A1%A8/","year":"2021"},{"content":"引言  计算机可以处理形式化的语言。人工智能的一大难题就是将如何将非形式化的传达给计算机。 人工智能AI系统需要具备自己获取知识的能力，即从原始数据中提取模式的能力，这种高能力被称为机器学习。 简单机器学习的性能很大程度上取决于给定数据的表示，表示数据的选择对机器学习性能产生很大的影响。例子：笛卡尔坐标系和极坐标系下的数据表示会有不同的划分途径。p3 使用机器学习发掘表示本身，称为表示学习(representation learning)，经典例子：AutoEncoder。 设计特征、设计用于学习的特征的算法时，目标通常是分理处额能解释观察数据的变差因素(factors of variation)，这些因素时不可观察到的量或者力，会影响到可观测的量。人工智能中的困难主要源于多个变差因素同时影响着我们能观察到的数据。 从原始数据提取高层次、抽象的特征是比较困难的。深度学习(deep learning)通过其他比较简单的表示来表示复杂表示。解决了表示学习中的核心问题。输入展示在可见层，一系列从图像中提取到的越来越多的特征叫隐藏层。例子：多层感知机。 深度学习两个度量视角，一个是提取数据的正确表示。另一个是多步骤的计算机程序，多顺序指令的执行帮助计算机理解输入。 维恩图：deeplearning-\u0026gt;Represenation learning-\u0026gt;Machine Learning-\u0026gt;AI 深度学习的趋势。最早有控制论、联结主义。控制论很类似傅里叶变换，使用不同的函数和不同的参数来表示一个函数。   第一次高潮、退潮\n 控制论：使用一组n个输入，并将它们与一个输出y相关联。这个模型希望学习到一个权重。使得$f(x,w)=x_1w_1+\u0026hellip;+x_nw_n$。显然模型需要正确设置权重之后才能使用。 感知机是第一个能根据每个类别的输入样本来学习权重的模型。 但诸如上面所讲的线性模型，无法学习最简单的异或。这引起了神经网络的第一次退潮。    第二次高潮退潮\n 神经科学被认为是深度学习的一个重要灵感来源。但已经不再是该领域的主要指导。我们连大脑最简单的部分还远没有理解。 小故事：将雪貂的视觉神经和大脑听觉区域相连，它们可以学会取用听觉区域去“看”。这暗示这大多数的哺乳动物的大脑能够使用单一的算法就可以解决其大脑可以解决的大部分不同的任务。拥有这个假设，深度学习团队同时研究多个领域是很常见的。 新认知机受哺乳动物的视觉系统结构的启发，后来成为CNN的基础。目前大多数神经网络都基于整流线性单元模型。 神经科学是神经网络的重要灵感来源。但线代深度学习从许多其他领域获取灵感：线性代数、信息论、数值优化等等。 第二次浪潮：联结主义、并行分布处理。联结主义思想：当网络将大量的简单的饿计算单元连接在一起时可以实现智能行为。联结主义重要的成就就是反向传播算法的普及。 分布式表示：每一个输入都应该由多个特征表示，并且每一个特征都应该参与到多个可能的输入的表示。    第三次浪潮：\n 深度信念网络使用一种贪婪逐层预训练的策略来有效的训练。同样的策略可以用来训练其他许多类型的深度网络。深度学习的第三次浪潮才开始。 虽然已开始着眼于无监督学习技术，但目前更多的兴趣点仍是比较传统的监督学习算法和深度模型充分利用大型标注数据的能力。     与日俱增的数据量， 联结主义的主要见解之一是，当动物的需多神经元在一起工作的时候，会变得聪明。单独神经元或者小集合的神经元不是特别有用。几十年内，我们的机器学习模型中的每个神经元的连接数量已经和哺乳动物的大脑在同一数量级上。自从引入隐藏层后，神经元数量每2.4年增加一倍。21世纪50年代，人工智能的神经元可以达到与人脑相同的数量级。目前的网络，相对原始的脊椎动物如青蛙还小。 强化学习，在没有人类指导的情况下，通过试错来学习执行任务。深度学习爷显著改善了机器人强化学习的性能。  符号表  $\\odot$ hadamard乘积。 逐元素相乘  ","id":25,"section":"posts","summary":"引言 计算机可以处理形式化的语言。人工智能的一大难题就是将如何将非形式化的传达给计算机。 人工智能AI系统需要具备自己获取知识的能力，即从原始数","tags":[],"title":"0 引言","uri":"https://yanyuLinxi.github.io/2021/10/0-%E5%BC%95%E8%A8%80%E5%92%8C%E7%AC%A6%E5%8F%B7%E8%A1%A8/","year":"2021"},{"content":" 常见概念  特征分解 奇异值分解    常见概念 标量 向量 矩阵 张量\n  矩阵加法：C = A + b。表示向量b和矩阵的每一行相加。这种隐式复制的方式，称为广播。boardcasting\n  元素对应乘积： A $\\odot$ B hadamard 乘积\n  由一组解线性组合而成的解称为 生成子空间。表示原始向量线性组合后能抵达的点的集合。\n  矩阵列向量线性相关的矩阵，称为奇异的。\n  范数： 形式上 $L^p 定义为 ||x||_p = (\\sum_i|x_i|^p)^{\\frac{1}{p}}$ 。 满足的性质：\n $f(x) = 0 -\u0026gt; x=0$ $f(x+y) \\leq f(x) + f(y)$ (三角不等式) $\\forall \\alpha \\in R, f(\\alpha x) = |\\alpha|f(x)$    当p=2的时候，$L^2$就是欧几里得范数。表示从原点到x确定的点的欧几里得距离。经常表示为$||x||$。平方$L^2$范数对x中每个元素的导数取决于对应的元素。而$L^2$范数对每个元素的导数却和整个向量相关。但是平方L2范数在原点附近增长的十分缓慢。L1范数在0附近增长则正相关。\n  最大范数$L^{\\infty} =\u0026gt; ||x||_\\infty = max|x_i|$\n  Frobenius范数，衡量矩阵的大小。\n  特征分解  ***对角矩阵（diagonal matrix）***只在主对角线上含有非零元素，其他位置都是零。我们用 diag(v) 表示一个对角元素 由向量 v 中元素给定的对角方阵。计算乘法 diag(v)x，我们只需要将 x 中的每个元素 xi 放大 vi 倍。换 言之，diag(v)x = v⊙ x\n 对角方阵的逆矩阵存在， 当且仅当对角元素都是非零值 但通过将一些矩阵限制为对角矩阵，我们可以得到计算代价较低的（并且简明扼要的）算法    对称（symmetric）矩阵是转置和自己相等的矩阵\n  单位向量（unit vector）是具有单位范数（unit norm）的向量：$||x||_2 = 1$.\n  如果 x⊤y = 0，那么向量 x 和向量 y 互相正交（orthogonal）。如果这些向量不仅互相正交，并且范数都为 1，那么我们称它们 是标准正交（orthonormal）。\n 正交矩阵（orthogonal matrix）是指行向量和列向量是分别标准正交的方阵, 这意味着$A^{−1} = A^⊤$    特征分解（eigendecomposition）是使用最广的矩阵分解之一，即我们将矩阵分 解成一组特征向量和特征值.方阵 A 的特征向量（eigenvector）是指与 A 相乘后相当于对该向量进行缩放 的非零向量 v:$Av=\\lambda v$.标量 λ 被称为这个特征向量对应的特征值（eigenvalue）\n 每个实对称矩阵都可以分解成实特征向量和实特征值 按照惯例，我们通常按降序排列 Λ 的元素。在该 约定下，特征分解唯一当且仅当所有的特征值都是唯一的。    矩阵是奇异的当且仅当含 有零特征值\n  所有特征值都是正数的矩阵被称为正定（positive definite）；所有特征值都是非 负数的矩阵被称为半正定（positive semidefinite）。半正定矩阵受到关注是因为它们保证 ∀x, x⊤Ax ≥ 0。此外， 正定矩阵还保证 x⊤Ax = 0 ⇒ x = 0。\n  奇异值分解  被称为奇异值分解（singular value decomposition, SVD），将矩阵分 解为奇异向量（singular vector）和奇异值（singular value）。每 个实数矩阵都有一个奇异值分解，但不一定都有特征分解。\n  对角矩阵 D 对角线上的元素被称为矩阵 A 的奇异值（singular value）。矩阵 U的列向量被称为左奇异向量（left singular vector），矩阵 V的列向量被称右奇异向量（right singular vector）。 事实上，我们可以用与 A 相关的特征分解去解释 A 的奇异值分解。A 的左奇 异向量（left singular vector）是AA⊤ 的特征向量。A的右奇异向量（right singular vector）是 A⊤A 的特征向量。A 的非零奇异值是 A⊤A 特征值的平方根，同时也是 AA⊤ 特征值的平方根。\n  奇异值分解是类似的，只不过这回我们将矩阵 A 分解成三个矩阵的乘积$A = UDV^⊤$.矩阵 U和 V都定义为正交 矩阵，而矩阵 D 定义为对角矩阵。注意，矩阵 D 不一定是方阵。A 的奇异值（singular value）。矩阵 U的列向量被称为左奇异向量（left singular vector），矩阵 V的列向量被称右奇异 向量（right singular vector）。SVD最有用的一个性质可能是拓展矩阵求逆到非方矩阵上。\n  Moore-Penrose 伪逆. 当矩阵 A 的列数多于行数时，使用伪逆求解线性方程是众多可能解法中的一 种。特别地，x = A+y 是方程所有可行解中欧几里得范数 ∥x∥2 最小的一个。当矩阵 A 的行数多于列数时，可能没有解。在这种情况下，通过伪逆得到的 x 使得 Ax 和 y 的欧几里得距离 ∥Ax− y∥2 最小。\n  迹运算返回的是矩阵对角元素的和.多个矩阵相乘得到的方阵的迹，和将这些矩阵中的最后一个挪到最前面之后相 乘的迹是相同的。\n  行列式，记作 det(A)，如果行列式是 0，那么空间至少沿着某一维完全收缩了，使其失去了所有的 体积。如果行列式是 1，那么这个转换保持空间体积不变。\n  ***主成分分析（principal components analysis, PCA）***是一个简单的机器学习算 法，可以通过基础的线性代数知识推导。对于每个点 x(i) ∈ Rn，会有一个对应的 编码向量 c(i) ∈ Rl。如果 l 比 n 小，那么我们便使用了更少的内存来存储原来的数据。f(x) = c；我们也希望找到一 个解码函数，给定编码重构输入，x ≈ g(f(x))。了简化解码器，我们使用矩阵乘 法将编码映射回 Rn，即 g(c) = Dc，其中 D ∈ Rn×l 是定义解码的矩阵。\n 目前为止所描述的问题，可能会有多个解. 为了使问 题有唯一解，我们限制 D 中所有列向量都有单位范数 首先我们需要明确如何根据每 一个输入 x 得到一个最优编码 c∗。一种方法是最小化原始输入向量 x 和重构向量 g(c∗) 之间的距离。我们使用范数来衡量它们之间的距离。在PCA算法中，我们使用 L2 范数： 我们可以用平方 L2 范数替代 L2 范数，因为两者在相同的值 c 上取得最小值。这是因为 L2 范数是非负的，并且平方运算在非负值上是单调递增的。 这使得算法很高效：最优编码 x 只需要一个矩阵-向量乘法操作。为了编码向量， 我们使用编码函数$f(x) = D^⊤x$.进一步使用矩阵乘法，我们也可以定义PCA重构操作：r(x) = g(f(x)) = DD⊤x. 具体来讲，最优的 d 是 X⊤X最大特 征值对应的特征向量。以上推导特定于 l = 1 的情况，仅得到了第一个主成分。更一般地，当我们希望 得到主成分的基时，矩阵 D 由前 l 个最大的特征值对应的特征向量组成。这个结论 可以通过归纳法证明，我们建议将此证明作为练习。 从线性代数的角度总结：PCA是最小化压缩后的矩阵和原始矩阵之间的距离。通常使用平方$L^2$范数来表示这个距离。经过在一维的运算，最优的d是$X^TX$最大特征值对应的特征向量。通过归纳法得知压缩矩阵D由前l个最大的特征直对应的特征向量组成。 从其他角度还有更广泛的解释： 方差：表示数据的分散程度。协方差表示两个变量的相关性。 思想方法：  将坐标轴移动到数据的中心，旋转坐标轴，使得数据在c1轴上的方差最大，即全部n个数据个体在该方向投影最分散。C1为第一主成分。 找一个C2，使得C2和C1的协方差（相关系数）为0，上市的数据在该方向的方差尽量最大。 依次类推。   数学方法：  将原始数据按列组成 n 行 m 列矩阵 X 将 X 的每一行进行零均值化，即减去这一行的均值； 求出协方差矩阵 求出协方差矩阵的特征值及对应的特征向量； 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 k 行组成矩阵 P； 即为降维到 k 维后的数据。   优缺点：  保留了主要信息。这个主要信息未必是重要信息。很可能是非主要信息取了决定性作用。 起到了降噪降维的作用。且PCA后的特征相互独立。       ","id":26,"section":"posts","summary":"常见概念 特征分解 奇异值分解 常见概念 标量 向量 矩阵 张量 矩阵加法：C = A + b。表示向量b和矩阵的每一行相加。这种隐式复制的方式，称为广播。boar","tags":[],"title":"线性代数","uri":"https://yanyuLinxi.github.io/2021/10/1-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/","year":"2021"},{"content":"学习计划  神经网络的学习，我们首先找到一本pdf进行学习，然后我们在学习的过程中持续关注这方面的东西，寻找下一阶段的学习目标。 吴恩达、李宏毅。 先看这本书。然后我们系统的往后面学习。 2019 年 12 个深度学习最佳书籍清单！值得收藏 - 红色石头的文章 - 知乎https://zhuanlan.zhihu.com/p/60574682 用一个板块来记录。  时间计划 我们从早上8点及更早的时刻来学习这个知识。每一章节做一个笔记目录记录学习到的知识。\n如果打算一个月读完的话。一天必须要读30面左右。这个肯定是很难的。所以除了早上的一个小时，我们有空就读读。然后积累知识。\n我们学习50分钟，最后10分钟做笔记。\n视频计划 大林轩田教授的《机器学习基石》和《机器学习技法》\n","id":27,"section":"posts","summary":"学习计划 神经网络的学习，我们首先找到一本pdf进行学习，然后我们在学习的过程中持续关注这方面的东西，寻找下一阶段的学习目标。 吴恩达、李宏毅。","tags":[],"title":"深度学习计划","uri":"https://yanyuLinxi.github.io/2021/10/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%A1%E5%88%92/","year":"2021"},{"content":"现有方案 仅有的一个针对开源代码恶意代码的检测。提供了一个巨大的开源库，但是其是通过检测github库的字段、md文件等信息。\n开源代码 恶意代码的特征：  大部分功能是正常的，小部分功能为异常行为。且针对之前安卓恶意代码的经验，几千个函数当中，仅仅有一两个函数存在恶意行为。 要尽量提取到语义信息以抗混淆。避免更改变量名就可以躲过检查  通过控制流、数据流等信息来提取语义信息。   有人向开源库中投毒，导致某一两个commit是包含病毒的。  启示：可以针对commit为单位来检测是否包含病毒。单个commit 或者 同作者的多个commit。   参考目前的方案，通过社区来挖掘恶意代码的情报。  针对Usenix的和面向开源社区漏洞情报挖掘平台来判断其是否包含恶意代码   动态检测存在问题  很多开源库是API调用，很难将所有API遍历来分析恶意代码。   恶意行为较为集中。恶意代码复用较多，且行为类似。都包括截取部分特征，向某个网站、ip发送网络请求等等。来自安卓恶意代码的经验。  目前的方案： 和江帅讨论了下：\n 针对恶意的行为建立匹配特征。  然后通过恶意行为特征匹配查找是否包含恶意行为。恶意行为特征的提取尽量考虑到抗混淆部分。 或者通过神经网络来判断行为是否是恶意的。   针对社区的情报判断其是否包含恶意代码  恶意代码作者。 恶意代码讨论帖子。    难点：  没有数据集。仅有恶意代码库，具体哪里是恶意的，并没有具体的标签。 需要对恶意行为进行统计，并设计特征提取方案。 数据集得发邮件要啊。  Notes：\n 考虑被调用函数的语义信息。 联合多个函数。内联函数判别多个函数是否是恶意的。  ","id":28,"section":"posts","summary":"现有方案 仅有的一个针对开源代码恶意代码的检测。提供了一个巨大的开源库，但是其是通过检测github库的字段、md文件等信息。 开源代码 恶意代码","tags":[],"title":"源代码恶意代码识别","uri":"https://yanyuLinxi.github.io/2021/10/%E6%BA%90%E4%BB%A3%E7%A0%81%E6%81%B6%E6%84%8F%E4%BB%A3%E7%A0%81%E8%AF%86%E5%88%AB/","year":"2021"},{"content":"目录：   1. 综述翻译  1.1 发表于   2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释  1. 综述翻译 我们在哪里可以找到恶意软件源代码？这个问题的动机是一个真正的需求：缺乏恶意软件源代码，这阻碍了各种类型的安全研究。我们的工作受到以下见解的推动：公共档案，如 GitHub，拥有数量惊人的恶意软件存储库。利用这个机会，我们提出了 SourceFinder，这是一种有效识别恶意软件源代码存储库的监督学习方法。我们使用来自 GitHub 的 97K 存储库评估和应用我们的方法。首先，我们展示了我们的方法使用标记数据集以 89% 的准确率和 86% 的召回率识别恶意软件存储库。其次，我们使用 SourceFinder 来识别 7504 个恶意软件源代码存储库，这可以说是最大的恶意软件源代码数据库。最后，我们研究了恶意软件存储库及其作者的基本属性和趋势。此类存储库的数量似乎每 4 年增加一个数量级，并且 18 位恶意软件作者似乎是具有良好在线声誉的“专业人士”。我们认为我们的方法和我们的大型恶意软件源代码存储库可以研究的催化剂，目前是不可能的。\n1.1 发表于 2020 USENIX\n2. Tag 3. 任务描述 4. 方法 我们打算将我们的数据集和工具用于研究目的。我们的愿景是创建社区驱动的参考中心，它将提供：(a) 恶意软件源代码存储库，(b) 社区审查的标签和反馈，以及 (c) 用于收集和分析恶意软件存储库的开源工具。我们的目标是用更多的软件档案扩展我们的数据库。尽管作者可以开始隐藏他们的存储库（见第 8 节），但我们认为我们已经检索到的数据库可能会对某些类型的研究产生重大影响。\nGitHub 中的存储库具有以下数据字段：a) 标题，b) 描述，c) 主题，d) README 文件，e) 文件和文件夹，f) 创建日期和最后修改日期，g) 分支，h) 观察者， i) 明星，以及 j) 关注者和关注者，\n建立基本事实：由于没有可用的基本事实，我们需要建立自己的。由于这是一项相当技术性的任务，我们选择了领域专家，而不是根据最近的研究 [22] 推荐的 Mechanical Turk 用户。我们使用三位计算机科学家从我们的每个数据集 RD137 和 RD50 以及来自 RD1 的 600 个存储库中手动标记 1000 个存储库，这些存储库是我们以均匀随机方式选择的。评委被指示对每个存储库进行彻底的独立调查。确保地面实况的质量。为了提高地面实况的可靠性，我们采取了以下措施。首先，我们要求法官仅在确定存储库是恶意的或良性且独特的情况下才对其进行标记，否则不对其进行标记。我们只保留法官一致同意的存储库。其次，通过手动检查删除重复的存储库，并从最终标记的数据集中排除以避免过度拟合。值得注意的是，我们在每个具有数百个存储库的数据集中仅发现了 3-5 个数量级的极少数重复项。通过这个过程，我们从每个查询的相应恶意软件存储库开始，建立了三个独立的标记数据集，分别命名为 LD137、LD50 和 LD1，如表 2 所示。虽然标记数据集不是 50-50，但它们代表了两个类相当不错，因此将所有内容标记为一个类的幼稚解决方案的性能会很差。相比之下，我们的方法表现得足够好，正如我们将在第 5 节中看到的。由于没有可用的数据集，我们认为我们通过手动努力制作了足够大小的数据集。\nAmazon Mechanical Turk：亚马逊众包网站，雇佣远程众包工作者来执行计算机无法完成的离散任务。\n步骤 1. 数据预处理：与任何自然语言处理 (NLP) 方法一样，我们从文本的一些初始处理开始，以提高解决方案的有效性。\na. 字符级预处理：我们通过删除特殊字符（例如标点符号和货币符号）来处理字符级“噪音”，并修复 Unicode 和其他编码问题。 b. 字级预处理：我们按照以下最佳实践消除或聚合字词自然语言处理[32]。首先，我们删除文章词和其他本身不具有重要意义的词。其次，我们使用词干技术处理屈折词。即我们希望降低数据的维数通过将具有相同“root”的单词分组。例如，我们将“organizing”、“organized”、“organize”和“organizes”这些词组合为一个词“organize”。第三，我们过滤掉常见的文件和文件夹名称我们不希望对我们的分类有帮助，例如“LEGAL”、“LICENSE”、“gitattributes”等。 c. 实体级过滤：我们过滤可能对描述存储库范围没有帮助的实体。具体来说，我们删除号码、URL 和电子邮件，这在文中经常出现。我们发现这种过滤提高了分类性能。未来，我们可以考虑挖掘 URL 和其他信息，例如人名、公司或 YouTube 频道的名称，以识别作者、验证意图并发现更多恶意软件活动。\n步骤 2. 存储库字段：我们考虑存储库中可以是数字或文本的字段。基于文本的字段需要处理才能将它们转换为分类特征，我们将在下面解释这一点。我们使用并评估以下文本字段：标题、描述、主题、文件和文件夹名称以及自述文件字段。文本字段表示：我们考虑两种技术来通过分类中的特征来表示每个文本字段。\na. Bag ofWords (BoW)：词袋 (BoW) 模型是最广泛使用的文档表示之一。文档表示为其单词出现的次数，不考虑语法和词序 [69]。该模型通常用于文档分类，其中每个单词的频率用作训练分类器的特征值 [41]。我们使用带有计数向量化器和 TF-IDF 向量化器的模型来创建特征向量。 b. 词嵌入：词嵌入模型是文档中每个词的向量表示：每个词映射到实数的 M 维向量 [43]，或者等效地投影到 M 维空间中。 良好的嵌入可确保含义接近的单词在嵌入空间中具有附近的表示。 为了创建文档向量，词嵌入遵循两种方法（i）基于频率的向量化器（无监督）[55]和（ii）基于内容的向量化器（监督）[37]。\n步骤 3. 选择字段：另一个关键问题是在我们的分类中使用存储库中的哪些字段。 我们对第 2 节中列出的所有领域进行试验，并在下一节中解释我们的发现。\n步骤 4. 选择 ML 引擎：我们设计分类器将存储库分为两类：(i) 恶意软件存储库和 (ii) 良性存储库。 我们系统地评估了许多机器学习算法 [7,44]：朴素贝叶斯 (NB)、逻辑回归 (LR)、决策树 (CART)、随机森林 (RF)、K-最近邻 (KNN)、线性判别分析 (LDA) ) 和支持向量机 (SVM)。\n步骤 5. 检测源代码存储库：我们还想确定存储库中是否存在源代码，作为向社区提供恶意软件源代码的最后一步。\n我们提出了一种启发式方法，它在实践中似乎效果很好。 首先，我们要识别存储库中包含源代码的文件。 为此，我们首先检查它们的文件扩展名。 如果文件扩展名是已知的编程语言之一：Assembly、C/C++、Batch 他已知的编程语言：Assembly、C/C++、Batch File、Bash Shell Script、Power Shell 脚本、Java、Python、C#、Objective-C 、Pascal、Visual Basic、Matlab、PHP、Javascript 和 Go，我们将其标记为源文件。 其次，如果存储库中的源文件数量超过源百分比阈值（SourceThresh），我们认为存储库包含源代码。\n5. 解决了什么问题（贡献） 难点： (a) 从可能庞大的档案中收集一组适当的存储库， (b) 识别包含恶意软件的存储库。\n首先，几项研究分析了软件存储库以查找使用和限制，而没有任何关注恶意软件 [14]。 其次，一些努力维护恶意软件二进制文件的数据库，但没有源代码 [2, 3]。 第三，许多努力试图从二进制文件中提取更高级别的信息，例如提升到中间表示（IR）[19]，但是重新创建源代码真的很困难 [10]。 事实上，此类研究将受益于我们的恶意软件源代码档案，以评估和改进他们的方法。 从软件工程的角度来看，一项有趣的工作 [8] 将 150 个恶意软件源代码存储库的演变与良性软件的演变进行了比较。\n贡献： (a) 我们提出了 SourceFinder，这是一种以高精度识别恶意软件源代码存储库的系统方法， (b) 我们创建了可以说是最大的非商业恶意软件源代码存档，拥有 7504 个存储库， (c) 我们研究存储库生态系统的模式和趋势，包括时间和以作者为中心的属性和行为。\n我们确定多产和有影响力的作者。我们发现 3% 的作者拥有超过 300 名粉丝。我们还发现 0.2% 的作者拥有超过 7 个恶意软件存储库，其中最多产的网络威胁作者创建了 336 个存储库。 G。我们识别并分析了 18 名专业黑客。我们发现了 18 位恶意软件存储库的作者，他们似乎围绕他们的活动创建了一个品牌，因为他们在安全论坛中使用相同的用户名。例如，用户 3vilp4wn（发音为 evil-pawn）是 GitHub 中键盘记录恶意软件的作者，作者使用相同的用户名在 Hack This Site 论坛中推广该恶意软件\n6. 实验结果 正如我们在第 5 节中讨论的那样。我们表明我们分类恶意软件存储库使用存储库中的五个字段，具有 89% 的准确率、86% 的召回率和 87% 的 F1 分数\n总结：通过文档判断是否是恶意代码，虽然包含了恶意代码的库，但是并没有具体哪个地方是恶意代码的标签。\n7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 ","id":29,"section":"posts","summary":"目录： 1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专","tags":["论文阅读笔记"],"title":"SourceFinder Finding Malware Source Code From Publiclyu Available Repositories in Github","uri":"https://yanyuLinxi.github.io/2021/10/sourcefinder-finding-malware-source-code-from-publiclyu-available-repositories-in-github/","year":"2021"},{"content":"从大到小\n   从大到小 命名规范 样例     包名 全小写 sklearn   模块名 全小写，下划线分割 sklearn.model_selection   类名 首字母大写、驼峰(类名不加下划线) ThisIsMyClass, TruncatedSVD   函数名 全小写、用下划线增加可读性 read_pickle_file   变量名 全小写、用下划线增加可读性（函数名和变量名不可区分） my_variable   常量名 全大写、用下划线增加可读性 COUNTER_NUMBER    ","id":30,"section":"posts","summary":"从大到小 从大到小 命名规范 样例 包名 全小写 sklearn 模块名 全小写，下划线分割 sklearn.model_selection 类名 首字母大写、驼峰(类名不加下划线) ThisIsMyClass, TruncatedSVD 函数名 全小写、用下划线增加可读性","tags":[],"title":"Python命名规范","uri":"https://yanyuLinxi.github.io/2021/10/python%E5%91%BD%E5%90%8D%E8%A7%84%E8%8C%83/","year":"2021"},{"content":"标准化和归一化  缩放到均值为0，方差为1（Standardization——StandardScaler()） 缩放到0和1之间（Standardization——MinMaxScaler()） 缩放到-1和1之间（Standardization——MaxAbsScaler()） 缩放到0和1之间，保留原始数据的分布（Normalization——Normalizer()） 1就是常说的z-score归一化，2是min-max归一化。 归一化Normalization，只是缩放到0，1 标准化Standardization，缩放到0-1，还改变了方差。所以改变了分布。不一定是正态分布。  Notes:\n 归一化和标准化的选择差异  当数据范围有严格要求的时候，使用归一化。不涉及距离度量、协方差计算则归一化。 标准化更通用，无从下手，则用标准化。数据极端变化用标准化。分类算法、聚类算法是标准化效果更好。    二分类一般都是输出一个值。 输出两个值的话，权重量增加了一倍。得不偿失。不用。在结尾层使用sigmoid激活函数。\n现在二分类一般都是两个值。\n但是如果你是使用交叉熵损失函数的话，那几个分类，就是输出几个值。\none-hot 的作用和意义 One-hot主要用来编码类别特征，即采用哑变量（dummy variables）对类别进行编码。 它的作用是避免因将类别用数字作为表示而给函数带来抖动。 直接使用数字会将人工误差而导致的假设引入到类别特征中，比如类别之间的大小关系，以及差异关系等等\nTP FP FN TN P recall Accuracy tp = 预测为正，实际为正 fp = 预测为正，实际为负 fn = 预测为负，实际为正 tn = 预测为负，实际为负\np = tp/(tp+fp) R = tp/(tp+fn) acc = (tp+tn)/(tp+tn+fp+fn)\n","id":31,"section":"posts","summary":"标准化和归一化 缩放到均值为0，方差为1（Standardization——StandardScaler()） 缩放到0和1之间（Standar","tags":[],"title":"神经网络杂碎知识点记录","uri":"https://yanyuLinxi.github.io/2021/10/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9D%82%E7%A2%8E%E7%9F%A5%E8%AF%86%E7%82%B9%E8%AE%B0%E5%BD%95lingsui/","year":"2021"},{"content":"调研到的降维方法记录：  PCA tSNE UMAP  5维的数据使用了300个树？？   mds Multidimensional Scaling (mds),  ","id":32,"section":"posts","summary":"调研到的降维方法记录： PCA tSNE UMAP 5维的数据使用了300个树？？ mds Multidimensional Scaling (mds),","tags":[],"title":"DR特征降维总结","uri":"https://yanyuLinxi.github.io/2021/10/dr%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4%E6%80%BB%E7%BB%93/","year":"2021"},{"content":"注意 里面有很多需要其他库的神经网络。比如keras。比如tensorflow。为了避免最后太过复杂，这些自己写最好。\n样例 生成污染数据 contamination = 0.1 # percentage of outliers n_train = 200 # number of training points n_test = 100 # number of testing points X_train, y_train, X_test, y_test = generate_data( n_train=n_train, n_test=n_test, contamination=contamination)  X_train (numpy array of shape (n_train, n_features)) – Training data.\nX_test (numpy array of shape (n_test, n_features)) – Test data.\ny_train (numpy array of shape (n_train,)) – Training ground truth.\ny_test (numpy array of shape (n_test,)) – Test ground truth.\n一般模式 clf = models()\nclf.fit(X_train)\ny_train_scores = clf.decision_scores_ y_test_scores = clf.decision_function(X_test)\nAPI  fit(X) 填充检测器。y 在无监督方法中被忽略。用以训练网络 decision_function() 输出Y的异常分数 predict() 用来拟合 Y predict_proba() 用来输出Y的异常概率。  attribute  decision_scores_ 训练数据的异常值。值越高越不正常。 labels_ 训练数据的二进制标签。0表示正常。1代表异常。  其他API metrics  from pyod.utils.data import evaluate_print  evaluate_print(clf_name, y_train, y_train_scores) 输出ROC和precision值。   可视化  visualize(clf_name, X_train, y_train, X_test, y_test, y_train_pred,y_test_pred, show_figure=True, save_figure=False)     combination  四种模型组合 最大值平均，平均值最大。最大值。平均值。  先获取每一个样本所有点的异常分数 然后进行归一化 utils.utility.standardizer 然后进行计算分数 average maximization aom moa等    保存 from joblib import dump, load # save the model dump(clf, 'clf.joblib') # load the model clf = load('clf.joblib')  Fast Train with SUOD from pyod.models.suod import SUOD # initialized a group of outlier detectors for acceleration detector_list = [LOF(n_neighbors=15), LOF(n_neighbors=20), LOF(n_neighbors=25), LOF(n_neighbors=35), COPOD(), IForest(n_estimators=100), IForest(n_estimators=200)] # decide the number of parallel process, and the combination method # then clf can be used as any outlier detection model clf = SUOD(base_estimators=detector_list, n_jobs=2, combination='average', verbose=False)  ","id":33,"section":"posts","summary":"注意 里面有很多需要其他库的神经网络。比如keras。比如tensorflow。为了避免最后太过复杂，这些自己写最好。 样例 生成污染数据 contamination = 0.1 #","tags":[],"title":"PyOD学习笔记","uri":"https://yanyuLinxi.github.io/2021/10/pyod%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","year":"2021"},{"content":"203 移除链表元素 头节点是什么呢。头节点就是一个空的节点但指向第一个节点。保证了在处理其他节点的时候保持操作一致。返回的时候返回头节点的下一个节点。\n头指针是指向第一个节点\n707 设计链表 基本功题目\n 用好头指针 不要重复造轮子 可以在leetcode中自己定义类。  206 反转链表 写的越来越得心应手了。目前看来没什么问题。在纸上思考好后再写就行。\n可以使用临时变量存储部分值。\n24 交换两个节点 想清楚一点。尽量想快一点。\n多用头节点。 即使没传进来头节点。可以凭空创造一个头节点。\n19 删除链表的倒数第N个节点 使用虚拟头节点。想清楚就行。\n160 intersection of two Linked Lists 想清楚已经有的条件是什么。就从已经有的条件中进行分析。\n这道题将尾部进行对齐。然后判断是否相同。\n当然你投机取巧使用赋值也是不会做时的办法了。\n这道题的解法真的非常有意思\n网上存在的解法差异就是抵消差异，然后开始遍历。 有一种解法非常有意思： https://leetcode.com/problems/intersection-of-two-linked-lists/discuss/49785/Java-solution-without-knowing-the-difference-in-len! 我们可以使用两次迭代来做到这一点。在第一次迭代中，我们将一个链表的指针在到达尾节点后重置到另一个链表的头部。在第二次迭代中，我们将移动两个指针，直到它们指向同一个节点。我们在第一次迭代中的操作将帮助我们抵消差异。所以如果两个链表相交，那么第二次迭代的交点一定是交点。如果两个链表根本没有交集，那么第二次迭代的相遇指针一定是两个链表的尾节点，为空。 //if a \u0026amp; b have different len, then we will stop the loop after second iteration while( a != b){ //for the end of first iteration, we just reset the pointer to the head of another linkedlist a = a == null? headB : a.next; b = b == null? headA : b.next; } 真的非常棒。\n142 linked list cycle 这题非常有意思。判断链表是否有环。就要考虑有环的情况下会发生什么。\n如果有环，我们会在里面绕圈圈。所以我们可以设置两个指针。一个快指针。一个慢指针。\n快指针走两步。慢指针走一步。这样如果有环，两个指针一定会相遇。链表中能用到的就是指针。应该善用指针。\n现在就需要找到环入口。\n这里就要用到数学知识。这里的数学知识虽然比较简单但是很巧妙。最终得到的结论也是： 从入口到循环节点的距离等于相遇节点的距离到循环节点的距离。所以可以设置两个指针，同时出发。相遇时就找到了循环节点了。\n可以说非常巧妙。\n能学到的东西就是，善用指针。善用已知条件构造合理的等式。\n 如果有环，则龟兔赛跑一定会相遇。类似于分针和秒针。 根据条件设立实验条件。  ","id":34,"section":"posts","summary":"203 移除链表元素 头节点是什么呢。头节点就是一个空的节点但指向第一个节点。保证了在处理其他节点的时候保持操作一致。返回的时候返回头节点的下一个节","tags":[],"title":"Leetcode2_链表","uri":"https://yanyuLinxi.github.io/2021/10/leetcode2_%E9%93%BE%E8%A1%A8/","year":"2021"},{"content":"目录：   1. 综述翻译  1.1 发表于   2. Tag 3. 任务描述 4. 方法  基于密度的方法 基于统计的方法  基于参数的  GAUSSIAN MIXTURE MODEL METHODS REGRESSION METHODS   非参数方法 其他统计方法 优缺点   基于距离的方法 基于聚类的方法 基于聚类的   5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释  1. 综述翻译 检测异常值是一个重要问题，已在各个研究和应用领域进行了研究。研究人员继续设计稳健的方案，以提供有效检测异常值的解决方案。在本次调查中，我们对 2000 年至 2019 年异常值检测方法的进展进行了全面而有组织的回顾。首先，我们提供了异常值检测的基本概念，然后将它们从不同的异常值检测技术中分类为不同的技术，例如距离- 、聚类、密度、集成和基于学习的方法。在每个类别中，我们介绍了一些最先进的异常值检测方法，并在性能方面进一步详细讨论它们。其次，我们描述了它们的优缺点和挑战，为研究人员提供每种技术的简要概述，并推荐解决方案和可能的研究方向。本文介绍了异常值检测技术的当前进展，并提供了对不同异常值检测方法的更好理解。最后的开放研究问题和挑战将为研究人员提供未来异常检测方法的清晰路径\n1.1 发表于 IEEE Access\n2. Tag 3. 任务描述 4. 方法 尽管在定义异常值时存在模糊性和复杂性，但通常可以将其描述为与其他数据点显着不同的数据点或不模仿其他点的预期典型行为的点 [5]。与异常值相反的数据点称为内点。\n 基于统计的方法 基于统计的技术在标记或识别异常值方面的基本思想取决于与分布模型的关系。这些方法通常分为两大类——参数方法和非参数方法。 • 基于距离的方法。基于距离的检测算法的基本原理侧重于观测之间的距离计算。一个点被视为离其附近邻居很远的离群点。 • 基于密度的方法。这些方法的核心原理是在低密度区域可以找到异常值，而在密集邻域中可以找到内部值。 • 基于聚类的方法。基于聚类的技术的关键思想是应用标准聚类技术从给定数据中检测异常值。异常值被认为是不在任何大型或密集集群内或附近的观测值。 • 基于图的方法 基于图的方法基于使用图技术来有效地捕获互连实体的相互依赖性以识别异常值。 • 基于集成的方法 集成方法侧重于组合不同模型的结果以生成更稳健的模型以有效检测异常值的想法。它们有助于回答异常值是否应该基于线性模型、基于距离或其他类型的模型的问题。 • 基于学习的方法 基于学习的方法，例如主动学习和深度学习，其基本思想是通过应用这些学习方法来学习不同的模型来检测异常值。  基于密度的方法 基于密度的离群点检测方法的核心原理是在低密度区域可以找到离群点，而假设非离群点（inliers）出现在密集的邻域中。与其最近的邻居有很大不同的对象，即那些远离最近邻居的对象，被标记并始终被视为异常值。他们将本地点的密度与其本地邻居密度进行比较。与基于距离的方法相比，在基于密度的异常值检测方法中，应用了更复杂的机制来对异常值进行建模。\n提出了局部异常因子（LOF）方法，这是第一个基本的松散相关的基于密度的聚类异常值检测方法之一。该技术利用了 k 最近邻。在每个点的 KNN 集中，LOF 利用局部可达性密度 (lrd) 并将其与该 KNN 集的每个参与者的邻居的可达性密度进行比较。\nTang 等人介绍了对 LOF [8] 和简化的 LOF [79] 的改进。 [80]，他们称之为基于连接的离群因子（COF）。 该方法与 LOF 非常相似，唯一的区别是计算记录密度估计的方式。 COF 使用链接距离作为最短路径来估计邻居的局部密度，而 LOF 使用欧几里得距离来选择 K-最近邻居。 这种方法的缺点是对数据分布的间接假设，这会导致不正确的密度估计。 作者提出的关键思想是基于将“低密度”与“孤立性”区分开来。 孤立性被定义为一个物体与其他物体的连通程度。\n在基于密度的方法中，使用的密度估计是非参数的； 它们不依赖于假设的分布来拟合数据。\n在最广为人知的基于密度的方法之一 LOF [8] 中，必须注意的是，在局部异常值不显着的异常值检测过程中，该算法可能会产生大量误报。通常，由于基于密度的方法是非参数的，对于高维数据空间，样本量被认为太小 [27]\n基于统计的方法 基于统计的方法通常分为两大类——参数方法和非参数方法。 两种方法的主要区别在于前者对给定数据中的底层分布模型有一个假设，并从已知数据中估计分布模型的参数。 后一种方法没有任何关于分布模型的先验知识的假设 [98]\n我们将目前使用统计方法检测异常值的一些研究分为三类 - 参数方法、非参数方法和其他类型的统计技术。\n基于参数的 GAUSSIAN MIXTURE MODEL METHODS 对于这种具有底层分布模型假设的方法，异常值检测采用的两种众所周知的方法是高斯混合模型和回归模型\n高斯模型是用于检测异常值的最流行的统计方法之一。 在这个模型中，训练阶段使用最大似然估计 (MLE) 方法 [100] 来执行高斯分布的均值和方差估计。 在测试阶段，应用了一些统计不一致测试（箱线图、均值方差测试）。 杨等人。 [101]，介绍了一种具有全局最优 Exemplar-BasedGMM（高斯混合模型）的无监督异常值检测方法\n可以降低这种计算复杂性的算法可以为未来的研究提供更大的可扩展性。 2015 年，为了更稳健的异常值检测方法，Tang 等人提出了使用具有局部保留投影的 GMM。 [102]。他们结合使用 GMM 和子空间学习在能量分解中进行稳健的异常值检测。在他们的方法中，子空间学习的局部保留投影（LPP）用于有效地保留邻域结构，然后揭示数据的内在流形结构。\n[103] 主成分分析（PCA）方法。本研究解决了先前方法 LOF [8] 和 Tang 等人的研究空白\nREGRESSION METHODS 多年来，使用回归技术进行异常值检测的一些标准方法包括使用马氏距离进行阈值处理、具有双平方权重的稳健最小二乘法、混合模型，然后是替代的振动贝叶斯回归方法 [26]\n非参数方法 核密度估计方法：核密度估计 (KDE) 是一种用于检测异常值的常见非参数方法 [107]。 Latecki 等人在 [108] 中提出了一种使用核函数进行异常值检测的无监督方法。异常值检测过程是通过将每个点的局部密度与邻居的局部密度进行比较来执行的。\n其他统计方法 如箱线图、修剪平均值、极端学生化偏差和狄克逊型检验 [40] .其中，Trimmed mean 更能抵抗异常值，而为了识别单个异常值，Extreme Studentized Deviate test 是正确的选择。 Dixon 型检验具有在样本量较小时表现良好的优点，因为无需假设数据的正态性。巴内特等人。\n优缺点  它们在数学上是可以接受的，并且一旦模型建立起来就有一个快速的评估过程。这是因为大多数模型都是以紧凑的形式制作的，并且在给定概率模型的情况下它们表现出改进的性能。 这些模型通常适合定量实值数据集或一些定量有序数据分布。可以将序数数据更改为合适的值进行处理，从而缩短复杂数据的处理时间。 即使仅限于特定问题，它们也更容易实现。  缺点，挑战和差距：\n 由于它们的依赖性和参数模型中分布模型的假设，由于缺乏有关潜在分布的先验知识，所产生的结果的质量对于实际情况和应用而言大多是不可靠的。 由于大多数模型适用于单变量特征空间，因此它们通常不适用于多维场景。在处理多变量数据时，它们会产生很高的计算成本，这反过来又使大多数统计非参数模型成为实时应用程序的糟糕选择。 在直方图技术中，多元数据的一个基本缺点是无法捕捉不同属性之间的相互作用。这是因为他们不能同时分析多个特征。一般来说，一些流行的统计方法不适用于处理非常高维的数据。需要设计统计技术来支持能够同时分析多个特征的高维数据。 当面临维度增加的问题时，统计技术采用不同的方法。这会导致处理时间增加和误报发送数据的分布。  当捕获正确的分布模型时，基于统计的方法可以在异常值检测过程中有效。在一些现实生活中，例如，在传感器流分布中，没有可学习的先验知识。在这种情况下，当数据不遵循预定分布时，它可能变得不切实际。因此，非参数方法最有吸引力，因为它们不依赖于分布特征的假设。对于无法假设数据分布的大数据流也是如此。对于数据集中均匀分散的异常值，使用统计技术变得复杂。因此，参数方法不适用于大数据流，但对于非参数方法，它们适用。此外，定义标准分布的阈值以区分异常值具有更高的标记不准确概率。对于参数情况，使用高斯混合模型，\n基于距离的方法 最常用的基于距离的异常值检测定义以局部邻域、k-最近邻 (KNN) [121] 和传统距离阈值的概念为中心。它与 k-最近邻分类不同。这些方法主要用于检测全局异常值。最初，搜索每条记录的 k 最近邻居，然后这些邻居用于计算异常值分数。他们主要检查给定对象邻域信息的性质，以确定它们是否靠近邻居或是否具有低密度\n[76] 提出了一种基于局部距离的异常值检测方法，称为基于局部距离的异常值因子（LDOF）。他们的研究表明，与 LOF [8] 相比，在邻居大小范围内的性能有所提高。对成对距离计算的需求是 (O(k2))\n它们简单易懂，因为它们大多不依赖于假设分布来拟合数据。 ii.在可扩展性方面，它们在多维空间中的扩展性更好，因为它们具有强大的理论基础，并且与统计方法相比，它们的计算效率更高。\n缺点、挑战和差距： i．在高维空间方面，它们与基于统计和基于密度的方法有一些相似的缺点，因为它们的性能由于维数灾难而下降。数据中的对象通常具有离散的属性，这使得定义这些对象之间的距离具有挑战性。 ii.当使用基于距离的方法时，诸如邻域和 KNN 搜索之类的搜索技术在高维空间中是一项昂贵的任务。在大型数据集中，可扩展性也不具有成本效益。 三、现有的大多数基于距离的方法无法处理数据流的原因是它们难以维护局部邻域中的数据分布以及在数据流中寻找KNN。这是专门设计用于处理数据流的方法的一个例外。\n基于聚类的方法 基于聚类的技术通常依赖于使用聚类方法来描述数据的行为。为此，包含比其他集群少得多的数据点的较小规模的集群被标记为异常值。需要注意的是，聚类方法与异常值检测过程不同。聚类方法的主要目的是识别聚类，而异常值检测是检测异常值。基于聚类的技术的性能高度依赖于聚类算法在捕获正常实例的聚类结构方面的有效性\n 分区聚类方法：也称为基于距离的聚类算法。 ii. 分层聚类方法：它们将对象集划分为不同级别的组并形成树状结构。 为了分组到不同的级别，它们通常需要最大数量的集群。 三、 基于密度的聚类方法：它们不需要像分区方法那样初始给出聚类的数量； 比如K-Means。 给定集群的半径，他们可以将集群建模为密集区域。 密度聚类方法的一些例子 包括 DBSCAN [153] 和 DENCLUE [154]。 iv． 基于网格的聚类方法 v. 高维数据的聚类方法： CLIQUE [157], HPStream [158]  基于聚类的 与其他相关方法相比，基于集成的方法通常用于机器学习，因为它们的性能相对更好 近年来，已经引入了几种技术，包括：（I）Bagging [37] 和 boosting [184] 用于分类问题（ii）隔离森林 [192] 用于并行技术。 (iii) 对于顺序方法 [185] 和极限梯度提升异常值检测 (XGBOD) [183]​​ 和用于混合方法的袋装异常值表示集成 (BORE) [186]。\n典型：\n isolation Forest  5. 解决了什么问题（贡献）  我们介绍了不同的最新异常值定义、不同的种类、原因、当代检测和处理过程，以及最新的挑战和应用领域。与其他调查不同，我们添加了需要更多关注的新应用领域。 我们扩展了异常值检测算法的类别，并在之前的调查中增加了不同的方法。我们介绍最先进的算法，讨论它们并突出它们的优点和缺点。我们主要引用和讨论在大多数重要调查 [26]、[33] 之后所做的最新研究。 与之前的调查相比，我们通过介绍最近方法的优缺点、公开挑战和不足，显着扩展了对每个不同类别的讨论。我们还总结了一些最先进算法的性能、解决的问题、缺点和可能的解决方案。 我们提出了一些评估异常值检测算法的当代开放挑战。然后我们介绍了标准工具和一些通常用于异常值检测研究的基准数据集。我们通过讨论 OD 工具选择和选择合适数据集的挑战来扩展我们的讨论。 我们确定了一些挑战，并最终为未来的研究推荐了一些可能的研究方向。  6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 ","id":35,"section":"posts","summary":"目录： 1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 基于密度的方法 基于统计的方法 基于参数的 GAUSSIAN MIXTURE MODEL METHODS REGRESSION METHODS 非参数方法 其他统计方法 优缺点 基于距离的方法 基于","tags":["论文阅读笔记"],"title":"Progress in Outlier Detection Techniques a Survey","uri":"https://yanyuLinxi.github.io/2021/10/progress-in-outlier-detection-techniques-a-survey/","year":"2021"},{"content":"seaborn结合pandas来画好看的图。\n最后网页显示的统计图，可以用seaborn画。\n基本使用  seaborn中两种函数，第一种返回当前设置，第二种设置  管理图表样式 axes_style, set_style 返回样式、设置样式\n主题 darkgrid 白线灰底。不影响数据表示\nwhitegrid 白底，简洁大方，数据元素较大使用。\ndark 没有格子的灰底\nwhite 没有格子的白底\nticks 给轴线分割线段\n布局 使用plotting_context()返回布局。set_context来设置布局。\n布局按相对大小排序分别是：paper, notebook, talk,和poster\n影响标题、线型等。默认时notebook。\n临时设置布局 with sns.plotting_context()\n配色方案  使用color_palette()和set_palette()建立配色方案  color_palette()可以接受的颜色参数形式\n  HTML十六进制字符串（hex color codes）\n color = \u0026lsquo;#eeefff\u0026rsquo;    合法的HTML颜色名字（HTML color names）\n color = \u0026lsquo;red\u0026rsquo;/\u0026lsquo;chartreuse\u0026rsquo;    归一化到[0, 1]的RGB元组（RGB tuples）\n color = (0.3, 0.3, 0.4)    色环  共用API：  sns.palplot(色环) 显示色环 sns.color_palette(色环) 获取色环 sns.set_palette(色环) 设置色环， 和color_palette同参数 每个色环有自己的专门设置的函数，比如hsl: sns.hsl_palette()   Qualitative color palettes 当需要区分离散的数据集，且数据集之间没有内在的顺序  使用色环代码：  sns.color_palette(\u0026ldquo;hls\u0026rdquo;, 10) hls最为常用, husl 在亮度和饱和度上更平均  第一个参数是色环，第二个参数是颜色个数。 l 亮度 s 饱和度。   或者使用sns.huso_palette()函数来调用，两个函数类似。  参数 start 开始 rot number of rotations as_cmap=True 返回colormap对象。 dark, light 控制亮度。     sns.hls_palette(8, l=.3, s=.8) # l是亮度（lightness），s是饱和度（saturation） 使用hls循环配色 最为常用。   Color Brewer颜色循环。 在某些情况下颜色会循环重复 某些颜色循环系统对色盲比较友好（尤其是红绿色盲）  Paired Set2   sequential color palettes 当数据集的范围从相对低值（不感兴趣）到相对高值（很感兴趣）  \u0026ldquo;Blues\u0026rdquo; \u0026ldquo;BuGn_r\u0026rdquo; 添加r倒置，逆序。绿色。 \u0026ldquo;GnBu_d\u0026rdquo; 添加后缀d，则颜色加深 \u0026ldquo;cubehelix\u0026rdquo; 颜色线性变化，打印后也能区分不同颜色，对色盲友好。  cubehelix专属函数: sns.cubehelix_palette可以进一步设置这个色环的更多参数  可以通过dark, light控制亮度、暗度。 通过reverse参数控制是否reverse     sns.light_palette(\u0026ldquo;green\u0026rdquo;) 从明亮向暗产生渐变。 返回可以通过set_palette()设置的颜色 sns.dark_palette(\u0026ldquo;purple\u0026rdquo;)从暗向明产生渐变   Diverging color palettes。当数据集的低值和高值都很重要，且数据集中有明确定义的中点时  BrBG 两端的颜色应该具有相似的亮度和饱和度，中间点的颜色不应该喧宾夺主 coolwarm 常用的diverging 调色  center=\u0026ldquo;dark\u0026rdquo; 将中间设置为黑色。 sep参数controls the width of the separation between the two ramps in the middle region of the palette   diverging_palette 函数使用\u0026rsquo;husl\u0026rsquo;颜色系统，需要在函数中设置两个hue参数，也可以选择设置两端颜色的亮度和饱和度   color_palette()与set_palette()的关系，类似于axes_style()和set_style()的关系 set_palette()的参数与color_palette()相同。 区别在于，set_palette()会改变配色方案的默认设置，从而应用于之后所有的图表。总之就这两个函数来选择配色方案。 临时设置配色方案: with sns.color_palette(\u0026ldquo;PuBuGn_d\u0026rdquo;):   sns.choose_colorbrewer_palette()能够以互动的方式测试、修改不同的参数 只能用于Jupyter Notebook  绘图 绘制单变量分布图  sns.displot()绘制分布图  kind=\u0026ldquo;hist\u0026quot;直方图 kind=\u0026ldquo;kde\u0026quot;核密度图， kind=\u0026ldquo;ecdf\u0026rdquo; rug=True为每个观察值添加一个tick kde=True 打印核密度   sns.histplot()函数，绘制直方图， sns.kdeplot()  相对于sns.distplot()能够设置更多选项 设置shade参数，填充核密度线下方区域 bw_method 确定要使用的平滑带宽的方法 bw_adjust 参数和bin很像。乘法缩放使用 选择的值的因子 cut 正如您在上面看到的，高斯 KDE 过程的性质意味着估计超出了数据集中的最大值和最小值。可以使用 cut 参数控制绘制曲线超过极值的距离。但是，这仅影响曲线的绘制方式，而不影响曲线的拟合方式。 ax参数选择图表绘制在哪个坐标系内 n_levels=60 通过n_levels参数，增加轮廓线的数量，达到连续化核密度图的效果   rugplot() 绘制出现点  绘制双变量分布图  sns.jointplot  散点图 默认绘制散点图  (x=\u0026ldquo;x\u0026rdquo;, y=\u0026ldquo;y\u0026rdquo;, data=df) 注意Seaborn与DataFrame联合使用，data参数指定DataFrame，x、y参数指定列名   六边形图（hexbin plot）  六边形颜色的深浅，代表落入该六边形区域内观测点的数量，常应用于大数据集，与white主题结合使用效果最好 sns.jointplot(x=x, y=y, kind=\u0026ldquo;hex\u0026rdquo;, color=\u0026ldquo;k\u0026rdquo;) # kind参数设置六边形图   核密度图（kernel density estimation）  kind=\u0026ldquo;kde\u0026rdquo;   而sns.jointplot()只能单独绘制，无法添加在其他图表之上 sns.jointplot()绘制后返回JointGrid对象对象，可以通过JointGrid对象来修改图表，例如添加图层或修改其他效果  g = sns.jointplot g.plot_joint(plt.scatter, c=\u0026ldquo;w\u0026rdquo;, s=30, linewidth=1, marker=\u0026quot;+\u0026quot;)       其他画图api  FacetGrid  一次花多个图。    可视化数据集的成对关系  sns.pairplot() PairGrid对象  g = sns.PairGrid(iris)    API  despine()移除右侧上方的横线  默认上右。传入left等参数控制哪边被移除   临时设置主题：with sns.axes_style()  ","id":36,"section":"posts","summary":"seaborn结合pandas来画好看的图。 最后网页显示的统计图，可以用seaborn画。 基本使用 seaborn中两种函数，第一种返回当前设","tags":[],"title":"Seaborn学习笔记","uri":"https://yanyuLinxi.github.io/2021/09/seaborn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","year":"2021"},{"content":"常用库  yellowbrick 用来可视化画图的。 seaborn高端matplotlib画图的。 prettyplotlib 配合seaborn来画漂亮的图。 plotly Express 可视化。 tqdm进度条。 linux后台运行  https://www.cnblogs.com/kaituorensheng/p/3980334.html    机器学习库  pyOD 用来做异常检测的。 plotly 可视化eda神器。  学习地址https://github.com/datawhalechina/wow-plotly     pip 更新注意 pip install selectivesearch -i http://pypi.douban.com/simple \u0026ndash;trusted-host pypi.douban.com 啊我吐了，一直retrying还有可能事因为pip版本太高。 python -m pip install pip==20.2 -i http://pypi.douban.com/simple \u0026ndash;trusted-host pypi.douban.com 通过该命令降级。 pip 安装报错试试这个。\npython 语法记录  params = dict(aa=bb, cc=dd) 来记录参数。  ","id":37,"section":"posts","summary":"常用库 yellowbrick 用来可视化画图的。 seaborn高端matplotlib画图的。 prettyplotlib 配合seaborn来画漂亮的图。 plotly Express 可视化。 tqdm进度条。 lin","tags":[],"title":"常用其他python库","uri":"https://yanyuLinxi.github.io/2021/09/%E5%B8%B8%E7%94%A8%E5%85%B6%E4%BB%96python%E5%BA%93/","year":"2021"},{"content":"Tag:\n异常值检测; 离群值检测; Outlier;\n低维特征  Numeric Outlier Z-score  基于高斯分布（正态分布）外的值标记为异常\nhttps://mp.weixin.qq.com/s?__biz=MzIzODExMDE5MA==\u0026amp;mid=2694182460\u0026amp;idx=1\u0026amp;sn=a4842775394946bb643006e2e7c67be9#rd 多维高斯特征离群点检测\n使用 Mahalanobis 距离检测多元离群点 使用 $x^2$ 统计量检测多元离群点  高维特征 1. DBSCAN\n在DBSCAN聚类技术中，所有数据点都被定义为核心点（Core Points）、边界点（Border Points）或噪声点（Noise Points）。\n核心点是在距离ℇ内至少具有最小包含点数（minPTs）的数据点； 边界点是核心点的距离ℇ内邻近点，但包含的点数小于最小包含点数（minPTs）； 所有的其他数据点都是噪声点，也被标识为异常值；\n2. Isolation Forest\n该方法是一维或多维特征空间中大数据集的非参数方法，其中的一个重要概念是孤立数\n孤立数是孤立数据点所需的拆分数。通过以下步骤确定此分割数\n 随机选择要分离的点“a”； 选择在最小值和最大值之间的随机数据点“b”，并且与“a”不同； 如果“b”的值低于“a”的值，则“b”的值变为新的下限； 如果“b”的值大于“a”的值，则“b”的值变为新的上限； 只要在上限和下限之间存在除“a”之外的数据点，就重复该过程；  与孤立非异常值相比，它需要更少的分裂来孤立异常值，即异常值与非异常点相比具有更低的孤立数。因此，如果数据点的孤立数低于阈值，则将数据点定义为异常值。\n即大于孤立数的特征被标记为异常。\n其他方法 有哪些比较好的做异常值检测的方法？ - 张戎的回答 - 知乎 https://www.zhihu.com/question/38066650/answer/107801822\n 基于矩阵分解的异常值检测  通过PAC进行降维。然后再恢复到原始空间。\n基于聚类的方法 基于聚类的方法 基于聚类的方法是一类无监督的检测方法，通过考察数据点与之间的关系检测异常值。考虑数据 样本中的数据点 大判所该数据点是否属于某个簇，如果不属于任何族，则认为是异常值 计算该数据点与最近的族之间的距离，如果距离很远，则认为是异常值 大判所该数据点是否是小或者稀硫簇的部分，如東是，则该簇中的所有点都是异常值。 上述三条中的第一条可以采用基于密度的聚类方法(如 DBSCAN)进行计算。第二条可以采用k means聚类方法。第三条寻找小族和稀疏簇一般采用 FINDCBLOFI算法，其方法为 通过设置一个参数 Alpha( Ovelalpha\\le1)$S来区別大和小族，至少包含数据集中数据点占比为 S\\alpha$的族是大族，其余的为小族 对每个数据点计算基于簇的局部류常因子（ CBLOF月：对于大族的点， CBLOFT为族的大小和该点与 疾的相似性的乘积；对于小族的点， CBLOF为小族的大小和该点于最近的大簇的相似性的乘积 点与族的相似性代表了点属于族的概率，因此 BLOFE的值可以检辺远离任何簇的异常值，具有最低 CBLOF值的点被认为是异常值  从论文中总结的 Anomaly Detection unsupervised Ensemebles\n  AE autoEncoder\n input-\u0026gt; input/4 -\u0026gt; input/8 -\u0026gt; input/4 -\u0026gt; input -\u0026gt; output    IF Isolation Forest\n tree: 200. max sample size: 256.    LODA\n 400 histograms $1/ \\sqrt{d}$ sparsity    LOF Local Outlier Factor\n number of neighbors is 20.    One-class SVM\n  Subspace Anomaly Detection\n  KDE 核密度检测。\n  离群算法原理总结 Isolation Forest 原理: 隔离：具体来说，该算法利用一种名为孤立树iTree的二叉搜索树结构来孤立样本。由于异常值的数量较少且与大部分样本的疏离性，因此，异常值会被更早的孤立出来，也即异常值会距离iTree的根节点更近，而正常值则会距离根节点有更远的距离。\n检测离群点： 对于如何查找哪些点是否容易被孤立，iForest使用了一套非常高效的策略。假设我们用一个随机超平面来切割数据空间, 切一次可以生成两个子空间（想象拿刀切蛋糕一分为二）。之后我们再继续用一个随机超平面来切割每个子空间，循环下去，直到每子空间里面只有一个数据点为止。直观上来讲，我们可以发现那些密度很高的簇是可以被切很多次才会停止切割，但是那些密度很低的点很容易很早的就停到一个子空间了。\n具体： iForest 由 T 个 iTree 组成，每个 iTree 是一个二叉树结构。该算法大致可以分为两个阶段，第一个阶段我们需要训练出 T 颗孤立树，组成孤立森林。随后我们将每个样本点带入森林中的每棵孤立树，计算平均高度，之后再计算每个样本点的异常值分数\n 第一阶段，步骤如下  从训练数据中随机选择Ψ个点样本点作为样本子集，放入树的根节点 随机指定一个维度（特征），在当前节点数据中随机产生一个切割点 p（切割点产生于当前节点数据中指定维度的最大值和最小值之间） 以此切割点生成了一个超平面，然后将当前节点数据空间划分为2个子空间：把指定维度里小于 p 的数据放在当前节点的左子节点，把大于等于 p 的数据放在当前节点的右子节点 在子节点中递归步骤(2)和(3)，不断构造新的子节点，直到子节点中只有一个数据（无法再继续切割）或子节点已到达限定高度。 循环(1)至(4)，直至生成 T 个孤立树iTree   第二阶段  获得T个iTree之后，iForest训练就结束，然后我们可以用生成的iForest来评估测试数据了。对于每一个数据点 xi，令其遍历每一颗孤立树iTree，计算点 xi 在森林中的平均高度 h(xi) 对所有点的平均高度做归一化处理。异常值分数的计算公式如下所示 https://www.cnblogs.com/guoyaohua/p/isolation_forest.html     总结： 评价：  相较于LOF，K-means等传统算法，孤立森林算法对高纬数据有较好的鲁棒性 Forest具有线性时间复杂度。因为是Ensemble的方法，所以可以用在含有海量数据的数据集上面。通常树的数量越多，算法越稳定。 iForest不适用于特别高维的数据。由于每次切数据空间都是随机选取一个维度，建完树后仍然有大量的维度信息没有被使用，导致算法可靠性降低。高维空间还可能存在大量噪音维度或无关维度(irrelevant attributes)，影响树的构建 iForest仅对Global Anomaly敏感，即全局稀疏点敏感，不擅长处理局部的相对稀疏点(Local Anomaly)。  LODA 没有找到相关介绍\nLOF 原理 LOF通过计算一个数值score来反映一个样本的异常程度。这个数值的大致意思是：一个样本点周围的样本点所处位置的平均密度比上该样本点所在位置的密度。比值越大于1，则该点所在位置的密度越小于其周围样本所在位置的密度，这个点就越有可能是异常点。\n总结 是一种无监督的离群检测方法，是基于密度的离群点检测方法中一个比较有代表性的算法。\n评价 ","id":38,"section":"posts","summary":"Tag: 异常值检测; 离群值检测; Outlier; 低维特征 Numeric Outlier Z-score 基于高斯分布（正态分布）外的值标记为异常 https://mp.weixin.qq.com/s?__biz=MzIzODExMDE5MA==\u0026amp;mid=2694182460\u0026amp;idx=1\u0026amp;sn=a4842775394946bb643006e2e7c67be9#rd 多维高斯特征离群点检测 使用 Mahalanobis 距离检测多元离群点 使用 $x^2$ 统计","tags":["科研学习笔记"],"title":"Outlier异常值检测技术记录","uri":"https://yanyuLinxi.github.io/2021/09/outlier%E5%BC%82%E5%B8%B8%E5%80%BC%E6%A3%80%E6%B5%8B%E6%8A%80%E6%9C%AF%E8%AE%B0%E5%BD%95/","year":"2021"},{"content":"概览 setting 不同风格    plt.style.available 打印样式列表 plt.style.use(\u0026lsquo;seaborn\u0026rsquo;) 使用seaborn风格的图。\n  参数color可选项： b:blue c:cyan g:green k:black m:magenta r:red w:white y:yellow\n  结束时 plt.legend() 在图上放置图例。\n设置坐标轴 plt.xlim(-1, 3.5) #设置x坐标在-1到3.5\nplt.xlabel(\u0026lsquo;degree\u0026rsquo;); plt.ylabel(\u0026lsquo;rms error\u0026rsquo;) 设置横纵坐标的名称。\n图 scatter plt.scatter(x, y, s=None float array, c array-like color map, marker, cmap)\nc是一组数，标注了每个点根据标签使用不同的色彩。\ns是 marker size in points**2 就是粗细 cmap 就是色彩图。 optional: rainbow; Blues; spring\n线图 plt.plot\n plot可以接收多个x,y参数，只要依次给过去就行。在show前绘制的所有plot都能画在一张图上。  柱状图 plt.bar\n热力图 plt.heat\n其他图 plt.box箱图 plt.hist 直方图 plt.pie 饼图 plt.area 面积图\nimshow subplots    fig, ax = plt.subplts(row, columns, figsize=(subplots figsize))\nrow是只子图占用父图多宽。 ax[i]就是子图对象，在上面画图 ax[i].scatter(x[:, 0], x[:, 1])等\n  plt.figure(figsize=(row, columns)) ax = fig.add_subplot(row, column, idx ) idx 从1开始增加。 ax 画图 如 ax.plot\n参数  nrows, subplot行 ncols 列 sharex sharey subplot中x、y共享。同时影响所有plot    图的其他信息  plt.legend(loc) 画图例。在图上哪个地方画图例。  画线时标记label就会显示legand。或者legend有方法后添加图例 label = nolegend 就不会显示图例 loc=0就是自动选择best位置来显示。 ncol=2 控制legand有几列。   plt.grid(True) 显示图片背景中的格子  plt.grid(b=True, which=\u0026lsquo;major\u0026rsquo;, axis=\u0026lsquo;both\u0026rsquo;) which指定绘制的网格刻度类型（major、minor或者both） axis指定绘制哪组网格线（both、x或者y）   设置横纵轴  plt.axis() # shows the current axis limits values；如果axis方法没有任何参数，则返回当前坐标轴的上下限 (1.0, 4.0, 0.0, 12.0) plt.axis([0, 5, -1, 13]) # set new axes limits；axis方法中有参数，设置坐标轴的上下限；参数顺序为[xmin, xmax, ymin, ymax] 同样的方法可以用xlim, ylim来设置。   设置标题  plt.title(\u0026lsquo;Simple plot\u0026rsquo;)   保存图片  plt.savefig(\u0026lsquo;plot123_2.png\u0026rsquo;, figsize=[8.0, 6.0], dpi=200)   设置样式  设置颜色  plt.plot(x, y, \u0026ldquo;color value\u0026rdquo;) 可以为16进制字符串，可以为灰度值，可以为rgb三元组，可以为别名         颜色 别名 HTML颜色名 颜色 别名 HTML颜色名     蓝色 b blue 绿色 g green   红色 r red 黄色 y yellow   青色 c cyan 黑色 k black   洋红色 m magenta 白色 w white    alpha 设置透明度 参数:ls 设置线形。 \u0026ldquo;-\u0026ldquo;实线， \u0026lsquo;:\u0026lsquo;虚线, \u0026lsquo;\u0026ndash;\u0026lsquo;破折线 \u0026lsquo;steps\u0026rsquo;阶梯线, \u0026lsquo;-.\u0026lsquo;点划线 \u0026lsquo;None\u0026rsquo;什么都不画。表示实线 lw 表示线宽。 marker 设置标志。用来标志数据的位置。    标记 描述 标记 描述     \u0026lsquo;1\u0026rsquo; 一角朝下的三脚架 \u0026lsquo;3\u0026rsquo; 一角朝左的三脚架   \u0026lsquo;2\u0026rsquo; 一角朝上的三脚架 \u0026lsquo;4\u0026rsquo; 一角朝右的三脚架   \u0026lsquo;v\u0026rsquo; 一角朝下的三角形 \u0026lsquo;\u0026lt;\u0026rsquo; 一角朝左的三角形   \u0026lsquo;^\u0026rsquo; 一角朝上的三角形 \u0026lsquo;\u0026gt;\u0026rsquo; 一角朝右的三角形   \u0026rsquo;s\u0026rsquo; 正方形 \u0026lsquo;p\u0026rsquo; 五边形   \u0026lsquo;h\u0026rsquo; 六边形1 \u0026lsquo;H\u0026rsquo; 六边形2   \u0026lsquo;8\u0026rsquo; 八边形     \u0026lsquo;.\u0026rsquo; 点 \u0026lsquo;x\u0026rsquo; X   \u0026lsquo;*\u0026rsquo; 星号 \u0026lsquo;+\u0026rsquo; 加号   \u0026lsquo;,\u0026rsquo; 像素     \u0026lsquo;o\u0026rsquo; 圆圈 \u0026lsquo;D\u0026rsquo; 菱形   \u0026rsquo;d\u0026rsquo; 小菱形 \u0026lsquo;\u0026rsquo;,\u0026lsquo;None\u0026rsquo;,\u0026rsquo; \u0026lsquo;,None 无   \u0026lsquo;_\u0026rsquo; 水平线 ' '      更多的plot设置:\n   参数 描述 参数 描述     color或c 线的颜色 linestyle或ls 线型   linewidth或lw 线宽 marker 点型   markeredgecolor 点边缘的颜色 markeredgewidth 点边缘的宽度   markerfacecolor 点内部的颜色 markersize 点的大小     设置背景色  设置背景色，通过向plt.axes()或者plt.subplot()方法传入axisbg参数，来设置坐标轴的背景色   坐标轴刻度：  xticks()和yticks()方法。设置坐标轴的刻度。   其他图  柱状图 bar 水平柱状图 barh 箱线图 boxplot 散点图 scatter  s设置散点大小 c设置散点颜色。 marker设置散点形状   阶梯图 step 条形图 bar 条带图 fill_between 直方图 hist  bins 分类数量 normed 归一化处理 orientation 水平或垂直图 align 居中或者左右。 cumulative 累计直方图   误差条 errorbar 饼图 pie 极坐标图 polar 网格线 rgrids() thetagrids()   图形中的文字放置：  plt.text(0.1, -0.04, \u0026lsquo;sin(0)=0\u0026rsquo;); # 位置参数是坐标 plt.annotate()在途中加注释 annotate()绘制箭头   matplotlib对象分为三层：  FigureCanvas。直接使用pyplot as plt绘制的。 fig = plt.figure() 使用图像绘制的 Axes axes = fig.add_axe() 从属关系，plt最大，axes最小。都从上一级调用 使用axes和fig.add_subplot()的区别。subplot不能控制具体的位置。无法设置嵌套的结构。   twinx() twiny() 从其他数据那里复制x坐标和y坐标。 sharex sharey 共享x、y坐标轴 设置对数坐标轴 set_yscale, semilogx loglog etc. 设置中央坐标轴  ax.spines ax.set_position   设置等高线: pcolor contour() contourf() 地形图 3D 图 曲面图  映射等高线    API  plt.fill_between  x, x+i, x-i, facecolor=\u0026ldquo;基础色\u0026rdquo;, alpha：透明度    ","id":39,"section":"posts","summary":"概览 setting 不同风格 plt.style.available 打印样式列表 plt.style.use(\u0026lsquo;seaborn\u0026rsquo;) 使用seaborn风格的图。 参数color可选项： b:blue c:cyan g:green k:black m:magenta r:red w:white y:yellow 结束时 plt.legend() 在图上放置图例。 设置坐标轴 plt.xlim(-1, 3.5) #设置","tags":[],"title":"Matplotlib学习笔记","uri":"https://yanyuLinxi.github.io/2021/09/matplotlib%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","year":"2021"},{"content":"API 创建数组  numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0)  np.linspace(2.0, 3.0, num=5) output =\u0026gt; array([2. , 2.25, 2.5 , 2.75, 3. ])   np.random  random.rand(d0, d1, \u0026hellip;, dn) d为size np.random.randn(d0, d1, \u0026hellip;, dn)  Return a sample (or samples) from the “standard normal” distribution.   random.randint(low, high=None, size=None, dtype=int)  np.random.randint([1, 5, 7], 10) 多个下限，一个上限。      数组操作  np.newaxis 功能上等同于 np.expand_dims  a.shape 2, 3 print(np.expand_dims(a, 2).shape) =\u0026gt; (2, 3, 1) print(a[:, :, np.newaxis].shape) =\u0026gt; (2, 3, 1)   a = ndarry, b=ndarry  a[b] = 在a中用坐标b去取值。即a[[1,2,3,4]]这样子    判断操作  numpy.all(a, axis=None, out=None, keepdims=, *, where=)  Test whether all array elements along a given axis evaluate to True. example: np.all(y == y_pred)   numpy.any(a, axis=None, out=None, keepdims=, *, where=)  Test whether any array element along a given axis evaluates to True.    计算操作  np.log()取自然对数。 array.std(1)计算第一维的标准偏差standard deviation array.mean(1)求第一维的均值。 value_counts()检查数据。   ","id":40,"section":"posts","summary":"API 创建数组 numpy.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0) np.linspace(2.0, 3.0, num=5) output =\u0026gt; array([2. , 2.25, 2.5 , 2.75, 3. ]) np.random random.rand(d0, d1, \u0026hellip;, dn) d为size np.random.randn(d0, d1, \u0026hellip;, dn) Return a sample (or samples) from the “standard normal” distribution. random.randint(low, high=None, size=None, dtype=int) np.random.randint([1, 5,","tags":[],"title":"Numpy学习笔记","uri":"https://yanyuLinxi.github.io/2021/09/numpy%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","year":"2021"},{"content":" 概览  整体框架  unsupervised  unsupervised methods   cluster   API validation API metrics models embedding Cluster Overview Unsupervised dimensionality reduction  PCA Random projections  johnson_lindenstrauss_min_dim GaussianRandomProjection SparseRandomProjection   Feature agglomeration      概览 整体框架 # set numpy seed. when it comes to random functions, please leave a function to set seed. # load data x, y = sklearn.data # train test data split. # create model entity model = sklearn.model(params) # model train model.fit(x, y) # which contains the train process. # model test results = model.predict(test) # predict the test result # metrics results_proba = model.predict_proba(test) # output the probablity of the each label. models.score() # scores are between 0 and 1, with a larger score indicating a better fit. # for unsupervised method model.transform() # transform new data into new basis model.fit_transform() # performs a fit and a transform on the same input data. model.predict()  unsupervised est = KMeans(4) est.fit(X) out = est.predict(X)\nor est.fit_predict(x)\nunsupervised methods  Isolation Forest  params  n_jobs = -1 means all cpu. contamination means the amount of contamination of the dataset. it means the threshold of whole data. n_estimators: the number of base estimators. max_samples: the number of samples to draw from X to train each estimator   function:  decision_function(X). 返回样本的异常评分。 score_samples(X) return abnormal score of samples.The lower, the more abnormal.      问题：\n 无监督的 fit_predict 和 fit 后 predict有什么区别？  无监督学习基本使用fit_predict。 也不是，fit_predict如其名字，就是先预测。然后进行predict。我们按照神经网络中的做法。8预测，2个predict。   max_samples的作用  cluster clust.cluster_centers_ 是簇类的中心\nAPI validation API  sklearn.metrics.confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)  Compute confusion matrix to evaluate the accuracy of a classification. y_true = [2, 0, 2, 2, 0, 1]    y_pred = [0, 0, 2, 2, 0, 2] confusion_matrix(y_true, y_pred) array([[2, 0, 0], [0, 0, 1], [1, 0, 2]])\n      sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)[source]  Split arrays or matrices into random train and test subsets random_state保证每次划分结果都一样。   model_selection.cross_val_score交叉验证评估分数  通过不停将测试组和训练组分组来评估模型分数。 参数cv表示数据折叠数量。 返回分数。   model_selection.validation_curve  cv是交叉验证的值。 确定不同参数值的训练和测试分数。   learning_curve  确定不同训练集大小的交叉验证训练和测试分数。   GridSearchCV使用网格搜索模型指定参数。  metrics  accuracy_score  Accuracy classification score. In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.    models  model select     svm  sklearn.svm import SVC clf = SVC(kernel=\u0026ldquo;linear\u0026rdquo;) clf = SVC(kernel=\u0026ldquo;rbf\u0026rdquo;)   random forest  sklearn.tree import DecisionTreeClassifer() sklearn.ensemble.RandomForestClassifier sklearn.ensemble.RandomForestRegressor sklearn.ensemble.RandomTreesEmbedding 无监督随机森林 clf = rfclf(n_estimators=100, random_state=0)   cluster  KMeans  fit(x)计算k均值聚类 fit_predict(x)计算样本的聚类中心，并预测聚类索引。     异常检测：  sklearn.ensemble.IsolationForest sklearn.covariance.EllipticEnvelope sklearn.svm.OneClassSVM sklearn.neighbors.LocalOutlierFactor     embedding  manifold:  sklearn.manifold.Isomap() Isomap Embedding. 用来做embedding的。将特征映射到2维后，进行画图。 Non-linear dimensionality reduction through Isometric Mapping    mainfold就是来做映射embedding的\nmake_blobs  函数是为聚类产生数据集 产生一个数据集和相应的标签    Cluster Overview https://scikit-learn.org/stable/modules/clustering.html 中文: https://sklearn.apachecn.org/docs/master/22.html#k-means\n聚类是把相似的对象通过静态分类的方法分成不同的组别或者更多的子集（subset），这样让在同一个子集中的成员对象都有相似的一些属性。\n一些常见聚类方法简介：\n k-means  以空间中的k个点为中心进行聚类。对最靠近它们的对象归类。\nKNN  一个对象的分类由其邻居的多数表决确定。k个最近邻居中最常见的分类决定赋予了该对象的类别。\nUnsupervised dimensionality reduction PCA 使用奇异值分解将data进行线性分解，来将其映射到一个低维的空间。来去除特征之间的相关性。 不支持稀疏输入。输入的数据居中化，但是并没有缩放。\n一般来说先标准化后再进行pca分析。\nParams\n n_components:  Number of components to keep. if is not set, n_components=min(n_samples, n_features) copy bool, True means copy data and transform whiten bool, when true whitening will remove some information from the transformed signal.会从转换后的信号中去除一些信息。 svd_solver auto full arpack randomized  auto: selected based on X.shape and n_components full: full SVD arpack: SVD truncated randomized: run randomized SVD   random_state    Attribute:\n components_  principal axes in feature space. The components are soted by explained_variance_ 具有最大方差的成分   explained_variance_ 保留的n个成分和各自的方差百分比 n_components_ 保留的成分个数n  API:\n fit transform(X)  将数据X转换为降维的数据。模型fit后，对新输入的数据，可以使用transform来降维。    Random projections johnson_lindenstrauss_min_dim JL随即投影 API： https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.johnson_lindenstrauss_min_dim.html#sklearn.random_projection.johnson_lindenstrauss_min_dim\n原理解释： 通俗版JL引理： 塞下N个向量，只需要$O(logN)$维空间。 高维空间中任意两个向量几乎都是垂直的 从$N(0, 1/n)$采样出来的$n*n$矩阵几乎是一个正交矩阵 Params:\n n_samples:  Number of samples   eps:  Maximu distortion rate    Return:\n n_components:  最小的组件来保证很好的最小大小来保守估计随即子空间的最小大小。保证随机投影的有界失真。    GaussianRandomProjection 高斯随机投影\nparams:\n n_components 目标投影空间的维度。  可以根据样本数量和johnson-Lindenstrauss引理给出的界限自动调整。嵌入的质量由参数eps控制   eps  当n_components设置为“自动”时，根据 Johnson-Lindenstrauss 引理控制嵌入质量的参数。 eps 默认0.1 越小则损失的越少。最终需要的特征数量越多。   random_state  Attrs:\n n_components_:  具体组件数。   其他具体看官方  API：\n fit transform get_params 获取参数保存 set_params 设置参数。  SparseRandomProjection 稀疏随机投影随机矩阵\nparams:\n n_components 同上 density:  auto the value is set as recommended   eps  Parameter to control the quality of the embedding according to the Johnson-Lindenstrauss lemma when n_components is set to ‘auto’.   dense_output  if True will output dense output even input is sparse.   random_state  attr:\n n_components_ components_  Random matrix used for the projection. Sparse matrix will be of CSR format.    Api:\n 同上。  Feature agglomeration ","id":41,"section":"posts","summary":"概览 整体框架 unsupervised unsupervised methods cluster API validation API metrics models embedding Cluster Overview Unsupervised dimensionality reduction PCA Random projections johnson_lindenstrauss_min_dim GaussianRandomProjection SparseRandomProjection Feature agglomeration 概览 整体框架 # set numpy seed. when it comes to random functions, please leave a function to set seed. # load data x, y = sklearn.data # train test data split. # create model entity model =","tags":[],"title":"Scikit Learn学习笔记","uri":"https://yanyuLinxi.github.io/2021/09/scikit-learn%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","year":"2021"},{"content":"目录：   1. 综述翻译  1.1 发表于   2. Tag 3. 任务描述 4. 方法  Overview Data Collection and Pre-processing ML for Data Analytics   5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释  1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 Overview 恶意行为和内部威胁检测系统的建议方法如图1所示。 系统流程如下：\n 数据采集：多源数据采集，统一格式存储。两个主要来源是：  • 用户活动，例如网络流量、电子邮件、文件日志。 • 组织结构和用户资料信息。   数据预处理：对聚合后的数据进行处理，构建表示不同粒度级别的用户活动和个人资料信息的特征向量。 基于构建的特征向量，采用ML算法进行数据分析。 结果以不同的格式呈现，并向系统分析员提供详细的分析。  该系统旨在在安全分析师的参与/监督下进行许多步骤，特别是在初始检测中，其中调查恶意行为和异常活动的早期迹象。人类分析师不仅在分析系统警告和警报方面发挥重要作用，而且在执行必要的操作以在攻击后将系统恢复到“正常”运行方面发挥着重要作用。在这里报告的工作中，我们假设 CERT 提供的基准数据集（第 IV-A 部分）用于评估图 1 的数据分析组件的特定目的。具体来说，我们有兴趣评估在有限基础上训练的 ML 算法检测未知恶意内部人员的真相。为此，采用监督学习算法从获得的关于恶意/正常用户行为的知识（基本事实）中学习。然后，我们探索学习到的解决方案在检测未知恶意内部案例时能够泛化的能力如何。使用监督学习的好处是我们不需要假设数据集群总是与不同的行为同义。这可能会导致比无监督学习/异常检测算法 [13]（第 IV-C2 节）更高的精度。\n 采用监督学习??\n  基于用户的检测？“简而言之，我们认为突出恶意用户而不是事件的结果代表了更重要的系统性能衡量标准。”\n 此外，我们的分析将区分检测到的恶意操作和检测到的恶意用户，两者不一定相同。也就是说，组织内用户角色的多样性会影响所执行操作的数量/类型，包括正常的和恶意的。在许多情况下，用户操作可能会随着时间的推移而变化，并且需要考虑多个上下文，以便处理有关可疑行为的警报 [34]。因此，在这种情况下，高恶意实例检测率可能不一定会转化为检测到所有恶意内部人员。此外，如果将许多不同的正常用户标记为异常，看似很小的误报率可能仍然需要安全分析师的大量关注。简而言之，我们认为突出恶意用户而不是事件的结果代表了更重要的系统性能衡量标准。\n最后，提出了几种措施，例如每个恶意内部人员的检测延迟，或对每个恶意内部人员警报的支持。通过提供这些措施，我们旨在为安全分析师提供更好的支持，并为所提议的系统在实际场景中的成功应用做出积极贡献。\nData Collection and Pre-processing 数据收集和预处理对于内部威胁检测尤其重要，而且对于一般的网络安全任务也至关重要。良好的监控程序与足够的数据收集相结合，可以成功应用 ML 技术并支持安全分析师做出正确的决策。从组织环境中收集的数据可能来自各种来源，并且有许多不同的形式 [4]、[35]。本研究假设组织数据收集在两个主要类别中：(i) 活动日志数据，以及 (ii) 组织结构和用户信息。第一类数据来自不同的日志系统，例如网络流量捕获、防火墙日志、电子邮件、Web 和文件访问。这些代表通常需要及时收集和处理的实时数据源，以便快速检测和响应恶意和/或异常行为。第二类数据代表背景或上下文数据，可以是员工信息、组织中的角色、与其他用户的关系。在许多情况下，该类别还包含更复杂的数据，例如用户的心理测量和行为模型。为了协助数据处理和特征构建，为组织中的每个用户创建了用户上下文模型。模型由与每个用户相关的辅助信息组成，例如分配的机器、与其他用户的关系、角色、工作时间、允许访问等。基于用户上下文模型，可以从传入的数据中快速有序地创建总结用户行为的特征向量。\n 特征提取：从收集到的数据和用户上下文模型中，可以进行特征提取以创建适合训练 ML 算法的数据向量。 首先，给定聚合条件 c，例如持续时间或执行的操作数，基于用户 id 聚合来自不同来源的数据。 随后，对聚合数据进行特征提取，生成固定长度 N 的数值向量 x_c，也称为数据实例，汇总用户动作。 每个向量都包含用户信息——主要是以数字格式编码的分类数据，用于为 ML 算法提供上下文——以及两种类型的特征：   给定聚合条件c，基于用户id聚合来自不同来源的数据。生成固定长度N的数值向量XC。包含频率特征和统计特征。\n • 频率特征，即用户在聚合期间执行的不同类型操作的计数，例如发送的电子邮件数量、下班后访问文件的数量或在共享 PC 上访问的网站数量。 • 统计特征，即数据的描述性统计，例如均值、中位数、标准差。 统计功能中汇总的数据示例包括电子邮件附件大小、文件大小和访问过的网站中的字数。\n图 2 展示了在这项工作中使用 CERT 数据集的情况下的特征创建过程。 该过程允许创建由许多细节组成的信息丰富的特征，例如 PC、时间和特定于动作的特征。 图 2 中显示的最多三个连接的信息组合在一起以生成一个特征，例如共享 PC 上的操作数、下班后的 HTTP 下载数、已发送电子邮件的平均附件大小。 因此，构建的功能集本质上是对信息片段的枚举。1 HTTP 和文件功能要求我们在企业环境中定义可能有助于内部威胁检测的网站和文件类别集。 此外，精心设计的用于收集活动信息的分类方案直接有助于保护隐私的用户监控，因为在数据预处理中不会检查用户访问的特定网站和文件及其内容[36]。\n2）数据粒度：基于上述数据聚合条件c，提取的特征可以具有不同级别的粒度。我们探讨了 c 的两个主要标准：持续时间和执行的操作数量。表一根据不同的粒度级别总结了本研究中提取的数据类型。在持续时间的情况下，假设用户活动的三个数据聚合：周、日或会话 [27]，[29]。用户周和用户日数据实例汇总了用户在相应时间段内的活动。这些粗粒度类型的数据提供了一天或一周内行为的高级概述，其特征计数高于会话和子会话数据。因此，它们可以通过减少提取的数据实例的数量来潜在地加速学习过程。另一方面，用户会话数据点通过捕获用户在 PC 上的操作，从登录到相应的注销，提供更高的数据保真度；或从一次登录到下一次登录。基于会话的数据可用于隔离恶意操作，因为恶意用户倾向于在特定会话中执行恶意操作，而同一天或同一周的其他会话可能仍然正常 [8]。此外，由于会话的持续时间通常比一天短得多，因此当检测到恶意实例时，这种数据类型还可以允许更快的系统响应。\n 三个数据聚合。周、日或会话。这些粗粒度类型的数据提供了一天或一周内行为的高级概述，其特征计数高于会话和子会话数据。另一方面，用户会话数据点通过捕获用户在 PC 上的操作，从登录到相应的注销，提供更高的数据保真度；或从一次登录到下一次登录。基于会话的数据可用于隔离恶意操作，因为恶意用户倾向于在特定会话中执行恶意操作，而同一天或同一周的其他会话可能仍然正常 这句话的意思就是会话比周、日更合适。\n 由于会话可能持续数小时并包含数百个操作，因此我们进一步探索了每个数据实例中汇总的数据量与对恶意行为的潜在系统响应时间之间的平衡。这是通过使用持续时间和执行的动作数量作为将用​​户会话数据实例分成子会话数据实例的标准来完成的。通过这种方式，我们可以控制嵌入到每个数据实例中的信息量。因此，如果基于 ML 的系统能够成功地从短暂的子会话数据中学习以检测恶意行为，则系统的响应时间可能会得到改善。如表 I 所示，根据持续时间，从用户在 PC 上的会话开始时间起每 i 小时创建一个用户子会话 Ti 数据实例。类似地，从登录操作开始，用户在 PC 上的每 j 个操作后都会创建一个用户子会话 Nj 数据实例。 i 和 j 越小，数据的保真度越高，但实例中汇总的用户活动信息量也越少。在第 IV-A 节中进行了实证分析，以确定 CERT 数据集的 i 和 j 的最合适值。\n 为控制会话包括在里面的时间，来控制信息量。控制i，j（每i个小时，每j个动作的session量）。\n  总结下，特征提取，提取了两方面的特征，统计特征（发的邮件里的文本数）和频率特征（访问次数）。 提取了两方面的特征，用户信息，和组织结构信息。属于哪一组。\n  有这个用户的信息组。就是这个人的信息属于同一个组。需要查看一个组的用户的操作是否相似。\n ML for Data Analytics 在本研究中，采用了以下四种众所周知且广泛使用的 ML 算法：逻辑回归、随机森林、神经网络和 XGBoost [7]、[13]、[37]、[38]。 下面给出了算法的简要描述，而更详细的描述可以在 [39] 中找到。\n5. 解决了什么问题（贡献） 6. 实验结果 从受限的数据中进行训练（400个正常、恶意用户前37周，50%的时间）。\n表 IV 和图 6 说明了 IF 实现的结果。结果清楚地表明，当标签信息（尽管有限）可用于训练 ML 算法时，监督学习中的引导搜索将实现卓越的性能，尤其是在 FPR 非常低的情况下。无监督学习算法不够好。\n 有监督可以更好的进行学习，哪怕是使用对比学习的自学习。\n  基于用户的报告效果更好。而且更贴近实际需求。当用户有周被报告为异常的话，就判定其为异常。\n  根据图10，EANOC 这些特征也对分类有帮助。根据图10定义部分特征。\n 每个版本都表征一个拥有 1000 到 4000 名员工的组织。这项工作中使用的数据集 (CERT r5.2) 的 5.2 版模拟了一个在 18 个月内拥有 2000 名员工的组织。 CERT r5.2 由用户活动日志组成，分类如下：登录/注销、电子邮件、Web、文件和 U 盘连接，以及组织结构和用户信息。\nCERT r5.2 中的每个恶意内部人员都属于四种流行的内部人员威胁场景之一：数据泄露（场景 1）、知识产权盗窃（场景 2、4）到 IT 破坏（场景 3）。\n 有四个场景实例。\n 图 3 显示了用户会话数据按动作数量、每个会话的持续时间以及两个特征之间的关系的分布。现在很明显，大多数用户会话数据的动作少于 300 个，超过一半的会话少于 100 个动作。因此，我们得出结论 j = {25, 50} 用于提取 usersubsession Nj 数据。另一方面，会话时长更接近于均匀分布，很大一部分持续时间超过 8 小时。此外，如图 3 所示，许多少于 50 个动作的会话可能会持续超过 10 个小时。因此，我们探索 i = {2, 4} 的值以按时间提取子会话数据。表 II 概述了数据类型以及正常和恶意用户的数量。可以看出，数据分布极度偏斜，恶意内部人员相关数据分别仅占用户周、用户日和用户会话数据的 0.39%、0.19% 和 0.18%。在子会话数据上，这个数字更小，从 0.09% 到 0.15% 不等。此外，在检查不同的内部威胁情景时，似乎存在不同的模式。场景 3（与 IT 破坏相关的恶意行为）拥有最少的用户和数据实例。另一方面，场景 2 和场景 4 中的恶意行为（针对不同类型的知识产权盗窃行为）跨越了很长的时间——8 周（超过 240 个恶意用户周数据实例/30 个用户）。这可能表明恶意内部人员试图通过长时间执行恶意操作来避免检测。然而，当考虑单个恶意会话时，场景 2 和 4 之间会出现不同的特征。虽然场景 4 的几乎所有单个恶意会话都很短（少于 2 小时或 25 个操作），但很大一部分场景 2 会话超过 50 个操作并且跨度超过 4 小时。\n 这里明显突出了会话时间、周、日都会展现出不同的检测效果，会从不同的方面对恶意行为进行检测。\n In this work, our aim is to obtain a realistic estimation of the proposed system’s performance on real-world networked systems, based on scenarios characterized by limitations to the amount of ground truth data available for training the ML algorithms.\n 以真实世界的ground-truth.\n 具体来说，在现实环境中，用于训练检测系统的标记（地面实况）数据很少。因此，真实情况只能从有限的一组经过验证的用户那里获得，而其他人的行为通常是未知的 [14]、[48]。为了模拟这种情况，我们假设了一个主要配置——即之后的现实条件——在给定的时间段内仅从有限的一组用户那里获得真实情况。因此，用于训练 ML 算法的真实数据仅限于 400 个识别出的“正常”和“恶意”用户（组织中的 2000 个用户）的数据，基于前 37 周 – 50% 的时间段数据集覆盖。根据用户数量，这允许 ML 算法从代表 18% 的“正常”用户和 34% 的恶意内部人员的数据中学习。值得注意的是，从检测器的角度来看，训练数据中的“正常”用户只能保证在前 37 周是良性的，而在测试周后期，他们可能会也可能不会变成“恶意”。此外，我们通过呈现仅从未知用户（即在前 37 周内未执行任何恶意操作的用户）获得的结果，进一步确保实验的真实性。通过从系统性能指标中排除已知的恶意用户，即训练数据中包含恶意行为的用户，我们认为所进行的评估反映了现实生活中的情况以及网络安全分析师的兴趣 [34]。评估结果是从一系列实验中获得的，其中每个设置（一种数据类型的 ML 算法）随机重复 20 次。\n在第一个实验中，为了显示传统 ML 应用程序与现实世界网络安全情况之间的对比，我们将上述现实设置与理想（传统）设置进行比较，其中使用整个数据集中随机 50% 的数据进行训练ML 算法。这是在三个数据粒度级别完成的：用户周、用户日和用户会话。\n第二个实验在所有上述数据粒度级别的现实设置中评估 ML 算法，以获得基于实例和基于用户的详细结果。对数据集中提供的每个内部威胁场景的结果进行详细分析。此外，在 CERT r5.2 上训练的模型还用于针对其他版本的 CERT 内部数据进行测试，以探索训练模型在新环境/未知环境下的表现的泛化（不同版本的 CERT 数据模拟不同的组织）。\n学习算法——有监督与无监督：虽然这项工作的重点是使用 ML 算法从有限数量的标记数据中学习以检测看不见的恶意内部人员，但在本节中，我们将在实际训练条件下比较所使用的 ML 算法和隔离森林的性能(IF) [53]，一种突出的无监督学习方法，最近已在许多网络异常检测工作中使用 [54]。 IF 假设异常数据实例比正常实例更容易与数据的其余部分隔离，因此到异常实例的相应叶子的路径长度更短。对于训练 IF 模型，针对每种数据类型调整树的数量。根据用于调查标记异常事件的不同可用预算，我们将警告数据实例的三个不同阈值（1%、5% 和 10%）假设为“异常”。表 IV 和图 6 说明了 IF 实现的结果。结果清楚地表明，当标签信息（尽管有限）可用于训练 ML 算法时，监督学习中的引导搜索将实现卓越的性能，特别是在非常低的 FPR 时。\n 有监督学习算法比无监督要好很多。\n ) 基于实例的结果：基于实例的结果显示在表 V 和图 5 中。总体上一个明显的趋势是 ML 算法的基于实例的性能正在下降（w.r.t. IDR 和 IF1）由于更高的数据粒度级别。具体而言，在以下情况下可以观察到显着差异 在几乎所有情况下比较不同数据粒度级别的基于实例的结果 (IF1)。例如，比较用户周和用户会话之间或用户会话和用户子会话 T2 之间的 RF IF1 都产生 p = 9e−5。图 6 进一步证明了观察结果，其中用户周数据的 AUC 高于用户会话数据。这可以通过嵌入在不同数据类型的每个数据实例中的信息量来解释（参见第 III-B 节），其中粗粒度的数据类型，例如用户周和用户日，涵盖更长的时间段并汇总更多的行为信息，即用户操作，而不是细粒度的数据类型，例如 user-session 和 user-subsession。此外，细粒度数据类型（第 IV-A 部分）中更大的实例计数和更高的不平衡数据分布也可能导致观察到的基于实例的结果的退化。 2) 基于用户的结果：表 V 和图 5 和 7 显示 ML 算法在不同数据粒度级别上基于用户的结果。与在基于实例的结果中观察到的趋势相反，基于用户的结果（UDR、UF1）通常对不同的数据粒度级别更加稳健。除了 XG 算法之外，在大多数情况下，数据类型的度量之间没有大的变化 (\u0026gt;5%)。此外，尽管检测到的恶意内部数据实例的比例相对较低（表 V），但分类器可以学习为大多数恶意内部人员（80% 到 90%）检测至少一个恶意实例。基于用户的结果报告也大大调整了误报率。例如，NN 仅实现了 0.14% 的 IFPR，但在用户会话数据上实现了 3.44% 的 UFPR。这些观察显示了简单地报告每个数据实例而不是每个用户的结果的缺点，前者可能不一定证明检测器检测恶意用户的能力的真实估计。在实践中，似乎基于用户的指标 证明使用细粒度数据类型的合理性，例如用户会话\n 这个想法是很有用的，基于用户的实例的检测要比基于操作的检测更容易检测出异常。\n 问题：怎么基于用户进行检测的？\n用户子会话数据没有优于用户会话数据。\n问题：用户会话数据是怎么构建的？\n通常在所有数据类型上，只有场景 2 中的内部人员被遗漏。值得注意的是，虽然 UDR 和 UFPR 与在 CERT r5.2 上观察到的相似，但鉴于 CERT r5.1 中恶意用户的数量较少，UPr 和 UF1 较低。另一方面，CERT r6.2 似乎提出了新的挑战，可能是由于不同的组织结构。值得注意的是，未检测到 CERT r6.2（场景 5）中的新内部威胁场景。该场景描述了“因裁员而大量减少的用户将文档上传到 Dropbox，并计划将其用于个人利益”的行为 [33]。这个场景不仅代表了训练模型的一种新的恶意行为，而且与其余四个场景相比，它显示出更少的突兀行为。在数据粒度上，用户会话表现出较低的性能 比来自 CERT r6.2 的其他数据类型。这表明在具有不同用户行为模型的不同组织中，用户数据的会话可能代表不同的操作过程。在这种情况下，更聚合的数据类型（例如用户日和/或用户周）可能会显示出更好的结果。总体而言，本节中的结果表明训练的模型对于组织中的内部威胁检测，只能用作不同环境中的初始检测步骤。特定模型需要从头开始重新训练或从现有模型发展而来以获得更高的准确性。此外，需要异常检测来识别新的恶意行为。\n 用户会话对于新的恶意行为效果不好。针对不同的恶意行为需要分别设立模型进行检测。\n 在这项研究中，提出了一种基于机器学习的系统，用于组织网络系统中的内部威胁检测。该工作对四种不同的 ML 算法（LR、NN、RF 和 XG）在多个数据粒度级别、有限的真实情况和不同的训练场景进行了基准测试，以支持网络安全分析师检测未知数据中的恶意内部行为。评估结果表明，所提出的系统能够成功地从有限的训练数据中学习并推广以检测具有恶意行为的新用户。该系统实现了很高的检测率和精度，特别是 当考虑基于用户的结果时。在四种 ML 算法中，RF 明显优于图 10. 用户会话数据中的特征重要性。表九 其他作品的结果 其他算法，在大多数情况下，它实现了高检测性能和 F1-score 以及低误报率。另一方面，NN 允许稍微更好的内部威胁检测性能，但代价是更高的误报率。在数据粒度上，用户会话数据提供高恶意内部检测率和最小延迟。另一方面，用户日数据显示在检测特定内部威胁场景（即场景 2 - 知识产权盗窃）方面的性能略好。此外，当应用于不同组织的数据时，它似乎更具普遍性。未来的工作将调查时间信息的使用 在用户操作中。具体来说，这项工作中的所有模型都提供了基于仅限于单个示例的状态描述的标签。使模型能够看到多个示例或保留状态（循环连接）有可能使模型成为可能 做出非马尔可夫决策。\n7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 ","id":42,"section":"posts","summary":"目录： 1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 Overview Data Collection and Pre-processing ML for Data Analytics 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是","tags":["论文阅读笔记"],"title":"Analyzing Data Granularity Levels for Insider Threat Detection Using Machine Learning","uri":"https://yanyuLinxi.github.io/2021/09/analyzing-data-granularity-levels-for-insider-threat-detection-using-machine-learning/","year":"2021"},{"content":"目录：   概览 1. 基于规则  1.1. 随机森林 1.2. SVM 1.3. 投影寻踪   2. 基于神经网络  2.1. AutoEncoder   3. 其他  3.1. 关联分析   4. 其他知识  回归问题    概览 https://scikit-learn.org/stable/modules/clustering.html 中文： https://sklearn.apachecn.org/docs/master/22.html#k-means\n1. 基于规则 1.1. 随机森林 众多树。每次选择部分信息进行训练。然后选择众数作为最终输出。\nhttps://mp.weixin.qq.com/s?__biz=MzU4ODcyMTI1Nw==\u0026amp;mid=2247483832\u0026amp;idx=1\u0026amp;sn=e23330ead0d312a94c926d54e92cbc67\u0026amp;chksm=fdd93cbecaaeb5a803191c98f0aadac658fbb3f7a14f40c3b5fc6e35f67d0bde9272740b3adc\u0026amp;mpshare=1\u0026amp;scene=23\u0026amp;srcid=\u0026amp;sharer_sharetime=1570582903129\u0026amp;sharer_shareid=8906c7c6e8077a7cd67e079a0339edc8#rd\nRF的回归问题，就是将所有决策树的输出取平均值。最多的还是用来处理分类问题。\n1.2. SVM 1.3. 投影寻踪 https://esl.hohoweiya.xyz/11-Neural-Networks/11.2-Projection-Pursuit-Regression/index.html\ncode：https://github.com/pavelkomarov/projection-pursuit\n2. 基于神经网络 2.1. AutoEncoder 3. 其他 3.1. 关联分析 4. 其他知识 回归问题 回归问题是机器学习三大基本模型中很重要的一环，其功能是建模和分析变量之间的关系。\n分类问题将回归问题离输出散化。\n回归问题将分类问题输出连续化\n一些常见聚类方法简介：\n k-means  以空间中的k个点为中心进行聚类。对最靠近它们的对象归类。\nKNN  一个对象的分类由其邻居的多数表决确定。k个最近邻居中最常见的分类决定赋予了该对象的类别。\n","id":43,"section":"posts","summary":"目录： 概览 1. 基于规则 1.1. 随机森林 1.2. SVM 1.3. 投影寻踪 2. 基于神经网络 2.1. AutoEncoder 3. 其他 3.1. 关联分析 4. 其他知识 回归问题 概览 https://scikit-learn.org/stable/modules/clustering.html 中文： https://sklearn.apachecn.org/docs/master/22.html#k-means 1. 基于规则 1.1. 随机森林 众多树。每","tags":["科研学习笔记"],"title":"无监督聚类算法学习总结","uri":"https://yanyuLinxi.github.io/2021/09/%E6%97%A0%E7%9B%91%E7%9D%A3%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/","year":"2021"},{"content":"目录：   1. 综述翻译  1.1 发表于   2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释  1. 综述翻译 内部人员通常会给组织造成重大损失，而且很难被发现。 目前，已经提出了各种方法来基于分析记录员工活动类型和时间信息的审计数据来实现内部威胁检测。 然而，现有的方法通常侧重于对用户的活动类型进行建模，而没有考虑活动时间信息。 在本文中，我们通过将时间点过程和循环神经网络相结合，提出了一种分层神经时间点过程模型，用于内部威胁检测。 我们的模型能够通过有效模拟活动时间、活动类型、会话持续时间和会话间隔信息的两级结构来捕获对所有活动历史的一般非线性依赖性。 两个数据集的实验结果表明，我们的模型优于仅考虑活动类型或时间信息的模型。\n1.1 发表于 2019 于 IEEE Internatioanl Conference on Big Data. C会。\n2. Tag Insider Threat; RNN;\n3. 任务描述 4. 方法 现有的大多数方法只关注操作类型（网络访问、发送电子邮件等）信息，而没有考虑关键的活动时间信息。在本文中，我们研究如何开发一个能够同时捕获活动时间和类型信息的检测模型。\n在文献中，标记时间点过程（MTPP）是一个通用的数学框架，用于对序列的事件时间和类型信息进行建模。它已被广泛用于预测地震和余震[9]。传统的 MTPP 模型对事件如何发生做出假设，但在现实中可能会违反这些假设。最近，研究人员 [10]、[11] 提出将时间点过程与循环神经网络 (RNN) 相结合。由于神经网络模型不需要对数据做出假设，因此基于 RNN 的 MTPP 模型通常比传统的 MTPP 模型获得更好的性能。\n我们提出了一种基于分层 RNN 的时间点过程模型，该模型能够捕获会话内和会话间时间信息。我们的模型包含两层长短期记忆网络 (LSTM) [12]，它们是传统 RNN 的变体。下层LSTM在会话内层捕获活动时间和类型，而上层LSTM在会话间层捕获时间长度信息。特别是，我们在较低级别的 LSTM 中采用了一个序列到序列模型，该模型被训练以在给定前一个会话的情况下预测下一个会话。上层 LSTM 将来自下层 LSTM 的编码器的第一个和最后一个隐藏状态作为输入来预测两个会话的间隔和下一个会话的持续时间。通过使用普通用户生成的活动序列训练所提出的分层模型，该模型可以利用较低级别的序列到序列模型、两个连续会话之间的时间间隔和会话持续时间来预测下一个会话的活动时间和类型来自上层 LSTM 的时间。\n5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 ","id":44,"section":"posts","summary":"目录： 1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专","tags":["论文阅读笔记","异常行为分析","Insider Threat","RNN"],"title":"Insider Threat Detection via Hierarchical Neural Temporal Point Process","uri":"https://yanyuLinxi.github.io/2021/09/insider-threat-detection-via-hierarchical-neural-temporal-point-process/","year":"2021"},{"content":"目录：   1. 综述翻译  1.1 发表于   2. Tag 3. 任务描述 4. 方法  概览 Graph Construction 关系详细定义 Graph Embedding Detection  k-means簇聚类算法简单讲解： log2Vec 的聚类算法   random walk   5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释  1. 综述翻译 内部员工的常规攻击和新兴的 APT 都是组织信息系统的主要威胁。现有的检测主要集中在用户的行为上，通常分析记录他们在信息系统中操作的日志。一般来说，这些方法中的大多数都考虑了日志条目之间的顺序关系并模拟用户的顺序行为。然而，他们忽略了其他关系，不可避免地导致在各种攻击场景下的表现不尽如人意。我们提出 log2vec，一种基于异构图嵌入的模块化方法。首先，它涉及一种启发式方法，该方法根据日志条目之间的不同关系将日志条目转换为异构图。接下来，它利用适用于上述异构图的改进图嵌入，可以自动将每个日志条目表示为低维向量。 log2vec 的第三个组件是一种实用的检测算法，能够将恶意和良性日志条目分成不同的集群并识别恶意条目。我们实现了 log2vec 的原型。我们的评估表明 log2vec 明显优于最先进的方法，例如深度学习和隐马尔可夫模型 (HMM)。此外，log2vec 显示了其在各种攻击场景中检测恶意事件的能力。\n1.1 发表于 2019 于 Computer and Communications Security (CCS) A会\n2. Tag Cyber Threat; Enterprise; Anomaly detection; GNN; Doc2Vec\n3. 任务描述 现代信息系统已成为当今企业和组织的重要且不可替代的组件。然而，这些系统经常面临来自内部员工的攻击风险，他们授权访问它们并故意使用这种访​​问来影响它们的机密性、完整性或可用性。同时，另一种新兴攻击，高级持续威胁 (advanced persistent threat APT) 也威胁着这些系统。具体来说，APTactors 最初会破坏目标系统中的帐户和主机，然后从这些主机中，他们会通过内网秘密地、持续地破坏多台主机并窃取机密信息\n常见攻击模式: 对抗模型包括以下三种企业和政府常见的攻击场景。 第一种情况是内部员工滥用职权进行恶意操作，例如访问数据库或应用服务器，然后破坏系统或窃取知识产权以谋取个人利益。 二、恶意内部人员获取其他合法用户的凭据 用户通过窥视或键盘记录器，并利用这个新身份来寻求机密信息或在公司中制造混乱,利用这个新身份窃取机密信息（即伪装攻击） 。这两种场景属于典型的内部员工攻击。 第三种攻击是 APTactor 破坏系统中的一台主机，并从该主机上持续破坏多台主机以提升其权限并窃取机密文件。\n现有的方法通常将用户的各种操作（也包括日志条目）转换为可以保存信息的序列，例如日志条目之间的顺序关系，然后使用顺序处理技术，例如。深度学习，从过去的事件中学习并预测下一个事件 [12, 47]。本质上，这些日志条目级方法对用户的正常行为进行建模，并将与其的偏差标记为异常。\n总之，我们面临三个问题：1）如何同时检测上述两种攻击场景，特别是考虑到检测系统的所有三种关系（sequential relationship among log entries, logical relationships among days, interactive relationship among hosts,）； 2）如何在APTscenario中进行细粒度的检测，特别是深度挖掘和分析主机内日志条目之间的关系； 3）如何在没有攻击样本的情况下对训练模型进行检测。\n4. 方法 概览 Log2vec 包含三个组件，如图 1 所示： (1) Graph构建。 Log2vec 构建异构图来整合日志条目之间的多种关系； (2) 图嵌入（也是图表示学习）。这是一种强大的图处理方法，可以根据每个操作在此类图中的关系来学习它们的表示（向量）。对用户的操作进行矢量化，可以直接比较他们的相似性以找出异常； (3) 检测算法，有效地将恶意操作分成单独的集群和弄清楚他们。\n 首先，log2vec 的第一个组件构建了一个异构图。这个数据结构是基于前面的三个关系构建的，这是现有方法在解决两种攻击场景中使用的主要关系[4,12,38,41,47,50,57]（对于问题1） . 其次，我们将日志条目划分为五个属性。根据这些属性，我们深入考虑主机内日志之间的关系，并设计出精细的规则来关联它们。这种设计使正常和异常的日志条目在这样的图中拥有不同的拓扑结构，可以被 log2vec 的后面组件捕获和检测（对于问题 2）。 第三，log2vec 的图嵌入和检测算法在没有攻击样本的情况下将日志条目表示并分组到不同的集群中，适用于数据不平衡的场景（对于问题 3）。此外，图嵌入本身可以为每个操作自动学习表示（向量），而不是手动提取特定领域的特征，从而独立于专家的知识。我们的改进版本可以进一步从上述异构图中差分提取和表示操作之间的多种关系。  Graph Construction Log2vec 的第一个组件是一种基于规则的启发式方法，用于将反映用户典型行为和暴露恶意操作的日志条目之间的关系映射到图形中。log2vec主要考虑了三种关系：（1）causal and sequential relationships within a day 一天内的因果关系和顺序关系； (2) logical relationships among days 多天内的逻辑关系 (3) logical relationships among objects.对象之间的逻辑关系。\n 我们将日志条目分为五个主要属性（主题、对象、操作类型、时间和主机），称为元属性（参见第 3.1 节）这就将构建了一个异构图的基本元素。 在设计关于这三种关系的规则时，我们会考虑这些元属性的不同组合，以关联更少的日志条目并将更精细的日志关系映射到图中。我们使用一个规则，将同一用户的日志条目按时间顺序连接起来（规则 A），将这种关系映射到图形中。 我们考虑两个元属性，主题和时间 在另一个例子中，我们考虑另一个元属性，操作类型，并使用一个规则，将同一用户和同一操作类型的日志条目按时间顺序连接起来（规则B），将一天内的设备连接操作串联起来。在生成对应于 3 天的三个设备连接序列后，我们使用其他规则根据它们的相似性将它们关联起来。 通过日志属性的不同组合，我们设计了涉及较少日志条目的各种行为序列，并将一天内和主机内的日志条目之间的多个更精细的关系映射到图中。 经过图嵌入和检测算法，log2vec 产生小簇来揭示异常操作。 在实践中，可疑集群中涉及的日志条目数量非常少，甚至等于1。因此，log2vec在上述两种攻击场景中对用户的行为进行了更精细的挖掘。 根据 log2vec 中的每条规则，我们将日志条目转换为序列或子图，所有这些都构成了一个异构图。 每个规则，对应一个边类型，是从一个特定的关系派生出来的，如上例。 由于不同的关系在各种场景中扮演不同的角色，我们使用多种边类型而不是权重来区分它们。  关系详细定义 A log entry： \u0026lt; sub, obj,A,T,H \u0026gt; 其中sub是用户的集合。obj 是对象的集合，例如文件、移动存储设备和网站； A是操作类型的集合，如文件操作、浏览器使用等； T 是时间的集合，H 是主机的集合，例如计算机或服务器。\n此外，sub、obj、A 和 H 都有自己的属性集。sub 的属性涉及角色（例如系统管理员）和他所属的组织单位（例如现场服务部门）。 obj 的属性可能包括文件类型和大小。 H 的属性是服务器的功能（例如文件服务器）。也就是说，它把一个登录操作当作如下方式，一个用户（子）登录到（A）一个目的主机（obj） 源一 (H)，就像用户在服务器中写入文件一样。\n关系涉及三类：（1）一天内的因果关系和顺序关系； (2) 天之间的逻辑关系； (3)对象之间的逻辑关系。\n Rule1 (edge1): log entries ofthe samedayare connectedin chronological order with the highest weight (value 1).\nRule1（edge1）：按时间顺序连接当天的日志条目，权重最高（值为1）。\n  Rule2 (edge2): log entries ofthe same host and the same day are connected in chronological order with the highest weight (value 1).\n规则2（edge2）：同一主机同一天的日志条目按时间顺序连接，权重最高（值为1）。\n  Rule3 (edge3): log entries ofthe same operation type, the same host and the same day are connected in chronological order with the highest weight (value 1)\n规则 3（边 3）：相同操作类型、相同主机和同一天的日志条目按时间顺序连接，权重最高（值 1）\n 不同于恶意的行为模式。为了比较从 rule1∼rule3 导出的用户日常行为序列，我们提出 rule4∼rule6，包括分别对应于 rule1∼rule3 的元属性的元属性。通过这些规则，log2vec 分别隔离了异常的行为序列， 来自图中第 3.2.1 节中提到的各种场景。\n Rule4 (edge4): daily log sequences are connected and their weights are positively related to similarity ofthe numbers oflog entries that they contain.\nRule4（edge4）：每日日志序列是连通的，它们的权重与它们包含的日志条目数量的相似性呈正相关。\n  Rule5 (edge5): daily log sequences ofthe same host are connected and weights are positively related to similarity ofthe numbers oflog entries that they contain.\nRule5（edge5）：同一主机的每日日志序列是相连的，权重与它们包含的日志条目数量的相似性呈正相关。\n  Rule6 (edge6): daily log sequences of the same operation type and the same host are connected and weights are positively related to similarity ofthe numbers of log entries that they contain.\nRule6（edge6）：相同操作类型和相同主机的每日日志序列是相连的，权重与它们包含的日志条目数量的相似度呈正相关。\n  Rule7 (edge7): log entries of accessing the same destination host from the same source host with the same authentication protocol are connected in chronological order with the highest weight (value 1).\nRule7（edge7）：使用相同认证协议从同一源主机访问同一目标主机的日志条目按时间顺序连接，权重最高（值为1）。\n  Rule8a (edge8): log sequences of the same destination host and source one with different authentication protocols are connected and weights are positively related to the similarities of the numbers of log entries that they contain.\nRule8a (edge8)：同一目标主机的日志序列和具有不同身份验证协议的源 1 连接在一起，权重与它们包含的日志条目数量的相似性呈正相关。\n  Rule8b (edge8): log sequences of different destination hosts or source ones with the same authentication protocol are connected and weights are positively related to the similarities of the numbers of log entries that they contain.\nRule8b（edge8）：具有相同认证协议的不同目的主机或源主机的日志序列相连，权重与它们包含的日志条目数量的相似性呈正相关。\n  ule9 (edge9): log entries of the same host and accessing the same domain name are connected in chronological order with the highest weight (value 1).\nRule9（edge9）：同一主机访问同一域名的日志条目按时间顺序连接，权重最高（值为1）。\n  Rule10 (edge10): log sequences of the same host and different domain names are connected and weights are positively related to the similarities of accessing modes and numbers of log entries that they contain.\nRule10（edge10）：同一主机不同域名的日志序列是相连的，权重与访问方式的相似性和它们所包含的日志条目数呈正相关。\n 对于每个类别，我们根据元属性的不同组合提出了一些规则。 如图3所示，我们首先提出rule1∼rule3将一天内的日志条目连接成序列，对应关系（1）。 这三个规则从不同方面对用户的行为进行建模，例如 日期、主机和操作类型。 通过这种设计，将用户在陌生主机上进行的操作，或者属于很少进行操作的操作类型，在图中进行隔离。 然后，我们提出了规则 4∼规则 6，以根据关系 (2) 分别桥接这些日常序列。 异常行为序列将与良性行为序列分开。 这六个规则将图 2c 中的关系映射到图形中。 它们主要关联用户跨天在多个主机上的各种类型的操作。\n最后，提出了与关系（3）相对应的四个规则（rule7∼rule10），以考虑用户如何登录/入侵主机并向外部发送机密数据。 具体来说，它们构建了用户登录和网页浏览操作的模式。 有关登录操作的规则，规则 7 和规则 8，考虑如何在内联网内交互访问这些主机，例如图 2d 中的实例。 Web 操作规则，规则 9 和规则 10，侧重于用户通过 Internet 使用浏览器。 Intranet 和 Internet 是主要的入侵源，例如 登录主机或驱动下载。\nGraph Embedding 具体来说，图嵌入涉及图 1 中的随机游走和 word2vec。\n假设一个walker 位于图中的一个节点上，他根据每条边的权重和类型决定下一个要访问的节点。 由他生成的路径，一个节点序列，被视为这些节点的上下文（见第 4.1 节）。 例如，当一个步行者驻留在属于第1天或第2天设备连接序列的节点时（图2a），通过图构建生成，他很少会选择第3天序列中的节点（设备连接），因为链接低 重量。 同样，当他在第 3 天的序列中驻留节点时，他很少会到达第 1 天或第 2 天的序列。\nDetection Log2vec 采用聚类算法对上述向量进行分析，并将良性操作（日志条目）和恶意操作分为不同的集群（第 5.1 节）。\n在聚类之后，我们设置了一个阈值来识别恶意集群。 也就是说，大小小于阈值的集群被视为恶意（第 5.2 节）\nk-means簇聚类算法简单讲解：  定义总共有多少个簇 将每个簇心随机定在一个点上 将每个数据节点关联到最近簇重心所属的簇上 对于每一个簇找到其所有关联点的中心点（取每一个点坐标的平均值） 将上述点变为新的簇心 不停重复直至每个簇所拥有的点不变。  简单来说，聚类算法只是通过某种方式，将结果进行分类。\nlog2Vec 的聚类算法 传统的更新聚类中心的想法，如k-means，不适合内部威胁检测，因为它严重依赖于聚类中心和k的初始化，导致性能不理想。这里提出了一个新的聚类算法，见论文。\nrandom walk Log2vec 的改进是控制邻居节点 (neigh) 的数量并以不同比例的边类型 (ps) 集提取上下文，旨在解决不平衡数据集和各种攻击场景的问题。\n改进版的random walk\n5. 解决了什么问题（贡献）  提出了一种将log日志转为图结构的方法 设计了很多规则，使用这些规则，提出了异构图来构造图。异构图可以允许更多类型的节点互相交互，比如日志文件，用户，操作等。从实验结果来看，异构图会比同构图要好一点。  问题：\n 整个方案非常复杂，复现可行度需要画一个问号。  6. 实验结果 详见Table 3 和 Table 4.\n7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 ","id":45,"section":"posts","summary":"目录： 1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 概览 Graph Construction 关系详细定义 Graph Embedding Detection k-means簇聚类算法简单讲解： log2Vec 的聚类算法 random walk 5. 解决了什么问题（贡","tags":["论文阅读笔记","异常行为分析","Insider Threat","GNN","Doc2Vec"],"title":"Log2Vec a Heterogeneous Graph Embedding Based Approach for Detecting Cyber Threats Within Enterprise","uri":"https://yanyuLinxi.github.io/2021/09/log2vec-a-heterogeneous-graph-embedding-based-approach-for-detecting-cyber-threats-within-enterprise/","year":"2021"},{"content":"目录：   1. 综述翻译  1.1 发表于   2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释  1. 综述翻译 在当前的 Intranet 环境中，信息正变得更容易在广泛的互连系统中访问和复制。任何使用内联网计算机的人都可以访问他无权访问的内容。对于内部攻击者来说，窃取同事的密码或使用无人值守的计算机发起攻击相对容易。在这种情况下，常见的一次性用户身份验证方法可能不起作用。在本文中，我们提出了一种基于鼠标生物行为特征和深度学习的用户身份验证方法，可以准确高效地对当前计算机用户进行连续身份验证，从而应对内部威胁。我们使用一个有 10 个用户的开源数据集进行实验，实验结果证明了该方法的有效性。该方法大约每 7 秒完成一次用户认证任务，错误接受率为 2.94%，错误拒绝率为 2.28%。\n1.1 发表于 Security and communication Networks C刊 由于发的不是很好，所以大致看看。\n2. Tag Insider Threat; CNN;\n3. 任务描述 4. 方法 本文提出了一种基于鼠标动态行为和深度学习的持续身份认证方法来解决内部威胁攻击检测问题\n(i) 我们提出了一种使用鼠标动态行为和深度学习的新型连续身份认证方法。与现有方法相比，它实现了更高的准确性和更短的验证时间。 (ii) 我们不是从原始操作中手动提取特征来表征用户独特的鼠标行为特征，例如移动速度曲线，而是将鼠标动态行为映射到图片中。因此，可以保留鼠标行为的全部细节。 (iii) 我们构建了一个 7 层 CNN 网络来训练鼠标行为图片数据集。网络以少量数据（约18000张图片）收敛。此外，该网络可用于训练其他鼠标行为数据集并轻松实现身份验证\n我们提出了一种可以完全保留所有基本鼠标操作并使用深度学习进行用户身份验证。首先，我们通过特定的方法将鼠标生成的所有动作映射到图像上。然后我们通过 CNN 网络训练图像数据集以创建分类模型。在认证过程中，将用户的鼠标操作按照相同的方法进行映射，然后通过训练好的模型进行分类，从而达到用户识别的目的。我们的方法充分利用了深度学习的优势。首先，在将鼠标行为映射到图像的过程中，我们保留了所有基本的鼠标操作。使用CNN网络时既不需要人工提取特征来训练这些图像数据集，也不需要传统机器学习中的特征提取算法。卷积神经网络可以在训练中自动完成特征提取和抽象。其次，对于如何使用CNN对图像进行分类的问题，有很多成功有效的解决方案。为了利用这一优势，我们将鼠标动作映射到图片上基于鼠标行为的行为轨迹。这将基于鼠标行为的用户认证问题变成了经典的图像分类问题\n5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 ","id":46,"section":"posts","summary":"目录： 1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专","tags":["论文阅读笔记","异常行为分析","CNN","Insider Threat"],"title":"An Insider Threat Detection Approach Based on Mouse Dynamics and Deep Learning","uri":"https://yanyuLinxi.github.io/2021/09/an-insider-threat-detection-approach-based-on-mouse-dynamics-and-deep-learning/","year":"2021"},{"content":"目录：   1. 综述翻译  1.1 发表于   2. Tag 3. 任务描述 4. 方法  概述 细节  Feature Extraction Structured Stream Neural Network  DNN   RNN   输出 内部危害检测：   6. 实验结果 5. 解决了什么问题（贡献） 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释  1. 综述翻译 分析组织的计算机网络活动是早期检测和缓解内部威胁的关键组成部分，这是许多组织日益关注的问题。原始系统日志是流数据的典型示例，可以快速扩展超出人类分析师的认知能力。作为人类分析师的预期过滤器，我们提出了一种在线无监督深度学习方法，以实时检测系统日志中的异常网络活动。我们的模型将异常分数分解为个人用户行为特征的贡献，以提高可解释性，以帮助分析师审查潜在的内部威胁案例。使用 CERT 内部威胁数据集 v6.2 和威胁检测召回作为我们的性能指标，我们新颖的深度和循环神经网络模型优于主成分分析、支持向量机和基于隔离森林的异常检测基线。对于我们的最佳模型，我们数据集中标记为内部威胁活动的事件的平均异常分数为 95.53 个百分点，这表明我们的方法有可能大大减少分析师的工作量。\n1.1 发表于 aaai 2017\n2. Tag Insider Threat; LSTM; unsupervised;\n3. 任务描述 4. 方法 概述 我们提出了一个在线无监督深度学习系统来过滤系统日志数据以供分析师审查。 由于内部威胁行为千差万别，我们不会尝试对威胁行为进行明确建模。 相反，深度神经网络 (DNN) 和循环神经网络 (RNN) 的新变体经过训练，可以识别网络上每个用户的特征活动，并同时实时评估用户行为是正常还是异常。 考虑到流媒体场景，我们方法的时间和空间复杂度作为流持续时间的函数是恒定的； 也就是说，不会无限期地缓存任何数据，并且在将新数据输入到我们的 DNN 和 RNN 模型时进行检测。 为了帮助分析师解释系统决策，我们的模型将异常分数分解为人类可读的导致检测到的异常的主要因素的摘要（例如，用户在凌晨 12 点到早上 6 点之间将异常大量的文件复制到可移动媒体）。\n图 1 概述了我们的异常检测系统。\n 首先，来自系统用户日志的原始事件被输入到我们的特征提取系统中，该系统聚合它们的计数并为每个用户每天输出一个向量。 然后将用户的特征向量输入神经网络，创建一组网络，每个用户一个。 在我们系统的一种变体中，这些是 DNN； 另一方面，它们是 RNN。 在任何一种情况下，不同的用户模型共享参数，但对于 RNN，它们保持独立的隐藏状态。 这些神经网络的任务是预测序列中的下一个向量； 实际上，他们学会了为用户的“正常”行为建模。 异常与预测误差成正比，有足够的异常行为被标记以供分析师调查。  细节 Feature Extraction 我们的系统从这些来源中提取了两种信息：分类用户属性特征和连续“计数”特征。\ncategorical user attribute features分类用户特征是指用户在组织中的角色、部门和主管等属性。有关我们实验中使用的分类特征列表（以及每个类别中不同值的数量），请参见表 1\n除了这些分类特征之外，我们还累积了用户在某个固定时间窗口（例如 24 小时）内执行的 408 项“活动”的计数。计算活动的一个示例是下午 12:00到下午 6:00 之间来自可移动媒体的不常见非诱饵文件副本的数量。图 2 直观地列举了一组计数特征：只需沿着一条从右到左的路径，沿途在每个集合中选择一个项目。所有这些遍历的集合是计数特征的集合。对于每个用户 u，对于每个时间段 t，将分类值和活动计数串联起来变成一个414维的数字特征向量xu\n414=6+408\nStructured Stream Neural Network 我们系统的核心是两个神经网络模型之一，该模型将给定用户的一系列特征向量（每天一个）映射到用户序列中下一个向量的概率分布。 该模型以在线方式同时对所有用户进行联合训练。 首先，我们描述了我们的 DNN 模型，它没有明确地对任何时间行为进行建模，然后是 RNN，它可以。 然后我们讨论用于预测结构化特征向量和识别特征向量流中的异常的其余部分。\nDNN 根据前文所说，一共分成了T个时间段。对于每个时间段组成了vectors $x_t^u$, DNN将这些$x_t^u$转为$h_t^u$\nRNN 前面的T个特征值$h_t^u$按顺序送入LSTM。这样当前输入不仅与当前时间段有关，而且和前一个时间段有关。\n输出 RNN得到的特征$h_{t-1}$用来进行预测。\n对于408个统计量counter和6个角色特征C={R,P,F,D,T,S}分别进行预测\n Therefore, the P is actually the joint probability over the counter vector and each of the categorical variables: role (R), project (P), functional unit (F), department (D), team (T) and supervisor (S).\n 这里使用7个单层隐藏层来得到输出概率： 对于6个角色属性使用softmax函数进行归一化得到离散概率。 对于counter使用正态分布获取连续概率分布。\n 我们将六个分类变量的条件概率建模为离散的，而我们将计数的条件概率建模为连续的。对于离散模型，我们使用标准方法：类别 k 的概率只是向量 θ(V) 的第 k 个元素，其维度等于类别的数量。例如，有 47 个角色，所以 θ(R) ∈ R47。因为我们使用 softmax 输出激活来产生 θ(V)，所以元素是非负的并且总和为一\n  对于计数向量，我们使用多元正态密度。我们考虑两个变体。第一个，我们的模型输出平均向量 μ (θ(ˆx) = μ)，我们假设协方差 Σ 是恒等式。使用恒等协方差，最大化真实数据的对数似然相当于​​最小化平方误差。在第二个中，我们假设对角协方差，我们的模型输出均值向量和 Σ 对角线的对数。模型的这部分可以看作一个简化的混合密度网络（Bishop 1994）。\n 文中使用两种预测方式运算。\n 下一个时间步预测：用本时间步特征预测下一个时间步的值 同一个时间步预测：用本时间步特征预测本时间步的值。  使用本时间步值的原因：\n An auto-encoder is a parametric function trained to reproduce the input features as output. Its complexity is typically constrained to prevent it from learning the trivial identity function; instead, the network must exploit statistical regularities in the data to achieve low reconstruction error for commonly found patterns, at the expense of high reconstruction error for uncommon patterns (anomalous activity). Networks trained in this unsupervised fashion have been demonstrated to be very effective in several anomaly detection application domains (Markou and Singh 2003).\n 自动编码器是经过训练的参数函数，用于将输入特征再现为输出。 它的复杂性通常受到限制，以防止它学习琐碎的身份函数； 相反，网络必须利用数据中的统计规律来实现常见模式的低重构误差，但代价是不常见模式（异常活动）的高重构误差。 以这种无监督方式训练的网络已被证明在几个异常检测应用领域非常有效（Markou 和 Singh 2003）。\n内部危害检测： 我们模型的目标是检测内部威胁。我们假设以下条件：我们的模型产生异常分数，用于从最异常到最少对用户天数进行排名，然后我们将排名最高的用户天对提供给判断异常行为是否表明内部威胁的分析师。我们假设有一个每日预算，它规定了每天可以判断的最大用户日对数，并且如果向分析师呈现内部威胁的实际案例，他或她将正确检测到它。\n One key feature of our model is that the anomaly score decomposes as the sum over the negative log probabilities of our variables; the continuous count random variable further decomposes over the sum of individual feature terms: (xi − µi)/σi. This allows us to identify which features are largest contributors to any anomaly score;\n 我们模型的一个关键特征是异常分数分解为我们变量的负对数概率的总和；连续计数随机变量进一步分解单个特征项的总和：(xi − µi)/σi。这使我们能够确定哪些特征对任何异常分数的贡献最大；\n为了适应在线场景，我们对标准训练方案进行了重要调整。对于 DNN，主要区别在于每个样本只能观察一次的限制。对于 RNN 来说，情况更为复杂。我们同时训练多个用户序列，每次看到用户的新特征向量时都会反向传播和调整权重。从逻辑上讲，这对应于每个用户训练一个 RNN，其中权重在所有用户之间共享，但隐藏状态序列是每个用户。在实践中，我们通过使用补充数据结构训练单个 RNN 来实现这一点，该结构存储每个用户的过去输入以及隐藏和单元状态的有限窗口。每次将用户的新特征向量输入模型时，在计算前向传播和反向传播误差时，该用户的隐藏状态和单元状态将用于上下文。\n简单点讲，每个用户拥有自己的时间序列。\n6. 实验结果 训练和测试时，仅训练和测试了工作日，将周末分开了。因为工作日和周末的正常情况在性质上是不同的。如果需要，可以训练第二个系统模拟正常的周末行为。\n测试显示，当前时间步预测比下一个时间步预测效果要稍微更好点。 Cumulative Recall 累计召回率。 precision 衡量查准率。召回率衡量查全率。\n我们进行了两次分析以更好地理解我们系统的行为，使用我们最好的 DNN 模型来说明。首先，我们看看时间对模型异常概念的影响。由于模型开始时完全未经训练，所有用户的异常分数在最初几天都非常高。当模型看到用户行为的例子时，它很快就会知道什么是“正常”。图 4 显示了作为天函数的异常（在前几天的“老化”期之后开始，以保​​持 y 轴刻度易于管理）。百分比范围显示（根据当天的用户计算），恶意（内部威胁）用户天覆盖为红点。请注意，所有恶意事件都高于异常的第 50 个百分位，大多数都高于第 95 个百分位。\n在我们的第二个分析中，我们研究了每日预算的影响 关于最佳 DNN、最佳 LSTM 和三个基线模型的回忆。图 5 绘制了这些召回曲线。令人印象深刻的是，在每日预算为 425 的情况下，DNN-Diag、LSTM-Diag 和隔离森林模型都获得了 100% 的召回率。它还表明，使用我们的 LSTM-Diag 系统，只需 250 的预算即可获得 90% 的召回率（分析师需要考虑的数据量减少了 93.5%）。\n5. 解决了什么问题（贡献） 我们的模型试图解决将机器学习应用于网络安全领域（Sommer and Paxson 2010）的几个关键困难。\n 网络上的用户活动在几秒到几小时内通常是不可预测的，这导致难以找到“正常”行为的稳定模型。我们的模型以在线方式持续训练，以适应数据中不断变化的模式。 此外，恶意事件的异常检测尤其具有挑战性，因为攻击者经常试图密切模仿典型行为。我们将系统日志流建模为具有用户元数据的交错用户序列，以便为网络上的活动提供精确的上下文；例如，这允许我们的模型识别用户、同一角色的员工、同一项目团队的员工等的真正典型行为。 我们评估我们的模型在合成 CERT Insider Threat v6.2 上的有效性数据集（Lindauer et al. 2014；Glasser and Lindauer 2013），其中包括带有内部威胁活动行级注释的系统日志。真实威胁标签仅用于评估。  7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 ","id":47,"section":"posts","summary":"目录： 1. 综述翻译 1.1 发表于 2. Tag 3. 任务描述 4. 方法 概述 细节 Feature Extraction Structured Stream Neural Network DNN RNN 输出 内部危害检测： 6. 实验结果 5. 解决了什么问题（贡献） 7. 如何想到该方法 8. 我","tags":["论文阅读笔记","异常行为分析","RNN"],"title":"Deep Learning for Unsupervised Insider Threat Detection in Structured Cybersecurity DataStreams","uri":"https://yanyuLinxi.github.io/2021/09/deep-learning-for-unsupervised-insider-threat-detection-in-structured-cybersecurity-datastreams/","year":"2021"},{"content":"目录：   1. 综述翻译  1.1 发表   2. Tag 3. 任务描述 4. 方法  概览 特征提取： 自编码器   5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释  1. 综述翻译 近年来，恶意内部人员威胁已成为组织可能面临的最重要的网络安全威胁之一。由于内部人员具有逃避部署的信息安全机制（例如防火墙和端点保护）的天然能力，因此检测内部人员威胁可能具有挑战性。此外，与组织为入侵/异常检测目的收集的审计数据量相比，恶意内部人员的行为留下的数字足迹可能微不足道。为了从大量复杂的审计数据中检测内部威胁，在本文中，我们提出了一种检测系统，该系统使用深度自动编码器的集合来实现异常检测。集成中的每个自动编码器都使用特定类别的审计数据进行训练，这些数据准确地代表了用户的正常行为。原始数据和解码数据之间获得的重建误差用于衡量任何行为是否异常。在数据经过单独训练的自动编码器处理并获得各自的重建误差后，使用联合决策机制报告用户的整体恶意评分。使用用于内部威胁检测的基准数据集进行数值实验。结果表明，所提出的检测系统能够以合理的误报率检测所有恶意的内部人员行为。\n1.1 发表 2018-ICDMW，数据库B会的workshop\n2. Tag Insider threat; Autoencoders; Unsupervised\n3. 任务描述 4. 方法 概览 在我们提议的内部人员检测系统中，我们总共创建了四个检测器，分别用于四类审计数据，即登录/注销活动记录、文件操作、USB 设备操作和 http 活动记录。假设用户的行为在整个审计数据集中是一致的，则为审计数据的每个类别提取许多基于频率的特征。这些特征用作训练深度自动编码器的输入。\n当经过训练的自动编码器充当特定正常用户行为的基线模型时，任何受恶意行为影响的特征向量都应显着偏离基线模型，并应报告为异常。\n由于自动编码器由编码器和解码器组成，因此偏差是通过使用解码器重建编码特征向量时产生的误差来衡量的。在四个自编码器检测器运行完数据后，对每个检测器应用 top-N 推荐算法，产生四维归一化的顶部异常向量。\n然后，我们将异常向量归因于用户，并对每个用户的异常向量进行加权以获得恶意评分。随后再次应用top-N推荐算法以报告具有最高恶意评分的用户以供进一步调查。\n我们使用 CMU 的内部威胁测试数据集进行数值实验，以评估每个自动编码器的最佳构造，这些参数包括层数、激活函数类型和损失函数类型等各种参数。最后，我们证明了我们可以构建一个检测系统，该系统结合了不同的自动编码器架构，以可管理的误报率检测恶意内部攻击。特别是，本文做出了以下贡献：\n具体：\n 我们使用了基于主机的数据（即登录/注销活动、文件操作和 USB 设备操作）和基于网络的数据（即 http 活动）。 为每个类别的数据集提取了许多特征。这些特征被转发到自动编码器，从而形成一个对用户行为进行建模的神经网络。换句话说，每个自编码器代表给定审计数据集的特定基线模型，任何偏离模型的行为（即可检测的自编码器重建错误）都应标记为“异常”。 当所有自动编码器模型组合在一起时，可以从各种不同的审计数据视图中更好地识别用户的行为。一般而言，每个审计数据集可能仅提供微弱或不提供异常行为的指示。然而，当模型被用作异常行为的弱指标集合时，用户的恶意行为水平可能会被检测到。  概览总结：提取四种数据的特征=》自编码器重构，产生误差=》训练=》topN计算前N个恶意行为=》对每个用户使用topN计算恶意行为。\n特征提取：  时间粒度通常是一个需要考虑的关键因素，它可以确保提取的特征能够反映用户的正常行为，同时能够检测任何异常/异常变化。一方面，小粒度通常会引入大方差，从而产生过拟合的基线模型。另一方面，大的时间粒度将导致无法突出任何异常行为的欠拟合基线模型。在我们提出的系统中，我们将时间粒度设置为每小时，因为就文献中观察到的内容以及我们在实验中获得的内容而言，这是最合适的程度。所有特征均从用户每天（24 小时）的活动中提取 计算每个用户的每小时登录频率以描述用户的每日登录行为/模式。 直观地说，这种登录模式对于用户来说很常见，因为他们通常在固定时间开始一天，比如 8:00 AM 或 9:00 AM。 同样，每个登录事件都会有一个相应的注销事件。 生成的每小时登录/注销模式形成一个长度等于 48 的正整数向量 文件审计数据提供有关用户对文件的操作的信息； 例如，对此类文件进行的操作（例如，打开、写入、复制和删除）以及文件传输到的位置。 这里应用与登录/注销相同的概念，以提取文件操作特征，即获取每个用户的每种类型文件操作的每小时频率。 对于来自同一业务单元的一组用户，期望用户以类似的方式访问和操作文件，这体现在文件副本数量等特征上。 否则，如果用户执行文件复制的频率明显高于基线建议的频率（根据他/她自己的个人资料和他/她在组织中的同事的个人资料），则应引发关注事件。 随后将所有四种文件操作模式串联起来，生成由 96 个正整数组成的文件审计特征向量。 USB 设备审计数据记录用户是否已将拇指驱动器连接到/从其计算机主机断开连接，并记录访问了哪些目录。 将文件从用户计算机传输到拇指驱动器通常与恶意内部人员行为密切相关。 事实上，它已被确定为从计算机网络中窃取数据的三种最常见方法之一 [3]。 在所提出的系统中，我们将拇指驱动器连接和断开操作的每小时频率统计数据连接在一起，产生 48 个正整数的特征向量。 传统上，http 审计数据能够提供非常丰富的信息，包括用户如何访问 Internet、数据如何传入和传出网络，甚至计算机与网络的交互行为模式。 然而，我们的工作旨在以有效和高效的方式检测恶意内部人员，因此，我们只关注用户在操作层面的活动。 特别是，用户基于 http 的活动被限制为三种类型，即：访问、上传和下载。 访问活动表示用户正在浏览网页； 一旦用户通过 HTTP 协议传出数据，这个活动就会触发一个上传活动； 相反，通过 HTTP 协议接收数据会触发下载活动。 http 审计数据特征向量连接每个活动的每小时频率。 这会产生 72 个正整数的特征向量。 表一总结了审计数据的提取特征。 从每一类审计数据中提取的特征然后被转发到一个单独的自动编码器。 自编码器从每个特征向量中提取最常见的信息模式。 然后，通过研究自动编码器重建特征向量的程度，我们可以确定用户的日常行为是否与以前的行为不同，以及它与他/她的同龄人的行为有何不同。   总结：特征提取部分每一天都提取了24个小时的特征，每个日志文件每小时提取2-4个特征，总共就提取了48-96个特征。训练时使用前k个做训练，使用第k+1天做测试。\n自编码器 从异常检测的角度，我们希望将用户的正常行为表征为等同于该用户或用户组的基线模型。任何恶意行为都可能被识别为与基线模型的偏差。\n在对用户的正常行为建模时，我们面临两个挑战：1）审计数据中存在复杂的非线性关系（例如，用户登录计算机的时间不一定与用户实际在计算机上花费的时间相关），以及2）很少有标签（如果有的话）可以提前表明“好”和“坏”审计数据实例，这意味着我们被迫进行无监督或半监督\n我们使用深度自动编码器，因为它能够表示非线性关系，更重要的是，作为神经网络家族的一员，它用于无监督学习。\n自编码器的有效性在很大程度上取决于自编码器模型及其相关参数的正确构建。例如，需要考虑各种参数，例如要采用的损失函数、要选择的层数以及对于每一层要采用的最佳激活函数。这些参数可能会对性能产生显着影响。在这项研究中，我们优化了这些自动编码器参数，\n自编码器： 编码：$z=f(Wx+b)$\n解码: $x'=f'(W\u0026rsquo;z+b')$\nLoss: $L(x, x') = ||x-x'||^2=||x-f'(W'(f(Wx+b))+b')||^2$\n图 3 说明了从一天的登录/注销活动中提取的几个采样的正常/异常特征向量，其中每个像素代表登录/注销审计数据。一个 6×8 像素的图像表示发送特征向量的归一化值（从 0 到 1）。\n对于正常特征向量的情况，前两行分别可视化原始特征向量和重建特征向量。底部两行显示相同的登录/注销特征向量，但用于异常用户行为情况。从图的顶部两行，我们可以观察到法线特征向量看起来非常相似，这与我们假设存在可以表示常见用户行为的基线模型一致。底部两行的模式表明被恶意登录/注销操作污染的特征向量与正常特征向量不同，并且它们的重构形式放大了这种差异。直观地，我们能够通过测量其原始形式和重建形式之间的差异来检测异常特征向量。\n自编码器的架构设计可以对自动编码器的性能有显着影响 [13]。应考虑以下因素：1) 应使用的总层数，包括输入和输出层 2) 对于每一层，隐藏单元的数量和激活函数的选择，以及 3) 损失函数的选择被最小化。由于我们总共应用了四个自动编码器，四个审计数据类别中的每一个都有一个，因此可以通过为每个自动编码器使用不同数量的层来获得每个数据类别的最佳特征表示。\n例如，如图 3 所示，一般每个用户每天只有 2-3 次登录和注销计算机，导致特征向量非常稀疏，而登录和注销的频率是按小时计算的.在这种特殊情况下，正如我们的数值实验将显示的那样，2-3 层足以用于登录/注销自动编码器。\n图 3 表明这些特征不像从图像或文本中提取的特征那么复杂，并且其中一些特征具有很强的相关性。因此我们倾向于选择简单的激活函数，例如线性单元。为了最大化正常和异常用户行为之间的可分离性，我们试图使用一种损失函数来惩罚结构差异而不是数值差异\n然而，在实践中，一般很难选择一个合适的阈值，尤其是当数据集很大时[19][20]。在我们的例子中，我们使用 top-N 推荐算法 [21] 只生成 top-N 异常行为特征向量。换句话说，每个审计数据检测器使用自己训练的自动编码器对所有特征向量进行重建，按降序对重建错误进行排序，并仅报告前 N 个。最后，将来自四个检测器中每一个的前 N ​​个重构误差组合在一起以做出最终决定。由于每个检测器都关注特定攻击向量的结果，因此组合不同的审计数据检测器以识别内部攻击更有意义。 1 其中 μ 和 σ 分别表示重构误差的均值和标准差，k 对应于检测器索引（k = 1, 2, 3, 4）。 其次，然后将前 N 个归一化重建错误组合在一起，按用户和日期对齐。 当用户没有一个检测器的输入特征向量时，错误率归零值。 然后将平均重建错误计算为恶意分数，对应于每对用户和日期。 最后，再次应用top-N算法以报告具有最大恶意评分的前N对用户和天。 然后，计算机应急响应小组 (CERT) 分析员可以使用此用户日对订购来进一步调查异常行为。\n我们将误报率固定在 1% 到 10% 的范围内。图 5 说明了得到的 RoC 曲线，从中我们可以得出结论，我们可以通过仅调查总（组合）活动的 6% 来检测所有攻击（即，当 TPR=100% 时）。随后，我们还可以确定理论是什么 - 对于相同的检测精度，它可以达到的最小误报率。结果表明，通过调查 0.1% 的原始数据集，我们已经能够达到 66.7% 的真阳性率，通过调查 5.6% 的原始数据集，我们可以检测到所有的攻击。这样的结果证明了所提出的检测系统的实际可行性。在实际场景中，这意味着给定的组织每天监视 5000 个用户的行为，并且每个用户每天可能只生成一个事件（特征向量），那么最多只需要 300 个事件由 CERT 人类专家手动调查，这显着减少了分析师的整体认知工作量。\n检测总结：针对四个检测器的前N个输出进行平均加权组合。\n5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 ","id":48,"section":"posts","summary":"目录： 1. 综述翻译 1.1 发表 2. Tag 3. 任务描述 4. 方法 概览 特征提取： 自编码器 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点","tags":["论文阅读笔记","异常行为分析","Unsupervised"],"title":"Anomaly Based Insider Threat Detection Using Deep Autoencoders","uri":"https://yanyuLinxi.github.io/2021/09/anomaly-based-insider-threat-detection-using-deep-autoencoders/","year":"2021"},{"content":"目录：   1. 综述翻译 2. Tag 3. 任务描述 4. 方法  Deep Feedforward Neural Network RecurrentNeuralNetwork. Convolutional Neural Network GraphNeuralNetwork. Challenges   5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释  1. 综述翻译 内部威胁作为网络空间中最具挑战性的威胁之一，通常会给组织造成重大损失。虽然安全和数据挖掘社区对内部威胁检测问题已经研究了很长时间，但传统的基于机器学习的检测方法严重依赖特征工程，难以准确捕捉内部人员和普通用户之间的行为差​​异，原因是与底层数据特征相关的各种挑战，例如高维、复杂、异构、稀疏、缺乏标记的内部威胁，以及内部威胁的微妙和适应性。先进的深度学习技术提供了一种从复杂数据中学习端到端模型的新范式。在这个简短的调查中，我们首先介绍一个常用的内部威胁检测数据集，并回顾有关此类研究的深度学习的最新文献。现有研究表明，与传统的机器学习算法相比，深度学习模型可以提高内部威胁检测的性能。然而，应用深度学习来进一步推进内部威胁检测任务仍然面临一些限制，例如缺乏标记数据、自适应攻击。然后，我们讨论这些挑战，并提出有可能应对挑战并进一步提高深度学习性能的未来研究方向。 内部威胁检测。\n2021- Computer \u0026amp; Security 网络与信息安全的B会。\n2. Tag database insider threats; insider attack; deep learning\n3. 任务描述 在协调中心 (CERT/CC) 中，恶意内部人员被定义为“已经或已经授权访问组织的网络、系统或数据的现任或前任员工、承包商或业务合作伙伴，并故意超出或故意使用该组织的网络、系统或数据。以对组织信息或信息系统的机密性、完整性或可用性产生负面影响的方式访问。”\n最近的一项调查根据检测中使用的策略和特征将内部威胁检测技术进一步分为 9 类：（1）基于异常的方法，（2）基于角色的访问控制，（3）基于场景的技术，（4） 诱饵文件和蜜罐技术，(5) 使用心理因素进行风险分析，(6) 使用工作流进行风险分析，(7) 改进网络防御，(8) 通过访问控制改进防御，以及 (9) 过程控制以劝阻内部人员 .\n异常检测是识别与所有其他实例不同的实例，这是欺诈检测、入侵检测和视频监控等多种应用的重要问题[13]。异常在数据挖掘和统计文献中被称为异常、异常或异常值，粗略地说，内部威胁可以被视为一种异常。\n4. 方法 最近的一项调查 [12] 将深度分类基于标签的可用性将基于学习的异常检测分为三组，即监督、半监督和无监督的深度异常检测。在某些情况下，当正常和异常数据都可用时，有监督的深度异常检测方法被提议用于二元或多类分类 [10, 11]。一个更常见的场景是很容易收集到很多正常样本而只有少量异常样本可用，因此可以通过利用正常样本分离异常值来采用半监督深度异常检测[1, 67, 85 ]。当没有标记数据可用时，基于数据样本的内在属性，应用无监督的深度异常检测来检测异常\n有一个数据集：卡内基梅隆大学软件工程研究所的 CERT 部门维护着一个包含 1000 多个内部威胁真实案例研究的数据库，并使用包含叛徒实例和伪装活动的场景生成了一系列合成内部威胁数据集。 CERT 数据集包含记录计算机的官方日志文件 -。他们进一步将常用的11个数据分为五类：基于伪装者、基于叛徒、杂项恶意、替代伪装者和基于识别/认证。\n深度学习在内部威胁检测方面的潜在优势，可以总结如下。\n 表征学习。深度学习模型最显着的优势在于能够自动发现检测所需的特征。网络空间中的用户行为是复杂且非线性的。手动设计的功能很难捕获用户行为信息，而且效率低下。同时，具有浅层结构的学习模型，如 HMM 和 SVM，是相对简单的结构，只有一层用于将原始特征转换为可用于检测的高级抽象。这些浅层模型对于解决许多约束良好的问题是有效的，但是能力有限的浅层模型很难对复杂的用户行为数据进行建模。相比之下，深度学习模型能够利用深度非线性模块通过通用学习程序来学习表示。因此，使用深度学习模型来捕捉复杂的用户行为并精确检测用户的意图，尤其是那些恶意的意图是很自然的。 序列建模。 深度学习模型，例如循环神经网络 (RNN) 和新提出的 Transformer，在对序列数据（例如视频、文本和语音）进行建模方面表现出良好的性能 [27, 70]。 由于将审计数据中记录的用户活动表示为顺序数据是很自然的，因此利用 RNN 或 Transformer 捕获复杂用户行为的显着信息具有提高内部威胁检测性能的巨大潜力。 异构数据组合。 深度学习模型在融合的任务上也取得了出色的表现 异构数据，例如图像字幕 [16, 36]。 对于内部威胁检测，除了将用户活动数据建模为序列之外，其他信息，例如组织中的用户配置文件信息和用户结构信息，也很关键。 与仅使用单一类型的数据相比，结合所有用于内部威胁检测的有用数据有望获得更好的性能。 与传统的机器学习方法相比，深度学习模型更强大地结合异构数据进行检测。  Deep Feedforward Neural Network [48] Anomaly-based Insider Threat Detection using Deep Autoencoders 使用深度自动编码器来检测内部威胁。深度自动编码器由编码器和解码器组成，其中编码器将输入数据编码为隐藏表示，而解码器旨在基于隐藏表示重构输入数据。深度自动编码器的目标是使重构输入接近原始输入。由于组织中的大多数活动都是良​​性的，带有内部威胁的输入应该具有相对较高的重构误差。因此，深度自编码器的重构误差可以作为异常分数来识别内部威胁。利用自动编码器结构的另一个想法是，在根据重构误差学习隐藏表示之后，将一类分类器（例如一类 SVM）应用于学习到的隐藏表示以识别内部威胁 [45]\nRecurrentNeuralNetwork. 因此，已经提出了许多基于 RNN 的方法来模拟用户活动 [51, 69, 79, 80] 以进行内部威胁检测。基本思想是训练一个 RNN 模型来预测用户的下一个活动或活动周期。只要预测结果和用户的真实活动没有显着差异，我们就认为用户遵循了正常的行为。否则，用户活动是可疑的\n[69] Deep Learning for Unsupervised Insider Threat Detection in Struc- tured Cybersecurity Data Streams 提出了一个堆叠的 LSTM 结构来捕获一天中的用户活动，并采用用户活动的负对数似然作为异常分数来识别恶意会话。 [80]Insider_Threat_Detection_via_Hierarchical_Neural_Temporal_Point_Processes 不是仅使用活动类型（例如，网络访问或文件上传）进行内部威胁检测，而是提出了一种分层神经时间点过程模型来捕获用户会话中的活动类型和时间信息，然后得出异常分数基于预测结果与实际活动在类型和时间方面的差异。\nConvolutional Neural Network 最近一项关于内部威胁检测的研究通过分析鼠标生物行为特征提出了一种基于 CNN 的用户身份验证方法 [35]An Insider Threat Detection Approach Based on Mouse Dynamics and Deep Learning。所提出的方法将计算机上的用户鼠标行为表示为图像。如果发生身份盗用攻击，用户的鼠标行为将与合法用户不一致。因此，将 CNN 模型应用于基于鼠标行为生成的图像，以识别潜在的内部威胁。\nGraphNeuralNetwork. 最近的工作 [37]Anomaly Detection with Graph Convolutional Networks for Insider Threat and Fraud Detection. 采用 GCN 模型来检测内部人员。由于组织中的用户经常通过电子邮件或在同一设备上的操作相互联系，因此使用图结构来捕获用户之间的相互依赖关系是很自然的。除了以结构信息的邻接矩阵作为输入外，GCN还结合了丰富的用户画像信息作为节点的特征向量。在基于图结构应用卷积层进行信息传播后，GCN 采用交叉熵作为目标函数来预测图中的恶意节点（用户）。受图嵌入方法的启发，[47]Log2vec A Heterogeneous Graph Embedding Based Approach 中的研究提出 log2vec 来检测恶意活动。 Log2vec 首先通过将审计数据中的各种活动表示为节点，将活动之间的丰富关系表示为边来构建异构图，然后训练可以对活动关系进行编码的节点嵌入。最后，通过在节点嵌入上应用聚类算法，log2vec 能够将恶意和良性活动分成不同的集群并识别恶意活动。\nChallenges  Extremely Unbalanced Data. 与良性活动相比，来自内部人员的恶意活动在现实场景中极为罕见。 因此，内部威胁数据集是一个不平衡的数据集，这是训练深度学习模型的一大挑战。 一般来说，由大量参数组成的深度学习模型需要大量标记数据才能正确训练。 但是，在现实中收集大量的恶意内部人员是不可行的。 如何利用现有的小样本正确训练深度学习模型对于内部威胁检测任务至关重要。 Temporal Information in Attacks. 大多数现有的内部威胁检测方法只关注活动类型信息，例如将文件复制到可移动磁盘或浏览网页。 然而，仅仅根据用户进行的活动类型来检测攻击是不够的，因为相同的活动可能是良性的，也可能是恶意的。 一个简单的例子是，工作时间复制文件看起来很正常，但半夜复制文件却很可疑。 时间信息在分析用户行为以识别那些恶意威胁方面起着重要作用，如何合并这些时间信息具有挑战性 Heterogeneous Data Fusion. 除了时间信息，利用各种数据源并融合此类异构数据对于提高内部威胁检测也至关重要。 例如，在日常工作中处理文件的用户预见到他的潜在裁员并且有目的地将凭证文件复制到可移动磁盘的活动。 在这种情况下，考虑用户资料（即心理测量分数）或用户交互数据可能有助于识别潜在的内部威胁。 Subtle Attacks. 目前，大部分现有工作都将内部威胁检测任务视为异常检测任务，通常将异常样本建模为分布外样本。 现有模型通常在来自良性用户的样本上进行训练，然后应用于识别与观察到的良性样本不同的内部人员。 推导出阈值或异常分数来量化内部人员和良性用户之间的差异。 但是，在现实中，我们不能期望内部人员进行恶意活动的模式发生重大变化。 为了逃避检测，内部威胁是微妙且难以察觉的，这意味着内部人员和良性用户在特征空间中很接近。 传统的异常检测方法无法检测接近良性用户的内部人员。 Adaptive Threats. 内部人员总是改进攻击策略以逃避检测。 然而，基于学习的模型在训练后无法检测新的攻击类型。 当观察到新类型的攻击时，再次从头开始训练模型是低效的。 首先，通常需要一些时间来收集足够的样本来训练模型。 更重要的是，再训练策略不能确保及时发现和预防。 设计一个可以自适应地提高内部威胁检测性能的模型是一项重要且具有挑战性的任务。 Fine-grainedDetection. 现有的基于深度学习的方法通常会检测包含恶意活动的恶意会话。 然而，用户通常在一个会话中进行大量的活动。 这种粗粒度的检测面临着难以及时检测的问题。 因此，如何识别细粒度的恶意子序列或确切的恶意活动对于内部威胁检测很重要。 这也是一项非常具有挑战性的任务。 这是因为我们可以从每个活动中利用的信息非常有限，即我们只观察用户何时以及进行了哪些活动。 没有足够的信息，很难实现细粒度的内部威胁检测 EarlyDetection. 当前的方法侧重于内部威胁检测，这意味着恶意活动已经发生并且已经给组织造成了重大损失。 因此，一个新兴的话题是如何实现内部威胁的早期检测，即在潜在的恶意活动实际发生之前检测它们。 提出了几种通过使用通用 IT 安全机制来防御内部威胁的方法 [3, 66]，但没有基于学习的方法来实现早期检测。 主动识别在不久的将来很有可能进行恶意活动的用户至关重要，以便组织可以提前进行干预以防止或减少损失。 Explainability. 深度学习模型通常被视为黑匣子。 尽管深度学习可以在许多领域取得可观的表现，但模型工作的原因仍然没有得到充分利用。当员工被检测为内部人员时，了解模型做出此类预测的原因至关重要，因为员工通常是最重要的 组织中的宝贵资产。 特别是，深度学习模型无法在内部威胁检测上达到 100% 的准确度。 误报案例（将良性用户错误分类为内部人员）会严重影响员工对组织的忠诚度。 因此，模型的可解释性是向领域专家提供模型洞察力的关键，以便可以高可信度地执行进一步的操作。 Lack of Testbed. Lack of Practical Evaluation Metrics. 采用常用的分类指标，如真阳性率（TPR）、假阳性率（FPR）、准确率和召回率来评估内部威胁检测的性能。基于 TPR 和 FPR，可以通过将 FPR 和 TPR 分别设置为 x 和 y 轴来绘制接收器操作特征 (ROC) 曲线，该曲线表示真阳性和假阳性之间的权衡。理想情况下，我们期望内部威胁检测算法可以实现 TPR 为 1，FPR 为 0。目前，在文献中，ROC 曲线下面积（AUC）分数被广泛用于比较不同检测算法的性能 [45， 47、48、51、80]。另一个指标是精度-召回率 (PR) 曲线，它是召回率和精度作为 x 和 y 轴的图，用于评估不平衡的数据分类。与 ROC-AUC 相比，PR-AUC 在评估内部威胁检测算法方面可能更有用，因为 PR 曲线更关注分类器在少数类上的性能。然而，由于内部人员的数量极少以及相应的恶意活动，目前尚不清楚 ROC-AUC 或 PR-AUC 是否适用于评估内部威胁检测。例如，来自不同检测算法的 ROC-AUC 值通常很接近 [48,51,80]，这意味着很难根据 ROC-AUC 值确定更好的模型。  5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 ","id":49,"section":"posts","summary":"目录： 1. 综述翻译 2. Tag 3. 任务描述 4. 方法 Deep Feedforward Neural Network RecurrentNeuralNetwork. Convolutional Neural Network GraphNeuralNetwork. Challenges 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10.","tags":["论文阅读笔记","异常行为分析","survey"],"title":"Deep Learning for Insider Threat Detection Review Challenges and Opportunities","uri":"https://yanyuLinxi.github.io/2021/09/deep-learning-for-insider-threat-detection-review-challenges-and-opportunities/","year":"2021"},{"content":"目录：   1. 综述翻译 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释  1. 综述翻译 尽管有必要保护存储在数据库系统 (DBS) 中的信息，但现有的安全模型不足以防止滥用，尤其是合法用户的内部滥用。此外，现有的误用检测研究尚未充分解决 DBS 中误用检测的概念。 重刑。即使有可用的方法来保护存储在数据库系统中的信息不被滥用。它们很少被保安人员使用 因为组织的安全策略要么不精确，要么根本不知道。 本文介绍了一种称为 DEMIDS 的误用检测系统，该系统专为关系数据库系统量身定制。 DEMIDS 使用审计日志来派生描述 DBS 用户典型行为的配置文件。计算出的配置文件可用于检测滥用行为，尤其是内部人员滥用。此外，配置文件可以通过帮助安全人员定义/优化组织，作为组织安全再造的宝贵工具 安全策略并验证现有的安全策略，如果提出的方法有任何必要，那么用户的访问模式 通常形成一些工作范围，其中包括通常与查询中的某些值一起引用的属性集。 DEMIDS 认为 通过距离度量的概念，在给定的数据库模式中编码的数据结构和语义的领域知识。距离度量用于指导搜索描述用户工作范围的频繁项集。在 DEMIDS 中，使用数据库管理系统的数据管理和查询处理功能从审计日志中有效地计算出此类频繁项集\n2. Tag 3. 任务描述 (Carter and Katz 1996) revealed that in computer systems the primary security threat comes from insider abuse rather than from intrusion 然而，现实表明，这种强制执行组织安全策略的机制通常没有得到充分利用。 这有多种原因。 首先，安全策略通常不为人所知或没有很好地指定，因此很难甚至不可能将它们转换为适当的安全机制。 此观察结果适用于一般安全策略以及针对单个数据库用户和应用程序定制的策略。 其次，更重要的是，安全策略没有充分保护存储在数据库系统中的数据免受“特权用户”的侵害。 （Carter 和 Katz 1996）透露，在计算机系统中，主要的安全威胁来自内部人员滥用而不是入侵。 这一观察结果导致必须更加重视系统的内部控制机制，例如审计日志分析。\n4. 方法 所提出的方法的本质是，给定数据库模式和关联的应用程序，用户的访问模式将形成一些工作范围，包括某些属性集，这些属性集通常与查询中的某些值一起引用。工作范围的概念在概念上被频繁项集的概念所捕获，频繁项集是具有某些值的特征集。基于数据字典中编码的数据结构和语义（完整性约束）以及审计日志中反映的用户行为，DEMIDS 定义了距离度量的概念，用于度量一组属性相对于工作范围的接近程度。通过利用数据库管理系统的高效数据处理功能的新型数据挖掘方法，距离度量用于指导在审计日志中搜索频繁项集。滥用，例如篡改数据的完整性，然后可以通过将派生的配置文件与指定的安全策略或收集的有关用户的新信息（审计数据）进行比较来检测。\n不行，不是想要的。\n5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 ","id":50,"section":"posts","summary":"目录： 1. 综述翻译 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方","tags":["论文阅读笔记","异常行为分析"],"title":"DEMIDS a Misuse Detection System for Database Systems","uri":"https://yanyuLinxi.github.io/2021/09/demids-a-misuse-detection-system-for-database-systems/","year":"2021"},{"content":"目录：   1. 综述翻译 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释  1. 综述翻译 利用电商平台上的购物历史数据对用户购买行为进行预测有助于提升用户体验和营销效果。提出一 种基于ＣＮＮＬＳＴＭ的用户购买行为预测模型。使用“分段下采样”对样本数据进行均衡化处理以获得购买用户 和未购买用户均衡样本；使用ＣＮＮＬＳＴＭ组合网络实现用户属性、商品属性及用户行为特征的自动抽取与选择， 并以此对用户购买行为进行预测。在阿里巴巴移动电商平台数据集的实验结果表明，基于ＣＮＮＬＳＴＭ的预测模 型Ｆ１值比基准模型平均提升了７％ ～１１％，使用“分段下采样”样本均衡算法Ｆ１值提升了２％左右。\n2. Tag 用户行为预测; 电商; 卷积神经网络; LSTM; CNN;\n3. 任务描述 4. 方法 首先提取特征:\n将用户的历史数据制作成特征。这里不清不楚搞不清楚哪些东西作为了特征\n5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 ","id":51,"section":"posts","summary":"目录： 1. 综述翻译 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方","tags":["论文阅读笔记","异常行为分析","用户行为预测"],"title":"基于CNN LSTM的用户购买行为预测模型","uri":"https://yanyuLinxi.github.io/2021/09/%E5%9F%BA%E4%BA%8Ecnn-lstm%E7%9A%84%E7%94%A8%E6%88%B7%E8%B4%AD%E4%B9%B0%E8%A1%8C%E4%B8%BA%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B/","year":"2021"},{"content":"目录：   1. 综述翻译 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释  1. 综述翻译 内部攻击形成了对数据库管理系统的最大威胁。已经开发了许多机制来检测和防止称为检测数据库系统中的恶意活动 DEMIDS 的内部攻击。 DEMIDS 被认为是数据库安全系统的最后防御机制之一。已经开发了许多机制来检测和防止误用活动，例如删除和更新数据库系统上的数据。这些机制利用审计和分析方法来检测和防止恶意活动。然而，这些机制在检测滥用活动方面仍然存在问题，例如限制检测授权命令上的恶意数据。本研究将通过提出一种机制来解决这些问题，该机制利用项目之间的依赖关系，通过计算数据项之间的关系来检测和防止恶意数据。如果项目之间的关系数不允许任何修改或删除，则该机制将检测活动为恶意活动。检测、假阳性和假阴性率等评估参数用于评估所提出机制的准确性。\n2. Tag 数据库; 内部攻击; 攻击防范; insider attacks\n3. 任务描述 内部攻击分为合法访问和非法访问。 合法访问可以滥用其特权进行恶意操作。 非法访问试图利用系统的漏洞进行恶意操作。 恶意事务是破坏数据库完整性和可用性的内部攻击之一\n内部威胁被认为是威胁机密性、完整性和可用性的最危险的威胁。 滥用活动是内部威胁之一，被认为是最危险的威胁，重点是破坏关键和敏感数据。 已经开发了许多方法来检测和防止恶意和异常活动。 该项目专门针对恶意活动，例如删除或更新已批准的记录。\n数据库安全问题之一是在恶意活动中。 其中包括：使用恶意数据更新已批准的记录，以及删除已批准的记录。 本研究假设项目之间的依赖关系可用于检测和预防上述恶意活动。\ni- 如何表示依赖关系来检测和防止恶意活动？ ii- 如何使用依赖关系来检测和预防恶意活动？\n4. 方法 图 10.2：机制流程流程 根据提出的项间依赖算法，计算项之间的关系以及与这些关系相关的数据项之间的计算关系。例如，如果项目之间的关系总数大于或等于三个关系，则该属性使用较多且重要性较高。之后，检查项目中的数据。如果数据已经写入多个项目，则该项目被其他用户在其他地方使用，禁止更新或删除并归类为恶意。另一方面，如果项目之间的总关系等于 2（低重要性），并且这两个数据项已经被使用。因此，如果只有一项数据项上有更新或删除的命令，而没有其他项，则将确定为恶意命令。但是，如果对这两个数据项同时进行更新或删除，则判定为恶意，但会在数据库中通过并提交。建议的依赖算法如下： 当授权用户向数据库发送命令时，算法检查命令类型，如果插入则直接移动到数据库。但是，如果然后命令更新或删除，算法将首先检查项之间的依赖关系（TR）的总数，然后检查由关系依赖相关的数据项（TD）的总数。因此，如果TR大于或等于三个关系，则检查相关数据项，如果数据已经写入多个项，则该机制将检测该活动为恶意活动并阻止它并通知DBA以及将事件写入事件表。另一方面，如果 TR 等于两个关系，则检查 TD 如果写入多个项目，则检查两个数据项上的活动，如果并行活动则检测为恶意但可以传递到数据库，由于数据可能正确与否，但如果活动仅针对一个数据项，则检测为恶意活动并加以阻止，同时通知 DBA 并将该事件写入事件表中。算法 10.3 将解释所提议的项目之间的依赖算法。\n5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 ","id":52,"section":"posts","summary":"目录： 1. 综述翻译 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方","tags":["论文阅读笔记","异常行为分析"],"title":"Detection and Prevention of Malicious Activites on RdBMS Relational Database Management Systems","uri":"https://yanyuLinxi.github.io/2021/09/detection-and-prevention-of-malicious-activites-on-rdbms-relational-database-management-systems/","year":"2021"},{"content":"https://cloud.tencent.com/document/product/1130/49731\n功能记录   支持7，30，3个月的数据。\n  异常行为统计\n 异常时间登录 异常操作 异常ip登录 异常资源访问     告警列表\n  目前总结的几种检测异常的方法（全局，并不仅限于数据库）\n 偏离训练集统计分布的任何东西都被认为是异常。  最简单的统计学方法就是控制图。计算出训练集每个特性的平均和标准偏差，然后围绕平均值定义出阈值：k*标准偏差（k为通常在1.5到3.0之间的任意系数，取决于既定的算法保守程度）。在部署中正向或负向超出阈值的点就是异常事件的可疑备选。   聚类  其他方法往往属于聚类方法。因为训练集中缺失异常类，聚类算法听起来很适合异常检测任务。 算法在训练集上创建一些群集。部署中，当前数据点和群集间的距离被计算出来。如果距离高于给定阈值，该数据点即为异常事件的可疑备选。根据距离衡量方法和聚合规则，人们设计出了不同的聚合算法，创建了各种群集。 但是，该方法不适合时间序列数据，因为固定的群集无法捕获时间进程。   受监督的机器学习  受监督的机器学习算法竟然也能应用到异常检测上。而且，因为受监督的机器学习技术既能应用于静态分类，也能应用到时间序列预测问题，该方法能覆盖所有数据情况。不过，由于受监督的机器学习技术需要所有牵涉类型的样本集，我们还需做些调整。 在异常检测问题上，受监督的机器学习模型只能在“正常”数据上训练，比如，在描述系统“正常”运行情况的数据上训练。只有在分类/预测完成后，才能评估出输入数据是不是异常。依赖受监督机器学习技术的异常检测方法主要有两种。 其一是神经自联器(或自编码器)。该自联器经过训练，重生成输入模式到输出层。只要输入模式类似训练集中的样本——也就是 “正常”，该模式重生成就会运行良好。而当新的不一样的东西出现在输入层，系统就会卡壳。这种情况下，该网络将无法重生成足够的输入向量到输出层。如果计算网络的输入和输出差距，异常事件的差值必然高于 “正常” 事件的差值。此处，定义该距离度量的阈值就应当可以找出异常点备选。该方法对静态数据点应用良好，但不适用于时间序列数据。 其二是时间序列预测算法。算法模型经过训练，基于“正常”值训练集上的前n个样本历史，预测下一个样本的值。在部署中，如果过往历史来自于在“正常”情况下工作的系统，下一个样本值的预测将会相对准确，近似于真实样本值。如果过往历史样本来自于不再在“正常”情况下运行的系统，该预测值就会偏离实际值。这种情况下，计量出预测样本值与真实样本值之间的差距，就能圈定异常事件备选。      ","id":53,"section":"posts","summary":"https://cloud.tencent.com/document/product/1130/49731 功能记录 支持7，30，3个月的数据。 异常行为统计 异常时间登录 异常操作 异常ip登录 异常资源访问 告警列表 目前总结的几种检测异常的方法（全局，并","tags":["异常行为分析"],"title":"DBrain数据库异常分析功能分析","uri":"https://yanyuLinxi.github.io/2021/09/dbrain%E6%95%B0%E6%8D%AE%E5%BA%93%E5%BC%82%E5%B8%B8%E5%88%86%E6%9E%90%E5%8A%9F%E8%83%BD%E5%88%86%E6%9E%90/","year":"2021"},{"content":"目录：   1. 综述翻译 2. Tag 3. 任务描述 4. 方法  概述 定义 方法   5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释  1. 综述翻译 User Behavior Anomaly Detection for Database Based on Unsupervised Learning\nB刊\n检测数据库内部合法用户的异常行为，对防范内部攻击和数据泄露具有重要意义，然而面临如下挑战：攻击模式不确定，真实异常样例少，数据集缺少准确标注。人工设定值和规则难以有效应对复杂多样的异常。本文提出了一种基于无监督学习的用户行为异常检测方法，通过划定时间窗口统计提取特征，运用核密度估计算法分别从单维度、多维度建模，实现在海量的无标注历史日志中发现简单异常和复杂异常、在新的缆上数据中检测异常。真实数据实验表明，该方法能够有效检测出简单异常，实验中检测三种简单异常的平均严格查准率和宽松查准率分别达90%和100%;能够从多维度找出存在攻击嫌疑的复杂异常，实验中成功检测出了一种单维度无法检测出的新的复杂异常\n2. Tag 数据库; 异常行为检测;\nmalicious activites\n数据库异常检测\n数据库用户异常行为检测\n数据库 用户行为检测\n待看：https://wap.cnki.net/touch/web/Dissertation/Article/10286-1018037812.nh.html\n无监督 行为预测\n接下来的调研目标，分一部分到无监督算法。\n3. 任务描述 对数据库用户行为建模、检测异常\n4. 方法 概述  使用特征工程从原始日志中提取特征。 采用核密度估计算法从单维度、多维度训练得到某个用户正常行为的概率密度模型。计算合理的概率密度阈值 根据样本呢的概率密度是否低于阈值判断该样本是否偏离用户绝大多数正常行为的离群点，从而发出警告。  定义 本文将数据库用户行为“异常”定义如下：当用户的操作 行为严重偏离其绝大多数正常历史行为的轨迹时，或者当用 户的操作行为具有拖取数据滥用数据等攻击嫌疑，对数据库 的安全构成威胁时，称为“异常”\n简单(单维度)异常:观察单个指标，确定其为异常 复杂异常：需要多个指标组合起来分析，发现其异常之处。\n可疑异常率：历史数据中，异常样本占比例。\n方法  特征提取  用户角色 用户工作状态 用户发器访问位置 天 访问数据量 天、两小时 访问不同表总个数 天、两小时 发器鉴权请求失败比例 天、两小时 访问绝密级列比例 天、两小时    操作行为的统计特征  将用户执行的query语句按照时间窗口聚合。   方法：  对对每一个维度分别归一化。 核密度估计  分布概率密度函数f的估值  $f_b(x)=\\frac{1}{n}\\sum_{i=1}^nK_b(x-x_i)=\\frac{1}{nb}\\sum_{i=1}^{n}K(\\frac{x-x_i}{b})$ 其中K为核函数，是积分值为1的非负函数；b为带宽，是一个平滑参数。核密度估计算法可以简单地理解为将每个样本作为中心点对应的核函数的加权求和     超参自动搜索模块：GridSearchCV 概率密度判定阈值$S_0$，可以根据可疑异常率a求出。若假定数据库历史数据中只有1%出现某种简单异常，计算所有历史样本在改维度概率密度值并作升序排列，在1%后的样本即为简单异常。触发相应单维度异常告警。 多维度概率密度模型  当选取的m个特征之间相互独立时，样本的整体概率密度等于各个维度的概率密度值相乘。      5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 相关工作：\n 基于参考阈值的方法 基于关联规则挖掘的方法 基于免疫原理的方法 基于隐Markov的方法 基于聚类的方法 基于模式挖掘的方法 基于query语句向量化特征的方法 基于数据库应用层检测的方法  ","id":54,"section":"posts","summary":"目录： 1. 综述翻译 2. Tag 3. 任务描述 4. 方法 概述 定义 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于","tags":["论文阅读笔记","异常行为分析"],"title":"一种无监督的数据库用户行为异常检测方法","uri":"https://yanyuLinxi.github.io/2021/09/%E4%B8%80%E7%A7%8D%E6%97%A0%E7%9B%91%E7%9D%A3%E7%9A%84%E6%95%B0%E6%8D%AE%E5%BA%93%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E6%96%B9%E6%B3%95/","year":"2021"},{"content":"目录：   1. 综述翻译 2. Tag 3. 任务描述 4. 方法  预处理   5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释  1. 综述翻译 摘要——反编译，从二进制恢复源代码， 在许多需要分析或理解源代码不可用的软件的情况下很有用。源代码比二进制代码更易于人类阅读，并且有许多工具可用于分析源代码。现有的反编译技术通常会生成人类难以理解的源代码，因为生成的代码通常不使用程序员使用的编码习惯用法。与人工编写代码的差异也降低了分析工具对反编译源代码的有效性。解决反编译之间的差异问题 代码和人工编写的代码，我们提出了一种使用基于循环神经网络的模型反编译二进制代码片段的新技术。该模型学习源代码中出现的属性和模式，并使用它们生成反编译输出。我们在从 C 源代码编译的二进制机器代码片段上训练和评估我们的技术。我们在本文中概述的一般方法不是特定于语言的，并且几乎不需要或根本不需要语言及其属性或编译器如何运行的领域知识，从而使该方法可以轻松扩展到新的语言和结构。此外，该技术可以扩展并应用于传统反编译器不针对的情况，例如用于对孤立的二进制片段进行反编译；快速、按需反编译；特定领域的学习反编译；优化反编译的可读性；并恢复控制流结构、注释和变量或函数名称。我们表明这种技术产生的翻译通常是准确的或接近的，并且可以提供有用的图片\nDisassembler(反汇编)(Machine Code to Assembly): A disassembler is a software tool which transforms machine code into a human readable mnemonic representation called assembly language.\nDecompiler(反编译)(Binary Code to Source code): Software used to revert the process of compilation. Decompiler takes a binary program file as input and output the same program expressed in a structured higher-level language.\n2. Tag RNN; Decompiler; Binary Code; Translator; Abstract Syntax Tree(AST); AST\n3. 任务描述 将二进制转为source code。\n这个source code并不是truth source code，而是经过标记化的。就是将大多数字符串、变量、函数名进行替换，将其他的比如for this 等标识符标识为记号，然后进行还原。\n4. 方法 backbone神经网络是Sequence-to-Sequence 模型 RNN。\n数据集由C（source code）和与其一一匹配的二进制构成。\n预处理 接下来的步骤对binary code 和 source code 进行相似的处理：\n 将二进制和源代码进行序列化。  二进制采用byte-by-byte的方式组成token序列。（第二种方式：bite-by-bite，相比效果没前一种好） 源代码采用词法分析后转为token序列。   源代码替换  将字符串替换为STRING 将top-20 函数名保留，其他函数名替换为function 将top-100 变量名保留，其他变量替换为var_XXX. XXX 是变量名的序号。   标志化  将这些token转为one-hot向量。   分桶  将二进制按照长度分成四个桶。这一步是因为RNN需要定长的输入和输出。所以需要针对长度不统一的序列进行padding或者舍弃。    将匹配好的匹配对，送入RNN进行训练。\n5. 解决了什么问题（贡献） 6. 实验结果 实验结果的metrics：\n 我们比较预测的标记化输出 C 源代码由 RNN 针对标记化的已知基本事实值，采用两者之间的 Levenschtein 距离。\n  莱文斯坦距离，又称Levenshtein距离，是编辑距离的一种。指两个字串之间，由一个转成另一个所需的最少编辑操作次数。\n edit distance大约在0.7左右。\n7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释 ","id":55,"section":"posts","summary":"目录： 1. 综述翻译 2. Tag 3. 任务描述 4. 方法 预处理 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业","tags":["论文阅读笔记"],"title":"Using RNN for Dexompiliation","uri":"https://yanyuLinxi.github.io/2021/09/using-rnn-for-dexompiliation/","year":"2021"},{"content":"目录：   1. 综述翻译 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释  1. 综述翻译 二进制分析有助于许多重要的应用程序，如恶意软件检测和自动修复易受攻击的软件。在本文中，我们建议应用人工神经网络来解决二元分析中重要而困难的问题。具体来说，我们解决了函数识别问题，这是许多二进制分析技术中至关重要的第一步。尽管神经网络在过去几年经历了复兴，在视觉对象识别、语言建模和语音识别等多个应用领域取得了突破性的成果，但还没有研究人员尝试将这些技术应用于二进制问题。分析。使用来自先前工作的数据集，我们表明循环神经网络可以比最先进的基于机器学习的方法更准确和更高效地识别二进制文件中的函数。我们可以更快地训练模型一个数量级，并以数百倍的速度对二进制文件进行评估。此外，它在八个基准测试中的六个基准上将错误率减半，并且在其余两个基准上的表现相当。\n2. Tag Binary Code Analysis, RNN, NLP, Function Recognizing.\n3. 任务描述 Function start identification: Given C, find { f1,1, ·· · , fn,1}. In other words, recover the location of the first byte of each function.\n Function end identification: Given C, find { f1,l1, ·· · , fn,ln}. In other words, find the bytes where each of the n functions in the binary ends. The length of each function is not given. Function boundary identification: Given C, find {( f1,1, f1,l1) ·· · , ( fn,1, fn,ln )}. In other words, dis-cover the location of the first and last byte within each function This task is more than a simple com- bination of function start and end identification. If the starts and ends of functions have been identi- fied separately, they need to be paired correctly so that each pair contains the start and end of the same function. General function identification: Given C, find {( f1,1, f1,2, ·· · , f1,l1) ·· · , ( fn,1, fn,2, ·· · fn,ln )}; i.e.,determine the number of functions in the file, and all of the bytes which make up each function.  4. 方法 创建标签：\n 我们将代码 C 本身视为一个字节序列 C[0],C[1],···,C[l]，其中C[i]∈$Z_{256}$是序列中的第i个字节。 我们将二进制中的 n 个函数表示为 f1,····,fn。 我们将属于每个函数 fi 的代码字节的索引（即对应于在运行该函数时可能执行的指令的字节）标记为 fi,1,···,fi,li，其中 li 是fi 中的总字节数。 不失一般性，我们假设 fi,1 \u0026lt; fi,2 \u0026lt; ·· · \u0026lt; fi,k。 每个字节可能属于任意数量的函数，函数可以包含任何字节集，无论是否连续。\n 为什么使用字节而不是指令：\n 请注意，我们将代码和函数定义为字节集合而不是指令集合。 在 x86 和 x86-64 ISA 中，根据解码开始的偏移量，字节序列可以有许多合理的指令解码； 因此每个字节可能属于少数可能的指令。 以字节为单位工作使我们能够避免这种歧义。\n 将二进制按byte字节一个个送入双向LSTM判断，其是否是函数的开头和结尾。\n注意：论文分别训练了两个网络判断是否是函数开头或者结尾。\n5. 解决了什么问题（贡献） 论文中的一个示例很有意思：\n原文：\n在图 1 中，我们展示了一个简短的 C 函数示例以及在两个不同优化级别编译后的相应二进制代码。\n图 1b 中的代码包含非常清晰的函数开始和结束标记：push %rbp 和 mov %rsp,%rbp 的函数序言保存了调用者的堆栈帧，函数以 retq 结束，它在函数中的其他任何地方都没有出现。相比之下，图 1c 根本不使用堆栈，因此函数开始时对 edi 和 esi 中传递的函数参数进行了一些访问；寻找 push %rbp 会失败。此外，对参数的类似访问再次出现在函数体内，因此很难仅仅依靠它作为函数开始的标记。同样，retq 在代码中出现两次，因此当我们看到这条指令时预测函数结束将失败。\n这个例子说明了为什么函数识别会带来很多困难，简单的启发式方法不太可能足够，这与直觉可能建议的相反。\n6. 实验结果  按照 Bao 等人的程序进行操作。 我们为四种（架构、操作系统）配置中的每一种都训练了一个单独的模型。 为了报告可比较的结果，我们还使用了 Bao 等人的 10 倍交叉验证； 我们为四种配置中的每一种训练十个模型，其中十个模型中的每一个都使用不同的 10% 的二进制文件作为测试集。\n 实验效果看论文呢，基本非常好。准确率普遍95%以上\n论文提出了两个limitations：\n 在测试集没有出现的binaries上效果可能会较差 抗混淆性可能不够。如在一段话中插入连续无意义的NOP，不会影响函数功能，但是会影响神经网络的判断。  7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 论文中说的future work：\n 尽管我们已经看到了一些关于 RNN 在各种条件下的性能的实验证据，但我们缺乏对模型内部机制的清晰解释。一种可能的解释方法是通过特征向量结构的分析来进行，方法是在网络随时间演变时线性化网络状态，并分析线性化系统的哪些特征向量携带与任务相关的信息 [12]。这种分析可以了解网络在选择、集成和通信相关信息时如何忽略不相关信息，并允许识别线性化系统的哪些特征向量负责网络执行的这些任务。然而，如果对破坏模型准确性感兴趣的对手可以使用神经网络的参数，他们可能能够使用这种分析来更有效地添加额外的指令，这些指令与携带任务相关信息的特征向量不正交，从而阻止其传输并显着影响 RNN 的性能。\n 12. 注释 ","id":56,"section":"posts","summary":"目录： 1. 综述翻译 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方","tags":["论文阅读笔记"],"title":"Recognizing Functions in Binaries With Neural Networks","uri":"https://yanyuLinxi.github.io/2021/09/recognizing-functions-in-binaries-with-neural-networks/","year":"2021"},{"content":"登录 https://zhuanlan.zhihu.com/p/119999079\n704 二分法搜查数组 这个可以用递归处理的问题，就可以用循环处理。\n多思考极端元素时的处理办法。一个元素，两个元素等。\n注意处理边界，每次mid都应该根据值得区间减一或者加一。\n注意保持区间不变性。就是区间要么一直是左闭右开。要么左闭右闭。 推荐左闭右开。这样是符合大多数编程的习惯的。\n27 移除元素 这题开始没有意识到是什么类型\n往后面写，意识到了是双向指针排序的问题。类似于快排这样的处理。\n这里其实一开始的想法挺简单的，而且容易实现，没实现成功的原因就是对极限情况没考虑清楚。其实在极限情况下改一下就行了的。\n977 有序数组的平方 好好读题，题目给的数组本身就有序，所以有更简单的做法\n这里重写了下快速排序。\n209 长度最小的子数组 读题。读题一定要仔细。\n注意循环退出条件。思考极端情况。 思路清楚再动笔。 伪代码尽量贴近原生语言\n首先我们创建一个数组，它的第 i 个索引是给定 nums 数组中所有前一个元素及其自身的总和。 然后使用二分搜索搜索满足条件的下标索引。使用空间换时间。这个想法要有。\n59 螺旋矩阵II Initialize the matrix with zeros, then walk the spiral path and write the numbers 1 to n*n. Make a right turn when the cell ahead is already non-zero.\ndef generateMatrix(self, n): A = [[0] * n for _ in range(n)] i, j, di, dj = 0, 0, 0, 1 for k in xrange(n*n): A[i][j] = k + 1 if A[(i+di)%n][(j+dj)%n]: di, dj = dj, -di i += di j += dj return A 同样的想清楚再动手。别人为什么这么简介。 在思想上多下功夫。动笔时少下功夫。\n","id":57,"section":"posts","summary":"登录 https://zhuanlan.zhihu.com/p/119999079 704 二分法搜查数组 这个可以用递归处理的问题，就可以用循环处理。 多思考极端元素时的处理办法。一个元素，两个元素等。 注意处理边界，每次mid","tags":[],"title":"Leetcode1_数组","uri":"https://yanyuLinxi.github.io/2021/09/leetcode1_%E6%95%B0%E7%BB%84/","year":"2021"},{"content":"其他  使用Vscode写html，一个！号后可以选择模板。 引入Vue框架  \u0026lt;script src=\u0026quot;https://unpkg.com/vue@next\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;   练手：  https://gitee.com/panjiachen/vue-admin-template    基础 创建实例, 支持链式。返回同一个实例。 Vue.createAPP({}) .component(\u0026quot;\u0026quot;) .directive  挂载到组件 const app = Vue.createAPP() const vm = app.mount(\u0026quot;#app\u0026quot;) // app 是dom 元素id号. 与id绑定。class是样式。 // 与大多数应用方法不同的是，mount 不返回应用本身。相反，它返回的是根组件实例k。现在理解为返回实例。 //应该等app所有都建立完成后，再进行绑定到html的组件上。  钩子 // 存在一些生命周期的钩子是可以绑定在Vue上的 Vue.createApp({ data(){ return {count: 1} }, created(){ console.log('count is: ' + this.count) // \u0026quot;count is 1\u0026quot; 创建时调用 } // 其他的还有 mounted updated unmounted等等 })  data是一个函数是返回这个对象的所有预先声明的变量\ndata 返回一个字典，字典所以用 :\n插值，文本 \u0026lt;span\u0026gt; Message:{{msg}}\u0026lt;/span\u0026gt; // span 中放内容会实时更新 \u0026lt;span v-once\u0026gt; 这个里面放的内容不会更新\u0026lt;/span\u0026gt; \u0026lt;p\u0026gt;Using v-html directive: \u0026lt;span v-html=\u0026quot;rawHtml\u0026quot;\u0026gt;\u0026lt;/span\u0026gt;\u0026lt;/p\u0026gt; //这样才能放置html内容 //其中javascript脚本中，data[rawHtml]可以赋值为一段html。v-html后面跟的是data id。  Attribute //可以绑定属性 // 绑定属性前，前面一个没有引号，后面的有引号。 \u0026lt;div v-bind:id=\u0026quot;dynamicId\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; //赋值dynamicId在data内可以更改id这个值，如果绑定的是null 或者undefined 则不会包含该attribute。 \u0026lt;button v-bind:disabled=\u0026quot;isButtonDisabled\u0026quot;\u0026gt;按钮\u0026lt;/button\u0026gt; //如果 isButtonDisabled 的值是 truthy[1]，那么 disabled attribute 将被包含在内。如果该值是一个空字符串，它也会被包括在内，与 \u0026lt;button disabled=\u0026quot;\u0026quot;\u0026gt; 保持一致。对于其他 falsy[2] 的值，该 attribute 将被省略  \u0026lt;!-- html中的{{}} 可以填入data property 或者methods名。如{{name()}}。 同样的可以由v-bind:title=\u0026quot;name()\u0026quot;这样子--\u0026gt;  使用javascript表达式 {{ number + 1 }} {{ ok ? 'YES' : 'NO' }} {{ message.split('').reverse().join('') }} \u0026lt;div v-bind:id=\u0026quot;'list-' + id\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; //以上都是ok的。表明可以进行简单的运算 \u0026lt;!-- 这是语句，不是表达式：--\u0026gt; {{ var a = 1 }} \u0026lt;!-- 流控制也不会生效，请使用三元表达式 --\u0026gt; {{ if (ok) { return message } }}  指令。指令 (Directives) 是带有 v- 前缀的特殊 attribute。指令 attribute 的值预期是单个 JavaScript 表达式 (v-for 和 v-on 是例外情况，稍后我们再讨论)。指令的职责是，当表达式的值改变时，将其产生的连带影响，响应式地作用于 DOM。 \u0026lt;p v-if=\u0026quot;seen\u0026quot;\u0026gt;现在你看到我了\u0026lt;/p\u0026gt; \u0026lt;!-- 这里，v-if 指令将根据表达式 seen 的值的真假来插入/移除 \u0026lt;p\u0026gt; 元素。 --\u0026gt; \u0026lt;!-- 一些指令能够接收一个“参数”，在指令名称之后以冒号表示。例如，v-bind 指令可以用于响应式地更新 HTML attribute：--\u0026gt; \u0026lt;a v-bind:href=\u0026quot;url\u0026quot;\u0026gt; ... \u0026lt;/a\u0026gt; \u0026lt;!-- 在这里 href 是参数，告知 v-bind 指令将该元素的 href attribute 与表达式 url 的值绑定。--\u0026gt; \u0026lt;!-- 动态参数 也可以在指令参数中使用 JavaScript 表达式，方法是用方括号括起来： --\u0026gt; \u0026lt;a v-bind:[attributeName]=\u0026quot;url\u0026quot;\u0026gt; ... \u0026lt;/a\u0026gt; \u0026lt;!-- 如果data property中存在attributeName，其值为 \u0026quot;href\u0026quot;，那么这个绑定将等价于 v-bind:href --\u0026gt; \u0026lt;!-- 修饰符 (modifier) 是以半角句号 . 指明的特殊后缀，用于指出一个指令应该以特殊方式绑定。例如，.prevent 修饰符告诉 v-on 指令对于触发的事件调用 event. preventDefault() --\u0026gt; \u0026lt;form v-on:submit.prevent=\u0026quot;onSubmit\u0026quot;\u0026gt;...\u0026lt;/form\u0026gt;  缩写 \u0026lt;!-- 完整语法 --\u0026gt; \u0026lt;a v-bind:href=\u0026quot;url\u0026quot;\u0026gt; ... \u0026lt;/a\u0026gt; \u0026lt;!-- v-bind 缩写 --\u0026gt; \u0026lt;a :href=\u0026quot;url\u0026quot;\u0026gt; \u0026lt;!-- 动态参数的缩写 --\u0026gt; \u0026lt;a :[key]=\u0026quot;url\u0026quot;\u0026gt; ... \u0026lt;/a\u0026gt; \u0026lt;!-- 完整语法 --\u0026gt; \u0026lt;a v-on:click=\u0026quot;doSomething\u0026quot;\u0026gt; ... \u0026lt;/a\u0026gt; \u0026lt;!-- 缩写 --\u0026gt; \u0026lt;a @click=\u0026quot;doSomething\u0026quot;\u0026gt; ... \u0026lt;/a\u0026gt; \u0026lt;!-- 动态参数的缩写 (2.6.0+) --\u0026gt; \u0026lt;a @[event]=\u0026quot;doSomething\u0026quot;\u0026gt; ... \u0026lt;/a\u0026gt;  注意 \u0026lt;!-- 动态参数表达式有一些语法约束，因为某些字符，如空格和引号，放在 HTML attribute 名里是无效的。例如： --\u0026gt; \u0026lt;!-- 这会触发一个编译警告 --\u0026gt; \u0026lt;a v-bind:['foo' + bar]=\u0026quot;value\u0026quot;\u0026gt; ... \u0026lt;/a\u0026gt; \u0026lt;!-- 在 DOM 中使用模板时 (直接在一个 HTML 文件里撰写模板)，还需要避免使用大写字符来命名键名，因为浏览器会把 attribute 名全部强制转为小写： --\u0026gt; \u0026lt;!-- 在 DOM 中使用模板时这段代码会被转换为 `v-bind:[someattr]`。 除非在实例中有一个名为“someattr”的 property，否则代码不会工作。 --\u0026gt; \u0026lt;a v-bind:[someAttr]=\u0026quot;value\u0026quot;\u0026gt; ... \u0026lt;/a\u0026gt; \u0026lt;!-- 所有attribute都是小写 --\u0026gt;  Data property 组件的 data 选项是一个函数。Vue 在创建新组件实例的过程中调用此函数。它应该返回一个对象，然后 Vue 会通过响应性系统将其包裹起来，并以 $data 的形式存储在组\nconst app = Vue.createApp({ data() { return { count: 4 } } }) const vm = app.mount('#app') // vm变量表示的就是应用实例， 先这么理解把 console.log(vm.$data.count) // =\u0026gt; 4 console.log(vm.count) // =\u0026gt; 4 // 这些实例 property 仅在实例首次创建时被添加，所以你需要确保它们都在 data 函数返回的对象中。必要时，要对尚未提供所需值的 property 使用 null、undefined 或其他占位的值。 // 直接将不包含在 data 中的新 property 添加到组件实例是可行的。但由于该 property 不在背后的响应式 $data 对象内，所以 Vue // vm.data是响应式的。  methods 这里是methods。打错一个字都不行!!!\n我们用 methods 选项向组件实例添加方法，它应该是一个包含所需方法的对象：\nconst app = Vue.createApp({ data() { return { count: 4 } }, methods: { increment() { // `this` 指向该组件实例 this.count++ } } }) const vm = app.mount('#app') console.log(vm.count) // =\u0026gt; 4 vm.increment() console.log(vm.count) // =\u0026gt; 5  \u0026lt;!-- 这些 methods 和组件实例的其它所有 property 一样可以在组件的模板中被访问。在模板中，它们通常被当做事件监听使用： --\u0026gt; \u0026lt;button @click=\u0026quot;increment\u0026quot;\u0026gt;Up vote\u0026lt;/button\u0026gt;  函数防抖：将几次操作合并为一此操作进行。 \u0026hellip; 函数节流：使得一定时间内只触发一次函数。 这里稍微没看懂。后面再看。\n侦听器 watch。可以侦测属性变换。 watch 会监听属性是否发生变化。发生变化后执行某种操作。最好还是使用计算属性。能用计算属性就别用侦听器。 watch顾名思义就是监听变量值的变化。\n\u0026lt;div id=\u0026quot;watch-example\u0026quot;\u0026gt; \u0026lt;p\u0026gt; Ask a yes/no question: \u0026lt;input v-model=\u0026quot;question\u0026quot; /\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;{{ answer }}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt;  \u0026lt;script src=\u0026quot;https://cdn.jsdelivr.net/npm/axios@0.12.0/dist/axios.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; const watchExampleVM = Vue.createApp({ data() { return { question: '', answer: 'Questions usually contain a question mark. ;-)' } }, watch: { // whenever question changes, this function will run question(newQuestion, oldQuestion) { if (newQuestion.indexOf('?') \u0026gt; -1) { this.getAnswer() } } }, methods: { getAnswer() { this.answer = 'Thinking...' axios .get('https://yesno.wtf/api') .then(response =\u0026gt; { this.answer = response.data.answer }) .catch(error =\u0026gt; { this.answer = 'Error! Could not reach the API. ' + error }) } } }).mount('#watch-example') \u0026lt;/script\u0026gt;  v-model // 你可以用 v-model 指令在表单 \u0026lt;input\u0026gt;、\u0026lt;textarea\u0026gt; 及 \u0026lt;select\u0026gt; 元素上创建双向数据绑定。如： \u0026lt;input v-model=\u0026quot;question\u0026quot; /\u0026gt; // v-model 会忽略所有表单元素的 value、checked、selected attribute 的初始值而总是将当前活动实例的数据作为数据来源。你应该通过 JavaScript 在组件的 data 选项中声明初始值。 // v-model 绑定的输入表单会始终将数据与Vue实例中的数据。 \u0026lt;input v-model=\u0026quot;message\u0026quot; placeholder=\u0026quot;edit me\u0026quot; /\u0026gt; \u0026lt;p\u0026gt;Message is: {{ message }}\u0026lt;/p\u0026gt;  class 和style绑定。样式绑定 对象语法： {string: js_var_name} 意思是js_var_name是truty的时候，string才存在。\n数据类型：变量要么是对象，要么是字典形式的对象，即active 或者 {active:isActive}\n\u0026lt;div class=\u0026quot;static\u0026quot; :class=\u0026quot;{ active: isActive, 'text-danger': hasError }\u0026quot; \u0026gt;\u0026lt;/div\u0026gt;  data() { return { isActive: true, hasError: false } }  \u0026lt;!-- 渲染结果： --\u0026gt; \u0026lt;div class=\u0026quot;static active\u0026quot;\u0026gt;\u0026lt;/div\u0026gt;  绑定的数据对象不必内联定义在模板里：\n\u0026lt;div :class=\u0026quot;classObject\u0026quot;\u0026gt;\u0026lt;/div\u0026gt; data() { return { classObject: { active: true, 'text-danger': false } } } // 或者用计算属性 data() { return { isActive: true, error: null } }, computed: { classObject() { return { active: this.isActive \u0026amp;\u0026amp; !this.error, 'text-danger': this.error \u0026amp;\u0026amp; this.error.type === 'fatal' } } }  使用数组 \u0026lt;div :class=\u0026quot;[activeClass, errorClass]\u0026quot;\u0026gt;\u0026lt;/div\u0026gt;  data() { return { activeClass: 'active', errorClass: 'text-danger' } }  记住html 的 id位置输送的进去的都是变量名。html中的vue变量用{{}}取值。用\u0026quot;\u0026ldquo;将变量包裹起来。\n数组语法和对象语法可以嵌套：\n\u0026lt;div :class=\u0026quot;[{active:isActive}, errorClass]\u0026quot;\u0026gt;\u0026lt;/div\u0026gt;  条件渲染 v-if 指令用于条件性地渲染一块内容。这块内容只会在指令的表达式返回 truthy 值的时候被渲染。\n\u0026lt;h1 v-if=\u0026quot;awesome\u0026quot;\u0026gt;Vue is awesome!\u0026lt;/h1\u0026gt; \u0026lt;h1 v-else\u0026gt;Oh no 😢\u0026lt;/h1\u0026gt;  在 元素上使用 v-if 条件渲染分组 可以作为常见分组包裹在最外层\n因为 v-if 是一个指令，所以必须将它添加到一个元素上。但是如果想切换多个元素呢？此时可以把一个 元素当做不可见的包裹元素，并在上面使用 v-if。最终的渲染结果将不包含 元素。\n\u0026lt;template v-if=\u0026quot;ok\u0026quot;\u0026gt; \u0026lt;h1\u0026gt;Title\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Paragraph 1\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Paragraph 2\u0026lt;/p\u0026gt; \u0026lt;/template\u0026gt;  你可以使用 v-else 指令来表示 v-if 的“else 块”：\n\u0026lt;div v-if=\u0026quot;Math.random() \u0026gt; 0.5\u0026quot;\u0026gt; Now you see me \u0026lt;/div\u0026gt; \u0026lt;div v-else\u0026gt; Now you don't \u0026lt;/div\u0026gt; v-else-if，顾名思义，充当 v-if 的“else-if 块”，并且可以连续使用： \u0026lt;div v-if=\u0026quot;type === 'A'\u0026quot;\u0026gt; A \u0026lt;/div\u0026gt; \u0026lt;div v-else-if=\u0026quot;type === 'B'\u0026quot;\u0026gt; B \u0026lt;/div\u0026gt; \u0026lt;div v-else-if=\u0026quot;type === 'C'\u0026quot;\u0026gt; C \u0026lt;/div\u0026gt; \u0026lt;div v-else\u0026gt; Not A/B/C \u0026lt;/div\u0026gt; \u0026lt;!-- v-else 元素必须紧跟在带 v-if 或者 v-else-if 的元素的后面，否则它将不会被识别。 --\u0026gt;  v-show 另一个用于条件性展示元素的选项是 v-show 指令。用法大致一样：\n\u0026lt;h1 v-show=\u0026quot;ok\u0026quot;\u0026gt;Hello!\u0026lt;/h1\u0026gt; \u0026lt;!-- 不同的是带有 v-show 的元素始终会被渲染并保留在 DOM 中。v-show 只是简单地切换元素的 CSS property display。 --\u0026gt;  注意，v-show 不支持 元素，也不支持 v-else。\nv-if 是“真正”的条件渲染，因为它会确保在切换过程中，条件块内的事件监听器和子组件适当地被销毁和重建。\nv-if 也是惰性的：如果在初始渲染时条件为假，则什么也不做——直到条件第一次变为真时，才会开始渲染条件块。\n相比之下，v-show 就简单得多——不管初始条件是什么，元素总是会被渲染，并且只是简单地基于 CSS 进行切换。\n一般来说，v-if 有更高的切换开销，而 v-show 有更高的初始渲染开销。因此，如果需要非常频繁地切换，则使用 v-show 较好；如果在运行时条件很少改变，则使用 v-if 较好。\n切换少量：if。切换多次 show.\n不推荐同时使用 v-if 和 v-for\n当 v-if 与 v-for 一起使用时，v-if 具有比 v-for 更高的优先级\nv-for v-for 指令需要使用 item in items 形式的特殊语法，其中 items 是源数据数组，而 item 则是被迭代的数组元素的\n\u0026lt;ul id=\u0026quot;array-rendering\u0026quot;\u0026gt; \u0026lt;li v-for=\u0026quot;item in items\u0026quot;\u0026gt; {{ item.message }} \u0026lt;!-- 使用.访问属性 --\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt;  Vue.createApp({ data() { return { items: [{ message: 'Foo' }, { message: 'Bar' }] } } }).mount('#array-rendering')  在 v-for 块中，我们可以访问所有父作用域的 property。即和当前list property同一作用域的property\nv-for 还支持一个可选的第二个参数，即当前项的索引。\nlist使用[{},{}]这样的语法。\n{{}}只支持访问一个变量。\n当访问对象的时候，可以for (value, name, index) in object\n当访问list的时候 可以 for (value, index) in list\n列表可以添加value, index。访问字典对象时，可以value, name, index\n\u0026lt;ul id=\u0026quot;array-with-index\u0026quot;\u0026gt; \u0026lt;li v-for=\u0026quot;(item, index) in items\u0026quot;\u0026gt; {{ parentMessage }} - {{ index }} - {{ item.message }} \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt;  Vue.createApp({ data() { return { parentMessage: 'Parent', items: [{ message: 'Foo' }, { message: 'Bar' }] } } }).mount('#array-with-index')  在使用for时使用key。它在更新时，不移动dom元素，而是更新dom的内容。\n嵌套循环可以实时获取父作用域的变量。但要求有上下级关系。如嵌套的.或者等。\nv-for 也可以接受整数。在这种情况下，它会把模板重复对应次数\n\u0026lt;div id=\u0026quot;range\u0026quot; class=\u0026quot;demo\u0026quot;\u0026gt; \u0026lt;span v-for=\u0026quot;n in 10\u0026quot; :key=\u0026quot;n\u0026quot;\u0026gt;{{ n }} \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt;  类似于 v-if，你也可以利用带有 v-for 的 来循环渲染一段包含多个元素的内容。比如：\n\u0026lt;ul\u0026gt; \u0026lt;template v-for=\u0026quot;item in items\u0026quot; :key=\u0026quot;item.msg\u0026quot;\u0026gt; \u0026lt;li\u0026gt;{{ item.msg }}\u0026lt;/li\u0026gt; \u0026lt;li class=\u0026quot;divider\u0026quot; role=\u0026quot;presentation\u0026quot;\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/template\u0026gt; \u0026lt;/ul\u0026gt;  当 Vue 正在更新使用 v-for 渲染的元素列表时，它默认使用“就地更新”的策略。如果数据项的顺序被改变，Vue 将不会移动 DOM 元素来匹配数据项的顺序，而是就地更新每个元素，并且确保它们在每个索引位置正确渲染。\n这个默认的模式是高效的，但是只适用于不依赖子组件状态或临时 DOM 状态 (例如：表单输入值) 的列表渲染输出。\n为了给 Vue 一个提示，以便它能跟踪每个节点的身份，从而重用和重新排序现有元素，你需要为每项提供一个唯一 key attribute：\n\u0026lt;div v-for=\u0026quot;item in items\u0026quot; :key=\u0026quot;item.id\u0026quot;\u0026gt; \u0026lt;!-- content --\u0026gt; \u0026lt;/div\u0026gt;  数组操作和渲染相关 部分方法替换原list push() pop() shift() unshift() splice() sort() reverse()\n部分方法返回新数组 如 filter()、concat() 和 slice()\nexample1.items = example1.items.filter(item =\u0026gt; item.message.match(/Foo/))\n可以用计算属性 或者methods来帮助进行排序什么的。\n事件处理 然而许多事件处理逻辑会更为复杂，所以直接把 JavaScript 代码写在 v-on 指令中是不可行的。因此 v-on 还可以接收一个需要调用的方法名称。 除了直接绑定到一个方法，也可以在内联 JavaScript 语句中调用方法：\n\u0026lt;div id=\u0026quot;inline-handler\u0026quot;\u0026gt; \u0026lt;button @click=\u0026quot;say('hi')\u0026quot;\u0026gt;Say hi\u0026lt;/button\u0026gt; \u0026lt;button @click=\u0026quot;say('what')\u0026quot;\u0026gt;Say what\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; Vue.createApp({ methods: { say(message) { alert(message) } } }).mount('#inline-handler') 有时也需要在内联语句处理器中访问原始的 DOM 事件。可以用特殊变量 $event 把它传入方法： \u0026lt;button @click=\u0026quot;warn('Form cannot be submitted yet.', $event)\u0026quot;\u0026gt; Submit \u0026lt;/button\u0026gt; 事件处理程序中可以有多个方法，这些方法由逗号运算符分隔： \u0026lt;!-- 这两个 one() 和 two() 将执行按钮点击事件 --\u0026gt; \u0026lt;button @click=\u0026quot;one($event), two($event)\u0026quot;\u0026gt; Submit \u0026lt;/button\u0026gt;  事件修饰符 在事件处理程序中调用 event.preventDefault() 或 event.stopPropagation() 是非常常见的需求。尽管我们可以在方法中轻松实现这点，但更好的方式是：方法只有纯粹的数据逻辑，而不是去处理 DOM 事件细节。\n为了解决这个问题，Vue.js 为 v-on 提供了事件修饰符。之前提过，修饰符是由点开头的指令后缀来表示的。\n.stop .prevent .capture .self .once .passive\n\u0026lt;!-- 阻止单击事件继续传播 --\u0026gt; \u0026lt;a @click.stop=\u0026quot;doThis\u0026quot;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;!-- 提交事件不再重载页面 --\u0026gt; \u0026lt;form @submit.prevent=\u0026quot;onSubmit\u0026quot;\u0026gt;\u0026lt;/form\u0026gt; \u0026lt;!-- 修饰符可以串联 --\u0026gt; \u0026lt;a @click.stop.prevent=\u0026quot;doThat\u0026quot;\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;!-- 只有修饰符 --\u0026gt; \u0026lt;form @submit.prevent\u0026gt;\u0026lt;/form\u0026gt; \u0026lt;!-- 添加事件监听器时使用事件捕获模式 --\u0026gt; \u0026lt;!-- 即内部元素触发的事件先在此处理，然后才交由内部元素进行处理 --\u0026gt; \u0026lt;div @click.capture=\u0026quot;doThis\u0026quot;\u0026gt;...\u0026lt;/div\u0026gt; \u0026lt;!-- 只当在 event.target 是当前元素自身时触发处理函数 --\u0026gt; \u0026lt;!-- 即事件不是从内部元素触发的 --\u0026gt; \u0026lt;div @click.self=\u0026quot;doThat\u0026quot;\u0026gt;...\u0026lt;/div\u0026gt; 使用修饰符时，顺序很重要；相应的代码会以同样的顺序产生。因此，用 v-on:click.prevent.self 会阻止所有的点击，而 v-on:click.self.prevent 只会阻止对元素自身的点击  按键修饰符 在监听键盘事件时，我们经常需要检查详细的按键。Vue 允许为 v-on 或者 @ 在监听键盘事件时添加按键修饰符：\n\u0026lt;input @keyup.enter=\u0026ldquo;submit\u0026rdquo; /\u0026gt;\n可以用如下修饰符来实现仅在按下相应按键时才触发鼠标或键盘事件的监听器。 .ctrl .alt .shift .meta\n\u0026lt;input @keyup.alt.enter=\u0026ldquo;clear\u0026rdquo; /\u0026gt;\n.exact 修饰符允许你控制由精确的系统修饰符组合触发的事件。\n.left .right .middle 这些修饰符会限制处理函数仅响应特定的鼠标按钮\n表单输入绑定 v-model 会忽略所有表单元素的 value、checked、selected attribute 的初始值而总是将当前活动实例的数据作为数据来源。你应该通过 JavaScript 在组件的 data 选项中声明初始值。\nv-model 在内部为不同的输入元素使用不同的 property 并抛出不同的事件：\ntext 和 textarea 元素使用 value property 和 input 事件； checkbox 和 radio 使用 checked property 和 change 事件； select 字段将 value 作为 prop 并将 change 作为事件。\n使用v-model来将输出绑定数据。v-model后跟着就是data名称。\n多个复选框，绑定到同一个数组：\n\u0026lt;div id=\u0026quot;v-model-multiple-checkboxes\u0026quot;\u0026gt; \u0026lt;input type=\u0026quot;checkbox\u0026quot; id=\u0026quot;jack\u0026quot; value=\u0026quot;Jack\u0026quot; v-model=\u0026quot;checkedNames\u0026quot; /\u0026gt; \u0026lt;label for=\u0026quot;jack\u0026quot;\u0026gt;Jack\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026quot;checkbox\u0026quot; id=\u0026quot;john\u0026quot; value=\u0026quot;John\u0026quot; v-model=\u0026quot;checkedNames\u0026quot; /\u0026gt; \u0026lt;label for=\u0026quot;john\u0026quot;\u0026gt;John\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026quot;checkbox\u0026quot; id=\u0026quot;mike\u0026quot; value=\u0026quot;Mike\u0026quot; v-model=\u0026quot;checkedNames\u0026quot; /\u0026gt; \u0026lt;label for=\u0026quot;mike\u0026quot;\u0026gt;Mike\u0026lt;/label\u0026gt; \u0026lt;br /\u0026gt; \u0026lt;span\u0026gt;Checked names: {{ checkedNames }}\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt;  for 属性规定 label 与哪个表单元素绑定。 这个不是vue规定的，是html规定的。注意不是v-for\n但是有时我们可能想把值绑定到当前活动实例的一个动态 property 上，这时可以用 v-bind 实现，此外，使用 v-bind 可以将输入值绑定到非字符串\n\u0026lt;select v-model=\u0026quot;selected\u0026quot;\u0026gt; \u0026lt;!-- 内联对象字面量 --\u0026gt; \u0026lt;option :value=\u0026quot;{ number: 123 }\u0026quot;\u0026gt;123\u0026lt;/option\u0026gt; \u0026lt;/select\u0026gt; // 当被选中时 typeof vm.selected // =\u0026gt; 'object' vm.selected.number // =\u0026gt; 123  组件基础 const app = Vue.createApp({}) // 定义一个名为 button-counter 的新全局组件 app.component('button-counter', { data() { return { count: 0 } }, template: ` \u0026lt;button @click=\u0026quot;count++\u0026quot;\u0026gt; You clicked me {{ count }} times. \u0026lt;/button\u0026gt;` }) 组件是带有名称的可复用实例，在这个例子中是 \u0026lt;button-counter\u0026gt;。我们可以把这个组件作为一个根实例中的自定义元素来使用： \u0026lt;div id=\u0026quot;components-demo\u0026quot;\u0026gt; \u0026lt;button-counter\u0026gt;\u0026lt;/button-counter\u0026gt; \u0026lt;/div\u0026gt; app.mount('#components-demo') component拥有自己的data域。注意这个data域是子data域。同为template的子域是无法访问的。  通过prop 向子组件传递数据 Prop 是你可以在组件上注册的一些自定义 attribute。为了给博文组件传递一个标题，我们可以用 props 选项将其包含在该组件可接受的 prop 列表中：\n这里props就是一个预先申明你会传入哪些参数，所以template可以使用哪些参数。\nprops 传入的值并不是从父类继承的。是在创建组件的时候集成的。即这个时候传入value 用props接收。\n整个步骤是什么？ 父级：Vue 初始app 子级： html 组件 和 定义的component。component就是定义一段可复用的html。它应该是独立于实例的。所以它不访问实例内容。但是被实例mount的html是可以访问实例的。所以通过被mount的html访问实例。然后传值给component。\ncomponent的html 可以通过来绑定属性到slot上。然后在html上就可以通过v-slot:slotName=\u0026ldquo;variableName\u0026rdquo; 来访问。\n总的来说component是一个独立出来的component html， 和创建的实例隔离开来，互不相访问。但是 实例可以和挂载的html互相访问。html可以使用该实例的component来访问其变量。并通过slot互相交互。component的目的就是复用。\ntemplate和组件配合使用分发插槽等。\n$emit 组件的作用就是来可复用。所以html理论上不能访问组件的变量。应该由组件内部自己显示变量。\n数据流向，是从父html单向流向子组件的。子组件不应该修改父html的值。\njavascript传值都是传引用。所以值会引发连串修改。\n子组件使用props来接收父组件的传值。 子组件使用$emit向父组件来发送值，父组件通过子组件的$emit来监听传值。\n子组件 @click=$emit(\u0026lsquo;functionName\u0026rsquo;, args)\n父组件 @functionName=\u0026ldquo;balabala\u0026rdquo;\n这个意思是子组件点击的时候发送functionName。 父组件接收functionName后执行balabala 父组件可以通过$event来访问这个参数。或者将这个参数原封不动传给balabala。\n注意emit出去的需要用 字符串符号括起来。它发射的是字符串。\n然后发射的只能是小写字母。html不区分大小写。Vue区分。\n子组件传出单个参数时： // 子组件 this.$emit(\u0026lsquo;test\u0026rsquo;,this.param) // 父组件 @test=\u0026lsquo;test($event,userDefined)\u0026rsquo;\n子组件传出多个参数时： // 子组件 this.$emit(\u0026lsquo;test\u0026rsquo;,this.param1，this.param2, this.param3) // 父组件 arguments 是以数组的形式传入 @test=\u0026lsquo;test(arguments,userDefined)\u0026rsquo;\n// 父组件 @test=\u0026lsquo;test($event,userDefined)\u0026rsquo; //userDefined 为父组件的附加对象/参数 常写成 @test=\u0026lsquo;test\u0026rsquo; 这个时候$event默认传参了。\n当执行函数时，没有参数时，$event参数默认在第一个。 当有多个参数时，$event需要显示传参。位置随意。\n$emit 传出参数的也可以进行验证。\n插槽 在template可以使用插槽来代替html 插槽中的内容。\n但是注意，子作用域可以访问父作用域。 父作用域先渲染。 子作用域后渲染。 父级模板里的所有内容都是在父级作用域中编译的；子模板里的所有内容都是在子作用域中编译的。 子作用域的内容不可以互相访问。\n中的内容，如果html的template中有内容会替换，没内容会保留这里的内容。作为vue的基本模板。大多数的内容都可以放在template上。但少数如v-slot不能放在template上。\u0026lt;template v-slot:\u0026ldquo;定义的name\u0026rdquo;=\u0026ldquo;一个对象用来接收item的值\u0026rdquo;\u0026gt; 这样html可以操控布局应该怎样呈现数据。也可以访问子组件定义的值。\n在组件上使用v-model v-model是vue的语法\n正常 详细 \u0026lt;input :value=\u0026ldquo;searchText\u0026rdquo; @input=\u0026ldquo;searchText=$event.target.value\u0026rdquo;\u0026gt; 在组件上 \u0026lt;component :model-value=\u0026ldquo;searchText\u0026rdquo; @update:model-value=\u0026ldquo;searchText=$event\u0026rdquo;\u0026gt;\nv-###:asdf 使用v开头的都是vue的语法。后面跟着的asdf都不用引号的。\n动态组件 组件命了名后，如果有组件需要切换的话 来切换\n组件可以包裹信息，不让他消失。\n与Dom兼容关系 当你用驼峰大小写的时候，在html 属性上，因为不区分大小写，所以驼峰大小写会被解释为带一个横线\n建议直接使用kebab-case 来定义组件名称。就是全部小写带横线。\n局部注册 使用对象实例来注册局部组件。\nprops prop可以预先声明接受的值的类型。还可以进行预先的验证。 类型检查： String Number Boolean Array Object Date Function Symbol\n这个类型检查可python不同。python类型检查错了也行，但这里必须检查正确才行。\nprovide 这里provide :{} 和provide(){return{}}有什么区别？ 感觉是一样的。直觉告诉我，一个是属性，一个是函数。 感觉真的要学javascript啊。=》javascript找不到答案。\n这个provide 和 inject 并非是响应式的。\n使用vue.computed(()=\u0026gt;)来响应式更改值。\n没读懂 异步组件 非Prop的Attribute 模板引用 API 计算属性 \u0026lt;div id=\u0026quot;computed-basics\u0026quot;\u0026gt; \u0026lt;p\u0026gt;Has published books:\u0026lt;/p\u0026gt; \u0026lt;span\u0026gt;{{ publishedBooksMessage }}\u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; Vue.createApp({ data() { return { author: { name: 'John Doe', books: [ 'Vue 2 - Advanced Guide', 'Vue 3 - Basic Guide', 'Vue 4 - The Mystery' ] } } }, computed: { // 计算属性的 getter publishedBooksMessage() { // 这里的函数名就是data 的property了。 // `this` 指向 vm 实例 return this.author.books.length \u0026gt; 0 ? 'Yes' : 'No' } } }).mount('#computed-basics') //通过computed来计算得到data 的 property，函数名就是property，记住它是个函数，处于data层 // 你可能已经注意到我们可以通过在表达式中调用方法来达到同样的效果： methods: { calculateBooksMessage() { return this.author.books.length \u0026gt; 0 ? 'Yes' : 'No' } } // 我们可以将同一函数定义为一个方法而不是一个计算属性。两种方式的最终结果确实是完全相同的。然而，不同的是计算属性是基于它们的响应依赖关系缓存的。计算属性只在相关响应式依赖发生改变时它们才会重新求值。这就意味着只要 author.books 还没有发生改变，多次访问 publishedBookMessage 计算属性会立即返回之前的计算结果，而不必再次执行函数。 //计算属性默认只有 getter，不过在需要时你也可以提供一个 setter： computed: { fullName: { // getter get() { return this.firstName + ' ' + this.lastName }, // setter set(newValue) { const names = newValue.split(' ') this.firstName = names[0] this.lastName = names[names.length - 1] } } } // 现在再运行 vm.fullName = 'John Doe' 时，setter 会被调用，vm.firstName 和 vm.lastName 也会相应地被更新 // get是从需要的值到最终值。 // set是从最终值到初始值的改变。 // fullname是data property \u0026lt;/script\u0026gt;  补充知识 axios \u0026lt;script src=\u0026quot;https://cdn.jsdelivr.net/npm/axios@0.12.0/dist/axios.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt;  // 引入axios // 发送 POST 请求 axios({ method: 'post', url: '/user/12345', data: { firstName: 'Fred', lastName: 'Flintstone' } }); // 请求响应的处理在 then 和 catch 回调中，请求正常会进入 then ，请求异常则会进 catch axios({ method: 'post', url: '/user/12345', data: { firstName: 'Fred', lastName: 'Flintstone' } }).then(res =\u0026gt; { consloe.log(res) }).catch(err =\u0026gt; { console.log(err) }) // 通过 axios 发出的请求的响应结果中， axios 会加入一些字段，如下 { // `data` 由服务器提供的响应 data: {}, // `status` 来自服务器响应的 HTTP 状态码 status: 200, // `statusText` 来自服务器响应的 HTTP 状态信息 statusText: 'OK', // `headers` 服务器响应的头 headers: {}, // `config` 是为请求提供的配置信息 config: {}, // 'request' // `request` is the request that generated this response // It is the last ClientRequest instance in node.js (in redirects) // and an XMLHttpRequest instance the browser request: {} }  箭头函数 javascript中箭头函数就是匿名函数 x =\u0026gt; x * x 等价于 function (x) { return x * x; }\n判定符号 a==b 先类型转换后，再左右判断 a===b 直接左右判断是否相等。\n注意  html 中里是没有逗号的，用空格分隔。  ","id":58,"section":"posts","summary":"其他 使用Vscode写html，一个！号后可以选择模板。 引入Vue框架 \u0026lt;script src=\u0026quot;https://unpkg.com/vue@next\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; 练手： https://gitee.com/panjiachen/vue-admin-template 基础 创建实例, 支持链式。返回同一个实例。 Vue.createAPP({}) .component(\u0026quot;\u0026quot;) .directive 挂载到组件 const","tags":[],"title":"Vue学习","uri":"https://yanyuLinxi.github.io/2021/09/vue%E5%AD%A6%E4%B9%A0/","year":"2021"},{"content":"DeepVuler 面向开源社区的漏洞挖掘平台 目标 通过社区对代码漏洞的讨论进行分析，完成以下几个任务：\n 发现社区讨论的新漏洞情报，使用神经网络对漏洞讨论内容进行分析，并对漏洞进行评级和分类。 发现社区讨论的漏洞，已经被厂家安全更新，但是没有收录为CVE条目的。提醒厂家更新版本。 关注漏洞情报挖掘者和易损仓库，及时发现新的漏洞情报。  方法  使用神经网络对漏洞讨论内容进行检测，用于区分讨论内容是普通内容还是漏洞讨论内容。  数据采集和标注  采集Github讨论内容数据。通过Github的Commits和Security采集漏洞讨论内容。   特征分析和元特征抽取  提取60个元特征分为五类。文本、行为、用户、仓库、会话。  行为：帖子链接数量，帖子正向表情、负面表情 绘画：发帖时间，字数等。     PTFT深层特征提取模块  在每个空间特征内，五大特则会那个组选择处的特征进一步构建具有时空概念的三维特征。（本子原话，但是根本没说怎么组合的）  每一个帖子的特征向量按时间顺序送入注意力LSTM。     轻量级幻影残差网络。  进一步将特征送入卷积网络进行分类。目标是判断这是漏洞事件还是普通事件。 幻影网络来自华为2020CVPR:GhostNet: More Features from Cheap Operations。 主要贡献是通过线性操作来减少运算代价，加速网络。就是一种加速CNN。     使用FGA对差异性代码判断代码更新是普通更新还是安全性更新。  数据获取，从commit获取代码更新签后的代码和针对代码更新的描述  设计统计特征组，分别对代码描述和代码提取相关特征。共计40多个特征。 针对代码描述，先去除停用词，词形还原等预处理方式，送入word2vec获取100维特征。 针对代码，将代码的函数名、变量名进行替换，然后word2vec获取200维特征。 将这些特征先后拼接在一起。   使用神经网络进行学习  送入FCN(MLP)+BiGRU(BiLSTM) 进行二分类，判断是普通更新还是安全性更新。     使用知识图谱对漏洞挖掘者和易损仓库进行识别  老东西了，也不是擅长领域。粗略看了后也没有什么革新。略。    数据收集  根据CVE,NVD 收集数据。 github中监听Security模块。  亮点 设计方面：\n 立意足够有趣。通过commit判断代码是否为漏洞，并进行漏洞挖掘。是实用且有趣的思路。 跨模型种类的模型组合：RNN结合CNN。这是一种比较新颖的模型缝合方式。  模型方面：\n 设计了非常多的统计特征。 将统计特征和神经网络训练得到的特征连接起来共同训练。增加可解释性，也增加效果。 将文本特征和代码特征共同作为判断依据。  评价  立意新颖。 本子看似内容多，实则结构非常混乱，读的很糟心。比如序号1），a），I）混用，无法分清层级关系 很多重点内容关键内容模糊带过，无法理解。  “白泽”反诈骗网站智能侦察取证研判系统 目标 设计一个系统，完成以下目标：\n 根据已有的数据，在互联网中发掘诈骗网站 针对一个网页，判断其是否为诈骗网站  方法  根据警方提供的3000个诈骗网站，通过下面原理找出其他诈骗网站：  网站内容分析。各个诈骗网站会互相创建友情链接，或者调用相同的静态文件（同一个模板制作的诈骗网站）。 网站证书加密。很多诈骗网站使用证书加密。且大多使用多域名证书加密，即一个证书加密多个域名（250个），根据证书注册信息可以挖掘出其他网站 网站域名分析。网站域名简单且相似，大多为字母和数字的排列组合，如 https://www.40939b.com/ 等。根据所有域名生成词典，然后排列组合可以发掘其他域名。 网站注册。  网站集中注册。如果服务商某短时间内收到大量注册信息，其中一个是诈骗网站，其他可能都是 统一身份注册。同一注册人注册的其他网站大多为诈骗网站。   DNS解析。根据动态DNS技术，一个域名可以对应多个IP，通过PDNS（DNS反向解析）技术找出一个诈骗网站域名下的其他IP IP解析。为了节约成本，常常多个域名指向一个IP，通过被动DNS，可以找到指向同一个IP的多个不同域名 IP子网技术。诈骗分子往往大批量部署服务器。一个IP网段的子网段窝藏诈骗网站的概率往往很大。 使用域名生成器挖掘  根据遗传变异算法生成新的潜在网站域名集合。     使用TextCNN分析网站内容。  脚本访问IP获取网页HTML内容。 过滤出关键信息、链接等。 通过语义信息和结构信息分别提取出特征向量后组合，送入分类器判断，输出为诈骗网站的分数。    数据收集  由警方提供 根据已有数据集挖掘。  亮点  立意新颖。核心的两个功能，一个通过遗传算法生成域名合集，二通过TextCNN判断网站内容。两个功能都非常实用。 通过已有IP域名挖掘其他域名，然后使用神经网络进行检查。是传统安全和人工智能的比较好的结合。  评价  涉及的神经网络技术并不复杂，主要是有目标需求，然后针对需求进行分析并完成项目。 通过已有IP域名发掘其他诈骗网站IP，需要对诈骗网站进行详尽的分析、调查。本项目工作调查准备的时间可能会比项目实现的时间更多。 本子过于简洁，且章节之间没有逻辑。详细技术没有阐明。  遗传算法 设定：\n 染色体：问题的一组解，由若干基因组成（基因即为基础元素，变量） 适应度函数：遗传算法迭代N次，每次迭代生成若干染色体。判断染色体的适应度。淘汰适应度低的染色体，保留适应度高的染色体。 交叉：每次迭代生成的染色体的生成方式：  从上一代中选择两条染色体，选择爸爸的某一个位置切断，选择妈妈的某一个位置切断，拼接成新的染色体。 选择哪些染色体由轮盘赌注决定：  染色体i被选择的概率 = 染色体i的适应度/ 所有染色体的适应度之和。     变异：每次生成了一条新的染色体后，在新染色体上随机选择若干基因然后修改基因的值。 复制：每次保留上一代中适应度最高的几条染色体，原封不动传给下一代。  整体流程：\n 在算法初始阶段，它会随机生成一组可行解，也就是第一代染色体。 然后采用适应度函数分别计算每一条染色体的适应程度，并根据适应程度计算每一条染色体在下一次进化中被选中的概率 下面正式进入“进化”过程。  通过“交叉”，生成N-M条染色体； 再对交叉后生成的N-M条染色体进行“变异”操作； 然后使用“复制”的方式生成M条染色体；   到此为止，N条染色体生成完毕！紧接着分别计算N条染色体的适应度和下次被选中的概率。这就是一次进化的过程，紧接着进行新一轮的进化。  ","id":59,"section":"posts","summary":"DeepVuler 面向开源社区的漏洞挖掘平台 目标 通过社区对代码漏洞的讨论进行分析，完成以下几个任务： 发现社区讨论的新漏洞情报，使用神经网络对漏洞讨论内容进行","tags":["论文阅读笔记"],"title":"信息安全技术大赛两个项目汇报","uri":"https://yanyuLinxi.github.io/2021/09/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8%E6%8A%80%E6%9C%AF%E5%A4%A7%E8%B5%9B%E4%B8%A4%E4%B8%AA%E9%A1%B9%E7%9B%AE%E6%B1%87%E6%8A%A5/","year":"2021"},{"content":"英语学习方法：\n  跟读 你有什么相见恨晚的英语学习方法？ - 翘囤奶爸的回答 - 知乎 https://www.zhihu.com/question/26677313/answer/777772073\n  输入+输出\n  背课文+写作。\n","id":60,"section":"posts","summary":"英语学习方法： 跟读 你有什么相见恨晚的英语学习方法？ - 翘囤奶爸的回答 - 知乎 https://www.zhihu.com/question/26677313/answer/777772073 输入+输出 背课文+写作。","tags":["other study"],"title":"EnglishLearning","uri":"https://yanyuLinxi.github.io/2021/08/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0/","year":"2021"},{"content":"画图：  如何在论文中画出漂亮的插图？ - 隐生宙的回答 - 知乎 https://www.zhihu.com/question/21664179/answer/52278440  论文写作注意:  论文常用词汇i.e.，e.g.，etc.，viz.，et al.的前世今生 - 薛动谔的喵的文章 - 知乎 https://zhuanlan.zhihu.com/p/63640148  论文检索  登录web of science 右上角选择产品-\u0026gt;classic 选择dataset 为 web of science 就是sci检索合集。 journal citation report 可以搜索期刊排名。  Q1取前百分之二十 Q2取前百分之五十 取论文发表那一年的为哪一区。   EI Compendex收录，即为EI检索。大多会议是EI检索。 可以在engineering village中进行搜索。  科研工具 deepl 翻译软件 grammarly quillbot\n论文下载 知网前缀kns改成oversea可以下载pdf。\n","id":61,"section":"posts","summary":"画图： 如何在论文中画出漂亮的插图？ - 隐生宙的回答 - 知乎 https://www.zhihu.com/question/21664179/answer/52278440 论文写作注意: 论文常用词汇i.e.，e.g.，etc.，viz.，et al.的前世今","tags":["科研工具"],"title":"科研工具mark","uri":"https://yanyuLinxi.github.io/2021/07/%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7mark/","year":"2021"},{"content":"科研十条，学会讲故事，说他人想听的话 作者：刀可它盆盆 链接：https://www.zhihu.com/question/337000233/answer/1362826082 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n我老板有一个保留多年的习惯，就是每天看到好的文章和经验贴都会email发给他的一众学生阅读。他曾经发过一个“投稿十条”，赠人玫瑰手留余香，希望对你们写好SCI有帮助。第一条 令人信服：讲好你的故事一篇好的稿件就是围绕一个简单的要点讲一个故事。你写的一切都必须支持这个要点。我解释一下：就是你文章的亮点，你解决了什么问题，突破了什么最高值，发展了什么新材料，为什么你做的这个东西这么重要，你的数据是怎么证明你的结果的等等，围绕你的亮点有理有据的展开，别试图说你的工作有六七八个亮点，小心贪多你一个都讲不好。\n第二条 令人信服：学着写好绝大多数科学家都不是作家。因此，你不仅得花时间自学，还得学习他人。学着写好的一个有效的方法就是找出那些你喜欢阅读的文献，弄明白它写得究竟好在哪里，然后试着用自己的表达方式模仿出来就够了。我解释一下：别跟写考研英语作文似的，东一句西一句最后也能拿个基本分，SCI投稿写不好可能就失去了发到一个期刊的机会。打开相关领域的文章好好读一下，甚至写的过程中都要时不时看看人家的句子表述。不过大佬的话就可能下笔如有神，洋洋洒洒一晚上就写完一篇文章，不需要任何参考。\n第三条 令人信服：爱图稿件中的插图是传递你想表达的意思的关键，因此插图必须让那些对你论文数据不怎么熟悉的读者容易理解、读得懂。尽可能利用色彩搭配、图形组合和附图文字让插图越清晰越明了。我解释一下：不得不说现在发文章标准越来越高了，看人家的文章图示一个比一个抢眼。自己会几个绘图软件更好，不会的话就花钱找公司吧。\n第四条 不要藏掖: 投稿前要请人把关尽管将初稿给别人看并不太好，但是作为读者的别人往往会看出稿件中那些你往往忽视的地方。把初稿早点给周围的同事看，他们会挑出那些后来审稿人会指出的毛病，你就可以及早改正，这往往会让后来的审稿更容易通过。我解释一下：这点挺重要的，自己一遍又一遍的读，已经脑袋僵硬，视觉疲劳，很难看出来什么显而易见的问题。这时候找同行看看，受益良多，可以提升很多自己忽视的点。\n第五条 有备无患：做好原始数据及备份的打算越来越多的期刊要求作者提供稿件中的原始数据和结果的源代码。因此，你得养成数据备份、存储和保管好原始记录的习惯，万一需要你提供或分享数据时，你可以在别人面前嘚瑟。我解释一下：很多文章数据被质疑，期刊要求提供原始数据，结果作者说丢了的例子你们听过不少吧？\n第六条 有的放矢: 投给该投的期刊，多从读者角度思考你的论文是写给谁看的？换位思考总能提前帮助你找到最适合的某些期刊，并提前想到可能哪些专家会审你的稿。写论文时时刻刻把读者放心头，就会帮助你遣词、造句、表意、表达等更恰当更合适。我解释一下：很多文章投稿的时候都会让写五个左右关键词，或者投稿过程中让在线选自己工作所属领域。这些都要慎重，会影响编辑选哪个方向的专家审你的文章。\n第七条 简洁明了：写摘要要格外用心认真写摘要可能是写稿时的最后一个步骤，但是这往往最重要也值得你最用心对待。类似的摘要材料还有好多，譬如论文重要性陈述、几点干货提纲、图文摘要、可供社交媒体转载的总结等等。我解释一下：正文尤其是article形式的文章，里面的某一句话倒没那么重要，但是摘要一定要逐字逐句的改，短短几句话呈现整个文章的精华，审稿人都是通过摘要和示意图了解你的工作到底重不重要。\n第八条 知己知彼：你得知晓编辑过程中的各种可能期刊的编辑过程往往有些神秘兮兮，但是通过与共同作者的讨论、读一些有关的博客、与一些担任学术期刊编辑的学者交流，你总能对典型的编辑流程了解个七七八八。你知晓得越多，编辑过程对你而言就越不神秘，在接下来的投稿及应对编辑的过程中你就越有信心。我解释一下：我理解的这一条的意思是通过期刊显示的你文章的状态比如submitted/with editor/peer review/under consideration这种了解文章审理到了哪一步，不同数据库不一样，自己明白才能应对自如。\n第九条 开诚布公：不放过每一点改正、提升自己论文的机会如何面对各种审稿意见往往是个麻烦事，你原以为差不多完事的稿件现在要你重新来过，哪怕只是要求修改其中的一小部分，心里总是不舒服。但是，所有的编辑都认为，审稿及返修的过程对稿件都是有百利而无一害的。你收到的审稿意见值得你重视。即便有些审稿建议你坚持己见不作修改，起码可以通过修改表述更明晰的表达出你想传递给读者的信息。我解释一下：尊重审稿人非常重要，拿出虚心受教的态度来会让编辑更有好感，从而赢得机会。\n第十条 与人为善：与人打交道需要好态度尽管事关你自己的论文投稿，最后发表与否很大程度上取决于别人的态度和别人的服务，以及别人是否愿意真心为你投稿质量的提升义务奉献。换个角度就很容易理解了，比方说你可能就是一本期刊的编辑或者审稿人，或者你的同事拿他的论文初稿请你提意见的时候。我解释一下：别人请你提意见，你诚诚恳恳说几条，结果对方不但不改还态度不屑，你说气人不气人，肯定想立马毙掉他。你文章的审稿人也是这样想的。这一条似乎与上一条意思差不多吧。\n自我科研心得 1. 数据对神经网络更重要。模型的重要程度一般。  科研良好习惯： 作者：叶小飞 链接：https://www.zhihu.com/question/470047139/answer/2080775493 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n我有一套叶氏四大切换定律支撑着我的科研新鲜感！其实科研就像涮火锅，就算它再好吃，你再爱吃，如果逼着你你天天吃，顿顿吃，到后面一定会“见锅色变”。所以，保持科研新鲜感的诀窍就是学会切换二字，这个诀窍让我无论是在奔驰研究院工作还是在UCLA读博时都始终干劲十足。这切换二字可以扩展为四大方面, 即：多个科研课题之间的切换，输入与输出之间的切换，体力劳动与脑力劳动之间的切换，工作日与周末的切换。多个科研课题之间的切换每个科研人员在做研究的道路上一定会遇到这样的情景：某个课题已经有了很明确的思路，但是总感觉前路漫漫不知何时做完，熬的越久越觉得筋疲力尽；抑或卡在了某个节点上很久，左想右想找不到出路，感觉心力交瘁。今年我在开发一款协同驾驶仿真开源框架（OpenCDA) 时就经常遇到这两种情况。因为我几乎是一个人用python重写了一套完整的自动驾驶系统，写到后面多车协同规划时简直每天都要被bug淹没，可以说看到那块代码就想呕吐。但是我并没有硬着头皮蛮写，而是又在业余时间开了一个关于GAN（对抗神经网络）的小课题。我每周都会拿出一天来研究这个小课题，虽然看似我攻克主项目的时间变少了，但在那个阶段却让我效率得到极大提高。由于这两个项目领域不同，我在研究GAN的最新成果时感觉十分新鲜，就好像看惯了江南山水后忽然看到了大漠一般整个人都被refresh了。而当我再次回到之前的主课题时，疲劳感几乎一扫而光，又重新充满了无尽的斗志。最后这个OpenCDA历时半年后终于成功开源并在协同驾驶领域产生了一定的影响力，而我所研究的小课题也在不久前投了今年的AAAI。所以我个人认为，科研人员有自己深挖主方向固然重要，但你同时也可以花相对较少的时间开拓一个子方向（最好与主方向有一定的overlap), 这样你可以避免长期窝在同一个课题时产生不可逆的疲劳，通过来回切换保持新鲜感。同时，有时候研究一下不太相同的领域也能给你带来新的灵感，最经典的例子之一就是把NLP的transformer用到了vision任务上，开拓了一个新的大热方向。输入与输出的切换其实科研的本质就是一个不断输入（例如阅读论文，上网课、听讲座、看代码）与输出（例如写代码、头脑风暴、参与学术研讨）的过程。一旦这个输入与输出关系失去平衡，人就会陷入科研疲劳。举个实际的例子，当一个研究人员因为新接触一个领域而阅读大量文献时，往往会特别兴奋，但如果让他持续读这些文献一个月不做别的，他再看到这个论文一定会受不了，因为输入本身产生的正反馈较小，而科研人员需要正反馈来保持动力。同样，当一个人一直在对外输出，却不去接受新知识、新观点时，他所产生的idea一定都是翻来覆去那一套，时间一久，自然会失去新鲜感。所以我的做法是，每天输出（例如敲代码、码论文）到有一些疲倦时，会找个舒服的地方葛优躺一下，拿着提前打印好的最新的顶会论文或者录好的讲座（譬如我最近就在空余时间看特斯拉AI DAY）观赏起来，这个时候感觉特别享受。脑力劳动与体力劳动的切换一个人无论再怎么热爱科研，不停地进行脑力劳动而不让自己放松迟早会放空他所有的精力与热情。有的人喜欢通过补觉来休息大脑，但是科学研究表明，最好休息大脑的方式是进行体育锻炼。我一般每天科研进行到下午四点时就会觉得头昏脑胀，注意力涣散，进入了科研“贤者模式”。这个时候我就会离开座椅，去户外或者健身房活动一个小时，等我锻炼完再次回到书桌前，感觉自己又焕然一新。所以强烈建议各位小伙伴每周至少进行三到四次体育锻炼，时间段放在你科研疲劳的临界点即可。工作日与周末的切换这一条非常重要，因为它会提醒你，科研只是生活的一部分，而不是生活的全部。当一个人为了科研失去了全部的生活，他要么会变成一个偏执的工作狂，要么就会渐渐对所有事情失去兴趣，更不要说是新鲜感了。这一条我以前也不太懂，我一直以为巨佬们都是一周七天不停地工作，直到我看到了我的偶像周博磊老师的一篇回答， 发现周老师在非ddl时期工作与周末分的十分清楚。无论平时多忙，他到了周末就是去攀岩和搞乐队，只要没有ddl压力，他会把所有工作留在周一再做。国外名牌大学 PhD 的学习工作强度有多大？3505 赞同 · 132 评论回答后来我就尝试在不赶ddl的情况下，每周至少拿出一整天来完全放松，出去转一转，练练拳击、和哥们打打球，结果发现效果奇佳，每次到了周一都有一种一切都是崭新的感觉。写在最后保持科研新鲜感固然重要，更重要的是保持生活的新鲜感，愿诸君在努力科研的同时莫要忽视了生活。\n","id":62,"section":"posts","summary":"科研十条，学会讲故事，说他人想听的话 作者：刀可它盆盆 链接：https://www.zhihu.com/question/337000233/","tags":["科研记录","经验"],"title":"前人科研经验","uri":"https://yanyuLinxi.github.io/2021/07/%E5%89%8D%E4%BA%BA%E7%A7%91%E7%A0%94%E7%BB%8F%E9%AA%8C/","year":"2021"},{"content":"美食试错表 烤披萨：\n有肉很好吃。不要将鸡蛋打在中间，有水很难熟。饼底需要提前烤5分钟。玉米很难熟。放肉好了，蔬菜有点难熟。\n土豆饼\n土豆切丝，调味（记得给盐）。放入锅中煎。底部煎的焦一点更好吃。然后翻面，煎另一面，可以在煎好的面上撒上各种其他东西，洋葱，番茄酱，黑胡椒，芝士碎。\n炸鸡翅：\n提前改刀、腌制。直接空气炸锅炸。给上调料。\n炸五花肉：\n切片，瘦多，肥肉少的好吃，然后抹上调料。一勺生抽，两勺料酒，一勺耗油。用生菜卷着吃。就是一次能炸的不多。可以多炸一会儿，脆脆的好吃。\n酱炒一切 看视频做菜的时候，要能明白做这一步是要干什么。能想象出来最终味道是怎样的。\n","id":63,"section":"posts","summary":"美食试错表 烤披萨： 有肉很好吃。不要将鸡蛋打在中间，有水很难熟。饼底需要提前烤5分钟。玉米很难熟。放肉好了，蔬菜有点难熟。 土豆饼 土豆切丝，调味","tags":["厨艺学习"],"title":"厨神培养计划","uri":"https://yanyuLinxi.github.io/2021/07/%E5%8E%A8%E7%A5%9E%E5%9F%B9%E5%85%BB%E8%AE%A1%E5%88%92/","year":"2021"},{"content":" 记录自己的钢琴音乐学习之旅。\n 自我总结经验： 1. 练琴时，脑子要处理很多信息。绝对不是闭着眼睛弹 2. 还有人问练琴的意义。要明白练琴本身带来的快乐已经足够我去完成这件事。  它人经验： 1. https://www.douban.com/group/topic/15405739/ 别人的教学。 2. https://www.youtube.com/channel/UCCnATk0U9H4sz-Bx1pfm7fA/videos 零基础免费学钢琴 3. https://www.zhihu.com/question/52268773/answer/146625916 带着脑子弹琴。  它人经验总结： 自己学习安排。  现在从基础开始练习。练习拜厄。  ","id":64,"section":"posts","summary":"记录自己的钢琴音乐学习之旅。 自我总结经验： 1. 练琴时，脑子要处理很多信息。绝对不是闭着眼睛弹 2. 还有人问练琴的意义。要明白练琴本身带来的快乐已经","tags":["自学","钢琴学习","音乐学习"],"title":"钢琴学习记录","uri":"https://yanyuLinxi.github.io/2021/06/%E9%92%A2%E7%90%B4%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/","year":"2021"},{"content":" 这里记录一些读论文时的奇思妙想，留待有空的时候去验证。\n  漏洞检测  多任务图：无监督图embedding获取。  方法：  变量误用。 图生成。 将获取的图embedding做分类     强化学习等，引入代码中的工作。做代码生成或者代码还原。   多目标检测用于代码领域？  安全相关的神经网络方向  漏洞检测 恶意代码检测 僵尸网络检测。DDOS相关  其他的神经网络方向  使用神经网络来剪辑视频。  感觉难度很大，感觉也有很多事情可以做。   怎么用神经网络做人脸合成的？  神经网络模型的想法  不同模型的语义融合。  ","id":65,"section":"posts","summary":"这里记录一些读论文时的奇思妙想，留待有空的时候去验证。 漏洞检测 多任务图：无监督图embedding获取。 方法： 变量误用。 图生成。 将获取的图e","tags":["科研记录","奇思妙想"],"title":"论文奇思妙想","uri":"https://yanyuLinxi.github.io/2021/06/%E8%AE%BA%E6%96%87%E5%A5%87%E6%80%9D%E5%A6%99%E6%83%B3/","year":"2021"},{"content":"目录：   1. 综述翻译 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 12. 注释  1. 综述翻译 代码完成已成为集成开发环境的重要组成部分。当代代码完成方法依赖于抽象语法树（AST）来生成语法正确的代码。 但是，它们无法完全捕获编写代码的顺序和重复模式以及AST的结构信息。 为了缓解这些问题，我们提出了一种名为CCAG的新代码完成方法，该方法将部分AST的平坦序列建模为AST图。 CCAG使用我们提出的AST Graph Attention Block捕获AST图中的不同依存关系，以学习代码完成中的表示形式。 通过CCAG中的多任务学习优化了代码完成的子任务，并且无需调整任务权重即可使用不确定性自动实现任务平衡。 实验结果表明，CCAG比最先进的方法具有更好的性能，并且能够提供智能的代码完成功能。\n2. Tag 论文阅读笔记; 代码完成; 代码补全; 图神经网络; AST树; 抽象语法树;\n3. 任务描述  定义 AST：AST是一颗树，其中所有的非叶子节点都能对应于CFG中一个特定的非终结符，其中包含了特定的结构信息，如同IfStatement等。每一个叶子节点对应于CFG中的终结符。AST树可以轻易的转换成源代码。本文赋予了每个节点value和type。非叶子节点type为控制类型，如IfStatement，value为Empty。叶子节点value为变量值，type为变量类型。\n  定义 部分AST树：对于一个完整的AST树，部分树是完整树的一个子集。对于部分树中的每个节点n，其左序列(left sequence)节点都在该部分树中。节点n的左序列节点是指在树的先序深度优先遍历中比n早出现的所有节点。\n 本文的代码完成任务定义如下：\n 对于一个AST图的每一个部分图T\u0026rsquo;都存在一个最右节点$n_R$，部分图中的所有其他节点都是$n_R$的左序列节点，而在AST的先序深度优先遍历中紧跟于$n_R$后的那个节点则称为下一个节点(the next node)，本文的目标即在给出部分图的情况下，预测the next node的value和type。\n如下图所示，对于绿色的部分图，最右节点为NameLoad:b，代码补全的任务就是预测Return:EMPTY节点。 4. 方法 处理数据：  将部分AST树展平（flatten AST），即按照先序深度优先遍历，展开成一个序列。然后再将flatten AST 转换为AST Graph。遵循以下规则：\n 在flatten AST序列中相邻的节点，在AST Graph中会用node-node边来连接。 flatten AST中重复的节点会在AST Graph 中合并成一个节点。如果有多条边重合，则叠加为一条边，边的权重相应增加。 AST Graph中的边是无向边。保证权重双向传播。 在AST Graph中使用parent-child边来表示AST树中的父子关系，这个边是有向的。 Flatten AST转为AST Graph后丢失了在Flatten中的顺序信息，所以这里采用位置embedding来记录位置信息。对于序列$\\{n_1, n_2, n_3, n_2\\}$,假设n_2是最右节点，则$n_1, n_2, n_3$的位置embedding为全维度的3,0,1。位置embedding固定且不会被更新。  则节点$n_i$的初始embedding$h_i$为： $h_i = ReLU(W^{(p)}([t_i||v_i]+p_i)+b^{(p)})$ $t_i、v_i$是type embedding和value embedding，$p_i$是位置embedding。\n网络结构：  本文设立了一个ASTGab（graph attention block）来捕获图中的注意力关系：\n Neighbor Graph Attention Layer (NGAT)：  直接在AST graph上跑注意力网络。$e_{i,j}^{(n)} = a(W^{(n)}h_i,W^{(n)}h_j, w_{i,j})$,其中a为注意力机器。这里使用了多头注意力机制。 更新公式：$h_i^{(n)} = ReLU(\\frac{1}{M} \\sum_{m=1}^M \\sum_{j \\in N_i}\\alpha_{i,j}^{(n)}W^{(a)}h_j)$， 其中M为多头。$\\alpha=softmax(e)$。   Global Self-attention Layer(GSAT):  一个全局的自注意力机制。   Parent-child Attention Layer(PCAT):  在子节点和所有父节点之间添加注意力机制。具体公式可以看论文。这里不多介绍    将上述三个注意力机制串联起来作为一个block。多个block串联起来，并用残差捷径连接。来降低训练难度。效果如图所示： 输出：  由于最右节点（right-most）节点蕴含最多的信息，所以这里对每个节点都对最右节点做了个soft-attention： $$\\beta_{i,n_j} = z^{(t1)} \\delta ( W_1^{(t1)} h_i^{(r)} + W_2^{(t1)}h_{n_j}^{(r)} + b^{(t1)})$$ $$s_j = \\sum_{i \\in G_j} \\beta_{i, n_j} h_i^{(r)}$$\n其中$h_{n_j}$是最右节点的embedding，$h_i$是左序列节点中的节点。通过计算左序列中每个节点和最右节点的自注意力系数，再和该节点embedding相乘相加起来。得到一个图的embedding，用这个图的embedding去做分类。\nloss计算：  本文中每个AST Graph的节点既有value也有type，所以两种方法来预测这两个值：\n 分别使用两个模型预测这两个值。但论文觉得这两个值是有关联的，应该一起训练。 通过某种方式将两个loss结合起来：$L = w_v L_v + w_t L_t$  但是$w_v, w_t$这两个权重的设置影响会很大，如果设定为超参，则超参调试会很消耗时间。如果设定为可学习的，那学习的代价又太大了。所以这里采用了一个约束条件：$L \\approx \\frac{1}{\\theta^2}L_v + \\frac{1}{\\tau^2}L_t + \\log \\theta + \\log \\tau$。这个公式保证了两个任务权重是可以学习的，同时$\\log$的存在保证参数不能为负，当参数太大的时候，$\\frac{1}{\\theta^2}$又会过小。从而限制loss在一定的范围。\n5. 解决了什么问题（贡献） 论文提出了一种使用图网络学习embedding，并完成了code completion任务。 论文中提到的各种模型，各种网络结构有多少用，很难说。但是理论上有用，再加上结果正确就能提高说服力。\n这篇论文的贡献我认为还是有的。起码在这之前我一直怀疑到底图网络能不能做代码还原。该文如果实验没有差错的话，是证明了有用的。它这里用的图网络广泛使用了注意力机制。可以去思考下其他几种图网络能不能做代码还原呢？\n这里还有一个比较大的贡献点，就是它用图分类去做代码还原，用节点分类我始终认为学习道的知识不够，用图分类，就有更多的信息。而且图分类时将更多的关注集中在最右节点，因为最右节点包含了最多的信息，对于预测下一个节点来说。这里充分借鉴了RNN的思想。\n我前段时间仔细思考了下rnn，认为rnn学习到的就是一种概率。对于英文单词I，那下一个出现的此中：am，are，is，毫无疑问am的几率最高。同样高的还有have等单词。再根据上文等相关信息，判断到底是am还是have。\n这里我也认为是这样，最能决定下一个单词是什么的就是上一个单词，然后再根据相关上下文信息来帮助神经网络做出抉择。\n我认为不能把神经网络想的太过复杂。要帮助神经网络去减负。其他结构有没有用，有多少用得做消融实验，不能简单的下出结论。\n当然论文自己是做了类似结构的对比实验的。可以看到有一定优化。这个的意义是一定要保证整体模型能够学习到知识的情况下，再去做删删减减做优化。\n6. 实验结果 这里贴一张图。表示了论文的所有实验：\n可以看到CCAG效果提升还是很不错的。自己对比的实验中，多头确实效果会更好一点。这也是点小启示，要尽可能从多个维度学习信息。\n7. 如何想到该方法 对RNN比较熟悉。要理解RNN是怎么运作的。改进合理。网络的改进不宜大脚步。一步步改，确保能学到知识后，再慢慢改进改造结构。这篇论文很多参考文献都是Li，Liu，不出意外是一个在前人基础上慢慢改进的文章。\n8. 我能否想到该方法 可以，要多思考，多看相关论文。\n9. 创新点是什么 10. 如何用于本专业 11. 该方案还存在的问题 这里只说一个猜测，图结构中大多数控制节点的value都是empty，这会不会对accuracy的准确性有影响？毕竟预测empty可比较容易。\n12. 注释 启迪：\n 合适的理由（动机） + 更优的结果 = 有效的改进。改造一定要有动机。 不要凭空创造，从已知推未知更加有效。 实验时，多个类似结构相互对比，能够有效说明结构的合理性。 不要把神经网络想的太复杂。也别太小看神经网络。 保证整体模型能学习到东西的情况下，再去删删减减做优化，效率更高。一个小结构的优化最多可能百分之2百分之3.所以整体结构正确是很重要的一件事。  ","id":66,"section":"posts","summary":"目录： 1. 综述翻译 2. Tag 3. 任务描述 4. 方法 5. 解决了什么问题（贡献） 6. 实验结果 7. 如何想到该方法 8. 我能否想到该方法 9. 创新点是什么 10. 如何用于本专业 11. 该方","tags":["论文阅读笔记"],"title":"“Code Completion by Modeling Flattened Abstract Syntax Trees as Graphs”阅读笔记","uri":"https://yanyuLinxi.github.io/2021/04/code-completion-by-modeling-flattened-abstract-syntax-trees-as-graphs%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/","year":"2021"}],"tags":[{"title":"CNN","uri":"https://yanyuLinxi.github.io/tags/cnn/"},{"title":"Doc2Vec","uri":"https://yanyuLinxi.github.io/tags/doc2vec/"},{"title":"GNN","uri":"https://yanyuLinxi.github.io/tags/gnn/"},{"title":"index","uri":"https://yanyuLinxi.github.io/tags/index/"},{"title":"Insider Threat","uri":"https://yanyuLinxi.github.io/tags/insider-threat/"},{"title":"other study","uri":"https://yanyuLinxi.github.io/tags/other-study/"},{"title":"RNN","uri":"https://yanyuLinxi.github.io/tags/rnn/"},{"title":"survey","uri":"https://yanyuLinxi.github.io/tags/survey/"},{"title":"Unsupervised","uri":"https://yanyuLinxi.github.io/tags/unsupervised/"},{"title":"厨艺学习","uri":"https://yanyuLinxi.github.io/tags/%E5%8E%A8%E8%89%BA%E5%AD%A6%E4%B9%A0/"},{"title":"奇思妙想","uri":"https://yanyuLinxi.github.io/tags/%E5%A5%87%E6%80%9D%E5%A6%99%E6%83%B3/"},{"title":"异常行为分析","uri":"https://yanyuLinxi.github.io/tags/%E5%BC%82%E5%B8%B8%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/"},{"title":"用户行为预测","uri":"https://yanyuLinxi.github.io/tags/%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E9%A2%84%E6%B5%8B/"},{"title":"科研学习笔记","uri":"https://yanyuLinxi.github.io/tags/%E7%A7%91%E7%A0%94%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"title":"科研工具","uri":"https://yanyuLinxi.github.io/tags/%E7%A7%91%E7%A0%94%E5%B7%A5%E5%85%B7/"},{"title":"科研记录","uri":"https://yanyuLinxi.github.io/tags/%E7%A7%91%E7%A0%94%E8%AE%B0%E5%BD%95/"},{"title":"经验","uri":"https://yanyuLinxi.github.io/tags/%E7%BB%8F%E9%AA%8C/"},{"title":"自学","uri":"https://yanyuLinxi.github.io/tags/%E8%87%AA%E5%AD%A6/"},{"title":"论文阅读笔记","uri":"https://yanyuLinxi.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"title":"钢琴学习","uri":"https://yanyuLinxi.github.io/tags/%E9%92%A2%E7%90%B4%E5%AD%A6%E4%B9%A0/"},{"title":"音乐学习","uri":"https://yanyuLinxi.github.io/tags/%E9%9F%B3%E4%B9%90%E5%AD%A6%E4%B9%A0/"}]}