---
title: "随即森林和决策树"
date: 2021-11-17T16:10:22+08:00
tags : [

]
categories : [

]
series : []
aliases : []
draft: false
---

https://easyai.tech/ai-definition/decision-tree/
https://zhuanlan.zhihu.com/p/85731206 决策树更详细点。
https://blog.csdn.net/zjsghww/article/details/51638126 c4.5

# 什么是决策树？

决策树算法采用树形结构，使用层层推理来实现最终的分类。决策树由下面几种元素构成：

**根节点**：包含样本的全集
**内部节点**：对应特征属性测试
**叶节点**：代表决策的结果

预测时，在树的内部节点处用**某一属性值**进行判断，根据判断结果决定进入**哪个分支节点**，**直到到达叶节点处**，得到**分类结果**。


# 决策树学习的 3 个步骤
## 特征选择。
特征选择决定了使用哪些特征来做判断。在训练数据集中，每个样本的属性可能有很多个，不同属性的作用有大有小。因而特征选择的作用就是筛选出跟分类结果相关性较高的特征，也就是**分类能力较强的特征**。在特征选择中通常使用的准则是：**信息增益**。

## 决策树生成
选择好特征后，就从根节点触发，对节点计算所有特征的信息增益，选择信息增益最大的特征作为节点特征，根据该特征的不同取值建立子节点；对每个子节点使用相同的方式生成新的子节点，直到信息增益很小或者没有特征可以选择为止。

## 决策树剪枝
剪枝的主要目的是对抗「过拟合」，通过主动去掉部分分支来降低过拟合的风险。

# 3 种典型的决策树算法

## ID3 算法

ID3 是最早提出的决策树算法，他就是利用信息增益来选择特征的。
ID3 计算信息增益来对特征进行划分。信息增益基于信息熵。如果选取了这个特征，针对分类的信息熵减小了，则认为这个信息增益增大了。

ID3 步骤：

1. 初始化特征集合、数据集合。
2. 计算信息熵和所有特征的条件熵（在选取这个特征进行分类后，每一个子树的信息熵）。信息增益=信息熵-条件熵。选取特征。
   1. 信息熵：熵越大，信息越多熵越大。
      1. $H(x) = -\sum plogp$
         1. -plogp的图像，想象成一个0-1向上凸的半圆。
      2. 熵越大，混乱程度越高。最典型的，二个类个占一半。则混乱度max。
   2. 信息增益=信息熵-选择这个特征后的条件熵。
3. 根据信息增益对子节点进行划分
4. 重复2-3直至子集包含纯的标签。则为分支叶子节点。

ID3弊端：
1. 只能处理离散值。
2. 更倾向于选择特征种类多的特征。特征种类多的特征，信息增益会更大。**特征种类多，分类就多**，每个分类数据就越少，分类标签的**纯度就容易越高**。信息熵显然会低，**信息增益会更大。**
3. 无法处理缺失值
4. 没有剪枝，容易过拟合。

## c4.5

我自己总结一下。c4.5是为了解决ID3的一些弊端：
1. 对于连续值，进行离散化后排序，取相邻值进行平均并进行分节点。将**连续特征离散化**，假设 n 个样本的**连续特征 A 有 m 个取值**，C4.5 将其排序并取**相邻两样本值的平均数共 m-1 个划分点**，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点；
2. 选择使用信息增益率来代替信息增益。信息增益率=信息增益/信息内在信息。信息内在信息是指信息本身所拥有的信息。信息熵越大，信息越不确定。所以信息增益率越低。越不选它。
   1. 当叶子节点是纯的时候，就停止划分叶子节点。
   2. 引入信息增益率，可以减少对多分类特征的偏爱。
3. 对于缺失值。
   1. 在缺失特征下选择样本？使用没有缺失的样本子集
   2. 对于缺失特征的样本，按不同概率划分到不同的子节点。
4. 剪枝采用悲观后剪枝。
   1. 后剪枝：
      1. 训练完成后，进行剪枝，如果这个减去这个子树，效果保持或者不下降，则可以替换这个子树。后剪枝欠拟合风险小很多，泛化性能会优于预剪枝。但是时间开销大。
      2. 悲观后剪枝更麻烦点，优势就是只使用训练集。不使用测试集。
   2. 预剪枝：三种
      1. 节点数据比例低于某一阈值
      2. 节点特征都已经分裂。
      3. 节点划分前准确率比划分后准确率高。
   3. 预剪枝可以减少过拟合，减少训练时间。但容易带来欠拟合。

C4.5 算法最大的特点是克服了 ID3 对特征数目的偏重这一缺点

C4.5 缺点：
1. c4.5包括ID3用的都是多叉树。
2. c4.5只能用于分类。
3. 对于连续值，需要排序。对内存需求很大。数据不足以放进内存的时候，无法进行运行。
4. 熵模型有大量的耗时的对数运算。

# CART

CART针对前面的缺点的提升：
1. 采用二叉树运算快
   1. 正是因为CART树是二叉树，所以对于样本的有N>=3个取值的离散特征的处理时也只能有两个分支，这就要通过组合人为的创建二取值序列并取GiniGain最小者作为树分叉决策点
   2. 所以cart是不停的二分离散特征。若没有停止准则，则树会一直增长。
   3. 叶子节点的标签也可能有多个标签分类。
2. CART可以用于分类或者回归
3. CART使用Gini作为变量的不纯度量。减少对数运算。CART采用Gini系数来代替熵模型。基尼指数约等于熵模型的一阶泰勒展开。公式Gini = 1 - \sum p^2。 p为选择这个节点后的，标签选择概率的分类。
   1. Gini越小，不纯度越低。
4. CART 采用代理测试来估计缺失值，而 C4.5 以不同概率划分到不同节点中；
   1. 如何在特征值缺失的情况下进行划分特征的选择？
      1. 一开始只使用没有缺失的数据。后来如果缺失是20％，则惩罚20%的权重
   2. 选定该划分特征，模型对于缺失该特征值的样本该进行怎样处理？
      1. CART 算法的机制是为树的每个节点都找到代理分裂器。无论在训练数据上得到的树是否有缺失值都会这样做。在代理分裂器中，特征的分值必须超过默认规则的性能才有资格作为代理。
   3. 代理分裂：
      1. 一个分裂使用另一个预测属性。满足与主分裂最相似，切有着正的关联一簇额度量。来对缺失值分到哪一类进行判别。
5. CART 采用“基于代价复杂度剪枝”方法进行剪枝，而 C4.5 采用悲观剪枝方法。
   1. 采用一种“基于代价复杂度的剪枝”方法进行后剪枝，这种方法会生成一系列树，每个树都是通过**将前面的树的某个或某些子树替换成一个叶节点**而得到的，这种方法需要使用一个单独的测试数据集来评估所有的树，根据它们在测试数据集熵的分类性能选出最佳的树。
   2. 在所有的子树中，找出那些**使用了较多节点，**却使**错误率下降最低的子树**，把这些子树剪裁掉。
6. 对于连续值，与C4.5思想相同，**都是将连续的特征离散化**。区别在选择划分点时，**C4.5是信息增益率，CART是基尼系数。**。注意的是，与ID3、C4.5处理离散属性不同的是，如果当前节点为连续属性，**则该属性在后面还可以参与子节点的产生选择过程。**
   1. 回归模型中，采用均方差。
7. 预测：
   1. 回归采用叶子均值或者中位数
   2. 分类，采用概率最大的类别。


运行过程：
1. 分裂：分裂过程是一个二叉递归划分过程，其输入和预测特征既可以是连续型的也可以是离散型的，CART 没有停止准则，会一直生长下去；
   1. 一般有预剪枝的停止条件
      1. 控制深度、当前的节点数、分裂对测试集的准确度提升大小
      2. **限制树的高度**，可以利用交叉验证选择
      3. **利用分类指标**，如果下一次切分没有降低误差，则停止切分
      4. **限制树的节点个数**，比如某个节点小于100个样本，停止对该节点切分
2. 剪枝：采用代价复杂度剪枝，从最大树开始，每次选择训练数据熵对整体性能贡献最小的那个分裂节点作为下一个剪枝对象，直到只剩下根节点。CART 会产生一系列嵌套的剪枝树，需要从中选出一颗最优的决策树；
3. 树选择：用单独的测试集评估每棵剪枝树的预测性能（也可以用交叉验证）。

优点:
1. 



CART（Classification and Regression Tree）

这种算法即可以用于分类，也可以用于回归问题。CART 算法使用了基尼系数取代了信息熵模型。
CART是“Classification and Regression Tree”的缩写。

### cart小总结
1. cart既是分类也是回归树
2. cart分类时，采用基尼值。是回归时，采用最小方差作为分类依据。
   1. 回归时左右两子树的MSE和最小即为划分点
   2. 预测时，回归树为均值，分类树为概率最大的类别。
3. 依然是二叉树。

基尼值:GINI = 1-\sum p^2.
节点不纯（0,1标签相同，则gini最大为1/2.如果只有一个类别就是0。选择jini值作为分裂依据。

方差：方差越大，表示该节点的数据越分散，预测的效果就越差。如果一个节点的所有数据都相同，那么方差就为0,此时可以很肯定得认为该节点的输出值；如果节点的数据相差很大，那么输出的值有很大的可能与实际值相差较大。因此，无论是分类树还是回归树，CART都要选择使子节点的GIN值或者回归方差最小的属性作为分裂的方案。

https://www.bilibili.com/video/BV1Pk4y1C7m9

类别不平衡
CART 的一大优势在于：无论训练数据集有多失衡，它都可以将其子冻消除不需要建模人员采取其他操作。
这是进行剪枝的技术。通过子节点数据的分类核根节点数据的分类，就可以变相的对节点特征进行加权。确保每个类别的概率是1/k。
先验影响的是每个节点的类别赋值和树生长过程中分裂的选择。

# 决策树的优缺点
## 优点

决策树易于理解和解释，可以可视化分析，**容易提取出规则**；
可以同时处理标称型和数值型数据；
比较适合处理**有缺失属性的样本；**
能够处理不相关的特征；
测试数据集时，**运行速度比较快；**
在相对短的时间内能够对大型数据源做出可行且效果良好的结果。
## 缺点

容易发生**过拟合**（随机森林可以很大程度上减少过拟合）；
容易忽略数据集中属性的**相互关联**；
对于那些各类别样本数量不一致的数据，在决策树中，进行属性划分时，**不同的判定准则会带来不同的属性选择倾向**；信息增益准则对可取数目较多的属性有所偏好（典型代表ID3算法），而增益率准则（CART）则对可取数目较少的属性有所偏好，但CART进行属性划分时候不再简单地直接利用增益率尽心划分，而是采用一种启发式规则）（只要是使用了信息增益，都有这个缺点，如RF）。
ID3算法计算信息增益时结果偏向数值比较多的特征。


除了之前列出来的划分标准、剪枝策略、连续值确实值处理方式等之外，我再介绍一些其他差异：

1. **划分标准的差异**：ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。
2. **使用场景的差异**：ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；
3. **样本数据的差异**：ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；
4. **样本特征的差异**：ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征；
5. **剪枝策略的差异**：ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。

## 总结

一些问题记录

### ID3算法—>C4.5算法—> CART算法
**ID3**
   ID3算法没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途。
   ID3算法采用信息增益大的特征优先建立决策树的节点，偏向于取值比较多的特征
   ID3算法对于缺失值的情况没有做考虑
   ID3算法没有考虑过拟合的问题

**C4.5在ID3算法上面的改进**
   连续的特征离散化
   使用信息增益比
   通过剪枝算法解决过拟合

**C4.5的不足：**
   C4.5生成的是多叉树
   C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围。
   C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算

**CART算法**
   可以做回归，也可以做分类，
   使用基尼系数来代替信息增益比
   CART分类树离散值的处理问题，采用的思路是不停的二分离散特征。


## 决策树的目标函数是什么？

$C_α(T)=\sum_{t=1}^{|T|}N_tH_t(T)+a|T|$
$H_t(T)=−\sum_k\frac{N_{tk}}{N_t}log\frac{N_{tk}}{Nt}$

$H_t$是经验熵。就是信息熵、香农熵。当随机变量的取值越多，状态数越多，混乱度越高，纯度就学小。

## 防止过拟合
### 做bagging

### 对于决策树进行约束：根据情况来选择或组合

设置每个**叶子节点的最小样本数**，可以避免某个特征类别只适用于极少数的样本。

设置**每个节点的最小样本数**，从根节点开始避免过度拟合。

设置**树的最大深度**，避免无限往下划分。

设置**叶子节点的最大数量**，避免出现无限多次划分类别。

设置评估分割数据是的最大特征数量，避免每次都考虑所有特征为求“最佳”，而**采取随机选择的方式避免过度拟合。**

### 预剪枝(提前停止)：控制深度、当前的节点数、分裂对测试集的准确度提升大小

限制树的高度，可以利用交叉验证选择
利用分类指标，如果下一次切分没有降低误差，则停止切分
限制树的节点个数，比如某个节点小于100个样本，停止对该节点切分

### 后剪枝(自底而上)：生成决策树、交叉验证剪枝：子树删除，节点代替子树、测试集准确率判断决定剪枝

在决策树构建完成之后，根据加上正则项的结构风险最小化自下向上进行的剪枝操作. 剪枝的目的就是防止过拟合，是模型在测试数据上变现良好，更加鲁棒。

## 如果特征很多，决策树中最后没有用到的特征一定是无用吗？

不是无用的，从两个角度考虑：

**特征替代性**，如果可以已经使用的特征A和特征B可以提点特征C，特征C可能就没有被使用，但是如果把特征C单独拿出来进行训练，依然有效

决策树的每一条路径就是**计算条件概率的条件**，前面的条件如果包含了后面的条件，只是这个条件在这棵树中是无用的，如果把这个条件拿出来也是可以帮助分析数据.

# 优缺点

**优点:**

简单直观，生成的决策树很直观。
基本不需要预处理，不需要提前归一化，处理缺失值。
既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。
可以处理多维度输出的分类问题。
相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释
可以交叉验证的剪枝来选择模型，从而提高泛化能力。
对于异常点的容错能力好，健壮性高。
用白盒模型，可清洗观察每个步骤，对大数据量的处理性能较好，更贴近人类思维。

1. 简单直接，理论性强，有**很好的解释性**
2. 可以处理离散值、连续值
3. 对**异常值、缺失值**的容错能力好
4. 测试数据集时，**运行速度比较快；**


**缺点:**

决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。
决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。
寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。
有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。
如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。

1. 容易**过拟合。**
2. 容易陷入局部最优。因为最优决策树是NP难问题。
3. 不能处理较为复杂的关系，忽略特征的相互关联，**比如异或。**
4. **不同的判定准则会带来不同的属性选择倾向**



# 随机森林

随机森林属于bagging

## 构造随机森林的 4 个步骤
一个样本容量为N的样本，有放回的抽取N次，每次抽取1个，最终形成了N个样本。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。
当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。
决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。
按照步骤1~3建立大量的决策树，这样就构成了随机森林了。


