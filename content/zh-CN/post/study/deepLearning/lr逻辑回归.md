---
title: "逻辑回归lr"
date: 2021-12-11T15:47:19+08:00
tags : [

]
categories : [

]
series : []
aliases : []
draft: false
---

逻辑回归
个人理解 y = f(wx+b)
f 为 sigmoid激活函数 $1/(1+e^{-z})$

公式推导:
https://zhuanlan.zhihu.com/p/44591359
https://zhuanlan.zhihu.com/p/46928319

# 极大似然估计

1. 概率
   1. 在知道**分布规律**和**具体参数**的情况下，可以计算出某个事件发生的概率。
   2. 举例：抛硬币10次，5次正面朝上的概率。分布规律为二项分布。参数为正面朝上的概率为1/2。随意概率为$c^5_{10}*0.5^5*(1-0.5)^5=0.25$
2. 估计
   1. 不知道分布规律和具体参数，进行猜测。即为估计。
3. 极大似然估计
   1. 似然函数（联合概率）公式：将多个概率值相乘。$L=\prod_{i=1}^n p\{x=k_i|p\}$，k_i为事件。P_k为概率。
   2. 优化目标：$\hat{p}=argmax_p L$. 求一个参数使得出现现有样本可能性最大。
   3. 方法：取对数 $lnL$然后拟牛顿法，或者梯度下降。
4. 步骤：
   1. 确定分布/模型
   2. 进行多组实验并观察客观现象
   3. 写出似然概率（联合概率）
   4. 求解似然函数最大的凸优化问题，求的参数（梯度下降）

# 梯度下降

1. 原理：将参数沿着梯度相反的方向前进一个步长，就可以实现目标函数（loss function）的下降。
2. 方法：
   1. 使用损失函数，对每个参数（变量）求偏导数
   2. 将值带入，求下降方向$J(\theta)$
   3. 根据步长$\alpha$，进行一步梯度下降 $\theta <= \theta-\alpha J(\theta)$ 

# 交叉熵损失
1. 公式$f(w)=-\frac{1}{m}\sum_{i=1}^N(y^ilogf_w(x^i)+(1-y^i)log(1-x^i))$
2. 推导：
   1. 对于每一个样本，设它为1的概率为p，则为0的概率为1-p
   2. 即
   $$
      P(y|x)=
         \begin{cases}
         p& \text{y=1}\\
         1-p& \text{y=0}
         \end{cases}
   $$
   3. 上述式子不方便计算。其等价于$p(y_i|x_i)=p^{y_i}(1-p)^{1-y_i}$。 即当$y_i=1$时，该值为p,否则为1-p
   4. 其最大似然估计为$L=\prod_{i=1}^n p^{y_n}(1-p)^{1-y_n}$。在逻辑回归中，这里面只有一个参数就在p里面。
   5. 上个式子不好计算，所以我们取对数$F ( w ) = \ln ( P _ { 总} ) \\= \ln ( \prod _ { n = 1 } ^ { N } p ^ { y _ { n } }( 1 - p ) ^ { 1 - y _ { n } } ) \\= \sum _ { n = 1 } ^ { N } ( y _ { n } \ln ( p ) + ( 1 - y _ { n } ) \ln ( 1 - p ) )$。 最终为求使得这个值最大的w。添加一个符号即为求最小值。即可以用梯度下降。
3. 注意：在使用softmax归一化后，极大似然函数和交叉熵损失函数等价。
4. 凸函数判定：二阶导存在且为正。则为凸函数。凸函数意思是在凸集中局部最优等于全局最优。
   1. 二阶导存在且为正，则一阶导有负有正，则原函数局部最小就是全局最小。


# 线性回归
1. 线性回归(Linear Regression)是利用称为线性回归方程对线性问题进行建模。
公式：$f ( x ) = \theta _ { 0 } x _ { 0 } + \theta _ { 1 } x _ { 1 } + \theta _ { 2 } x _ { 2 }+... + \theta _ { n } x _ { n } = \theta ^ { T }$

2. 损失函数。
   1. 损失函数一般使用最小二乘法。$MSE=\frac{1}{2m} \sum(y-f(x))^2$
3. Lasso回归
   1. 目的：解决线性回归出现的过拟合的请况。本质：约束(限制)要优化的参数
   2. Lasso回归加入L1正则化（$\lambda$*绝对值和)
3. 岭回归加入L2正则化$\lambda$*平方和。
4. ElasticNet 回归：$\lambda(pL1+(1-p)L2)$
5. LWR( 局部加权)回归:
   1. 局部加权线性回归是在线性回归的基础上对每一个测试样本（训练的时候就是每一个训练样本）在其已有的样本进行一个加权拟合，权重的确定可以通过一个核来计算，常用的有高斯核（离测试样本越近，权重越大，反之越小）
   2. 具体的查资料

## 面试题：
1. 线性回归要求因变量服从正态分布吗
   1. 前提假设是因变量服从正态分布，此时效果达到最好。


# 公式推导

1. 背景知识
   1. **回归**：人们在测量事物的时候因为客观条件所限，求得的都是测量值，而不是事物真实的值，为了能够得到真实值，**无限次的进行测量**，最后通过这些测量数据计算**回归到真实值**，这就是回归的由来。通过**有监督的学习**，学习到由x到y的映射h，利用该映射关系对未知的数据进行预估，因为y为连续值，所以是**回归问题。**
   2. 逻辑回归使用回归函数，来解决**分类问题**，线性回归的结果Y带入一个非线性变换的Sigmoid函数中，得到[0,1]之间取值范围的数S，**S可以把它看成是一个概率值**，如果我们设置概率阈值为0.5，那么S大于0.5可以看成是正样本，小于0.5看成是负样本，就可以进行分类了。
   3. **线性回归的损失函数**。一般使用最小二乘法，则MSE误差平方为$(yi−hθ(xi))^2$找到合适的参数
2. 逻辑回归
   1. 逻辑回归=线性回归+sigmoid函数
   2. 公式: $z=wx+b, y=\frac{1}{1+e^{-z}}$。判别式是$f(x)=sign(y)$
   3. 损失函数（交叉熵损失函数）:$L = - \sum_i^n (ylogp+(1-y)log(1-p))$。 前面的y是标签，后面的p是预测出来的分布。
      1. 损失函数求导。损失函数使用最大似然估计:$L=- \sum _ { n = 1 } ^ { N } ( y _ { n } \ln ( p ) + ( 1 - y _ { n } ) \ln ( 1 - p ) )$,这里的$p=\frac{1}{1+e^{-z}}$就是这个值的概率。
      2. $\frac{\partial f(x)}{\partial z}=\frac{1}{1+e^{-z}}*\frac{e^{-z}}{1+e^{-z}}=p(1-p)$
      3. 最终求导结果为$\frac{\partial L}{\partial w}= -\frac{1}{n}\sum_{n=1}^{N}(y_i-\frac{1}{1+e^{-z}})x$
3. 案例：
   1. p(Y=1|x)=p(x)
   2. p(y=0|x)=1-p(x).p就是函数
4. 极大似然估计

推导W、b的梯度下降步骤。


## 当label={-1, +1}下的公式推导
https://github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning
1. 在label={-1,+1},则$\left. \begin{array}  { l  }  { p ( y = 1 | x ) = h _ { w } ( x ) } \\ { p ( y = - 1 | x ) = 1 - h _ { w } ( x ) } \end{array} \right.$
2. 由于sigmoid函数的特性：
   1. $\left. \begin{array}  { l  }  { h ( - x ) = 1 - h ( x ) } \\ 综上{ p ( y | x ) = h _ { w } ( y x ) } \end{array} \right.$
3. 仍然使用极大似然函数，$L=\prod p(y|x;w)=\prod h(yx)=\prod_{ i = 1 } ^ { m } \frac{ 1 }{ 1 + e ^ { - y _ { i } w x _ { i } }}$
4. 再求导计算即可。


# 面试相关

1. 如何多分类：
   1. 一对一：N个类别两两配对
   2. 一对多：每次将一个例作为正例。其他作为反例。
   3. 多对多。多个类正类，多个类反类。需要进一步的设置。
2. 逻辑回归优缺点：
   1. 优点
      1. LR的可解释性强、特征的权重可以看到不同特征的影响。模型效果不错，运行速度快（仅和特征数目有关）。方便调整阈值来做分类。
   2. 缺点：
      1. 特征工程复杂。需要较多的特征工程和归一化。
      2. 准确率不足（简单的线性回归，和SVM相比）
      3. 难以处理不平衡的数据。（没有不平衡数据的处理方案）
   3. 使用场景：大规模数据线性分类。
   4. 记忆：LR简单，解释性强，适用于大规模数据的线性分类。但由于简单所以准确率不足，对特征工程要求较高。

3. 逻辑回归训练技巧
   1. 特征离散化
4. **逻辑斯特回归为什么要对特征进行离散化**  （ 对特征离散化，让特征里没有连续值）
   1. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，**每个变量有单独的权重，相当于为模型引入了非线性**，能够提升模型表达能力，加大拟合
   2. 稀疏向量内积乘法运算速度快计算结果方便存储，容易扩展
   3. **离散化后的特征对异常数据有很强的鲁棒性**：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰。
5. **线性回归**与逻辑回归的区别
   1. 线性回归主要解决回归任务，逻辑回归主要解决分类问题。
   2. 线性回归的输出一半是连续的，逻辑回归的输出一般是离散的。
   3. 线性回归的损失函数是MSE,逻辑回归中，采用的是负对数损失函数
6. **为什么逻辑回归比线性回归要好**？
   1. 在特征到结果的映射中加入了一层sigmoid函数（非线性）映射，即先把特征线性求和，
   2. 另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在0,1间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。
   3. 逻辑回归的损失函数让它只有靠近决策取阈的样本影响大，所以鲁棒性更好。
   4. 总结：1. 加入非线性，2靠近决策区域影响更大，效果更好。3. 它在0-1进行预测，比线性回归解空间小。
7.  **LR为什么使用sigmoid函数？,为什么不使用MSE**
    1.  而Sigmoid能够把它映射到[0,1]之间。正好这个是概率的范围。
    2.  Sigmoid是连续光滑的。
    3.  Sigmoid也让逻辑回归的损失函数成为凸函数，这也是很好的性质。可以寻找到一个全局最优点进行下降。
    4.  使用MSE，则不是一个凸函数。根据损失函数可以得到。
8. **逻辑回归为什么不使用最小二乘法，而使用最大似然估计**。注意区分，这是损失函数，上一个问题是激活函数的问题。
   1. 在最小二乘法下，逻辑回归损失函数非凸。
   2. 非凸判定：二阶导存在且为正。则为凸函数。意思是在凸集中局部最优等于全局最优。
9.  **LR和SVM有什么不同吗**
    1.  相同
        1.  两个方法都可以增加不同的正则化项
        2.  LR和SVM都可以**处理分类问题，且一般都用于处理线性二分类问题**
    2.  不同：
        1.  LR是参数模型，SVM是非参数模型。**参数模型即需要知道数据的分布参数。**
        2.  **从目标函数来看**，区别在于逻辑回归采用的是交叉熵损失函数，SVM采用的是hinge loss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。
        3.  SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。
        4.  LR在特别是大规模线性分类时比较方便。**SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,**这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。
        5.  LR依赖数据分布。LR和所有的数据相关。SVM不依赖。
        6.  LR能做的 SVM能做，但可能在准确率上有问题，SVM能做的LR有的做不了。
    3.  
    4.  总结。模型类型，从损失函数，支持向量上面进行分析。
10. sigmoid作为激活函数的缺点（难算，非中心化，梯度消失）
