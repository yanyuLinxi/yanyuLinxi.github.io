---
title: "GBDT理论推导和面试题"
date: 2022-02-17T10:59:58+08:00
tags : [

]
categories : [

]
series : []
aliases : []
draft: false
---

# 加法模型

加法模型事梯度提升树、Adaboost的基础模型

公式: $f(x) = \sum_{m=1}^M \beta_m b(x;\gamma_m) $
其中$\beta_m$是分类器的权重，$\gamma_m$是分类器的参数。

损失函数：L(y, f(x))
回归问题，使用MSE平方误差
分类问题：使用指数函数、交叉熵损失函数

训练方式：使用前向分布算法。
不使用梯度下降，是因为M个基学习器的参数太多，复杂度太高，不适合梯度下降。所以分成M步，在训练当前基学习器时，前面的基学习器已经训练完毕，就是前向分布算法。

前向分布算法步骤：
1. 初始化第一个学习器。根据loss function的选择。
2. for m=1 to M:
   1. 极小化损失函数 $(\beta_m, \gamma_m) = argmin_{\beta_m, \gamma_m}\sum_{i=1}^NL(y_i,f_{m-1}(x_i)+\beta b(x_i;\gamma)$，得到参数$(\beta_m, \gamma_m)$
   2. 更新 $f_m(x)=f_{m-1}(x) + \beta_m b(x;\gamma_m)$
3. 得到加法模型 $f(x)=f_M(x)=\sum_{m=1}^M\beta_m b(x;\gamma_m)$



# Gradient Boosting Decision Tree

梯度提升树。依然采用加法模型和前向分布算法。

特点：
1. 其可以支持设置不同的可微损失函数可以处理各类学习任务（多分类、回归、Ranking等），应用范围大大扩展。
2. 梯度提升算法利用损失函数的负梯度作为残差拟合的方式

整体步骤和加法模型类似。
不一样的点：
1. 可以使用自定义的损失函数
2. 去拟合负梯度。
3. 基学习器选择回归树

步骤：
1. 计算当前损失函数负梯度的表达式
2. 构造新的训练样本$T_m={(x, r_m1), (x_2, r_m2)...}$ 根据负梯度构造新的训练样本。（即如何拟合负梯度的）
3. 让当前的基学习器去你和上述训练样本，得到$T(x;\theta_m)$


具体步骤：
1. 初始化第一个学习器。根据loss function的选择。$f_0(x)=argmin_c\sum_{i=1}^NL(y_i, c)$, 对损失函数求导。
2. for m=1 to M:
   1. 计算负梯度：$y_i = - \frac{\partial L(y_i,f_{m-1}(x_i))}{\partial f_{m-1}(x_i)}$
   2. 让基学习器去拟合负梯度
   3. 使用line search 确定一个p_m。用来最小化和标签的损失函数L
      1. 确定p_m，$p_m  = argmin_p \sum_i^N L(y_i, f_{m-1}(x_i)+ph_m(x_i;w_m))$ 来使得L最小
      2. p_m就是最终加法模型当中每个基学习器的权重。
   4. $f_m(x) = f_{m-1}(x) + p_mh_m(x; w_m)$

当采用回归树做基分类器时，回归树公式可以表达为$T(x;\theta)=\sum_j^J b_j I(x\in R_j)$。上述公式可以进一步优化:
$f_m(x) =  f_{m-1}(x) + \sum_j^J \gamma_jI(x\in R_jm), 令\gamma_j=p_m b_j$
此时，第三步训练得到$\gamma$，第四步变成纯加法。

公式推导：
1. 需要公式推导的部分，就是为何拟合负梯度是可行的
2. 带入平方误差、指数损失函数。可以算出初始值、负梯度。
   1. 注意平方误差时，负梯度就是残差。

# 问题

1. 优缺点

优点：1. 可以使用任意的损失函数，只要损失函数连续可导。这使得抗噪音能力较强。


2. 剩一个问题。基学习器如何去拟合负梯度的？

负梯度作为下一个基学习器的输入。

3. 为何拟合负梯度？
根据加法模型的损失函数得到。
$Loss=L(y, f_m(x))=L(y, f_{m-1}(x)+h_m(x;w_m))$

进行一阶泰勒展开: 

$Loss=L(y, f_{m-1}(x))+\frac{\partial L(y_i,f_{m-1}(x_i))}{\partial f_{m-1}(x_i)}h_m(x;w_m)$

所以让$h_m$去拟合负梯度。

# 笔记

1. GBDT二分类问题是Adaboost的特殊情况。
2. GBDT如何处理分类任务、回归任务、如何处理负梯度？
   1. 负梯度作为x的标签去拟合。
   2. 底层采用cart，所以分类使用基尼系数，回归使用mse
