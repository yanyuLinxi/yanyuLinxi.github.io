---
title: "Boost复习"
date: 2021-12-26T10:30:40+08:00
tags : [

]
categories : [

]
series : []
aliases : []
draft: false
---

# Boost
boost是一种加法模型，通过串联的叠加多个弱学习器，每个学习器来学习上一个学习器的偏差，来得到一个强学习器。

1. boost和bagging的区别

+ bagging的弱学习器使用部分特征。boost使用全部特征
+ bagging可以并行训练。boost只能串行训练
+ bagging解决方差，boost解决偏差。

## 提问：
1. boost描述不精准（方法，目的）
boost通过层层叠加多个基分类器，聚焦错误的样本，来减少偏差。
bagging使用多个基分类器，多次采样集体投票，减小方差。

2. boost、bagging区别
从头想到尾，
输入上，boost使用全部的数据集。bagging使用部分数据集
运行上，boost串联运行，bagging支持并行
运行方式上，bagging均匀采样样本比例相同，boost对错误的样本赋予更大的权重。
结果（基学习器组合）bagging每个基学习器权重相等。boost分类误差小的基学习器拥有更大的权重。
目的上：bagging关注于降低方差。boost关注于降低偏差

# adaboost

将若干个弱学习器通过加法模型连接起来，每一个弱学习器中，加大判断错误的样本的权重，减少判断正确的样本的权重。采用指数函数：$f(x) = e^{-yh(x)}$作为损失函数。

算法流程：
1). 初始化第一个基学习器
2). 对于1-T个基学习器
3).     计算误差率$\epsilon$
4).     计算基学习器在加法模型中的权重
5).     计算样本的分布
6). end For
7). $f(x) = sign(\sum \alpha h(x))$

1. 为什么采用指数函数

指数函数是-1到1的替代性函数。
指数函数拥有连续可微等优秀的特性。

2. 如何分配基学习器的权重？

$\alpha = \frac{1}{2}ln\frac{1-\epsilon}{\epsilon}$
通过公式$f(x) = e^{-y \alpha h(x)}$求得。   

3. 如何分配样本的权重？

$\D_t = \frac{\D_{t-1}exp(-y \alpha_t h_t(x))}{Z_t}$ 其中$Z_t$是归一化系数，值为$\sum exp(-y \alpha_t h_t(x))$

## 提问：
1. 优缺点
优缺点就是boost的优缺点。
优点：1. 能够叠加弱学习器迅速构造出强学习器。
缺点：1. 异常样本的权重会不停的增加，影响最终的表现。对异常样本敏感。容易过拟合。
      2. 无法并行运行，运行比较慢。

2. 描述


# GBDT

GBDT就是梯度提升树，和adaboost一样，通过加法模型串联多个弱学习器达到强学习器。不同的是，GBDT通过拟合负梯度来纠正偏差。

算法步骤：
1) 初始化第一个基学习器f_0。初始化方式根据损失函数来决定。
2) for 1 to T:
3)     计算f_{t-1}的负梯度 f_{t-1}'
4)     $\alpha = min_\alpha L(f_{t-1}', f_t(\alpha))$
5)     $\rho = min_\rho L(y, f_{t-1} + \rho f_t)$
6)     更新$f(x) = f_{t-1} + \rho f_t(\alpha)$
7) end For

1. 为什么要拟合负梯度
   
根据提升树的公式 $L(y, f(x)) = L(y, f_{t-1}(x) + f_t(x))$.对f_{t-1}(x)进行泰勒展开。即 $L(y, f(x)) = L(y, f_{t-1}(x)) + (梯度）f_t(x)$所以可以看到这里完全可以让f_t(x)去拟合负梯度。就可以降低损失，来达到训练效果。负梯度中错误的样本梯度大，正确的样本梯度小，所以也是在去加大错误样本的权重。

负梯度的引入，可以支持更多的损失函数。

残差是梯度的特殊情况，拟合负梯度效果更好。

1. $\alpha是什么 \rho是什么$

$\alpha$是基学习器拟合负梯度的参数。$\rho$是基学习器的步长。负梯度仅仅指明了方向，但是没有指明移动多长距离。

2. 如何优化这个算法？

对于基学习器$f_t$,其可以表示为叶子节点的值$f_t = \sum_{x\in R_j} b_j$, $b_j$为叶子节点学习的值，在CART中一般为叶子节点的均值。

所以$\rho = min_\rho L(y, f_{t-1}+ \sum_j \rho b_j$

另$\gamma_j = \rho b_j$ 就可以一次最小运算得到$\gamma$

最终公式变为 $f(x) = f_{t-1} + \sum_j \gamma_j$

## 提问
1. 描述

2. 优缺点 

3. 举例子

如果需要举例子，可以使用对数损失函数L = log(1+e^{-2yF})来举例。


# XGBoost

xgboost属于一种梯度提升树，相比于GBDT，它在损失函数中引入了二阶导和正则化。并且使用了新的增益计算函数。在运行过程中，通过预排序和多线程特征并行查找来加快速度，引入了近似分裂算法来减少决策树特征选择的复杂度，引入了列抽样、shrinkage等避免过拟合的方法。

算法流程

1. 如何使用二阶导信息的？

对于提升树的损失函数$L(y, f) = L(y, f_{t-1} + f_t) + \Omega(f_t)$其中$\Omega(f_t) = \gamma j + \frac{1}{2} \lambda \sum b_j ^2$，就是通过正则化方式，尽量减少叶子结点的数量和叶子节点的权重的L2正则化。其中$b_j$就是叶子节点的预测值。对损失函数进行f_{t-1}的二阶泰勒展开。

$$
L(y, f) = \sum_i^{N}(L(y, f_{t-1}) + g_i f_t + \frac{1}{2} h f_t^2) + \gamma j + \frac{1}{2} \lambda \sum b_j ^2
\\
L(y, f) = \sum_j L(y, f_{t-1}) + \sum_{x\in R_j}g_i b_j + \frac{1}{2} \sum_{x\in R_j}h_i b_j^2 + \gamma j + \frac{1}{2} \lambda \sum b_j ^2
\\
L(y, f) = \sum_j L(y, f_{t-1}) + G_j b_j + \frac{1}{2}(H_j+\lambda) b_j^2 + \gamma j
$$
其中g为一阶导，h为二阶导。将公式对$b_j$求导得到$b_j = -\sum_j \frac{G_j}{H_j+\lambda}$。这个值为叶子节点的权重。

将$b_j$带回式子得到**损失函数**$L = \sum_j -\frac{1}{2}\frac{G_j^2}{H_j+\lambda} +\gamma j$
所以**信息增益**计算方式为
$$ Gain = L - (L_{left} +L_{right})
\\
 = \frac{G_{left}^2}{H_{left}+\lambda} + \frac{G_{right}^2}{H_{right}+\lambda} - \frac{(G_{left}+G_{right})^2}{H_{left}+H_{right}+\lambda} - \gamma
$$


2. 为何引入二阶导信息？

1) 二阶导可以让结果更加精准。一阶导只提供了梯度方向，没有提供下降多少，二阶导提供了一阶导的变化趋势。可以使下降结果更加精确
2) 二阶导的引入，更方便的支持扩展性。只要二阶可导的函数都可以作为损失函数。

3. 新的增益如何计算的？

1) 使用Gain = L - L_left - L_right. $L = \frac{G^2}{H+\lambda}$

4. 如何剪枝

1) 后剪枝。生长到指定的max_depth，然后从底到上进行剪枝，当某个节点以后的增益都小于gamma，则剪掉。一定程度上避免了欠拟合
2) 达到max_depth停止。
3) 当叶子权重小于最小的叶子权重时，避免分裂。

5. 如何避免过拟合

1) 增加正则化项。控制复杂度。
2) 修改min_child_weight，分裂gamma等
3) 进行列抽样。
4) shrinkage。缩小当前叶子节点的权重，为后面的树留出训练空间。
5) 子采样。每轮计算不适用全部样本。

调参: 
1. 调整控制模型复杂度的参数：max_depth, min_child_weight, gamma
2. 调整随机性的参数，subsample。colsample（列采样）
3. 调整模型学习率，基学习器的数量等。

6. 如何处理缺失值

1） 训练时，使用没有缺失值的部分进行训练。缺失值则依次放入左右叶子节点，计算最大增益。作为默认的缺省方向。
2） 预测时出现缺失值，默认送入右节点。
树模型对缺失值敏感度低。样本缺失不影响最优特征的选择，所以树模型对缺失值敏感度低

7. 优点
（思考时和gbdt做对比）
1. 二阶导，精准度更高。且可扩展性更好
2. 正则化可以更好的避免过拟合。降低泛化误差。shrinkage缩减，削弱每棵树的影响，让后面有更大的学习空间。
3. 支持列抽样
4. 多线程特征查找速度更快。将特征预排序存为block。然后对每个特征的最有特征的查找使用多线程。
5. 近似分裂算法，降低特征查找时的时间复杂度。提供了可并行的近似算法，高效的生成候选分割点。
6. 缺失值的处理。

7. 缺点

1. 预排序消耗的时间空间大。
2. 在寻找分裂点的时候，仍然需要遍历所有的数据。

8.  对比

9.  多线程特征并行查找如何实现的？
对特征进行预排序，然后存为blcok结构。每次分裂的时候。启用多个线程对多个特征的最优分裂点进行查找。


## 提问

# LightGBM

LightGBM是GBDT的一个算法框架。为了解决GBDT速度慢、内存占用大的缺点。

有以下的改进：
1）基于Histogram的决策树算法
基于直方图的算法。特征划分多个区间，每个区间成为一个箱子。区间中的值更新为箱子的值。
优点：
1. 内存占用小
2. 计算代价小。相比xgb不需要遍历一个特征值就需要计算一次分裂的增益，只需要计算k次(k为箱子的个数)
3. 子直方图可以用父直方图做差得到。速度上加快。

2）单边梯度采样 Gradient-based One-Side Sampling(GOSS)

减少样本的角度出发。排除大部分小梯度的**样本**。为了保证分布，按照梯度大小排序，选取最大的a个数，和小特征中的b个数。b个数乘以权重(1-a)/b。然后用a+b来计算信息增益。就减少了1-a-b个数。

3）互斥特征捆绑 Exclusive Feature Bundling(EFB)

如果将部分特征进行捆绑，可以降低特征数量。将互斥的特征进行捆绑（互斥即不同时为0，这样两个值可以叠加。如果同时为0）。且允许一小部分的冲突。可以得到更少的绑定特征。特征如何捆绑，A和B的取值空间叠加。

4）带深度限制的Leaf-wise的叶子生长策略

level-wise每次分裂一层的叶子。不容易过拟合，但是比较低效。
leaf-wise，找到分裂增益最大的叶子节点进行分裂，降低的误差更多，但是更容易过拟合。所以限制高度。

5）直接支持类别特征(Categorical Feature)

实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，通过 one-hot 编码，转化到多维的0/1特征，降低了空间和时间的效率。

但我们知道对于决策树来说并不推荐使用 one-hot 编码，尤其当类别特征中类别个数很多的情况下，会存在以下问题
1） 会产生样本切分不平衡问题，导致切分增益非常小（即浪费了这个特征）。即这个特征没有用。
2） 会影响决策树的学习。因为就算可以对这个类别特征进行切分，独热编码也会把数据切分到很多零散的小空间上，

LightGBM是第一个直接支持类别特征的GBDT工具。

6）支持高效并行

特征并行、数据并行、投票并行

7）Cache命中率优化

大概知道。并不很了解。


优点：
速度更快:
1. 速度更快。比如使用直方图减少内存消耗。降低时间复杂度。
2. 使用单边梯度算法，减少了大量的计算。GOSS在进行数据采样的时候只保留了梯度较大的数据
3. 基于leaf-wise算法的增长策略。减少了很多不必要的计算量
4. 采用优化后的特征并行，数据并行方法加速计算。还可以使用投票并行
5. 对缓存进行了优化。
内存更小：
1. 使用直方图将特征转为bin，减少了内存消耗

缺点：
1. 可能决策树较深，产生过拟合。在leaf_wise熵增加了一个最大深度限制，防止过拟合
2. 不断降低偏差，所以对噪点铭感。
3. 在寻找最优解时，依据的是最优切分变量，没有将最优解是全部特征的综合这一理念考虑进去；

# 决策树

ID3
C4.5
CART

决策树根是树形结构的判决树，来完成分类。拥有以下特点：
1. 根节点拥有所有的样本
2. 叶节点包含决策结果
3. 每个节点包含样本集，根据属性测试的结果划分到左右子树中。

1. 划分测试：
ID3 信息增益。
信息熵：所含的信息量。信息熵越大，信息量越多，混乱程度越高。公式$=-\sum p log p$。p就是第k类样本（标签）所占比例。
信息增益就是**父节点的信息熵**减去（**左右子节点的信息熵**乘以**节点权重**）。由于是多分类问题，每个节点拥有的样本数量不同，所以乘以这个权重。

信息增益的缺陷就是容易偏爱特征种数多的特征。当一个特征种数等于样本个数，则每种特征都是纯的，那这个信息增益会非常大。

信息增益率 = 信息增益 / 固有值。固有值为每一类样本个数所占比例的熵。公式$=-\sum p log p, p=\frac{D^v}{D}$
信息增益率让特征选择更偏向种类少的特征。

基尼系数
$Gini(D) = 1-\sum p^2$ p为第k类样本所占比例。
基尼指数 = $\sum \frac{D^v}{D}Gini(D^v)$

2. 连续值的处理。对于属性a，在a上出现了n个不同的取值，排序，然后取每两个相邻元素的中位点进行划分。进行二分。二分后的特征后面可以继续划分。

3. 缺失值

4. 剪枝

# SVM

公式：$L = \frac{1}{2}||w||^2 + \sum\alpha_i(1-y_i(W^TX+b))$

怎么求解？先转为对偶问题，先求w，b再求$\alpha$。两个参数W，b求偏导，得到的值带入能求出$\alpha$, $W = \sum_i \alpha y_i x_i$

软间隔 $L = min (1/2 ||w||^2 + C\sum_i^n\mu)$  C为松弛变量。

$\mu > max(0, 1-y_i(WX+b))$ 这个就是合页损失。

# 逻辑回归

# k-means

# 随机森林

# 其他
- [ ] 各个部分面经