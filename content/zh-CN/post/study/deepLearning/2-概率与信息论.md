---
title: "2 概率与信息论"
date: 2021-11-08T09:39:21+08:00
tags : [

]
categories : [

]
series : []
aliases : []
draft: false
---

- [概率论意义](#概率论意义)
- [随机变量](#随机变量)
- [概率分布](#概率分布)
- [边缘概率](#边缘概率)
- [条件概率](#条件概率)
- [条件概率的链式法则](#条件概率的链式法则)
- [独立性和条件独立性](#独立性和条件独立性)
- [期望、方差、协方差](#期望方差协方差)
- [常用概率分布](#常用概率分布)
- [常用函数的有用性质](#常用函数的有用性质)
- [贝叶斯规则](#贝叶斯规则)
- [连续型变量的技术细节](#连续型变量的技术细节)
- [信息论](#信息论)
- [结构化概率模型](#结构化概率模型)


概率论是用于表示不确定性声明的数学框架。它不仅提供了量化不确定性的方 法，也提供了用于导出新的不确定性声明（statement）的公理

概率法则告诉我们 AI 系统如何推理。其次，我们可以用概率和统计从 理论上分析我们提出的 AI 系统的行为

# 概率论意义

1. **几乎所有的活动**都需要一些在**不确定性存在**的情况下进行推理的能力
2. **不确定性有三种可能的来源**
   1. 被建模系统内在的随机性
   2. 不完全观测。
   3. 不完全建模。当我们使用一些必须舍弃某些观测信息的模型时，舍弃的信息会 导致模型的预测出现不确定性。
3. **使用一些简单而不确定的规则要**比复杂而确定的规则更为实用
   1. ‘多数鸟儿都会飞’’ 这个简单的规则描述起来很简单很并且使用广泛
   2. ‘除了那些还没学会飞翔的幼鸟，因为生病或是受伤而失去了飞翔能力的 鸟，包括食火鸟 (cassowary)、鸵鸟 (ostrich)、几维 (kiwi，一种新西兰产的无翼鸟)等不会飞的鸟类……以外，鸟儿会飞’’，很难应用、维护和沟通，即使经过这么多的 努力，这个规则还是很脆弱而且容易失效。
4. 频率派概率和 贝叶斯概率
   1. 当我 们说一个结果发生的概率为 p，这意味着如果我们反复实验 (例如，抽取一手牌) 无限次，有 p 的比例可能会导致这样的结果
   2. 在医生诊断病人的例 子中，我们用概率来表示一种信任度（degree of belief），
   3. 前面那种概率，直接与事件发生的频 率相联系，被称为**频率派概率**（frequentist probability）；
   4. 而后者，涉及到确定性水 平，被称为**贝叶斯概率**（Bayesian probability）
   5. 表征信任度的概率，我们称为贝叶斯概率。
5. 概率可以被看作是用于处理不确定性的逻辑扩展。逻辑提供了一套形式化的规 则，可以在给定某些命题是真或假的假设下，判断另外一些命题是真的还是假的。**概率论提供了一套形式化的规则**，可以在给定一些命题的似然后，计算其他命题为真的似然。
   1. 似然：likelihood 即文言文版的可能性。


# 随机变量

1. **随机变量**（random variable）是可以随机地取不同值的变量
   1. 例如，x1 和 x2 都是随机变量 x 可能的取值
   2. 向量值变量，我们会将随机变量写成 x

# 概率分布

1. **概率分布**（probability distribution）用来描述随机变量或一簇随机变量在每一 个可能取到的状态的可能性大小。我们描述概率分布的方式取决于随机变量是离散 的还是连续的
2. **离散型变量的概率分布**可以用概率质量函数（probability mass function, PMF）我们通常用大写字母 P 来表示概率质量函数
   1. 有时为了使得PMF的使用不相互混淆，我们会明确写出随 机变量的名称：P(x = x)。
   2. 有时我们会先定义一个随机变量，然后用 ∼ 符号来说明它遵循的分布：x ∼ P(x)
3. 这种多个变量的概率分布被称 为**联合概率分布**（joint probability distribution）。P(x = x, y = y) 表示 x = x 和 y = y 同时发生的概率
4. 如果一个函数 P 是随机变量 x 的 PMF，必须满足下面这几个条件：
   1. P 的定义域必须是 x 所有可能状态的集合。
   2. ∀x ∈ x, 0 ≤ P(x) ≤ 1. 不可能发生的事件概率为 0，并且不存在比这概率更低 的状态。
   3. ∑ x∈x P(x) = 1. 我们把这条性质称之为归一化的（normalized）。
5. 考虑一个离散型随机变量 x 有 k 个不同的状态。我们可以假设 x 是**均匀 分布（uniform distribution）**的。通常用 x ∼ U(a, b) 表示 x 在 [a, b] 上是均匀分布的。
6. **连续型变量**和**概率密度函数**
   1. 当我们研究的对象是连续型随机变量时，我们用**概率密度函数**（probability density function, PDF）
   2. 如果一个函数 p 是概率密度函数，必须满足下面这几个条件
      1. p 的定义域必须是 x 所有可能状态的集合
      2. ∀x ∈ x, p(x) ≥ 0. 注意，我们并不要求 p(x) ≤ 1。
      3. ∫p(x)dx = 1.
   3. 概率密度函数 p(x) 并没有直接对特定的状态给出概率，相对的，它给出了落在 面积为 δx 的无限小的区域内的概率为 p(x)δx。

# 边缘概率

1. 但想要了解其中一个子集的概 率分布。这种定义在子集上的概率分布被称为**边缘概率分布**（marginal probability distribution）。

# 条件概率

1. 在很多情况下，我们感兴趣的是某个事件，在给定其他事件发生时出现的 概率。这种概率叫做条件概率。
2. 条件概率只在 P(x = x) > 0 时有定义
3. 计算一个行动的后果被称为干预 查询（intervention query）。**干预查询**属于**因果模型**（causal modeling）的范畴，我
们不会在本书中讨论

# 条件概率的链式法则

1. 任何多维随机变量的联合概率分布，都可以分解成只有一个变量的条件概率相 乘的形式： P(x(1), . . . , x(n)) = P(x(1))Πn i=2P(x(i) | x(1), . . . , x(i−1)).
2. 这个规则被称为**概率的链式法则**（chain rule）或者乘法法则（product rule）。

# 独立性和条件独立性

1. 两个随机变量 x 和 y，如果它们的概率分布可以表示成两个因子的乘积形式，并 且一个因子只包含 x 另一个因子只包含 y，我们就称这两个随机变量是相互独立的
2. 那么这两个随机变量 x 和 y 在给定随机变量 z 时是**条件独立**的（conditionally
3. 我们可以采用一种简化形式来表示**独立性和条件独立**性：x⊥y 表示 x 和 y 相互 独立，x⊥y | z 表示 x 和 y 在给定 z 时条件独立。

# 期望、方差、协方差

1. 函数 f(x) 关于某分布 P(x) 的**期望**（expectation）或者期望值（expected value）是指，当 x 由 P 产生，f 作用于 x 时，f(x) 的平均值
2. 我们假设 E[·] 表示对方括号内的所有随机变量的值求平均。 类似的，当没有歧义时，我们还可以省略方括号。
3. **方差（variance）**衡量的是当我们对 x 依据它的概率分布进行采样时，随机变 量 x 的函数值会呈现多大的差异：Var(f(x))
   1. 当方差很小时，f(x) 的值形成的簇比较接近它们的期望值。方差的平方根被称为**标准差**（standard deviation）。
4. **协方差（covariance）**在某种意义上给出了两个变量线性相关性的强度以及这些 变量的尺度
   1. **协方差的绝对值如果很大**则意味着变量值变化很大并且它们同时距离各自的均值很 远。
   2. 如果**协方差是正的**，那么两个变量都倾向于同时取得相对较大的值
   3. 如果**协方 差是负的**，那么其中一个变量倾向于取得相对较大的值的同时，另一个变量倾向于取得相对较小的值，
   4. 它们是有联系的，因为 两个变量如果相互独立那么它们的协方差为零，如果两个变量的协方差不为零那么它们一定是相关的
   5. **两个变量相互依赖但具有零协方差是可能的**。例如，假 设我们首先从区间 [−1, 1] 上的均匀分布中采样出一个实数 x。然后我们对一个随机 变量 s 进行采样。s 以 12 的概率值为 1，否则为-1。

# 常用概率分布

1. **Bernoulli 分布**（Bernoulli distribution）是单个二值随机变量的分布。它由单 个参数 ϕ ∈ [0, 1] 控制，ϕ 给出了随机变量等于 1 的概率。
2. **Multinoulli 分布**（multinoulli distribution）或者范畴分布（categorical distribution）
3. 实数上最常用的分布就是**正态分布**（normal distribution），也称为**高斯分布**（Gaussian distribution）
   1. 正态分布由两个参数控制，µ ∈ R 和 σ ∈ (0,∞)
   2. 当我们由于**缺乏关于某个实 数上分布的先验知识**而不知道该选择怎样的形式时，**正态分布是默认的比较好的选择，**其中有两个原因。
      1. 第一，我们想要建模的**很多分布的真实情况是比较接近正态分布的**。中心极限 定理（central limit theorem）说明很多独立随机变量的和近似服从正态分布。
      2. 第二，在具有相同方差的所有可能的概率分布中，**正态分布在实数上具有最大 的不确定性**。因此，我们可以认为正态分布是对模型加入的先验知识量最少的分布。
4. 我们常常把协方差矩阵固定成一个对角阵。一个更简单的版本是各向同性（isotropic）高斯分布，它的协方差矩阵是一个标量乘以单位阵。
5. **指数分布和 Laplace 分布**
   1. 我们经常会需要一个在 x = 0 点处取得边界点 (sharp point) 的 分布。为了实现这一目的，我们可以使用指数分布
   2. 一个联系紧密的概率分布是Laplace 分布（Laplace distribution），它允许我们 在任意一点 µ 处设置概率质量的峰值
6. 在一些情况下，我们希望概率分布中的所有质量都集中在一个点上。这可以通 过**Dirac delta 函数**（Dirac delta function）δ(x) 定义概率密度函数来实现。
7. 通过组合一些简单的概率分布来定义新的概率分布也是很常见的。一种通用的组 合方法是构造混合分布（mixture distribution）。
8. 一个非常强大且常见的混合模型是**高斯混合模型（Gaussian Mixture Model）**， 它的组件 p(x | c = i) 是高斯分布
   1. 高斯混合模型的参数指明了给每个组件 i 的**先验概率** （prior probability）αi = P(c = i)。
   2. ‘‘先验’’ 一词表明了在观测到 x 之前传递给模 型关于 c 的信念
   3. **P(c | x) 是后验概率**（posterior probability），因为它 是在观测到 x 之后进行计算的
9. **高斯混合模型**是**概率密度的万能近似器**（universal approximator），在这种意义下，任何平滑的概率密度都可以用具有足够多组件的**高斯混合模型以任意精度来逼近。**


# 常用函数的有用性质

1. **sigmoid 函数** 在变量取绝对值非常大的正值或负值时会出现饱和（saturate）现象，意味着函数会变得很平，并且对输入的微小改变会变得不敏感
2. 另外一个经常遇到的函数**softplus **函数（softplus function）
   1. softplus 函数名来源于它是另外一个函数的平滑（或 ‘‘软化’’）形式，这个函数是
      1. x+ = max(0, x).

# 贝叶斯规则

1. 我们经常会需要在已知 P(y | x) 时计算 P(x | y)。幸运的是，如果还知道 P(x)， 我们可以用贝叶斯规则（Bayes’ rule）来实现这一目的：
   1. 它通常使用 P(y) = 所以我们并不需要事先知道 P(y) 的信息。


# 连续型变量的技术细节

1. 对于我们的目的，测度论更多的是用来描述那些适用于 Rn 上的大多数点，却不 适用于一些边界情况的定理。测度论提供了一种严格的方式来描述那些非常微小的点集。这种集合被称为 **“零测度（measure zero）’**’ 的
2. 另外一个有用的测度论中的术语是 **“几乎处处（almost everywhere）’’**。某个性 质如果是几乎处处都成立的，那么它在整个空间中除了一个测度为零的集合以外都是成立的。


# 信息论

1. 信息论的基本想法是**一个不太可能的事件居然发生了，要比一个非常可能的事 件发生，能提供更多的信息**。
   1. 非常可能发生的事件信息量要比较少，并且极端情况下，**确保能够发生的事件 应该没有信息量**。
   2. **较不可能发生的事件具有更高的信息量。**
   3. **独立事件应具有增量的信息**。例如，投掷的硬币两次正面朝上传递的信息量， 应该是投掷一次硬币正面朝上的信息量的两倍。
2. 我们定义一个事件 x = x 的自信息（self-information）
   1. I(x) = −logP(x).
3. 自信息只处理单个的输出。我们可以用香农熵（Shannon entropy）来对整个概 率分布中的不确定性总量进行量化
   1. 当 x 是连续的，香农熵被称为微分熵（differential entropy）。
   2. 说明了**更接近确定性的分布是如何具有较低的香农熵**，而**更 接近均匀分布的分布是如何具有较高的香农熵。**
4. 如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可 以使用KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异：
   1. KL 散度衡量的是，当我们使用一种被设计成能够使 得概率分布 Q 产生的消息的长度最小的编码
   2. KL 散度为 0 当且仅当 P 和 Q 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是 ‘‘几乎处处’’ 相同的
5. 个和 KL 散度密切联系的量是交叉熵（cross-entropy）

# 结构化概率模型

1. 由 一些可以通过边互相连接的顶点的集合构成。当我们用图来表示这种概率分布的分 解，我们把它称为**结构化概率模型**（structured probabilistic model）或者**图模型（graphical model）**。
2. **有向（directed）模型使用带有有向边的图**，它们用条件概率分布来表示分解，
3. **无向（undirected）模型使用带有无向边的图，**它们将分解表示成一组函数
4. 随机变量的联合概率与所有这些因子的乘积成比例（proportional）——意味着因子的值越大则可能性越大
5. 这些图模型表示的分解仅仅是描述概率分布的一种语言。它们不是互 相排斥的概率分布族。有向或者无向不是概率分布的特性；它是概率分布的一种特
殊描述（description）所具有的特性


