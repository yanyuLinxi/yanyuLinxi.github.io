---
title: "LSTM网络"
date: 2022-03-05T21:38:47+08:00
tags : [

]
categories : [

]
series : []
aliases : []
draft: false
---

# 循环神经网络

1. 循环神经网络每一层训练采用相同的w和b。它的求解采用BPTT（基于时间的反向传播算法实现）。当把循环神经网络展开成T层的神经网络来理解，就和普通的反向传播没有任何区别了。
2. BPTT不能成功捕捉长距离关系，这一现象有主要原因是梯度消失：
   1. 梯度消失。梯度消失的原因和深度网络中类似，它意味着只有靠近输出的若干层才能起到真正的学习的作用。
3. 所以引入了长短时记忆网络、GRU来弥补梯度消失。

循环神经网络求导：
$net=Ux_t+Wh_{t-1}\\$
$h_t=f(net)\\$
$y=g(Vh_t)$, g为激活函数

## LSTM长短时记忆网络
https://zhuanlan.zhihu.com/p/32085405

$Z^{i,o,f}=tanh(w^{i,o,f}(x_t||h_{t-1})$通过sgmoid映射到[0,1]为一个概率值。

# 面试题

1. LSTM优缺点
   1. 优点：
      1. 缓解了梯度消失的问题。通过引入记忆单元c，避免了无休止的连乘。而是边加边乘。
   2. 缺点：
      1. 计算耗时，三个sigmoid
      2. 缓解长距离依赖、梯度消失问题，但是还不够。
2. 循环神经网络能使用relu激活函数嘛？
   1. 能，但是要做一定的限制，比如限制权重矩阵为单位阵附近。
   2. $\frac{\partial f}{\partial f_{i-1}}=\frac{\partial f}{\partial g_{i-1}}\frac{\partial g_{i-1}}{\partial f_{i-1}}, \frac{\partial f}{\partial g_{i-1}}=w_{i-1}$

3. 为什么LSTM使用tanh？
   1. 输出在[-1,1]之间，和大多时候，特征分布是以0为中心是吻合的
   2. Tanh在0附近拥有比sigmoid更大的梯度，更容易收敛
   3. 最开始的时候就是采用2sigmoid-1这种，后来实验发现tanh效果更好。
   4. sigmoid作为门控，基本是共同选择。在可穿戴设备中，对计算量有要求，此时sigmoid往往会替换为hard gate[0,1]门。只输出0，1
4. LSTM怎么解决梯度消失的？
   1. LSTM增加了更多回传梯度的路径，只要一条路径没有梯度消失，那么梯度消失的问题就得到了改善。
   2. LSTM为什么有梯度消失？
      1. 连乘效应
      2. 总梯度被近距离梯度所影响，远距离梯度信息被忽略。
   3. 这个要接着改。
5. GRU相比于LSTM的优点：
   1. LSTM使用了三个门：z,i,o:记忆门，输入门，输出门。C_t为内部记忆单元
   2. GRU使用了两个门r, z，重置门和更新门。重置门用于控制记忆的保留量，使用了同一个门控z就同时可以进行遗忘和选择记忆（LSTM则要使用多个门控）。
6. lstm三个门：记忆门，遗忘门（信息门），输出门。
   1. 输入门、遗忘门、输出门。
   2. 遗忘门。接受长期记忆C_{t-1}, 。决定保留和遗忘$C_{t-1}$的哪些部分
   3. 记忆门（信息门、输入门）确定当前的输入和隐藏状态中哪些信息应被保留下来加入长期记忆中。
   4. 输出门。根据当前状态，确定输出的值，并将信息保存到隐藏状态中去。
7. LSTM公式
   1. tanh(x)求导=1-tanh(x)^2
   2. $tanh= \frac{e*z-e{-z}}{e*z+e{-z}}$
   3. 图像是sigmoid扩充到[-1,1]的图像。
   4. 和sigmoid一样，z很大或者很小的时候梯度几乎为零。
8. RNN反向传播公式
   1. https://zhuanlan.zhihu.com/p/33594517
   2. h_t对h_{t-1}矩阵求导的矩阵称为雅可比矩阵，若该雅可比矩阵最大特征值大于1，随着反向传播链的增长，梯度会逐渐的增大。导致梯度消失或者爆炸。