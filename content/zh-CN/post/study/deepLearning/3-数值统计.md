---
title: "3 数值统计"
date: 2021-11-09T08:37:32+08:00
tags : [

]
categories : [

]
series : []
aliases : []
draft: false
---

机器学习算法通常需要大量的数值计算。这通常是指通过**迭代过程更新解的估 计值来解决数学问题的算法**，而不是通过解析过程推导出公式来提供正确解的方法

# 上溢下溢

1. **连续数学在数字计算机上的根本困难**是，我们需要通过有限数量的位模式来**表 示无限多的实数**。意味着我们在计算机中表示实数时，几乎总会引入一些**近似误 差。**
2. 一种极具毁灭性的舍入误差是**下溢（underflow）**。当接近零的数被四舍五入为 零时发生下溢。
3. 另一个极具破坏力的数值错误形式是**上溢（overflow）**。当大量级的数被近似为 ∞ 或 −∞ 时发生上溢。进一步的运算通常会导致这些无限值变为非数字。
4. 上溢和下溢进行数值稳定的一个例子是softmax 函数
5. Theano (Bergstra et al., 2010a; Bastien et al., 2012a) 就是这样软件包的一个例子，它能自动检测并稳定深度学习中许多常见的数值不稳定的表达式。
   

# 病态条件

1. 考虑函数 f(x) = A−1x。当 A ∈ Rn×n 具有特征值分解时，其条件数为$max_{i,j} |\frac{\lambda_i}{\lambda_j}|$。这是最大和最小特征值的模之比1。当**该数很大**时，矩阵求逆对输入的**误差特别敏感**。


# 基于梯度的优化方法

1. 我们把要最小化或最大化的函数称为**目标函数（objective function）**或**准则 （criterion）**。
   1. 当我们对其进行最小化时，我们也把它称为**代价函数（cost function）**、**损失函数（loss function）**或**误差函数（error function）。**
   2. 我们通常使用一个**上标 ∗**表示最小化或最大化函数的 x 值。如我们记 x∗ = arg min f(x)。
2. 这个函数的导数（derivative） 记为 f′(x) 或 dy/dx。导数 f′(x) 代表 f(x) 在点 x 处的斜率。
3. 因此**导数**对于最小化一个函数很有用，因为它告诉我们如何更改 x 来略微地改 善 y。这种技术被称为**梯度下降** （gradient descent）(Cauchy, 1847)。
4. f′(x) = 0 的点称为**临界 点**（critical point）或驻点（stationary point）。
   1. f′(x) = 0 的点称为**临界 点**（critical point）或**驻点**（stationary point）。
   2. f′(x) = 0 的点称为**临界 点**（critical point）或**驻点**（stationary point）。
   3. 有些临界点既不是最小点也不是最大点。这些点被称为**鞍点**（saddle point）。
5. 因此，我们通常寻找使 f 非常小的 点，但这在任何形式意义下并不一定是最小。
6. 我们需要用到**偏导数**（partial derivative）的概念
7. **梯度（gradient）**是相对一个向量求导的导数:f 的导数是包含所有偏导数的向量，记为 ∇xf(x)。梯度的第i 个元素是 f 关于 xi 的偏导数。
8. 在 u（单位向量）方向的**方向导数**（directional derivative）是函数 f 在 u 方向 的斜率。
9. 负梯度向量指向下坡。我们在负梯度方向上移动可以减小 f。这被称为**最速下降法** (method of steepest descent) 或**梯度下降**（gradient descent）。
10. 最速下降建议新的点为 x′ = x− ϵ∇xf(x) (4.5)其中 ϵ 为**学习率**（learning rate），是一个确定步长大小的正标量
    1. 最速下降在梯度的每一个元素为零时收敛（或在实践中，很接近零时）
    2. 但不断向更好的情况移动一小 步（即近似最佳的小移动）的一般概念可以推广到离散空间。递增带有离散参数的目标函数被称为**爬山**（hill climbing）算法 (Russel and Norvig, 2003)
11. 有时我们需要计算输入和输出都为向量的函数的所有偏导数。包含所有这样的 偏导数的矩阵被称为 **Jacobian 矩阵**
    1. 二阶导数告诉我们，一阶导数将如何随着输入 的变化而改变。它表示只基于梯度信息的梯度下降步骤是否会产生如我们预期的那样大的改善，因此它是重要的。我们可以认为，二阶导数是对曲率的衡量
    2. 如果这样的函数具有零二阶导数，那就没有曲率。也就是一条完全 平坦的线，仅用梯度就可以预测它的值。我们使用沿负梯度方向大小为 ϵ 的下降步， 当该梯度是 1 时，代价函数将下降 ϵ。如果二阶导数是负的，函数曲线向下凹陷 (向 上凸出)，因此代价函数将下降的比 ϵ 多。如果二阶导数是正的，函数曲线是向上凹陷 (向下凸出)，因此代价函数将下降的比 ϵ 少
12. 当我们的函数具有多维输入时，二阶导数也有很多。我们可以将这些导数合并 成一个矩阵，称为 **Hessian 矩阵**
13. Hessian **等价于**梯度的 Jacobian 矩阵。
14. 微分算子在任何二阶偏导连续的点处可交换，也就是它们的顺序可以互换：这意味着 Hi,j = Hj,i，因此 Hessian 矩阵在这些点上是对称的。因为 Hessian 矩阵是实对 称的，我们可以将其分解成一组实特征值和一组特征向量的正交基。在特定方向 d上的二阶导数可以写成 d⊤Hd。这个方向的二阶导 数就是对应的特征值。对于其他的方向 d，方向二阶导数是所有特征值的加权平均， 权重在 0 和 1 之间，且与 d 夹角越小的特征向量的权重越大。最大特征值确定最大二阶导数，最小特征值确定最小二阶导数。
    1.  Hessian 的特征值决定了**学 习率的量级**
15. 当 f′(x) = 0 且 f′′(x) > 0 时，x 是一个**局 部极小点**。同样，当 f′(x) = 0 且 f′′(x) < 0 时，x 是一个**局部极大点**。这就是所谓的**二阶导数测试（second derivative test）。**
16. 在临界点处（∇xf(x) = 0），我们通 **过检测 Hessian 的特征值来判断该临界点是一个局部极大点、局部极小点还是鞍点。** 当 Hessian 是**正定**的（所有特征值都是正的），则该临界点是**局部极小点**。因为方 向二阶导数在任意方向都是正的，参考单变量的二阶导数测试就能得出此结论。同 样的，当 Hessian 是**负定的**（所有特征值都是负的），这个点就是**局部极大点**。在多 维情况下，实际上我们可以找到确定该点是否为鞍点的积极迹象（某些情况下）。如 果 Hessian 的特征值中**至少一个是正的且至少一个是负的**，那么 x 是 f **某个横截面 的局部极大点**，却是**另一个横截面的局部极小点**。见图4.5中的例子。最后，多维二 阶导数测试可能像单变量版本那样是不确定的。**当所有非零特征值是同号的且至少 有一个特征值是 0 时**，这个检测就是**不确定的**。这是因为单变量的二阶导数测试在**零特征值对应的横截面上是不确定的。**
17. 维度多于一个时，鞍点不一定要具有 0 特征值：仅需要同时具有正特征值和负特征值。我 们可以想象这样一个鞍点（具有正负特征值）在一个**横截面内是局部极大**点，而在另一个**横截面内是局部极小点。**

18. 中期稍微总结一下：
    1.  jacobian矩阵是梯度矩阵，表明了下一步权重下降的方向。Hessian衡量梯度下降的速率。如果hessian值正定，则权重是局部极小点。如果hessian负定，则权重是局部极大点。当涉及多维的时候条件更苛刻一点。有非零特征值时，这个检测是不确定的。根据梯度下降的方向进行下降，就是梯度下降。证明需要看书。大约就是二阶导，高中学的那一套。


19. 在**多维情况下**，单个点在每个方向上的二阶导数时不同的，Hessian 的条件数衡量 这些二阶导数的变化范围。**当 Hessian 的条件数很差时**，梯度下降法也会表现得很差。**这是因为一个方向上的导数增加得很快，而在另一个方向上增加得很慢**。
    1. 梯度下降把时间浪费于在峡谷壁反复下 降，因为它们是最陡峭的特征。由于步长有点大，有超过函数底部的趋势，因此需要在下一次迭代时在对面的峡谷壁下降
20. 我们可以使用 **Hessian 矩阵的信息**来指导搜索，以解决这个问题。其中最简单 的方法是**牛顿法**（Newton’s method）。
    1. 当 f 是一个正定二次函数时，牛顿法只要应用一次式(4.12)就能直接跳到函数的最 小点。如果 f 不是一个真正二次但能在局部近似为正定二次，牛顿法则需要多次迭代应用式。
    2. 如式(8.2.3)所讨论的，当附近的临界点是最小点（Hessian 的所有特征值 都是正的）时牛顿法才适用，而梯度下降不会被吸引到鞍点(除非梯度指向鞍点)。
21. 仅使用梯度信息的优化算法被称为**一阶优化算法** (first-order optimization algorithms)，**如梯度下降**。使用 **Hessian 矩阵的优化算法被称为二阶最优化算法**(second-order optimization algorithms)(Nocedal and Wright, 2006)，**如牛顿法**
22. 在深度学习的背景下，限制**函数满足Lipschitz 连续**（Lipschitz continuous）或 其导数Lipschitz连续可以获得一些保证。最成功的特定优化领域或许是**凸优化**（Convex optimization）。凸优化通过更强 的限制提供更多的保证。凸优化算法只对凸函数适用，即 Hessian 处处半正定的函数。

# 约束优化

1. 在 x 的所有可能值下最大化或最小化一个函数 f(x) 不是我们所希望 的。相反，我们可能希望在 x 的某些集合 S 中找 f(x) 的最大值或最小值。这被称为**约束优化**（constrained optimization）。集合 S 内的点 x 被称可行（feasible）点。
2. 我们常常希望找到在**某种意义上小的解。**
3. 约束优化的一个简单方法是将约束考虑在内后简单地对梯度下降进行修改。
   1. 如 果我们**使用一个小的恒定步长 ϵ，**我们可以先取梯度下降的单步结果，然后将结果投影回 S。
   2. 如果我们使用线搜索，我们只能在**步长为 ϵ 范围内搜索**可行的新 x 点
   3. 或者 我们可以将线上的**每个点投影到约束区域**。
   4. 在梯度下降或线搜索前 将梯度投影到可行域的切空间会更高效 (Rosen, 1960)。
4. 一个更复杂的方法是设计一个不同的、无约束的优化问题，其解可以转化成原 始约束优化问题的解**Karush–Kuhn–Tucker（KKT）**方法2是针对约束优化非常通用的解决方案。
5. 我们引入一个称为**广义 Lagrangian**（generalized Lagrangian） 或**广义 Lagrange 函数**（generalized Lagrange function）的新函数。
6. 那么 S 可以表示为 S = {x | ∀i, g(i)(x) = 0 and ∀j, h(j)(x) ≤ 0}。其中涉及 g(i) 的等式称为等式约束（equality constraint），涉及 h(j) 的不等式称为不等式约束（inequality constraint）。
   1. 这是因为当约束满足时，广义lagrangian函数的值为f(x)的值。
   2. 当约束不满足时，广义lagrangian为无穷大。
7. 我们可以使用一组简单的性质来描述约束优化问题的最优点。这些性质称 为Karush–Kuhn–Tucker（KKT）条件
   1. 广义 Lagrangian 的梯度为零
   2. 所有关于 x 和 KKT 乘子的约束都满足。
   3. 不等式约束显示的 ‘‘互补松弛性’’：α⊙ h(x) = 0。


# 实例分析：线性最小二乘法

没看懂