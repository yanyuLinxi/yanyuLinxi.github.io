---
title: "线性代数"
date: 2021-10-19T08:03:54+08:00
tags : [

]
categories : [

]
series : []
aliases : []
draft: false
---

- [常见概念](#常见概念)
  - [特征分解](#特征分解)
  - [奇异值分解](#奇异值分解)

# 常见概念

标量
向量
矩阵
张量

1. 矩阵加法：C = A + b。表示向量b和矩阵的每一行相加。这种隐式复制的方式，称为广播。boardcasting

2. 元素对应乘积： A $\odot$ B  hadamard 乘积

3. 由一组解线性组合而成的解称为 ***生成子空间***。表示原始向量线性组合后能抵达的点的集合。

4. 矩阵列向量线性相关的矩阵，称为***奇异***的。

5. ***范数***： 形式上 $L^p 定义为 ||x||_p = (\sum_i|x_i|^p)^{\frac{1}{p}}$ 。
    满足的性质：
    1. $f(x) = 0 -> x=0$
    2. $f(x+y) \leq f(x) + f(y)$  (三角不等式)
    3. $\forall \alpha \in R, f(\alpha x) = |\alpha|f(x)$

6. 当p=2的时候，$L^2$就是***欧几里得范数***。表示从原点到x确定的点的欧几里得距离。经常表示为$||x||$。平方$L^2$范数对x中每个元素的导数取决于对应的元素。而$L^2$范数对每个元素的导数却和整个向量相关。但是平方L2范数在原点附近增长的十分缓慢。L1范数在0附近增长则正相关。

7. ***最大范数***$L^{\infty} => ||x||_\infty = max|x_i|$

8. ***Frobenius范数***，衡量矩阵的大小。

## 特征分解

9. ***对角矩阵（diagonal matrix）***只在主对角线上含有非零元素，其他位置都是零。我们用 diag(v) 表示一个对角元素 由向量 v 中元素给定的对角方阵。计算乘法 diag(v)x，我们只需要将 x 中的每个元素 xi 放大 vi 倍。换 言之，diag(v)x = v⊙ x
   1. 对角方阵的逆矩阵存在， 当且仅当对角元素都是非零值
   2. 但通过将一些矩阵限制为对角矩阵，我们可以得到计算代价较低的（并且简明扼要的）算法

10. ***对称（symmetric）矩阵***是转置和自己相等的矩阵

11. ***单位向量***（unit vector）是具有单位范数（unit norm）的向量：$||x||_2 = 1$.

12. 如果 x⊤y = 0，那么向量 x 和向量 y 互相正交（orthogonal）。如果这些向量不仅互相正交，并且范数都为 1，那么我们称它们 是标准正交（orthonormal）。
    1. 正交矩阵（orthogonal matrix）是指行向量和列向量是分别标准正交的方阵, 这意味着$A^{−1} = A^⊤$

13. ***特征分解（eigendecomposition***）是使用最广的矩阵分解之一，即我们将矩阵分 解成一组特征向量和特征值.方阵 ***A 的特征向量***（eigenvector）是指与 A 相乘后相当于对该向量进行缩放 的非零向量 v:$Av=\lambda v$.标量 λ 被称为这个特征向量对应的特征值（eigenvalue）
    1. 每个实对称矩阵都可以分解成实特征向量和实特征值
    2. 按照惯例，我们通常按降序排列 Λ 的元素。在该 约定下，特征分解唯一当且仅当所有的特征值都是唯一的。

14. 矩阵是奇异的当且仅当含 有零特征值

15. 所有特征值都是正数的矩阵被称为正定（positive definite）；所有特征值都是非 负数的矩阵被称为半正定（positive semidefinite）。半正定矩阵受到关注是因为它们保证 ∀x, x⊤Ax ≥ 0。此外， 正定矩阵还保证 x⊤Ax = 0 ⇒ x = 0。

## 奇异值分解

16. 被称为***奇异值分解***（singular value decomposition, SVD），将矩阵分 解为奇异向量（singular vector）和***奇异值***（singular value）。每 个实数矩阵都有一个奇异值分解，但不一定都有特征分解。

17. 对角矩阵 D 对角线上的元素被称为矩阵 A 的奇异值（singular value）。矩阵 U的列向量被称为左奇异向量（left singular vector），矩阵 V的列向量被称右奇异向量（right singular vector）。 事实上，我们可以用与 A 相关的特征分解去解释 A 的奇异值分解。A 的左奇 异向量（left singular vector）是AA⊤ 的特征向量。A的右奇异向量（right singular vector）是 A⊤A 的特征向量。A 的非零奇异值是 A⊤A 特征值的平方根，同时也是
AA⊤ 特征值的平方根。

18. 奇异值分解是类似的，只不过这回我们将矩阵 A 分解成三个矩阵的乘积$A = UDV^⊤$.矩阵 U和 V都定义为正交 矩阵，而矩阵 D 定义为对角矩阵。注意，矩阵 D 不一定是方阵。A 的奇异值（singular value）。矩阵 U的列向量被称为左奇异向量（left singular vector），矩阵 V的列向量被称右奇异
向量（right singular vector）。***SVD最有用的一个性质可能是拓展矩阵求逆到非方矩阵上。***

18. ***Moore-Penrose 伪逆***. 当矩阵 A 的列数多于行数时，使用伪逆求解线性方程是众多可能解法中的一 种。特别地，x = A+y 是方程所有可行解中欧几里得范数 ∥x∥2 最小的一个。当矩阵 A 的行数多于列数时，可能没有解。在这种情况下，通过伪逆得到的 x 使得 Ax 和 y 的欧几里得距离 ∥Ax− y∥2 最小。

19. ***迹运算***返回的是矩阵对角元素的和.多个矩阵相乘得到的方阵的迹，和将这些矩阵中的最后一个挪到最前面之后相 乘的迹是相同的。

20. ***行列式，记作 det(A)***，如果行列式是 0，那么空间至少沿着某一维完全收缩了，使其失去了所有的 体积。如果行列式是 1，那么这个转换保持空间体积不变。

21. ***主成分分析（principal components analysis, PCA）***是一个简单的机器学习算 法，可以通过基础的线性代数知识推导。对于每个点 x(i) ∈ Rn，会有一个对应的 编码向量 c(i) ∈ Rl。如果 l 比 n 小，那么我们便使用了更少的内存来存储原来的数据。f(x) = c；我们也希望找到一 个解码函数，给定编码重构输入，x ≈ g(f(x))。了简化解码器，我们使用矩阵乘 法将编码映射回 Rn，即 g(c) = Dc，其中 D ∈ Rn×l 是定义解码的矩阵。
    1. 目前为止所描述的问题，可能会有多个解. 为了使问 题有唯一解，我们限制 D 中所有列向量都有单位范数
    2. 首先我们需要明确如何根据每 一个输入 x 得到一个最优编码 c∗。一种方法是最小化原始输入向量 x 和重构向量 g(c∗) 之间的距离。我们使用范数来衡量它们之间的距离。在PCA算法中，我们使用 L2 范数：
    3. 我们可以用平方 L2 范数替代 L2 范数，因为两者在相同的值 c 上取得最小值。这是因为 L2 范数是非负的，并且平方运算在非负值上是单调递增的。
    4. 这使得算法很高效：最优编码 x 只需要一个矩阵-向量乘法操作。为了编码向量， 我们使用编码函数$f(x) = D^⊤x$.进一步使用矩阵乘法，我们也可以定义PCA重构操作：r(x) = g(f(x)) = DD⊤x.
    5. 具体来讲，最优的 d 是 X⊤X最大特 征值对应的特征向量。以上推导特定于 l = 1 的情况，仅得到了第一个主成分。更一般地，当我们希望 得到主成分的基时，矩阵 D 由前 l 个最大的特征值对应的特征向量组成。这个结论 可以通过归纳法证明，我们建议将此证明作为练习。
    6. ***从线性代数的角度总结***：PCA是最小化压缩后的矩阵和原始矩阵之间的距离。通常使用平方$L^2$范数来表示这个距离。经过在一维的运算，最优的d是$X^TX$最大特征值对应的特征向量。通过归纳法得知压缩矩阵D由前l个最大的特征直对应的特征向量组成。
    7. 从其他角度还有更广泛的解释： 
    8. 方差：表示数据的分散程度。协方差表示两个变量的相关性。
    9. 思想方法：
       1. 将坐标轴移动到数据的中心，旋转坐标轴，使得数据在c1轴上的方差最大，即**全部n个数据个体在该方向投影最分散**。C1为第一主成分。
       2. 找一个C2，使得C2和C1的协方差（相关系数）为0，上市的数据在该方向的方差尽量最大。
       3. 依次类推。
    10. 数学方法：
        1. 将原始数据按列组成 n 行 m 列矩阵 X
        2. 将 X 的每一行进行零均值化，即减去这一行的均值；
        3. 求出**协方差矩阵**
        4. 求出协方差矩阵的**特征值及对应的特征向量；**
        5. 将**特征向量按对应特征值大小从上到下按行排列成矩阵，取前 k 行组成矩阵 P；**
        6. 即为降维到 k 维后的数据。
    11. 优缺点：
        1. 保留了主要信息。**这个主要信息未必是重要信息**。很可能是非主要信息取了决定性作用。
        2. 起到了**降噪降维的作用**。且PCA后的**特征相互独立。**
    12. 补充。方差，表示数据的分散程度。
    13. 协方差理解为一致性分散程度有多大。在分散程度的相关性有多大。协方差为0，分散程度不相关。

22. 