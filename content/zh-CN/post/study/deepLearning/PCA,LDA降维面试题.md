---
title: "PCA,LDA降维面试题"
date: 2022-03-06T10:04:34+08:00
tags : [

]
categories : [

]
series : []
aliases : []
draft: false
---

# PCA降维
1. PCA的理论，旨在找到数据中的主成分分析，利用这些主成分表征原始数据，从而达到降维的目的
2. 在信号处理领域，认为信号拥有较大的方差，噪声拥有较小的方差。信号和噪声之比即为信噪比。信噪比越大说明数据的质量越好。PCA的目标就是找到一个投影向量。最大化投影方差。
3. 公式写在纸上，步骤如下：
   1. 对样本进行去中心化处理（将样本减去均值）
   2. 对样本求协方差矩阵
   3. 对协方差矩阵进行特征值分解，特征值从大到小跑列
   4. 选择特征值前d大对应的特征所对应的特征向量w1..wd, 进行映射。x[w1...wd]
   5. 投影后信息量占比:$\mu=\sqrt{\frac{\sum_{i=1}^d \lambda^2}{\sum_{i=1}^n \lambda^2}}$, $\lambda$是协方差矩阵的特征值。
4. 一般情况下，方差是$\sum(x-\mu)^2$, 协方差是$\sum_{i=1}(x_i-\mu_x)^T(y_i-\mu_y)$,方差可视作随机变量x关于其自身的协方差Cov(x, x).


# LDA线性判别分析
主要为分类服务。
核心就是，让投影之后的向量，同一类的样本更加几种。不同类的样本隔的更远
所以即最大化类间距离，和最小化类内方差。
类间距离即均值的差值。类内距离即各个类别的方差之和。将损失函数定义为类间距离和类内距离的比值。

$max J(w) = \frac{||w(\mu_1-\mu_2)||^2}{D1+D2}$
定义类间散度为$S_B=(\mu_1-\mu_2)(\mu_1-\mu_2)^T$, 类内散度为$S_w=\sum_i(x_i-\mu_i)(x_i-\mu_i)^T$
最终结果为
$S_w^{-1}S_Bw=\lambda w$即只要求样本的均值和类内方差，就能计算出最佳投影方向。

## 多个类的LDA
1. 计算类内样本的均值。总体均值向量
2. 计算类内散度，全局散度，类间散度=全局-类内
3. 对类内散度$S_w^{-1}S_b$进行特征分解，取最大的特征向量前d维进行映射。

# 面试题

1. pca的优缺点
   1. 优点：
      1. 计算方法简单。
      2. 各主要成分之间正交，可消除原始数据成分间相互影响的因素
   2. 缺点：
      1. 方差小的非主成分也可能对样本的差异有重要信息。降维丢弃可能对后续数据处理有影响。
2. 其他降维方法，LDA（线性判别分析），有监督学习下的降维方法。svd（奇异值分解）
   1. 无监督的时候使用PCA，有监督的时候使用LDA。
