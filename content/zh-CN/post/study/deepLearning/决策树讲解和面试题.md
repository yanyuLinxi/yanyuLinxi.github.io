---
title: "决策树讲解和面试题"
date: 2022-02-20T09:57:58+08:00
tags : [

]
categories : [

]
series : []
aliases : []
draft: false
---

# 决策树

1. 决策树算法采用树形结构，使用层层推理来实现最终的分类。决策树由下面几种元素构成：
   1. **根节点**：包含样本的全集
   2. **内部节点**：对应特征属性测试
   3. **叶节点**：代表决策的结果
2. 预测时，在树的内部节点处用**某一属性值**进行判断，根据判断结果决定进入**哪个分支节点**，**直到到达叶节点处**，得到**分类结果**。

# 增益计算
1. ID3：信息增益
   1. 某一个属性的信息熵 $Ent(D) = -\sum_{k=1}^y p_k log p_k$。其中在所有标签y中，第k类标签所占的比例为$p_k$。分类越纯，$p_k$就越大，信息熵就越小。即信息熵越小，则信息量越小，混乱程度越低，置信度越高。
   2. 属性a信息增益为:$Gain(D, a)=Ent(D)-\sum_{v=1}^{V}\frac{|D^v|}{|D|}Ent(D^v)$ 其中属性（特征）a的分类有v种，第v类的分类数量为$D^v$。
   3. 信息增益的缺点：偏爱属性分类多的特征。属性分类多的特征，每一分类中的越纯的可能性越大。信息熵就低，信息增益就高。所以偏爱特征分类多的特征。比如按照id来分类，则信息增益最大。
2. C4.5：信息增益率
   1. 为了解决上述缺点。将信息熵除以一个固有值。
   2. $Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$，其中$IV(a)=-\sum_{v=1}^V \frac{|D^v|}{|D|} log \frac{|D^v|}{|D|}$ 固有值为属性a的每一种类所占的样本的比例的熵。
   3. 信息增益率偏爱特征分类数量少的特征。所以C4.5的信息选择为，先选择信息增益大于平均值的属性。然后在其中选择信息增益率最大的属性。
3. CART：基尼系数
   1. 由于熵运算非常耗时。所以采用计算更快的基尼系数:$Gain(D)=\sum_{y=1}^Y\sum_{k'\neq k}p_kp_k'=1-\sum_{k=1}^yp_k^2$ 其中$p_k$为标签y中第k类所占的比例。Gini系数表示了随机抽两个样本，它们标签不一致的概率。Gini越小，纯度越高。基尼系数替代信息熵，则是CART计算信息增益的方法。注意：算信息增益时，基尼系数仍然需要乘以$\frac{|D^v|}{|D|}$
   2. 基尼系数约等于熵模型的一阶泰勒展开。
   3. 当CART为回归树时，采用MSE平方差代替基尼系数作为增益计算。

# 连续值的处理
1. C4.5和cart拥有相同的连续值处理
   1. 对于连续值，排序后，选取每两个值的平均值作为候选值。根据增益计算选取最佳增益的候选值进行分裂。详细点说：将连续特征离散化，假设 n 个样本的连续特征 A 有 m 个取值，C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点；
   2. 如果当前属性为连续属性，则该属性在后面还可以参与子节点的产生选择过程。

# 缺失值的处理
1. C4.5采用将缺失样本同时划分入所有子节点，为每个确实样本赋予不同权重。形式上等价于使用不同概率将缺失样本划入子节点。
   1. 属性的选择：首先仅使用非缺失样本进行属性选择。每个属性的计算$Gain(D, a) = \rho * Gain(D, a)$，其中$\rho=\frac{\sum_{x\in \hat{D}}w_x}{\sum_{x\in D} w_x}$，即非缺失值样本所占的比例。
   2. 缺失值样本的划分：将缺失值同时划分入左右子节点，样本权值调整为:$w_x=\hat{r_v}*w_x$， 其中$\hat{r_v}$为非缺失值样本中，该分类中，样本所占的比例。即17个样本，2个缺失。15个样本分为5,8,2.则每一类的$\hat{r_v}$比例为$\frac{5}{15},\frac{8}{15},\frac{2}{15}$
2. CART采用代理测试来估计缺失值
   1. 属性的选择：使用没有缺失值的样本进行划分，并进行惩罚。和C4.5类似。缺失为20%则惩罚20%的权重
   2. 缺失值样本的划分：为每个属性建立一个代理属性（不管是否缺失都会这么做）。代理属性满足和主分裂类似，且有着正关联。使用代理属性对缺失值进行划分。

# 剪枝
1. C4.5剪枝
   1. 预剪枝：在分裂时进行剪枝
      1. 限制树的节点个数：节点数据比例低于某一阈值
      2. 限制树的高度：节点特征都已经分裂。或者树到达一定高度。
      3. 利用分类指标：节点划分前准确率比划分后准确率高。
      4. 总结：常见树防止过拟合的方法。树高度、节点个数，指标。
      5. 预剪枝优点：节省大量的训练开销，降低过拟合。预剪枝缺点：存在欠拟合风险（存在当前性能下降，后续性能大幅提高的情形）
   2. 后剪枝：
      1. 使用测试集进行验证：对所有非叶子节点，自底向上的进行剪枝：如果这个节点不分裂，效果保持或者不下降，则可以替换这个子树。
      2. 优点：后剪枝欠拟合风险小很多，泛化性能会优于预剪枝。缺点：时间开销大。
   3. 悲观后剪枝TODO:
2. CART剪枝：
   1. 基于代价复杂度剪枝：
      1. 这种方法会生成一系列树，每个树都是通过将前面的树的某个或某些子树替换成一个叶节点而得到的，这种方法需要使用一个单独的测试数据集来评估所有的树，根据它们在测试数据集熵的分类性能选出最佳的树。
      2. 在所有的子树中，找出那些使用了较多节点，却使错误率下降最低的子树，把这些子树剪裁掉。
   2. 指标$R(T)=e(T)*p(T)$，e(T)指该节点的错分率。如二分类，1占13，0占7，则错分率为7/20（因为这个节点被判为1）。p(T)是该节点样本数占样本总数的比例。计算当前节点的R(T)，计算当前节点下的所有叶子节点的R(T)，得到代价复杂度$\alpha=\frac{R(T)-R(T_t)}{N-1}$N为该节点下叶子节点数量。对于这个值，分子越大说明分类效果越好。分母越小说明复杂度越低。整个值越大越好。
   3. 代价复杂度剪枝：
      1. 循环对代价复杂度参数最小的节点进行剪枝（有多个节点同时取到最小值时取叶子节点最多的节点），直到只剩下根节点，可得到一系列的剪枝数{T0, T1, T2, …, Tm}，其中T0为原始的决策树，Tm为根节点，Ti+1为Ti剪枝后的结果。
      2. 当树只有根节点时结束。记录下所有的树，根据实际误差获得最优决策树。

# 三个树的比较：

1. **划分标准的差异**：ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。
2. **使用场景的差异**：ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；
3. **样本数据的差异**：ID3 只能处理离散数据且缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；
4. **样本特征的差异**：ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征；
5. **剪枝策略的差异**：ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。


# 决策树的优缺点
1. 优点：
   1. 决策树易于理解和解释，可以可视化分析，**容易提取出规则**；
   2. 比较适合处理**有缺失属性的样本；**
   3. 测试数据集时，**运行速度比较快；**
2. 缺点：
   1. 容易发生**过拟合**（随机森林可以很大程度上减少过拟合）；
   2. 容易忽略数据集中属性的**相互关联**；
   3. 对于那些各类别样本数量不一致的数据，在决策树中，进行属性划分时，**不同的判定准则会带来不同的属性选择倾向**；

# 随机森林
随机森林属于bagging

步骤：
1. 样本选择：一个样本容量为N的样本，有放回的抽取N次，每次抽取1个，最终形成了N个样本。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。
2. 属性选择：当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。
3. 决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。
4. 按照步骤1~3建立大量的决策树，这样就构成了随机森林了。

# 面试题

1. 部分面试题：
   1. https://github.com/datawhalechina/daily-interview/tree/master/AI%E7%AE%97%E6%B3%95/machine-learning
2. 给出案例手算信息增益（ID3,C4.5,CART)(面试中遇到过)


# 参考资料
1. 《机器学习》周志华。（推荐）
2. https://zhuanlan.zhihu.com/p/133838427
3. https://zhuanlan.zhihu.com/p/85731206