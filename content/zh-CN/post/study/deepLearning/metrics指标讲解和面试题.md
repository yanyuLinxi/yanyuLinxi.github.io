---
title: "指标讲解和面试题"
date: 2022-02-20T15:20:36+08:00
tags : [

]
categories : [

]
series : []
aliases : []
draft: false
---

# 指标讲解和面试题

## 指标

1. TP, FP, TN, FN
   1. tp = 预测为正，实际为正
   2. fp = 预测为正，实际为负
   3. fn = 预测为负，实际为正
   4. tn = 预测为负，实际为负
   5. 后一个字母代表预测，前一个字母表示预测是正确还是错误
2. accuracy准确率: 表示预测能力（预测正、负）的好坏：accuracy=(tp+tn)/(tp+fp+fn+tn)
3. precision: 精确度。所有预测中对了多少个。precision=(tp)/(tp+fp)
4. recall：召回率。度量有多少个正例被正确预测：recall=tp/(tp+fn)
5. f1-score: $\frac{1}{f1}=\frac{1}{2}(\frac{1}{p}+\frac{1}{r})$，得到$f1=\frac{2*precision*recall}{precision+recall}$
   1. f1更多知识看百面机器学习，看https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/%E7%AC%AC%E4%BA%8C%E7%AB%A0_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80.md
   2. $\frac{1}{f_\beta}=\frac{1}{1+\beta^2}(\frac{1}{p}+\frac{\beta^2}{r})$
      1. $\beta$表示了recall对precision的重要程度。$\beta>1$则，recall有更大的影响。否则precision有更大的影响。
6. ROC曲线
   1. 真正例率：所有正例中被正确预测为正的比例：TPR(tp ratio)=(tp)/(tp+fn)
   2. 假正例率：所有负例中被错误的预测为正例：FPR(fp ratio)=(fp)/(fp+tn)
   3. 怎么记忆：真正和假正都是看“正”的比例，“真”即真的正例中被**正确**预测的比例。“假”即所有负例中被**错误**预测为正的比例。 tp,fp分别做分子。然后一个是所有真的里选。后面的是所有假的里面选。
   4. ROC曲线：横坐标FPR，纵坐标TPR。
      1. FPR为0，TPR为1，则证明所有样本正确分类。即是最好。
   5. ROC曲线的绘制
   6. ROC底下的AUC的计算：
      1. ​ AUC是衡量二分类模型优劣的一种评价指标，表示正例排在负例前面的概率。
      2. 计算面积。先排序score。然后按照梯形计算公式计算：$Auc=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)*(y_{i+1}+y_i)$
      3. 使用等价的Wilcoxon-Mann-Witney Test法。任意给一个正类样本和负类样本，正样本的score有多大概率大于负样本的score。方法：设正样本为M，负样本为N。总样本数为n。score从大到小排序，然后最大的样本设为n，第二位n-1。$AUC=\frac{\sum_{ispositive}rank_i-\frac{M(1+M)}{2}}{M*N}$. 统计正样本的rank，它代表了排在它下面的所有样本数量。减去和正样本的匹配对，就是和负样本的匹配对。rank求和-正样本和正样本匹配对个数求和就是正样本和比它值小的负样本匹配对个数。除以正样本*负样本就是AUC。
7. PR曲线
   1. 横坐标Precision，纵坐标Recall。
   2. 该曲线对应的AUC即为(AP Average Precision) AP 越高，模型性能越好。
   3. PR曲线易受类别分布影响。而ROC曲线不会
      1. 注意TPR用到的TP和FN同属P列，FPR用到的FP和TN同属N列，**所以即使P或N的整体数量发生了改变，也不会影响到另一列。**也就是说，即使正例与负例的比例发生了很大变化，ROC曲线也不会产生大的变化，而像Precision使用的TP和FP就分属两列，则易受类别分布改变的影响。

## 二分类转多分类

1. 在二分类转多分类，或者多个混淆矩阵（多个验证集）时如何计算。
   1. 宏平均（macro-average)：分别计算出各个指标后平均。如计算出每一类的precision、recall后再平均得到平均precision,平均recall。再求平均F1。
   2. 微平均（micro-average)：先平均TP、FP等参数。再计算各个metrics。
   3. 加权平均（weighted-average)：根据类别的样本数量得到权重（该类样本数/总样本数）。然后加权求宏平均。
2. 优缺点比较
   1. 宏平均平等看待每个类别，但是它的值会受稀有类别影响。
   2. 微平均F1平等考虑所有样本，所以它的值受到常见类别的影响比较大。
   3. 加权平均考虑了类别不平衡情况，它的值更容易受到常见类（majority class）的影响。


## 训练集和测试集分类法
1. 留出法
   1. 划分训练集、验证机（验证参数用）、测试集。验证集从训练集中划分，比例为(1/3-1/5)
2. 交叉验证k-fold
   1. 将数据集划分为k个大小相似的互斥子集。每次选k-1个作为训练集，1个作为验证集。总共进行k次训练和测试。
3. 自助法：有放回采样 
   1. 设数据集个数为m。对数据集进行有放回采样m次，得到m个样本的训练集。剩下没有被采样到的数据作为测试集。不被采样到的概率为$lim_{m->+\infty}(1-\frac{1}{m})^m=\frac{1}{e}$.极限情况下有36.8%的数据不被采样到。缺点：这种方式会改变数据集的分布，引入偏差。数据量足够的情况下，不采用这个。

## 面试题
1. 正确率可以评估分类算法嘛？
   1. 不能。比如正负样本不平衡的数据中。全部预测负也能达到99的正确率。显然这是个无用的分类器。所以仅仅看正确率不行。
2. 如何判别分类器的好坏？
   1. 看ROC曲线。在保证正确率的情况下，提升ROC。
3. f1-score和ROC的好坏。