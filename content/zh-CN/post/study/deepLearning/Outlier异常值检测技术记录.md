---
title: "Outlier异常值检测技术记录"
date: 2021-09-29T20:06:48+08:00
tags : [
    "科研学习笔记",
]
categories : [
    "科研学习笔记",
]
series : []
aliases : []
draft: false
---


Tag:

异常值检测; 离群值检测; Outlier; 


# 低维特征

1. Numeric Outlier
2. Z-score

基于高斯分布（正态分布）外的值标记为异常

https://mp.weixin.qq.com/s?__biz=MzIzODExMDE5MA==&mid=2694182460&idx=1&sn=a4842775394946bb643006e2e7c67be9#rd
多维高斯特征离群点检测

3. 使用 Mahalanobis 距离检测多元离群点
4. 使用 $x^2$ 统计量检测多元离群点

# 高维特征

**1. DBSCAN**

在DBSCAN聚类技术中，所有数据点都被定义为核心点（Core Points）、边界点（Border Points）或噪声点（Noise Points）。

核心点是在距离ℇ内至少具有最小包含点数（minPTs）的数据点；
边界点是核心点的距离ℇ内邻近点，但包含的点数小于最小包含点数（minPTs）；
所有的其他数据点都是噪声点，也被标识为异常值；


**2. Isolation Forest**

该方法是一维或多维特征空间中大数据集的非参数方法，其中的一个重要概念是孤立数

孤立数是孤立数据点所需的拆分数。通过以下步骤确定此分割数

+ 随机选择要分离的点“a”；
+ 选择在最小值和最大值之间的随机数据点“b”，并且与“a”不同；
+ 如果“b”的值低于“a”的值，则“b”的值变为新的下限；
+ 如果“b”的值大于“a”的值，则“b”的值变为新的上限；
+ 只要在上限和下限之间存在除“a”之外的数据点，就重复该过程；

与孤立非异常值相比，它需要更少的分裂来孤立异常值，即异常值与非异常点相比具有更低的孤立数。因此，如果数据点的孤立数低于阈值，则将数据点定义为异常值。

即大于孤立数的特征被标记为异常。


# 其他方法

有哪些比较好的做异常值检测的方法？ - 张戎的回答 - 知乎
https://www.zhihu.com/question/38066650/answer/107801822

1. 基于矩阵分解的异常值检测

通过PAC进行降维。然后再恢复到原始空间。

2. 基于聚类的方法
基于聚类的方法
基于聚类的方法是一类无监督的检测方法，通过考察数据点与之间的关系检测异常值。考虑数据
样本中的数据点
大判所该数据点是否属于某个簇，如果不属于任何族，则认为是异常值
计算该数据点与最近的族之间的距离，如果距离很远，则认为是异常值
大判所该数据点是否是小或者稀硫簇的部分，如東是，则该簇中的所有点都是异常值。
上述三条中的第一条可以采用基于密度的聚类方法(如 DBSCAN)进行计算。第二条可以采用k
means聚类方法。第三条寻找小族和稀疏簇一般采用 FINDCBLOFI算法，其方法为
通过设置一个参数 Alpha( Ovelalpha\le1)$S来区別大和小族，至少包含数据集中数据点占比为
S\alpha$的族是大族，其余的为小族
对每个数据点计算基于簇的局部류常因子（ CBLOF月：对于大族的点， CBLOFT为族的大小和该点与
疾的相似性的乘积；对于小族的点， CBLOF为小族的大小和该点于最近的大簇的相似性的乘积
点与族的相似性代表了点属于族的概率，因此 BLOFE的值可以检辺远离任何簇的异常值，具有最低
CBLOF值的点被认为是异常值


# 从论文中总结的

**Anomaly Detection unsupervised Ensemebles**

1. AE autoEncoder
   1. input-> input/4 -> input/8 -> input/4 -> input -> output
2. IF Isolation Forest
   1. tree: 200.
   2. max sample size: 256.
3. LODA
   1. 400 histograms
   2. $1/ \sqrt{d}$ sparsity
4. LOF Local Outlier Factor
   1. number of neighbors is 20.



5. One-class SVM

6. Subspace Anomaly Detection
7. KDE 核密度检测。

# 离群算法原理总结

## Isolation Forest

### 原理:

隔离：具体来说，该算法利用一种名为孤立树iTree的二叉搜索树结构来孤立样本。由于异常值的数量较少且与大部分样本的疏离性，因此，异常值会被更早的孤立出来，也即异常值会距离iTree的根节点更近，而正常值则会距离根节点有更远的距离。

检测离群点：
对于如何查找哪些点是否容易被孤立，iForest使用了一套非常高效的策略。假设我们用一个随机超平面来切割数据空间, 切一次可以生成两个子空间（想象拿刀切蛋糕一分为二）。之后我们再继续用一个随机超平面来切割每个子空间，循环下去，直到每子空间里面只有一个数据点为止。直观上来讲，我们可以发现那些密度很高的簇是可以被切很多次才会停止切割，但是那些密度很低的点很容易很早的就停到一个子空间了。

具体：
iForest 由 T 个 iTree 组成，每个 iTree 是一个二叉树结构。该算法大致可以分为两个阶段，第一个阶段我们需要训练出 T 颗孤立树，组成孤立森林。随后我们将每个样本点带入森林中的每棵孤立树，计算平均高度，之后再计算每个样本点的异常值分数
1. 第一阶段，步骤如下
   1. 从训练数据中随机选择Ψ个点样本点作为样本子集，放入树的根节点
   2. 随机指定一个维度（特征），在当前节点数据中随机产生一个切割点 p（切割点产生于当前节点数据中指定维度的最大值和最小值之间）
   3. 以此切割点生成了一个超平面，然后将当前节点数据空间划分为2个子空间：把指定维度里小于 p 的数据放在当前节点的左子节点，把大于等于 p 的数据放在当前节点的右子节点
   4. 在子节点中递归步骤(2)和(3)，不断构造新的子节点，直到子节点中只有一个数据（无法再继续切割）或子节点已到达限定高度。
   5. 循环(1)至(4)，直至生成 T 个孤立树iTree
2. 第二阶段
   1. 获得T个iTree之后，iForest训练就结束，然后我们可以用生成的iForest来评估测试数据了。对于每一个数据点 xi，令其遍历每一颗孤立树iTree，计算点 xi 在森林中的平均高度 h(xi) 对所有点的平均高度做归一化处理。异常值分数的计算公式如下所示 https://www.cnblogs.com/guoyaohua/p/isolation_forest.html
   2. 

### 总结：

### 评价：
1. 相较于LOF，K-means等传统算法，孤立森林算法对高纬数据有较好的鲁棒性
2. Forest具有线性时间复杂度。因为是Ensemble的方法，所以可以用在含有海量数据的数据集上面。通常树的数量越多，算法越稳定。
3. iForest不适用于特别高维的数据。由于每次切数据空间都是随机选取一个维度，建完树后仍然有大量的维度信息没有被使用，导致算法可靠性降低。高维空间还可能存在大量噪音维度或无关维度(irrelevant attributes)，影响树的构建
4. iForest仅对Global Anomaly敏感，即全局稀疏点敏感，不擅长处理局部的相对稀疏点(Local Anomaly)。


## LODA

没有找到相关介绍

## LOF

### 原理
LOF通过计算一个数值score来反映一个样本的异常程度。这个数值的大致意思是：一个样本点周围的样本点所处位置的平均密度比上该样本点所在位置的密度。比值越大于1，则该点所在位置的密度越小于其周围样本所在位置的密度，这个点就越有可能是异常点。

1，对于每个数据点，计算它与其他所有点的距离，并按从近到远排序
2，对于每个数据点，找到它的K-Nearest-Neighbor，计算LOF得分


### 总结
是一种无监督的离群检测方法，是基于密度的离群点检测方法中一个比较有代表性的算法。

### 评价

## DBSCAN

DBSCAN的核心思想是从某个核心点出发，不断向密度可达的区域扩张，从而得到一个包含核心点和边界点的最大化区域，区域中任意两点密度相连。


## Kmeans

K-means 是我们最常用的基于欧式距离的聚类算法

1. 选择初始化的 k 个样本作为初始聚类中心 [公式] ；
2. 针对数据集中每个样本 [公式] 计算它到 k 个聚类中心的距离并将其分到距离最小的聚类中心所对应的类中；
3. 针对每个类别 [公式] ，重新计算它的聚类中心 [公式] （即属于该类的所有样本的质心）；
4. 重复上面 2 3 两步操作，直到达到某个中止条件（迭代次数、最小误差变化等）。

初始化质心。重新计算质心。最后得到质心。