---
title: "KnnK最近邻居"
date: 2021-12-11T15:49:11+08:00
tags : [

]
categories : [

]
series : []
aliases : []
draft: false
---


# KNN步骤。

一般来说，KNN分类算法的计算过程：

1）计算待分类点与已知类别的点之间的距离

2）按照距离递增次序排序

3）选取与待分类点距离最小的K个点

4）确定前K个点所在类别的出现次数

5）返回前K个点出现次数最高的类别作为待分类点的预测分类

# 关键点
**算法超参数K**
**距离度量**，特征空间中样本点的距离是样本点间相似程度的反映
**分类决策规则，少数服从多数。**

距离：
1. 欧式距离
2. 曼哈顿距离。
3. 闵可夫斯基距离。 $(\sum(x_i-x_j) ^p) ^1/p$

# notes：
1. k一般选择奇数。

# 优缺点
**优点：**

1）**算法简单，理论成熟**，既可以用来做分类也可以用来做回归。

2）**可用于非线性分类。**

4）由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属的类别，**因此对于类域的交叉或重叠较多的待分类样本集来说，KNN方法较其他方法更为适合**。

5）**该算法比较适用于样本容量比较大的类域的自动分类**，而那些样本容量比较小的类域采用这种算法比较容易产生误分类情况。

6）预测时间复杂度为O(n)，训练时，排序时间复杂度为O(nlogN)

**缺点：**

1）需要算每个测试点与训练集的距离，**当训练集较大时，计算量相当大**，时间复杂度高，特别是特征数量比较大的时候。

2）**需要大量的内存，空间复杂度高**。

3）**样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少**），对稀有类别的预测准确度低。

4）**预测时速度比起逻辑回归之类的算法慢。**

