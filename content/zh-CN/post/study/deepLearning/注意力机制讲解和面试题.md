---
title: "注意力机制讲解和面试题"
date: 2022-03-04T18:53:46+08:00
tags : [

]
categories : [

]
series : []
aliases : []
draft: false
---

# 注意力机制
https://zhuanlan.zhihu.com/p/78655043
https://zhuanlan.zhihu.com/p/379033238

1. 注意力机制，用于从大量信息中筛选出有用的信息，就是注意力机制。最经典的图，就是qkv的结构图。分三个步骤
   1. Query与key送入注意力机器计算，得到相关系数
   2. 将相关系数送入softmax得到注意力分数。注意力分数描述了value的重要程度。
   3. 使用注意力分数对value进行加权求和
2. 整个注意力机制来源于翻译中的Seq2Seq2模型。是为了缓解梯度消失而设立的一种模型
   1. 对于seq2seq的输入$h_1...h_t$,送入rnn模型得到上下文向量s
   2. 输出的时候$y=f(y_{i-1}, s_{i-1})$意思就是由上一个输出和上一个隐藏层状态决定了当前的输出。
   3. 但有两个缺陷：对齐问题和长距离依赖问题
   4. 为了解决长距离，使用状态向量和所有的输入向量进行注意力计算
   5. 得到$y=f(y_{i-1}, s_{i-1}, c)$, c由$s_{i-1}和h_1...h_i$计算得到
   6. 根据选取计算的输入向量不同，可以分为soft、global、local、self
      1. soft：从h_1到h_i
      2. hard：选择一个计算隐藏状态
      3. global：所有的输入
      4. local：窗口内的输入
      5. self: 不通过状态向量进行计算，而直接从source、target之间进行计算。source乘以权重分别得到q,k,v然后和自己进行运算。
         1. self-attention的三个矩阵由当前矩阵得来的
         2. self-attention的注意力系数要除以d_k
   7. 公式：三步：送入注意力机器，归一化得到分数，加权求和。
      1. $e_{ij} = a(h_i, h_j)$
      2. $\alpha_{ij} = \frac{e_{ji}}{\sum e_{ji}}$
      3. $c_j=\sum \alpha_{ji}h_i$
   8. 常见的注意力机器:
      1. 引入神经网络$a=LeakyReLU(a[h_i||h_j])$
      2. 向量点积。
         1. Transformer中还除以了根号d，防止注意力系数太大。：
            1. $a(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V$
         2. 为什么可以用点乘？
            1. 点乘反映了一个向量在两一个向量上的投影长度。是一个标量。两个向量越相似，则点乘结果就越大。
      3. cos相似性
   9. 多头注意力机器：设置不同的权重矩阵相乘，计算多个注意力特征，通过求平均或者并列得到多头注意力特征。（一般求并列）
      1. 原理：将特征映射到多个特征空间。提取了不同层面的注意力信息。
      2. 也可以将原有的高维空间转化为多个低维空间并再最后进行拼接，形成同样维度的输出，借此丰富特性信息，降低了计算量
3. 如何介绍Attention
   1. 介绍定义。从所有信息中抽取重要的。
   2. 介绍三个步骤
   3. 介绍注意力机器
   4. 介绍soft、hard、local等。
## 面试题
1. self-attention
   1. 取消了中间的向量，直接在source和target之间进行注意力计算
   2. 这里的source和target往往是一个东西。即QKV三个矩阵是由同一个输入特征计算出来的。x*w=q,k,v
2. 注意力机器
3. 注意力的公式
4. 为什么要用多头
   1. 原论文中说，多头可以让模型去关注不同方面的信息，类似于CNN中的多个滤波器
5. 为什么注意力机制缩放了d_k
   1. 当维度很大时，点积结果会很大，会导致softmax的梯度很小。为了减轻这个影响，对点积进行缩放。