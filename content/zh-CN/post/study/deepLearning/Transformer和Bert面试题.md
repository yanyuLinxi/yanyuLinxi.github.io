---
title: "Transformer和Bert面试题"
date: 2022-03-10T10:02:58+08:00
tags : [

]
categories : [

]
series : []
aliases : []
draft: false
---

# Transformer 和 Bert 面试题


## 资料

1. 视频资料
   1. https://www.bilibili.com/video/BV1Di4y1c7Zm
2. 面试资料
   1. https://zhuanlan.zhihu.com/p/148656446


## 讲解

### 来源
1. 机器翻译
   1. 机器翻译常用RNN模型。RNN模型的缺点
   2. RNN模型需要一个一个送入进去，无法并行
   3. RNN模型的梯度信息被近距离梯度信息主导，远距离梯度信息被忽略。
2. Transformer两列的结构
   1. 左边6个（或更多）Encoder。Encoder的结构相同。但每个Encoder参数是不共享的。
   2. 右边6个（或更多）Decoder。Decoder和Encoder结构不同。
3. 具体结构
   1. Encoder：
      1. 输入->position encoding
      2. ->多头注意机制
      3. -> position+多头注意力机制（残差结构）并进行norm
      4. -> 送入FF网络
      5. -> ff输出和第三步相加（残差）并进行norm
   2. Decoder
      1. （解码器前一个时刻的输出）输入->position encoding
      2. 送入 masked 多头注意力机制
      3. 输出和第一步残差并norm
      4. 接收encoder输出和第三步输出送入多头注意力
         1. 注意这里接收的encoder输出，是所有encoder计算完后产生的输出，不是同层encoder的输出。
      5. 和第3步的残差并norm
      6. 送入FF网络
      7. FF输出和第5步残差
   3. Decoder最终输出->线性层->softmax
   4. 总结Encoder：可以分为三个部分，输入部分，注意力机制部分，前馈神经网络部分。后面两个部分都有残差和norm的结构。
   5. Encoder和Decoder差别。Decoder中间多了一层交互层，输入为Decoder上一层输出和同层Encoder输出。且Decoder第一层是mask操作。

### 位置编码
1. Encoder 输入部分
   1. Embedding
   2. 位置编码

2. 为什么需要位置编码
   1. 相比于RNN来说，Transformer对于一个句子中的值是一起处理的（一起处理的好处加快了速度，但忽略了序列关系），而不是一个个处理的，所以需要加入位置编码来告诉网络每一个的位置。
3. 位置编码公式
   1. 对于一个长度为512的词特征向量。对于词向量中的奇数位置使用sin编码，对偶数位置使用cos编码。得到一个新的512维度向量。
   2. 上述两个512维度的向量相加
4. 为什么这样编码有用？
   1. $$\{ \begin{array}  { l  }  { \sin ( \alpha + \beta ) = \sin \alpha \cos \beta + \cos \alpha \sin \beta } \\ { \cos ( \alpha + \beta ) = \cos \alpha \cos \beta - \sin \alpha \sin \beta } \end{array}$$
   2. $${ P E ( p o s + k , 2 i ) = P E ( p o s , 2 i ) * P E ( k , 2 i + 1 ) + P E ( p o s , 2 i + 1 ) * P E ( k , 2 i ) } \\  P E ( p o s + k , 2 i + 1 ) = P E ( p o s , 2 i + 1 ) * P E ( k , 2 i +1) - PE(pos, 2i) * PE(k, 2i)$$
   3. 所以对于pos+k的位置向量的某一维度2i或2i+1而言，可以表示为pos位置和k位置的位置向量的2i和2i+1的线性组合。这样的线性组合意味着向量中蕴含了相对位置信息。
5. 注意
   1. 这种相对位置信息会在注意力机制中消失。
   2. 相对位置信息即$PE_{pos+k}$，可以由PE_{pos}线性表示。但经过点乘后$PE_{pos}W_qW_kPE_{pos+k}$，这两个值就没有相对位置信息了，两个w可以看成两个线性变化（实验得到的）。
6. 


### 多头注意力机制

1. 基础的注意力机制
   1. 自注意力机制，见Attention章节更详细的知识。
   2. 多头注意力机制。
2. 多头注意力机制
   1. 与其做一个单一的注意力函数，不如将整个query映射到低维（使用Linear）。然后使用多个注意力机制进行计算，最后再并起来，送入Linear
   2. 两个好处：
      1. 原始的单一注意力方式并没有很多可以学习的参数。可学习的参数少，说明网络的容量小。
      2. 希望不同的注意力头可以学习到不同的投影方法，去匹配不同的注意力模式。和卷积中多个核很像。
   3. 可以类比CNN中同时使用多个滤波器的作用，直观上讲，多头的注意力有助于网络捕捉到更丰富的特征/信息。

### 残差和layerNorm

1. 输入x1, x2
2. 经过self-attention得到 z1, z2
3. 进行残差 f1= x1+z1, f2=x2+z2
   1. 残差为什么起作用，公式一写就明白了。它增加了一条梯度下降的路径。缓解了梯度消失。
   2. 小括号中的1表明短路机制可以无损地传播梯度，而另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。https://zhuanlan.zhihu.com/p/31852747
4. 进行layernorm
   1. 为什么用layernorm不使用BN
      1. BN优点：
         1. 解决ICS问题（数据分布）
         2. 解决梯度消失问题
      2. BN缺点：
         1. batch小的时候效果差。
         2. 在RNN中效果比较差（句子的输入是动态的，句子长度不一致，且任意一个词都可以放在第一个位置，所以batchnorm保存的信息不够好。所以BN效果不好）
   2. LayerNorm对同一个样本内的所有值做归一化
      1. 这种归一化就学习到了句子中的分布

### 前馈神经网络

1. 接着上一步
   1. z1+Feed Forward,  z2+Feed Forward。 FeedForward又叫多层感知器。多层的FC全连接。
   2. 加上残差和归一化
2. 前馈神经网络就是全连接+非线性激活函数。然后多层累加。
3. 在实现中，就是单隐藏层的MLP。维度扩大四倍再缩小四倍。实现中，mlp是对每个词做运算得到输出。

### Decoder
1. MASK
   1. 需要对当前值和之前的值进行mask
   2. 因为预测的时候，无法知道当前值后面的值
   3. 如何做mask
      1. 在decoder端计算注意力时，仍然计算，但在做softmax前，将第t时刻以后的注意力权重设置为极大的负数。这样softmax后这个值就变为了0。
2. 交互层
   1. Encoder的输出（K，V）矩阵。是所有单词输出汇总计算k,v矩阵。这个过程和在Encoder中计算K/v矩阵是一样的。
   2. Decoder生成Q矩阵。


### 运行阶段

1. Encoder：
   1. 长度为n的句子。输入是n个词的embedding，【我，爱，机器，学习】
   2. 计算position encoding。
   3. 然后计算多头注意力机制
2. Decoder:
   1. 第二个多头注意力中，key、value来自于编码器输出，query来自上一层masked attention.
   2. masked注意力层的输出向量维度仍然是不变的
   3. decoder的每一次输入都是上一个时间状态理应的输出，第一个时间状态下，上一个时间状态使用<bos>表示。所以未【<bos >】，【<bos >, i,】,【<bos >, i,love】这样。为了方便，变为【<bos >, i,love, machine, learning】，然后在第一个attention 的时候，使用mask。
3. 词映射到向量时的这个embedding和decoder输入的embedding和输出时softmax前的embedding，这些是共享权重，减少训练难度。这个embedding的权重除以了根号d，d为512。为了和position编码相加。

### TRM面试题讲解

1. 所有Encoder和所有decoder做交互？
2. Encoder怎么生成Q矩阵的？
3. 和RNN的区别。
   1. RNN是使用上一刻的语义信息辅助当前的语义信息。
   2. Attention是关注全局的语义信息（Encoder）提取出来后，再通过mlp映射到相应语义空间。
4. position embedding？
5. 正则化
   1. 在进入残差和layernorm前都做0.1的dropout。词嵌入、输入也用dropout
   2. label smoothing，就是标签的value只用达到0.1的置信度就行。
6. 并行
   1. 6个encoder、decoder都是串行。
   2. encoder两个子模块之间是串行。子模块本身是可以并行的。
7. self-attention的Q、K为什么要乘以一个矩阵
   1. Q、K本身的点积是为了计算相似度。但原空间的相似度不够，为了使模型有更强的表现力，所以进行映射
   2. 如果Q、K保持相同。则Q、K的计算拥有自反性，即Q、K计算出来的矩阵是对称的。实际上，我是男孩，“我”对“男孩”的重要性应该低于“男孩”修饰“我”的重要性。所以乘上了矩阵
   3. 实际上reformer也提出来，Q、K可以是同一个。Q、K相乘以后再乘一个矩阵。其实还是在做线性映射。
8. 为什么attention会除以$\sqrt{d_k}$
   1. 对于输入[a, 10a, 100a]在输入的数据量级很大的时候，softmax会将几乎所有的概率分布分配给最大值对应的标签。会造成梯度消失为0，参数更新困难。
   2. 为什么使用$\sqrt{d_k}$:对于两个向量q、k，均值0，方差1。它们乘积后，均值0，方差为$d_k$。方差越大也就说明，点积的数量级越大（以越大的概率取大值）。那么一个自然的做法就是把方差稳定到1，做法是将点积除以$\sqrt{d_k}$
   3. attention两种形式，加法Add然后映射到tanh，第二种乘法Mul。Add运算中的矩阵乘法，和Attention中的矩阵乘法不同。前者中只有随机变量X和参数矩阵W相乘，但是后者中包含随机变量X和随机变量X之间的乘法。对于Mul来说，如果S和h都分布在[0,1]，在相乘时引入一次对所有位置的∑求和，整体的分布就会扩大到[0,dk], 反过来看Add,右侧是被tanh0钳位后的值，分布在[-1,1]。整体分布和dk没有关系。
   4. https://www.cnblogs.com/hongdoudou/p/12594430.html
9. Bert模型attention部分如何加速。即如何一个矩阵完成多个投影。
10. 多头运算加速
    1. 原始特征 batch, seqlen, hidden，映射成hidden/head的特征。则可以优化为：
    2. batch, head, seqlen, hidden/head. 这样可以进行多头的快速运算。
    3. 注意力的运算：
       1. hidden: B, S, Hidden
       2. Q, K, V: B, S, Hidden2
       3. Q*K=> S, Hidden2 * Hidden2, S => B, S, S
       4. Q*K*V=> S, S* S, Hidden2 = > S, Hidden2
       5. 最终C=>B, S, Hidden2
       6. 无论多少维，最后运算的时候都是二维的。
11. attention相比于lstm的优点
    1. attention将任意两个词的距离拉到了1
    2. attention的运算可以并行化。
12. 位置编码技术
    1. RPE(相对位置编码)
    2. 多头时，将key绝对位置转为相对query的位置。
    3. 复数域函数。
13. 并行化
    1. Transformer的并行化主要体现在self-attention模块，在Encoder端Transformer可以并行处理整个序列，并得到整个输入序列经过Encoder端的输出，但是rnn只能从前到后的执行
    2. Decoder 交互的时候不能并行化，其他的时候可以。
14. bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？
    1.  BERT和transformer的目标不一致，bert是语言的预训练模型，需要充分考虑上下文的关系，而transformer主要考虑句子中第i个元素与前i-1个元素的关系。Transformer主要是用来做机器翻译的。


# Bert

## 简单介绍
1. bert是用了transformer的encoder侧的网络，作为一个文本编码器，使用大规模数据进行预训练，预训练使用两个loss，一个是mask LM，遮蔽掉源端的一些字，然后根据上下文去预测这些字
   1. 可能会被问到mask的具体做法，15%概率mask词，这其中80%用[mask]替换，10%随机替换一个其他字，10%不替换，至于为什么这么做，那就得问问BERT的作者了\{捂脸}）
2. 一个是next sentence，判断两个句子是否在文章中互为上下句，然后使用了大规模的语料去预训练。
   1. roberta证明nsp这个loss并没有起到什么作用。

和GPT最大的区别在于它是双向的。

## 概要
1. 用了双向信息和Transformer来做预测。
2. 以前的预训练模型
   1. 基于特征：将学到的特征和输入一起放入网络训练。
   2. 基于微调：将训练好的权重，在下一层进行微调。
3. 使用两个任务:
   1. 带掩码的语言模型。masked language model.可以从双向看句子。
   2. 下一个句子预测。
4. 贡献：
   1. 针对于Elmo的改动，Elmo使用了双向的信息，但是使用的是RNN的老旧模型。GPT使用了Transformer的结构，但是使用的是单向的模型。所以把它们结合起来。
   2. 使用了大量无标签的数据集进行预训练，比在小批量有标签的数据集上训练效果更好。

## 模型
1. 预训练
   1. 在没有标签上的数据上进行训练
   2. 参数量
      1. 嵌入层：输入是30K，输出是隐藏层大小H。
      2. 自注意力块：多头注意力机制。
         1. 投影矩阵：H*H。投影后合并。
         2. 拿到输出concat后，再做一次投影，矩阵为H*H
      3. MLP层：H * 4H + 4H * H = 8H^2
2. 输入/输出。
   1. 输入不再是一个句子，而是一个句子对。
   2. 使用wordpiece进行切词。如果一个词根可以表示一个词，则用词根来表示词。
   3. 输入进行处理：行首添加[CLS]，两个句子中间添加[SEP]。来表示这是第一个句子还是第二个句子。 
   4. 在词送入网络时，添加一个嵌入层。
      1. token embedding层。节点、句子的嵌入
      2. segment embedding层。用来标识该词时第一个句子还是第二个句子。cls是第一个句子，sep是第二个句子。输入就是2.
      3. 位置编码。和句子长度相关。位置编码的one-hot。对应值为1. 
3. 预训练的任务
   1. MLM任务。
      1. 对于句子中的词，有15%的词会被替换为[MASK]。
      2. 为了避免预训练和微调时数据的不一致性。这15%的词中，80%被替换为[MASK], 10%的概率替换成随机的词源， 10%的概率什么也不干。
   2. 预测下一个句子
      1. 输入序列中有两个句子，50%样本就是下一个句子，50%的概率是随机选择一个句子。增大QA和语言推理中的信息。
4. 微调
   1. 要么使用对应词元做输出。
   2. 要么使用[cls]做输出。NSP任务也是用cls的embedding做预测的。
5. 实验
   1. glue：将CLS的输出拿出来接入softmax
   2. QA：对于一段话中，找出语句中的开头和结尾。
6. 微调设置
   1. 3 epoch， 学习率5e-5,  batchsize 32. 增大epoch数量。（epoch太少是没办法达到好的效果）
7. 消融实验
   1. bert微调的特征比直接使用特征进行训练效果要好。（大家都得做微调）

写作贡献：
1. 突出你的主要贡献点。
2. 分析你的贡献点带来的优点和缺点。

## bert模型详解
1. 每个词的embedding加上位置编码，加上句子编码。（并不是concat，是直接相加）
2. 乘以Q、K、V三个矩阵得到Q, K, V
3. 使用一个词的Q对其他的key进行计算，计算的结果和V相加得到当前词的注意力信息。
4. Transformer中的一个encoder对应Bert中的一个Trm。（输入是所有句子。输出是当前词的注意力特征）。（实际上可以理解一层的encoder是共享的）bert会使用基于所有层中的左右两侧的语句信息。
5. bert可以调节的参数 L, H, A. L是网络的层数，Transformer block的数量。A是多头的数量。filter是


## roberta的改进
1. 使用更长的训练时间、  100K-》500K steps
2. 更大的batch、 从256增加到了8K
3. 更多的数据
4. 移除了NSP
   1. 使用了全句子，不跨文档。
5. 序列更长
6. 动态调整mask。 
   1. 原本的bert使用静态mask。
   2. 定义了dupe_factor，每个数据复制dupe_factor份，拥有不同的mask，在不同的epoch中使用。
   3. 第三种动态：每次训练的时候，才会随机进行mask。
7. 文字编码
   1. 使用bytes-level：使用bytes而不是unicode作为subword的基本单位。
   2. 与wordpiece的区别：
      1. wordpiece选择提升语言模型概率最大的相邻子词加入此表
      2. BPE选择聘书最高的相邻子词合并。
   3. 好处不会出现unknown，但会增加数据集大小。


## 词构造算法
1. BPE(roberta使用的)
   1. 步骤：
      1. 准备足够大的训练语料，并确定期望的Subword词表大小；
      2. 将单词拆分为成最小单元。比如英文中26个字母加上各种符号，这些作为初始词表；
      3. 在语料上统计单词内相邻单元对的频数，选取频数最高的单元对合并成新的Subword单元；每次合并后词表大小可能出现3种变化：
         1. +1，表明加入合并后的新子词，同时原来的2个子词还保留（2个字词分开出现在语料中）。
         2. +0，表明加入合并后的新子词，同时原来的2个子词中一个保留，一个被消解（一个子词完全随着另一个子词的出现而紧跟着出现）。
         3. -1，表明加入合并后的新子词，同时原来的2个子词都被消解（2个字词同时连续出现）。
      4. 重复第3步直到达到第1步设定的Subword词表大小或下一个最高频数为1.
   2. 编码
      1. 得到Subword词表后，针对每一个单词，我们可以采用如下的方式来进行编码：
      2. 将词典中的所有子词按照长度由大到小进行排序；
      3. 对于单词w，依次遍历排好序的词典。查看当前子词是否是该单词的子字符串，如果是，则输出当前子词，并对剩余单词字符串继续匹配。
      4. 如果遍历完字典后，仍然有子字符串没有匹配，则将剩余字符串替换为特殊符号输出，如”<unk>”。
      5. 单词的表示即为上述所有输出子词。
   3. 解码
      1. 解码过程比较简单，如果相邻子词间没有中止符，则将两子词直接拼接，否则两子词之间添加分隔符。
2. wordpiece（bert使用的）
   1. 与BPE算法类似，WordPiece算法也是每次从词表中选出两个子词合并成新的子词。与BPE的最大区别在于，如何选择两个子词进行合并：BPE选择频数最高的相邻子词合并（构造词阶段），而WordPiece选择能够提升语言模型概率最大的相邻子词加入词表。
   2. 方法：
      1. 计算两个词的互信息量，让预料中以相邻方式同时出现的子词进行合并。
3. ULM（Unigram Language Model）
   1. ULM是减量法,即先初始化一个大词表，根据评估准则不断丢弃词表，直到满足限定条件。
   2. 

上述三种方法的使用
如何使用上述子词算法？一种简便的方法是使用SentencePiece。

SentencePiece还能支持字符和词级别的分词。更进一步，为了能够处理多语言问题，sentencePiece将句子视为Unicode编码序列，从而子词算法不用依赖于语言的表示。

## 参考资料

1. https://www.cnblogs.com/ffjsls/p/12257158.html
2. 词构造算法 https://zhuanlan.zhihu.com/p/198964217
3. https://zhuanlan.zhihu.com/p/363466672 trm面试题
4. https://zhuanlan.zhihu.com/p/151412524 Bert面试题
5. https://zhuanlan.zhihu.com/p/95594311 Bert面试题
6. trm模型图 https://zhuanlan.zhihu.com/p/44121378