---
title: "0 引言"
date: 2021-10-19T08:17:36+08:00
tags : [

]
categories : [

]
series : []
aliases : []
draft: false
---

# 引言

1. 计算机可以处理形式化的语言。人工智能的一大难题就是将如何将非形式化的传达给计算机。
2. 人工智能AI系统需要具备自己获取知识的能力，即从原始数据中提取模式的能力，这种高能力被称为机器学习。
3. 简单机器学习的性能很大程度上取决于给定数据的表示，表示数据的选择对机器学习性能产生很大的影响。例子：笛卡尔坐标系和极坐标系下的数据表示会有不同的划分途径。p3
4. 使用机器学习发掘表示本身，称为表示学习(representation learning)，经典例子：AutoEncoder。
5. 设计特征、设计用于学习的特征的算法时，目标通常是分理处额能解释观察数据的变差因素(factors of variation)，这些因素时不可观察到的量或者力，会影响到可观测的量。人工智能中的困难主要源于多个变差因素同时影响着我们能观察到的数据。
6. 从原始数据提取高层次、抽象的特征是比较困难的。深度学习(deep learning)通过其他比较简单的表示来表示复杂表示。解决了表示学习中的核心问题。输入展示在可见层，一系列从图像中提取到的越来越多的特征叫隐藏层。例子：多层感知机。
7. 深度学习两个度量视角，一个是提取数据的正确表示。另一个是多步骤的计算机程序，多顺序指令的执行帮助计算机理解输入。
8. 维恩图：deeplearning->Represenation learning->Machine Learning->AI
9. 深度学习的趋势。最早有控制论、联结主义。控制论很类似傅里叶变换，使用不同的函数和不同的参数来表示一个函数。
   1. 第一次高潮、退潮
       1. 控制论：使用一组n个输入，并将它们与一个输出y相关联。这个模型希望学习到一个权重。使得$f(x,w)=x_1w_1+...+x_nw_n$。显然模型需要正确设置权重之后才能使用。
       2. 感知机是第一个能根据每个类别的输入样本来学习权重的模型。
       3. 但诸如上面所讲的线性模型，无法学习最简单的异或。这引起了神经网络的第一次退潮。

   2. 第二次高潮退潮
      1. 神经科学被认为是深度学习的一个重要灵感来源。但已经不再是该领域的主要指导。我们连大脑最简单的部分还远没有理解。
      2. 小故事：将雪貂的视觉神经和大脑听觉区域相连，它们可以学会取用听觉区域去“看”。这暗示这大多数的哺乳动物的大脑能够使用单一的算法就可以解决其大脑可以解决的大部分不同的任务。拥有这个假设，深度学习团队同时研究多个领域是很常见的。
      3. 新认知机受哺乳动物的视觉系统结构的启发，后来成为CNN的基础。目前大多数神经网络都基于整流线性单元模型。
      4. 神经科学是神经网络的重要灵感来源。但线代深度学习从许多其他领域获取灵感：线性代数、信息论、数值优化等等。
      5. 第二次浪潮：联结主义、并行分布处理。联结主义思想：当网络将大量的简单的饿计算单元连接在一起时可以实现智能行为。联结主义重要的成就就是反向传播算法的普及。
      6. 分布式表示：每一个输入都应该由多个特征表示，并且每一个特征都应该参与到多个可能的输入的表示。
   3. 第三次浪潮：
      1. 深度信念网络使用一种贪婪逐层预训练的策略来有效的训练。同样的策略可以用来训练其他许多类型的深度网络。深度学习的第三次浪潮才开始。
      2. 虽然已开始着眼于无监督学习技术，但目前更多的兴趣点仍是比较传统的监督学习算法和深度模型充分利用大型标注数据的能力。
10. 与日俱增的数据量，
11. 联结主义的主要见解之一是，当动物的需多神经元在一起工作的时候，会变得聪明。单独神经元或者小集合的神经元不是特别有用。几十年内，我们的机器学习模型中的每个神经元的连接数量已经和哺乳动物的大脑在同一数量级上。自从引入隐藏层后，神经元数量每2.4年增加一倍。21世纪50年代，人工智能的神经元可以达到与人脑相同的数量级。目前的网络，相对原始的脊椎动物如青蛙还小。
12. 强化学习，在没有人类指导的情况下，通过试错来学习执行任务。深度学习爷显著改善了机器人强化学习的性能。



# 符号表

1. $\odot$ hadamard乘积。 逐元素相乘