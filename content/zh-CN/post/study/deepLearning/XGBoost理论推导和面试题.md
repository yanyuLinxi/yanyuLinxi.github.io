---
title: "XGBoost理论推导和面试题"
date: 2022-02-17T16:33:05+08:00
tags : [

]
categories : [

]
series : []
aliases : []
draft: false
---

停止增长的条件：
1. 叶子节点的分裂gain<=0
2. 叶子节点只有一个样本的时候，就没必要划分了。
3. 限制叶子节点个数

计算出叶子节点的b_j。计算完后叶子节点的权重就为b_j

步骤：
1. 

公式：https://zhuanlan.zhihu.com/p/46683728
1. $$f ( x ) = \sum _ { j = 1 } ^ { J } b _ { j } I ( x \in R _ { j } )$$
2. 将所有的样本加起来可以表示为：
$$\sum _ { i = 1 } ^ { N } f ( x _ { i } ) = \sum _ { j = 1 } ^ { J } \sum _ { x\in R_j} b _ { j }$$
3. 所以公式推导为：
$$L ^ { m } = \sum _ { i = 1 } ^ { N }  [g _ { i } f _ { m } ( x _ { i } ) + \frac { 1 } { 2 } h _ { i } f_m^2 ( x _ { i } )] + \gamma J  + \frac { 1 } { 2 } \lambda \sum_{j=1}^Jb_j^2$$

xgboost的训练方式和gbdt类似。计算m-1 f(x)的损失函数在训练样本点的一阶导、二阶导。然后用来分裂每个节点，生成树。最后加到模型中去f_{m-1}(x)+f_m(x)。再计算损失函数，拟合下一个。

# 优化

优化思路：1. 压缩特征数（列采样） 2. 压缩每个特征下，特征的种类数（即特征的分裂点的数量）

1. 预防过拟合
   1. 正则化
   2. 限制节点数量
   3. 限制Gain的增长。当Gain < gamma则不分裂
   4. 列采样。随机采样特征
      1. 按树随机。每一颗树筛选出来特征采样后，一整颗树的特征都不变
      2. 按层随机。每一层叶子节点分裂时，都进行采样。
   5. shrinkage 学习率
      1. 加入学习率到基学习器中。$y=y_{m-1}+\eta f(x)$
      2. 有助于防止过拟合。一般取0.1.
2. 速度优化：
   1. 近似分裂算法：
      1. 使用二阶导代替样本的权重，然后根据权重划分桶。设定一个值$\epsilon$,每个桶的权重值和应该小于$\epsilon$。会得到$\frac{1}{\epsilon}$个切分点。
      2. 为什么使用二阶导？二阶导可以看成是样本的权重。同时二阶导越大，说明该值附近变换越快，越应该进行切分。
      3. 和采样类似。有全局策略和局部策略。
         1. 全局策略。第一次计算得到的分裂点，后面的分裂只会用这几个分裂点。
         2. 局部策略。每次分裂节点时，都会重新计算分裂点。优点是可以根据节点中的样本数计算分裂点。
   2. 系统设计
      1. 核外块运算：
         1. 块拆分。将不能存储在内存的数据放在磁盘中。并且拆分成块，以提高磁盘IO率。
         2. 块压缩。每个块存储时会压缩存储。读取时解压读取。
         3. 开启线程在运算数据的同时对数据进行读取。
      2. 分块并行。
         1. 预先对所有的特征进行按列预排序，并存储索引。这样每次分裂只需要一次线性扫描就能获得最大的Gain
         2. 分布式运行：将所有的特征分成N各块，发送给各个机器执行。最后将执行结果返回给调度中心。
      3. 缓存命中率优化。由于特征存储为block结构。所以访问不是顺序访问的。为此在block后面开一个buffer，存储他们的一阶导和二阶导。直接读取一阶导和二阶导。这样命中率就会提高。
3. 对缺失值的处理：
   1. 所有的计算对非缺失值进行计算。计算完成后，缺失值整体放入左右，计算增益。设置最大增益方向为缺省（默认）方向放入缺失值。
   2. 预测中遇到缺失值，默认放入右节点。


# 题目

参考：https://mp.weixin.qq.com/s?__biz=MzI1MzY0MzE4Mg==&mid=2247485159&idx=1&sn=d429aac8370ca5127e1e786995d4e8ec&chksm=e9d01626dea79f30043ab80652c4a859760c1ebc0d602e58e13490bf525ad7608a9610495b3d&scene=21#wechat_redirect

https://github.com/datawhalechina/daily-interview/blob/master/AI%E7%AE%97%E6%B3%95/machine-learning/XGBoost.md


1. xgboost优缺点：
   1. XGBoost是大规模、分布式的通用梯度提升(GBDT)库。它是一个加法模型，每次迭代都学习一棵CART树来拟合之前 t-1 棵树的预测结果与训练样本真实值的残差。XGBoost对GBDT进行了一系列优化，比如损失函数进行了二阶泰勒展开、目标函数加入正则项、支持并行、默认缺失值处理等，在可扩展性和训练速度上有了巨大的提升，但其核心思想没有大的变化。
   2. 优点：
      1. 精度更高、灵活性更好（不同的基分类器，不同的损失函数）、正则化、缺失值、训练速度快（可并行）
   3. 缺点
      1. 和lgbm相比，节点分裂过程中需要遍历所有特征
      2. 存储空间消耗较大。
2. XGBoost特征重要性。
   1. 三种方式判断特征重要性。
   2. freq：特定的特征在模型树种发生分裂的次数的百分比
   3. gain：平均增益。特征在所有树种作为分裂节点的信息增益之和除以该特征出现的频次。
   4. cover：节点样本的二阶导数和处理特征出现的频次。平均二阶导数和。二阶导和越大，说明该节点对方向做出了较多的指导。
   5. xgboost一般采用gain。


3. 介绍一下XGB，怎么调参防止过拟合
   1. 决策树方面：
      1. 决策树节点最小样本权重和的阈值（这里指的是二阶导，官网api这么定义的）
      2. 树的最大深度
      3. 树节点的分裂阈值（增益阈值）
   2. 公式
      1. 正则化。目标函数添加正则化项。调整正则化系数
      2. 调整学习率 shrinkage
   3. 数据方面：
      1. 子采样
      2. 列抽样

   第一类参数：用于直接控制模型的复杂度。包括max_depth，min_child_weight，gamma 等参数
   第二类参数：用于增加随机性，从而使得模型在训练时对于噪音不敏感。包括subsample，colsample_by树
   还有就是直接减小learning rate，但需要同时增加estimator 参数。

4. 比较LR和GBDT，说说什么情景下GBDT不如LR
   1. 在特征高维稀疏的情况下。逻辑回归线性模型控制权重。xgboost正则化控制树的大小。此时xboost只需要一个节点就能分类所有样本。正则化的作用很小。但是逻辑回归的正则化就不容易过拟合

5. xgboost如何选择分裂点
   1. 对特征值进行排序，存储为block结构。方便重复使用
   2. 使用近似分裂算法，根据样本的二阶导选择常数个分裂点。
   3. 采用特征并行的方法利用多个线程分别计算每个特征的最佳分裂点。进行分裂。

6. xgboost缺失值
   1. 默认使用不缺失的样本进行分裂
   2. 缺失样本整体放入左右节点，最大增益节点就为默认的分裂方向
   3. 预测值中出现没见过的缺失值，则放入右节点

7. xgboost不平衡数据
   1. 为正负样本设置不同权重。scale_pos_weight参数

8. xgboost 为什么快？
   1. 四点
   2. 特征预排序+block缓存，多线程并行
   3. 核外块运算
   4. 近似分裂算法
   5. 命中缓存优化