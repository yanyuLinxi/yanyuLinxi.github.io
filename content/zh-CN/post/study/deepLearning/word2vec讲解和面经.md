---
title: "Word2vec讲解和面经"
date: 2022-02-26T16:10:37+08:00
tags : [

]
categories : [

]
series : []
aliases : []
draft: false
---

# word2vec 简介

讲的非常好：https://zhuanlan.zhihu.com/p/27234078
负采样：https://www.cnblogs.com/pinard/p/7249903.html

## 介绍
Word2Vec是从大量文本语料中以无监督的方式学习语义知识的一种模型
通过一个嵌入空间使得语义上相似的单词在该空间内距离很近。

它的输入都是one-hot编码，网络层包括两层隐藏层来学习一个滑动窗口内单词的共现关系。输出层是一个概率分布。

## 详细定义

word2vec是一个3层的神经网络。输入层和输出层都可以看做词汇表的one-hot表示

CBOW: 每个滑动窗口是一个测试用例。输入 层V_1, V_m ，输出层 V_c 。即用中心词 的上下文(不包含 V_c )为输入，最大化预测输出为中心词 V_c的概率。

Skip-Gram: 每个滑动窗口是w个测试用例。输入层 V_c ,输出层 V_i 。即用中心词，最大化预测输出层为上下文词汇的概率。

$$Z ^ { [ 1 ] } = \sum _ { i } ^ { 4 } W _ { i * } x _ { i } + b _ { i }$$

$$h = A ^ { [ 1 ] } = \frac { 1 } { 4 } \sum _ { i } z _ { i } ^ { [ 1 ] }$$

$$Z ^ { [ 2 ] } = h * W ^ { \prime }$$

$$\hat{y} = a = softmax ( Z ^ { [ 2 ] } ) = \frac { e ^ { [ 2 ] } } { \sum _ { k } ^ { [ 2 ] } }$$

再与 W'乘积后softmax归一化。因为输出层词汇的one-hot可以对应 y=[0,0..1..0] ，对于softmax输出一般采用最小化负的交叉熵的似然: ,然后使用梯度下降即可更新。

损失函数
$$E = - \log  p ( W _ { 0 } | W _ { 1 } ) = - \log \frac { e x p ( u _j ) } { \sum _ { k \in V } e x p ( u _ { k } ) } = \log \sum _ { k \in v } exp(u_k)-u_j$$

这里的j是目标词真实的下标。u_j表示第j个词就是目标词的可能性。就是使目标次出现频率最高的概率的乘积。就是极大似然估计。

CBOW具体前向传播步骤：
1. 输入层: 输入C个单词x： x1k,⋯,xCk，并且每个 x 都是用 One-hot 编码表示，每一个 x 的**维度为 V**（词表长度）。
2. 输入层到隐层
   1. 首先，共享矩阵为 W_{V×N} ，V表示词表长度，W的每一行表示的就是一个N维的向量（训练结束后，W的每一行就表示一个词的词向量）。
   2. 然后，我们把所有输入的词转x化为对应词向量，然后取平均值，这样我们就得到了隐层输出值 ( 注意，隐层中无激活函数，也就是说这里是线性组合)。 其中，隐层输出 h 是一个N维的向量 。
   3. $h=\frac{1}{C}W^T(x1+x2+⋯+xc)$
3. 隐层到输出层：隐层的输出为N维向量 h ， 隐层到输出层的权重矩阵为 W′N×V 。然后，通过矩阵运算我们得到一个 V×1 维向量u=W′T∗h
4. 其中，向量 u 的第 i 行表示词汇表中第 i 个词的可能性，然后我们的目的就是取可能性最高的那个词。因此，在最后的输出层是一个softmax 层获取分数最高的词，那么就有我们的最终输出：$P(wj|context)=yi=\frac{exp(uj)}{\sum_{k∈V}exp(uk)}$


Skip-gram:
Skip-Gram与CBOW的方法略有不同，每个词对（中心词，上下文词）都是一个训练样本

## 步骤
skip-gram处理步骤：

1.确定窗口大小window，对每个词生成2*window个训练样本，(i, i-window)，(i, i-window+1)，...，(i, i+window-1)，(i, i+window)

2.确定batch_size，注意batch_size的大小必须是2*window的整数倍，这确保每个batch包含了一个词汇对应的所有样本

3.训练算法有两种：层次Softmax和Negative Sampling

4.神经网络迭代训练一定次数，得到输入层到隐藏层的参数矩阵，矩阵中每一行的转置即是对应词的词向量



### 损失函数
$$E= - log p(w_1, w_2, \cdots, w_C | w_I) \\ = - log \prod_{c=1}^C P(w_c|w_i) \\ = - log \prod_{c=1}^{C} \frac{exp(u_{c, j})}{\sum_{k=1}^{V} exp(u_{c,k}) } \\ = - \sum_{c=1}^C u_{j,c} + C \cdot log \sum_{k=1}^{V} exp(u_k)$$


## 层次softmax， 哈夫曼树。

https://zhuanlan.zhihu.com/p/59396559

https://zhuanlan.zhihu.com/p/56139075

层次Softmax是一个二叉树结构，每个**非叶子节点是个二分类器**，每个**叶子节点对应词汇表中的单词**（ N个词汇，N-1 个二分类器）二分类器是逻辑回归。

二叉树是根据**词频**构建的哈夫曼树，词频越大的叶子节点距离根节点的路径越短。

softmax层的求和项被一系列二分类器代替，如果当前节点到目标叶子节点往左走，该节点的输出为二分类器的输出$\theta$ ,否则输出为$1-\theta$ 。最终输出为各个二分类器节点输出的乘积。$o ( h * W _ { 1 } ) * ( 1 - o ( h * W _ { 2 } ) ) * ( 1 - o ( h * W _ { 3 } ) )$.我们期望最大化上述路径乘积，即损失Cost 为最小化其负的log梯度。

所以层次Softmax相当于自动完成了归一化操作，故与普通Softmax可以等价

采用Softmax之后，需要需要更新参数的数量从 N降低到 logN ，对于一个大语料来说，这个改进可以说很大了。只有路径上的权重进行了更新。所以更新速度较快。

### 损失函数

$l^w$项连乘，即从根节点到叶子节点一功有$l^w$个节点，求负对数后，变成连加。

$$L ( w , j ) = ( 1 - d _ { j } ) \cdot \log [ \delta ( x _ { i v } w _ { j - 1 } ) ] + d _ { j } ^ { w } \cdot \log [ 1 - \delta ( xw ) ]$$
就是按照词频构建二叉树，然后每一个非叶子节点就是而分类器，输出$\theta$,往左走输出$\theta$,往右走输出$1-\theta$，$d_j$表示向左走，即为0， $d_j$表示向右走，表示值为1.

最终计算出来的梯度信息是上下文单词的梯度信息，word2vec直接将梯度信息应用到每个窗口单词上面去。

## Negative Sampling(负采样)
1. 负采样每次让一个训练样本仅仅更新一小部分的权重，这样就会降低梯度下降过程中的计算量。
2. 当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新
3. 采用二元逻辑回归来求解模型参数。通过负采样，得到neg个负例。接下来就是采用一个正例和negative负例，来使用逻辑回归求解
   1. $f = w_1x+b$
   2. $f_2 = w_2f+b$
   3. $p = sigmoid(f_2) = \frac{1}{1+e^{-f_2}}$,这是正例的概率，负例的概率就是1-p。然后对$w_1, w_2$进行梯度更新
   4. 损失函数为$L=\sum y_i log(\theta(xw))+(1-y_i)log(1-\theta(xw))$
4. word2vec中负采样
   1. 高频词采样率大，低频词采样率少。
   2. 公式：$p=\frac{f(w)^{\frac{3}{4}}}{\sum f(w)^{\frac{3}{4}}}$, f(w)为单词出现频率。
   3. 长度为1的线段，词频越大，则词对应的长度越大。
   4. 最后线段等份成m份。取neg个位置，每个位置对应一个词
5. cbow 负采样
   1. 上下文分别预测窗口词的平均去预测对应词和负采样的词
6. skip-gram负采样
   1. 分别使用当前窗口词去预测对应词和负采样的词。


## word2vec
输入是one-hot向量，B，N。N是字典大小。这里的N不是特征大小，如果是NLP的话，就是字典大小。就是整个集和中不重复样本数的个数。映射成one-hot向量
输出是经过两层隐藏层后，输出也是字典大小的向量，表示概率。


## word2vec优化
1. 输入的时候不会相乘，而是“查表”
2. word2vec在学习词向量之间的共现关系
3. 优化
   1. 对常见单词组合或词组作为单个words来处理
   2. 对高频词进行抽样减少训练样本个数
      1. 对于我们在训练原始文本中遇到的每一个单词，它们都有一定概率被我们从文本中删掉，而这个被删除的概率与单词的频率有关。
      2. 出现频率越高，越容易被删除
   3. 使用negative sample，这样每个训练样本只会更新一小部分权重。


## 面试题

1. word2vec看起来跟自动编码解码器很像，它们两者有什么关联吗？
word2vec和自编码器的差别。
word2vec是最大化共现概率。
自编码器是最小化重构误差。

2. cbow和skip-gram哪个更好
参数完全相同的情况下，大语料库cbow好。
这里使用skip-gram是速度更慢一些，但效果更好一点，因为中心词训练的次数更多。
skip-gram训练次数更多。所以学到的东西理应更多。

3. word2vec中softmax的优化
   1. softmax时间消耗主要在指数计算上，指数计算可以采用多项式逼近的方法，还可以采用查表法计算。将输入的值限制到一个范围内，这样查表就能得到高精度值。

4. softmax求导
   1. $e^{z_j}$为第j个词的概率。对输入z_j求偏导为1，其余的$z_k$对z_j求偏导为0
   2. a = softmax(z), a对z求偏导后=a(1-a)

$E = - log , p(w_1, w_2, \cdots, w_C | w_I) \\ = - log \prod_{c=1}^C P(w_c|w_i) \\ = - log \prod_{c=1}^{C} \frac{exp(u_{c, j})}{\sum_{k=1}^{V} exp(u_{c,k}) } \\ = - \sum_{c=1}^C u_{j^c} + C \cdot log \sum{k=1}^{V} exp(u_k)$



# 随机游走
https://zhuanlan.zhihu.com/p/66836312

1. DeepWalk

DeepWalk提出了“随机游走”的思想，这个思想有点类似搜索算法中的DFS，从某一点出发，以深搜的方式获得一个节点序列。

这个序列即可以用来描述节点。

2. node2vec

node2vec的作者针对DeepWalk不能用到带权图上的问题，提出了概率游走的策略

在一个图中，假设上一步走到的节点是t ，当前处于节点 v ，则下一步游走的概率为$\alpha$满足

$$\alpha = \{ \begin{array}  { l  }  { \frac { 1 } { p } \quad d = 0 } \\ { 1 \quad d = 1 } \\ { \frac { 1 } { q } \quad d = 2 } \end{array}$$

d = 0 表示上一步走过的节点。d=1表示与节点t和节点v距离相同的节点（距离都为1）。d=2表示其他节点。

使用p和q控制了游走的宽度优先和深度优先。在调参的效果下可以取得比word2vec更好的效果

第一个节点由于没有上一个节点，所以直接选取游走节点。

3. metapath2vec

应用于异质图。异质网络比同质网络包含了更多的语义信息，如果仅仅使用DeepWalk和node2vec这类方法，只能将不同的节点进行不同编号，并进行随机游走。但是这种方式忽略了节点的**类型信息**，而将作者、论文、会议都视作同一类型的节点，丢失了大量语义信息。

元路径可以理解为预定义的节点类型序列，比如APV表示【作者（Author）-论文（Paper）-期刊（Venue）】

通过设计首尾类型相同的元路径，我们可以不断地重复基于相同元路径的游走

