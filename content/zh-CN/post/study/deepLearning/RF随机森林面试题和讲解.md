---
title: "RF随机森林面试题和讲解"
date: 2022-03-03T10:41:13+08:00
tags : [

]
categories : [

]
series : []
aliases : []
draft: false
---

# 随机森林 Random Forest
一种基于树模型的Bagging的优化版本，一棵树的生成肯定还是不如多棵树，因此就有了随机森林，解决决策树泛化能力弱的特点。

1. 描述：
   1. 多次随机取样，多次随机取属性，选取最优分割点，构建多个(CART)分类器，投票表决

## 算法流程
1. 输入为样本集D=(x，y1)，(x2，y2)…(xm，ym)，弱分类器迭代次数T。
2. 对于t=1，2…T
   1. 对训练集进行第t次随机有放回采样，共采集m次，得到包含m个样本的采样集Dt
   2. 用**采样集Dt训练第t个决策树模型Gt(x)**，在训练决策树模型的节点的时候，**在节点上所有的样本特征M中选择一部分样本特征m, 使得m<<M**，在这些随机选择的部分样本特征中**选择一个最优的特征**来做决策树的左右子树划分
3. 如果是分类算法预测，则T个弱学习器投出**最多票数的类别**或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的**回归结果进行算术平**均得到的值为最终的模型输出。

## 面试题

1. 优缺点：
   1. 优点：
      1. 训练可以高度并行化，对于大数据时代的大样本训练速度有优势（并行速度快）
      2. 由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型（在样本维度高，存在缺失值的情况下能维持高效）
      3. 对部分特征缺失不敏感，如果有很大一部分的特征遗失，仍可以维持准确度。
      4. 由于采用了随机采样，训练出的模型的方差小，泛化能力强。（引入随机性，泛化能力强）
      5. 在训练后，可以给出各个特征对于输出的重要性（可以给出特征重要性）
      6. 相对于Boosting系列的Adaboost和GBDT， RF实现比较简单。（实现简单）
      7. 非常稳定。即使数据中出现了一个新的数据点，也只会对部分决策树，很难影响所有决策树
      8. 总结3个：实现简单，并行速度快，引入随机性泛化性能好（减少方差，减少确实特征影响），非常稳定
   2. 缺点：
      1. 数据小、特征少的情况下，不一定取得好的效果。
      2. 在某些噪音比较大的样本集上，RF模型容易陷入过拟合。（观测现象）
      3. 取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。
2. 随机森林的随机性体现在哪里？
   1. 多次有放回的随机取样，多次随机选择特征。引入了随机性，提升了泛化性能。
3. RF为什么不容易过拟合
   1. 随机森林中的每一颗树都是过拟合的，拟合到非常小的细节上
   2. 随机森林通过引入随机性，使每一颗树拟合的细节不同
   3. 所有树组合在一起，过拟合的部分就会自动被消除掉。
4. 为什么RF的训练效率优于bagging？
   1. 因为在个体决策树的构建过程中，Bagging使用的是“确定型”决策树，bagging在选择划分属性时要对每棵树是对所有特征进行考察；而随机森林仅仅考虑一个特征子集。仅仅考虑特征子集，所以效率高。
5. 随机森林需要剪枝吗？
   1. 不需要，后剪枝是为了避免过拟合，随机森林随机选择变量与树的数量，已经避免了过拟合
6. RF与 GBDT 的区别
   1. 随机森林将多棵决策树的结果进行投票后得到最终的结果，对不同的树的训练结果也没有做进一步的优化提升，将其称为Bagging算法。
   2. GBDT用到的是Boosting算法，在迭代的每一步构建弱学习器弥补原有模型的不足。GBDT中的Gradient Boost就是通过每次迭代的时候构建一个沿梯度下降最快的方向的学习器。
7. RF与Adaboost区别：
   1. 相同之处
      1. 二者都是Bootstrap自助法选取样本。
      2. 二者都是要训练很多棵决策树。
   2. 不同之处
      1. Adaboost是基于Boosting的算法，随机森林是基于Bagging的算法。（算法上不同
      2. Adaboost后面树的训练，其在变量抽样选取的时候，对于上一棵树分错的样本，抽中的概率会加大。（样本选择上不同）
      3. 在预测新数据时，Adaboost中所有的树加权投票来决定因变量的预测值，每棵树的权重和错误率有关；随机森林按照所有树中少数服从多数树的分类值来决定因变量的预测值（或者求取树预测的平均值）。（预测形式上不同）
8. RF参数调整：
   1. **n_estimators:**随机森林建立子树的数量。 较多的子树一般可以让模型有更好的性能，但同时让你的代码变慢。需要选择最佳的随机森林子树数量
   2. **max_features：**随机森林允许单个决策树使用特征的最大数量。 增加max_features一般能提高模型的性能，因为在每个节点上，我们有更多的选择可以考虑。
   3. max_depth： 决策树最大深度
   4. **min_samples_split：**内部节点再划分所需最小样本数 内部节点再划分所需最小样本数
   5. min_samples_leaf： 叶子节点最少样本
   6. max_leaf_nodes： 最大叶子节点数
   7. min_impurity_split： 节点划分最小不纯度 
   8. RF自己的参数，主要是前两个决定树的数量和最大特征数。后面的都是基学习器决策树的参数。
9.  特征重要性
   9. 袋外数据(OOB)： 大约有1/3的训练实例没有参与第k棵树的生成，它们称为第k棵树的**袋外数据**样本。
   10. 在随机森林中某个特征X的重要性的计算方法如下：
      1. 对于随机森林中的每一颗决策树，使用相应的OOB(袋外数据)来计算它的**袋外数据误差**，记为errOOB1。
      2. 随机地对袋外数据OOB**所有样本的特征X加入噪声干扰**(就可以随机的改变样本在特征X处的值)，再次计算它的**袋外数据误差**，记为errOOB2。
      3. 假设随机森林中有N棵树，那么对于**特征X的重要性为(errOOB2−errOOB1/N)**，之所以可以用这个表达式来作为相应特征的重要性的度量值是因为：若给某个特征随机加入噪声之后，袋外的准确率大幅度降低，则说明这个特征对于样本的分类结果影响很大，也就是说它的重要程度比较高。