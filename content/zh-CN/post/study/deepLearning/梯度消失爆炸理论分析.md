---
title: "梯度消失爆炸理论分析"
date: 2022-03-05T20:47:34+08:00
tags : [

]
categories : [

]
series : []
aliases : []
draft: false
---

# 梯度消失和爆炸

资料：https://zhuanlan.zhihu.com/p/33006526

由于链式法则和反向传播，在反向传播过程中需要对激活函数进行求导，如果导数大于1，那么随着网络层数的增加梯度更新将会朝着指数爆炸的方式增加，就是梯度爆炸。

举个例子：
$f_{i+1}=g(f_i*w_i+b_i)$, g为激活函数，在进行多层之后，我们进行BP反向传播：$w=w-\alpha \nabla w$, $\nabla w = \frac{\partial f_4}{\partial f_3}\frac{\partial f_3}{\partial f_2}\frac{\partial f_2}{\partial f_1}*x$，其中$\frac{\partial f_2}{\partial f_1}$就是激活函数g，这个如果求导之后大于1，则这个梯度会越来越大，造成梯度爆炸，小于1，这个会越来越小。造成梯度消失。

## 解决方案
1. 预训练+微调。逐层预训练。避免了长距离的反向传播。然后再微调
2. 梯度剪切。当梯度爆炸超出阈值的时候，进行剪切，限制在一定范围内
3. 使用其他的激活函数：relu, leakrelu激活函数，他们求偏导后值是1，这样就不能存在梯度消失、爆炸了。但缺点是，当值为负的时候，梯度为0。整条线都不会训练。且输出分布不是以0为中心的。
   1. 不以0为中心的坏处：https://liam.page/2018/04/17/zero-centered-active-function/
4. 使用batchnorm。每一层将输入限制在一定范围内，就避免了消失或者爆炸的问题。整个链式法则中包括输入x和激活函数，当激活函数位于不敏感区域的时候，梯度下降比较困难。batchnorm将输入归一化到[0,1]附近，将梯度从饱和区拉到了非饱和区。梯度更新就会更加容易。避免了梯度消失或者爆炸。
5. 使用新的模型结构，长短时记忆网络，GRU，残差结构
6. 总结五个：从避免长链式法则、限制梯度范围，限制梯度消失原因（激活函数求导大于1或小于1）、限制输入、新模型五个方面解决。

# 反向传播的流程

## 前向传播
定义一层网络
z = wx+b
f = g(z)
多层网络前向传播计算得到了最终的输出$f_n$。

则多层网络之后,计算误差
L=Loss(f_n,y)
然后求每一层的w。
比如四层网络求第2层的梯度。
$\frac{\partial f_4}{\partial z_3}\frac{\partial z_3}{\partial f_2}\frac{\partial f_2}{\partial z_2}\frac{\partial z_2}{\partial w_2}$
这样可以看出来，前面的部分值是可以复用的。所以链式法则减少了很多的运算量。

同时也可以看出这里的$\frac{\partial z_3}{\partial f_2}=w_2$和激活函数的导数都参与了运算。所以可以解释梯度消失和梯度爆炸的原因。

反向传播是损失函数对当前层的权重求导。
然后求下一层的梯度的时候，可以用到本层计算过的部分值。就是反向传播的流程。