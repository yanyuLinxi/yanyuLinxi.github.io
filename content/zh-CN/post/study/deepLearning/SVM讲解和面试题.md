---
title: "SVM讲解"
date: 2021-12-03T19:08:27+08:00
tags : [

]
categories : [

]
series : []
aliases : []
draft: false
---

# SVM

https://www.bilibili.com/video/BV1Y7411P7nd
讲的非常好。

概念:
1. **超平面**（最优分割面）能使支持向量和分割线最小距离最大的平面。
2. 点到直线距离=$\frac{Ax+By+b}{\sqrt{A^2+B^2}}$。 直线到直线的距离=$\frac{b1-b2}{\sqrt{A^2+B^2}}$
3. 正确分类样本点：$y_i(w^Tx_i+b)>=1, i=1,2,...,m$。$y_i$取+1，-1，

从二维扩展到多维空间中时，将 D0和D1完全正确地划分开的  就成了一个超平面。为了使这个**超平面更具鲁棒性**，我们会去找最佳超平面，以**最大间隔把两类样本分开的超平面**，也称之为最大间隔超平面。两类样本分别分割在该超平面的两侧；**两侧距离超平面最近的样本点到超平面的距离被最大化了。**

样本中距离超平面最近的一些点，这些点叫做**支持向量。**

在决定最佳超平面时只有**支持向量**起作用，而其他数据点并不起作用

**目标函数**：
在这个条件下 $yi(W^T x_i +b )>= 1$ 使得 $d = \frac{|W^T x +b|}{||w||}$最大，就是||w||最小。$\gamma=\frac{2}{||w||}$被称为间隔。这个w是直线的参数。同时使得$y_i(w^Tx_i+b)>=1, i=1,2,...,m$

这个不好求解。所以我们需要引入拉格朗日乘子法，对每条约束添加拉格朗日乘子$\alpha_i>=0$：

（SVM核心数学问题）：$L(w,b,\alpha) = \frac{1}{2} ||w||^2 - \sum_{i=0}^N \alpha_iy_i(wx_i+b-1)$
使得距离最大，则w最小，使得L最小，则后面一项最高为0.所以求W最小值和距离最大问题等价。
对偶问题 $min_{w, b}max_{\alpha}L = max_{\alpha}min_{w,b}$

## 拉格朗日乘子法
综合可知，在相切点，圆的梯度向量和曲线的梯度向量平行：
联立就是拉格朗日乘子法。
在某个条件下，求某个函数的最小值。则两个函数的梯度是平行的。

拉格朗日函数：求某个函数在某个约束条件下的极值。则这个函数和约束条件的法线平行。可以构造方程组求导$（f=\lambda g）$ , g(x,y) = 0

将约束条件乘以一个常数，与原函数求和得到一个新的函数，来求极值。
构造拉格朗日函数 L = fx + $\lambda g(x)$
再求偏函数。
限制条件再一次来到了要求lambda 乘以g(x)等于0.

# 对偶问题

对偶问题更易求解，由下文知对偶问题只需优化一个变量$\alpha$且约束条件更简单；
能更加自然地引入核函数，进而推广到非线性问题

## 拉格朗日函数
综合可知，在相切点，圆的梯度向量和曲线的梯度向量平行：
联立就是拉格朗日乘子法。

## 线性不可分
我们刚刚讨论的硬间隔和软间隔都是在说样本的完全线性可分或者大部分样本点的线性可分。

但我们可能会碰到的一种情况是样本点不是线性可分的.


我们用 x 表示原来的样本点，用 $\theta(x)$ 表示 x 映射到特征新的特征空间后到新向量。

## 优点
有严格的数学理论支持，可解释性强，不依靠统计方法，从而简化了通常的分类和回归问题；能找出对任务至关重要的关键样本（即：支持向量）；采用核技巧之后，可以处理非线性分类/回归任务；最终决策函数只由少数的支持向量所确定，计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”。
## 缺点
训练时间长。当采用 SMO 算法时，由于每次都需要挑选一对参数，因此时间复杂度为  ，其中 N 为训练样本的数量；当采用核技巧时，如果需要存储核矩阵，则空间复杂度为   ；模型预测时，预测时间与支持向量的个数成正比。当支持向量的数量较大时，预测计算复杂度较高。

面经：
1. SVM 原理

SVM 是一种二类分类模型。它的基本模型是在特征空间中寻找间隔最大化的分离超平面的线性分类器

1. 当训练样本线性可分时，通过硬间隔最大化，学习一个线性分类器，即线性可分支持向量机；
2. 当训练数据近似线性可分时，引入松弛变量，通过软间隔最大化，学习一个线性分类器，即线性支持向量机；
3. 当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性支持向量机。

# SVM自我总结

1. 超平面：n维欧式空间中，n-1维的线性子空间。是空间中的直线的推广。
2. SVM的目标就是找到一个超平面，使得两个平行的分割数据的超平面距离最大。这样健壮性最好。两条直线的距离为$1/||W||$，等价于$1/2||w||^2$最小。约束条件是$1-y(x^Tw+b)<0$
3. 使用拉格朗日函数，转换为求等式的最值：$min_{w,b} max_{\lambda} L = 1/2||w||^2 + \sum \lambda [1-y(x^Tw+b)]$.
4. $min_{w,b} max_{\lambda} L>=max_{\lambda}min_{w,b} L$。当L为凸优化问题时，这个问题转为强对偶问题。
5. 在三个条件下转为强对偶问题。
   1. 原问题凸函数
   2. 原问题X受线性约束
   3. 满足KKT条件,对于不等式 $\lambda h(x)$：
      1. $\lambda$大于等于0，
      2. h(x)小于等于0. 
      3. $\lambda h(x)=0$
      4. L对每个x的偏导为0.
6. 所以转换为强对偶问题。此时求偏导得到 
   1. $W=\sum_i^n \lambda_i y_i x_i$  **重点**
   2. $0 = -\sum_i^n \lambda_i y_i$   **重点**
   3. 解出$b=y_k-\sum_{i=1}^{N}\lambda_iy_ix_i^Tx_k$
7. 将这个值带入原始。得到最终的优化方程：
   1. $L(w,b,\lambda) = \sum_i^n\lambda_i-1/2\sum_i^m\sum_j\lambda_i y_i \lambda_j y_j x_i x_j$   **重点**
   2. 这里为什么出现了i，j因为上一步的W是对所有的n。这里的sum也是对所有的n两个n不是相同的取值。
   3. 转换为了$\lambda$的最大值。
   4. 由KKT条件可以知道，这个时候要么$\lambda=0$要么$1-y(x^Tw+b)=0$所以，解只和支持向量有关。和$1-y(x^Tw+b)<0$的点无关。
8. 这个问题的求解是一个二次规划问题。
   1. 就是对多参数的一个求解。这里有m个$\lambda$参数。
   2. 对于常见的二次规划问题，我们采用SMO（启发式）固定剩下所有参数求一个参数。由于b的公式限制了$\sum\lambda_iy_i=0$所以我们固定n-2个，来求两个值
9. 最终得到**决策函数**：
   1. $f = \sum_i^n\lambda_iy_iX_i^TX+b$
10. **软间隔**。为了提高泛化性，引入软间隔。允许少量样本不满足约束
    1. 修改约束条件$y(Wx+b)>1-\mu$
    2. 损失函数使用hinge损失（合页损失）$l_{hinge} = max(0, 1-z)$
    3. $L = min (1/2 ||w||^2 + C\sum_i^n\xi)， s.t. \xi>=0, y_i(W^Tx_i+b)>=1-\xi$
       1. $\mu > max(0, 1-y_i(WX+b))$ 这个就是合页损失。
       2. C为松弛变量。
           1. 当c越大的时候，损失越大，越在意这个异常值。所以越不平滑
           2. C越小，损失越小，对异常值越不在乎，所以越平滑。
    4. 将约束条件根据拉格朗日定理带入，可以重新求得解。和原解不同的是$\lambda$的取值范围。具体可以参考西瓜书。
11. **核函数**
    1. $K(x_1, x_2)  = K(x_1)*K(x_2) = K(x_1*x_2)$
    2. 因此我们不需要显式地定义映射K(x)是什么而只需事先定义核函数K(x_1*x_2)即可。
    3. 决策函数 $f = \sum_i^n\lambda_iy_i K(x^T,x_i)+b$
    4. 常用核函数：
        1. 线性核 $K(x_i, x_j)=x_i^Tx_j$
        2. 高斯核 $K(x_i, x_j)=exp(-\frac{||x_i-x_j||^2}{2\theta^2})$
12. 优点：
    1. 理论清晰。**逻辑完善**
    2. 仅支持向量影响数据，减少运算，无需依赖全部数据。**泛化能力强。**
    3. 核函数可以支持非线性问题。
    4. SVM是凸优化问题。求得的解是**全局最优。**
13. 缺点：
    1. 二次规划将涉及M阶矩阵运算，M为样本个数，**不适合大数据集**。SMO缓解这个问题
    2. SVM**只适用于二分类。**
    3. SVM**对缺失值敏感**。


# 面试题

1. 优缺点：
   1. 优点：
      1. 可以解决小样本下机器学习的问题。（支持向量就可以做出决策，不需要全部样本）
      2. 提高泛化性能。（软间隔）
      3. 解决高维、非线性问题（核技巧）
   2. 缺点：
      1. 对缺失数据敏感（缺失数据如果是支持向量，则会影响决策）
         1. 这里说的缺失数据是指缺失某些特征数据，向量数据不完整。SVM 没有处理缺失值的策略。
      2. 内存消耗大（支持向量计算复杂）
      3. 运行调参麻烦（支持向量计算复杂
2. SVM的支持向量是什么
   1. 超平面（最优分割面）能使支持向量和分割线最小距离最大的平面。
   2. 样本中距离超平面最近的一些点，这些点叫做支持向量。
   3. 线性可分支持向量机利用间隔最大化求得最优分离超平面，这时，解是唯一的。另一方面，此时的分隔超平面所产生的分类结果是最鲁棒的，对未知实例的泛化能力最强
3. SVM核函数和意义
   1. 当样本在原始空间线性不可分时，可将样本从原始空间映射到一个更高维的特征空间，使得样本在这个特征空间内线性可分。而引入这样的映射后，所要求解的对偶问题的求解中，无需求解真正的映射函数，而只需要知道其核函数。即不需要知道K(x1)* K(x2)，只需要知到K(x1* x2) 的值。所以不用定义非线性的映射函数。
   2. 结论：一方面数据变成了高维空间中线性可分的数据，另一方面不需要求解具体的映射函数，只需要给定具体的核函数即可，这样使得求解的难度大大降低。
4. SVM 核函数之间的区别
   1. SVM 核函数一般选择线性核和高斯核(RBF 核)。
   2. 线性核：主要用于线性可分的情形，参数少，速度快，对于一般数据，分类效果已经很理想了。
   3. RBF 核：主要用于线性不可分的情形，参数多，分类结果非常依赖于参数。
   4. 如果 Feature 的数量很大，跟样本数量差不多，这时候选用线性核的 SVM。
   5. 如果 Feature 的数量比较小，样本数量一般，不算大也不算小，选用高斯核的 SVM。
5. 为什么转换为对偶问题
   1. 对偶问题往往更易求解，
   2. 可以自然引入核函数，进而推广到非线性分类问题。
6. 处理多分类
   1. 一对多：就是对每个类都训练出一个分类器，设定为目标类为一类，其余类为另外一类。这样针对k个类可以训练出k个分类器，当有一个新的样本来的时候，用这k个分类器来测试，那个分类器的概率高，那么这个样本就属于哪一类。 
   2. 一对一：任意两个类训练出一个分类器，如果有k类，一共训练出C(2,k) 个分类器，这样当有一个新的样本要来的时候，用这C(2,k)个分类器来测试，每当被判定属于某一类的时候，该类就加一，最后票数最多的类别被认定为该样本的类。
   3. 多对多：了解不多。
7. SVM可以用梯度下降嘛？
   1. 可以。在优化后的对偶问题中，是一个最小化损失函数的目标。但SVM是凸优化问题，用SMO效果会更好。
8. SVM SMO是什么算法？
   1. 这里简单介绍。具体请看视频
   2. 解决这样一个有多变量（n个αi）的优化问题确实比较困难，但是如果能多次迭代，每次选择两个变量优化，同时把其他变量看做是固定的常数，这样“分而治之”的话，问题似乎就容易多了。
   3. 到这里，SMO算法的大致介绍总算是说完了。我把它的步骤概括式的总结一下：
      1. 初始化αα，一般情况下令初始的αiαi全部为0；
      2. 选取优化变量α1α1和α2α2，执行相关的优化计算，得到更新后的α1,α2α1,α2；
      3. 开始新的一轮迭代，重复执行上面的第2步，知道全部的αiαi满足公式(14)的KKT条件以及公式(1)中的约束条件；
   4. 总结：每次优化两个变量，固定其他变量为常数。

