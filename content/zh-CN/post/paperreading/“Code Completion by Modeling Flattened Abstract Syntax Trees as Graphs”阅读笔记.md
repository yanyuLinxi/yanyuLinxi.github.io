---
title: "“Code Completion by Modeling Flattened Abstract Syntax Trees as Graphs”阅读笔记"
date: 2021-04-13T10:40:02+08:00
tags : [
    "论文阅读笔记",
]
categories : [
    "论文阅读笔记",
]
series : []
aliases : []
draft: false
math: true
---

# 目录： <!-- omit in toc -->
- [1. 综述翻译](#1-综述翻译)
- [2. Tag](#2-tag)
- [3. 任务描述](#3-任务描述)
- [4. 方法](#4-方法)
- [5. 解决了什么问题（贡献）](#5-解决了什么问题贡献)
- [6. 实验结果](#6-实验结果)
- [7. 如何想到该方法](#7-如何想到该方法)
- [8. 我能否想到该方法](#8-我能否想到该方法)
- [9. 创新点是什么](#9-创新点是什么)
- [10. 如何用于本专业](#10-如何用于本专业)
- [11. 该方案还存在的问题](#11-该方案还存在的问题)
- [12. 注释](#12-注释)

# 1. 综述翻译

代码完成已成为集成开发环境的重要组成部分。当代代码完成方法依赖于抽象语法树（AST）来生成语法正确的代码。 但是，它们无法完全捕获编写代码的顺序和重复模式以及AST的结构信息。 为了缓解这些问题，我们提出了一种名为CCAG的新代码完成方法，该方法将部分AST的平坦序列建模为AST图。 CCAG使用我们提出的AST Graph Attention Block捕获AST图中的不同依存关系，以学习代码完成中的表示形式。 通过CCAG中的多任务学习优化了代码完成的子任务，并且无需调整任务权重即可使用不确定性自动实现任务平衡。 实验结果表明，CCAG比最先进的方法具有更好的性能，并且能够提供智能的代码完成功能。

# 2. Tag

论文阅读笔记; 代码完成; 代码补全; 图神经网络; AST树; 抽象语法树; 

# 3. 任务描述

> 定义 ***AST***：AST是一颗树，其中所有的非叶子节点都能对应于CFG中一个特定的非终结符，其中包含了特定的结构信息，如同IfStatement等。每一个叶子节点对应于CFG中的终结符。AST树可以轻易的转换成源代码。本文赋予了每个节点value和type。非叶子节点type为控制类型，如IfStatement，value为Empty。叶子节点value为变量值，type为变量类型。

> 定义 ***部分AST树***：对于一个完整的AST树，部分树是完整树的一个子集。对于部分树中的每个节点n，其左序列(left sequence)节点都在该部分树中。节点n的左序列节点是指在树的先序深度优先遍历中比n早出现的所有节点。

本文的代码完成任务定义如下：

&emsp;&emsp;对于一个AST图的每一个部分图T'都存在一个最右节点$n_R$，部分图中的所有其他节点都是$n_R$的左序列节点，而在AST的先序深度优先遍历中紧跟于$n_R$后的那个节点则称为下一个节点(the next node)，本文的目标即在给出部分图的情况下，预测the next node的value和type。

如下图所示，对于绿色的部分图，最右节点为NameLoad:b，代码补全的任务就是预测Return:EMPTY节点。
![任务描述](/researchPng/c/code_completion_by_Modeling_task_show.png)

# 4. 方法

### 处理数据： <!-- omit in toc -->

将部分AST树展平（flatten AST），即按照先序深度优先遍历，展开成一个序列。然后再将flatten AST 转换为AST Graph。遵循以下规则：

   1. 在flatten AST序列中相邻的节点，在AST Graph中会用node-node边来连接。
   2. flatten AST中重复的节点会在AST Graph 中合并成一个节点。如果有多条边重合，则叠加为一条边，边的权重相应增加。
   3. AST Graph中的边是无向边。保证权重双向传播。
   4. 在AST Graph中使用parent-child边来表示AST树中的父子关系，这个边是有向的。
   5. Flatten AST转为AST Graph后丢失了在Flatten中的顺序信息，所以这里采用位置embedding来记录位置信息。对于序列$\\{n_1, n_2, n_3, n_2\\}$,假设n_2是最右节点，则$n_1, n_2, n_3$的位置embedding为全维度的3,0,1。位置embedding固定且不会被更新。

则节点$n_i$的初始embedding$h_i$为：
$h_i = ReLU(W^{(p)}([t_i||v_i]+p_i)+b^{(p)})$
$t_i、v_i$是type embedding和value embedding，$p_i$是位置embedding。

### 网络结构： <!-- omit in toc -->

本文设立了一个ASTGab（graph attention block）来捕获图中的注意力关系：

  1. Neighbor Graph Attention Layer (NGAT)：
     1. 直接在AST graph上跑注意力网络。$e_{i,j}^{(n)} = a(W^{(n)}h_i,W^{(n)}h_j, w_{i,j})$,其中a为注意力机器。这里使用了多头注意力机制。
     2. 更新公式：$h_i^{(n)} = ReLU(\frac{1}{M} \sum_{m=1}^M \sum_{j \in N_i}\alpha_{i,j}^{(n)}W^{(a)}h_j)$， 其中M为多头。$\alpha=softmax(e)$。
  2. Global Self-attention Layer(GSAT):
     1. 一个全局的自注意力机制。
  3. Parent-child Attention Layer(PCAT):
     1. 在子节点和所有父节点之间添加注意力机制。具体公式可以看论文。这里不多介绍

将上述三个注意力机制串联起来作为一个block。多个block串联起来，并用残差捷径连接。来降低训练难度。效果如图所示：
![model_show](/researchPng/c/code_completion_by_Modeling_model_show.png)

### 输出： <!-- omit in toc -->

由于最右节点（right-most）节点蕴含最多的信息，所以这里对每个节点都对最右节点做了个soft-attention：
$$\beta_{i,n_j} = z^{(t1)} \delta ( W_1^{(t1)} h_i^{(r)} + W_2^{(t1)}h_{n_j}^{(r)} + b^{(t1)})$$
$$s_j = \sum_{i \in G_j} \beta_{i, n_j} h_i^{(r)}$$

其中$h_{n_j}$是最右节点的embedding，$h_i$是左序列节点中的节点。通过计算左序列中每个节点和最右节点的自注意力系数，再和该节点embedding相乘相加起来。得到一个图的embedding，用这个图的embedding去做分类。

### loss计算： <!-- omit in toc -->

本文中每个AST Graph的节点既有value也有type，所以两种方法来预测这两个值：

1. 分别使用两个模型预测这两个值。但论文觉得这两个值是有关联的，应该一起训练。
2. 通过某种方式将两个loss结合起来：$L = w_v L_v + w_t L_t$

但是$w_v, w_t$这两个权重的设置影响会很大，如果设定为超参，则超参调试会很消耗时间。如果设定为可学习的，那学习的代价又太大了。所以这里采用了一个约束条件：$L \approx \frac{1}{\theta^2}L_v + \frac{1}{\tau^2}L_t + \log \theta + \log \tau$。这个公式保证了两个任务权重是可以学习的，同时$\log$的存在保证参数不能为负，当参数太大的时候，$\frac{1}{\theta^2}$又会过小。从而限制loss在一定的范围。

# 5. 解决了什么问题（贡献）

论文提出了一种使用图网络学习embedding，并完成了code completion任务。
论文中提到的各种模型，各种网络结构有多少用，很难说。但是理论上有用，再加上结果正确就能提高说服力。


这篇论文的贡献我认为还是有的。起码在这之前我一直怀疑到底图网络能不能做代码还原。该文如果实验没有差错的话，是证明了有用的。它这里用的图网络广泛使用了注意力机制。可以去思考下其他几种图网络能不能做代码还原呢？

这里还有一个比较大的贡献点，就是它用图分类去做代码还原，用节点分类我始终认为学习道的知识不够，用图分类，就有更多的信息。而且图分类时将更多的关注集中在最右节点，因为最右节点包含了最多的信息，对于预测下一个节点来说。这里充分借鉴了RNN的思想。

我前段时间仔细思考了下rnn，认为rnn学习到的就是一种概率。对于英文单词I，那下一个出现的此中：am，are，is，毫无疑问am的几率最高。同样高的还有have等单词。再根据上文等相关信息，判断到底是am还是have。

这里我也认为是这样，最能决定下一个单词是什么的就是上一个单词，然后再根据相关上下文信息来帮助神经网络做出抉择。

我认为不能把神经网络想的太过复杂。要帮助神经网络去减负。其他结构有没有用，有多少用得做消融实验，不能简单的下出结论。

当然论文自己是做了类似结构的对比实验的。可以看到有一定优化。这个的意义是一定要保证整体模型能够学习到知识的情况下，再去做删删减减做优化。

# 6. 实验结果

这里贴一张图。表示了论文的所有实验：

![result_show](/researchPng/c/code_completion_by_Modeling_result_show.png)

可以看到CCAG效果提升还是很不错的。自己对比的实验中，多头确实效果会更好一点。这也是点小启示，要尽可能从多个维度学习信息。

# 7. 如何想到该方法

对RNN比较熟悉。要理解RNN是怎么运作的。改进合理。网络的改进不宜大脚步。一步步改，确保能学到知识后，再慢慢改进改造结构。这篇论文很多参考文献都是Li，Liu，不出意外是一个在前人基础上慢慢改进的文章。

# 8. 我能否想到该方法

可以，要多思考，多看相关论文。

# 9. 创新点是什么

# 10. 如何用于本专业

# 11. 该方案还存在的问题

这里只说一个猜测，图结构中大多数控制节点的value都是empty，这会不会对accuracy的准确性有影响？毕竟预测empty可比较容易。


# 12. 注释

启迪：

1. 合适的理由（动机） + 更优的结果 = 有效的改进。改造一定要有动机。
2. 不要凭空创造，从已知推未知更加有效。
3. 实验时，多个类似结构相互对比，能够有效说明结构的合理性。
4. 不要把神经网络想的太复杂。也别太小看神经网络。
5. 保证整体模型能学习到东西的情况下，再去删删减减做优化，效率更高。一个小结构的优化最多可能百分之2百分之3.所以整体结构正确是很重要的一件事。



