---
title: "OrderMatters Semantic Aware Neural Networks"
date: 2022-03-16T09:22:29+08:00
tags : [
    "论文阅读笔记",
]
categories : [
    "论文阅读笔记",
]
series : []
aliases : []
draft: false
math: true
---

# 目录： <!-- omit in toc -->
- [1. 综述翻译](#1-综述翻译)
  - [1.1 发表于](#11-发表于)
- [2. Tag](#2-tag)
- [3. 任务描述](#3-任务描述)
- [4. 方法](#4-方法)
- [5. 解决了什么问题（贡献）](#5-解决了什么问题贡献)
- [6. 实验结果](#6-实验结果)
- [7. 如何想到该方法](#7-如何想到该方法)
- [8. 我能否想到该方法](#8-我能否想到该方法)
- [9. 创新点是什么](#9-创新点是什么)
- [10. 如何用于本专业](#10-如何用于本专业)
- [11. 该方案还存在的问题](#11-该方案还存在的问题)
- [12. 注释](#12-注释)
- [Related work](#related-work)

# 1. 综述翻译

二进制代码相似性检测，其目标是在不访问源代码的情况下检测相似的二进制函数，是计算机安全中的一项基本任务。传统方法通常使用图匹配算法，速度慢且不准确。最近，基于神经网络的方法取得了巨大的成就。二元函数首先表示为具有手动选择块特征的控制流图（CFG），然后采用图神经网络（GNN）来计算图嵌入。虽然这些方法有效且高效，但它们无法捕获二进制代码的足够语义信息。在本文中，我们提出语义感知神经网络来提取二进制代码的语义信息。特别是，我们使用 BERT 对一个令牌级任务、一个块级任务和两个图级任务的二进制代码进行预训练。此外，我们发现 CFG 节点的顺序对于图相似性检测很重要，因此我们在邻接矩阵上采用卷积神经网络 (CNN) 来提取顺序信息。我们用四个数据集对两个任务进行实验。结果表明，我们的方法优于执行最先进的模型。

## 1.1 发表于
AAAI

# 2. Tag

# 3. 任务描述

# 4. 方法

我们提出了一个包含三个组件的整体框架：语义感知建模、结构感知建模和顺序感知建模。


在**语义感知建模**中，我们使用 NLP 模型来提取二进制代码的语义信息。CFG 块中的标记被视为单词，块被视为句子。

我们采用 BERT (Devlin et al. 2018) 来预训练令牌和块。与 BERT 相同，我们屏蔽标记以对屏蔽语言模型任务 (MLM) 进行预训练，并提取所有相邻块以对邻接节点预测任务 (ANP) 进行预训练。

此外，因为我们的最终目标是生成整个图表示，所以我们添加了两个图级任务。一是判断两个采样的block是否在同一个图中，我们称之为block inside graph task（BIG）。另一个是区分块属于哪个平台/优化，称为图分类任务（GC）

我们发现额外的任务可以帮助提取更多的语义信息并更好地学习块表示。在对块嵌入进行预训练后，我们在图级任务上对其进行微调。

**在结构感知建模中**，我们使用 MPNN (Gilmer et al. 2017) 和 GRU (Cho et al. 2014) 更新功能。 (Xu et al. 2018) 已经证明，图神经网络可以具有与 Weisfeiler-Lehman 测试一样的区分能力 (Weisfeiler and Lehman 1968)。我们发现在每一步使用 GRU 可以存储比仅使用 tanh 函数更多的信息。

在**顺序感知建模**中，我们尝试设计一种架构来提取 CFG 的节点顺序信息。即在邻接矩阵上使用 CNN。我们发现只有 3 层 CNN 表现良好。我们进一步探索了其他 CNN 模型，例如 Resnet (He et al. 2016)，并讨论了 CNN 模型可以从邻接矩阵中学到什么。


我们模型的输入是二进制代码函数的 CFG，其中每个块都是具有中间表示的令牌序列。 我们模型的整体结构如图 3 所示。在语义感知组件上，该模型将 CFG 作为输入，并使用 BERT 对令牌嵌入和块嵌入进行预训练。 在结构感知组件上，我们使用带有 GRU 更新功能的 MPNN 来计算图语义和结构嵌入 gss。 在 order-aware 组件上，模型以 CFG 的邻接矩阵作为输入，采用 CNN 计算图 order embedding go。 最后，我们将它们连接起来并使用 MLP 层来计算图嵌入

Bert任务
MLM是一个token级别的任务，它在输入层屏蔽token并在输出层预测它们。邻接节点预测任务（ANP）是一个块级任务
在 ANP 任务中，我们提取一个图上的所有相邻块，并在同一个图中随机抽取几个块来预测两个块是否相邻。

图内块任务（BIG）和图分类任务（GC）。
我们在同一个图中/不在同一个图中随机采样块对，并在 BIG 任务中预测它们。这个任务帮助模型理解块和图之间的关系，

为了使模型能够区分这些差异，我们设计了图分类任务（GC）。 GC 使模型对不同平台、不同架构或不同优化选项中的块进行分类。



# 5. 解决了什么问题（贡献）

# 6. 实验结果

# 7. 如何想到该方法

# 8. 我能否想到该方法

# 9. 创新点是什么

# 10. 如何用于本专业

# 11. 该方案还存在的问题

# 12. 注释


# Related work
一些研究尝试使用图核方法（Weisfeiler 和 Lehman 1968；Borgwardt 等人 2005）。最近（Xu et al. 2017）提出了一种基于 GNN 的模型，称为 Gemini，

它比以前的方法获得了更好的结果。但它使用手动选择的特征来表示 CFG 块，可能没有足够的语义信息。

(Zuo et al. 2018) 在这项任务上使用 NLP 模型。他们把一个token当作一个词，把一个块当作一个句子，并使用LSTM对句子的语义向量进行编码。为了确保具有相同语义信息的块具有相似的表示，他们使用孪生网络并计算 CFG 对的余弦距离。他们认为从同一段源代码编译的两个基本块是等效的。为了获得基本事实块对，他们修改编译器以添加基本块特殊注释器，该注释器为每个生成的装配块注释一个唯一 ID。

